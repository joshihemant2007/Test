In this course, Node.js: The Big Picture, we begin with the places you'll commonly find Node being used, its technology origins, and a brief history of the project. We then examine what it means to write asynchronous code, and how Node both expects and embraces this approach with its own APIs and constructs. From there, we look at how to both modularize your own application code as well as leverage Node's vast 3rd party module ecosystem via npm. Finally, we wrap up with a discussion on assembling your own "tool belt" of Node.js tools and strategies to jump start your journey.


Course Overview
Course Overview
Hello, my name is Paul O'Fallon, and welcome to my course, Node.js : The Big Picture. I began building applications for the web back when that meant putting Perl scripts in a cgi-bin directory, and nothing has intrigued me as much as Node.js. Its promise of JavaScript beyond the boundaries of the browser captures the imagination of both front-end and back-end developers alike. This course is a very broad overview of Node.js, and while it may be helpful to know a little JavaScript, no prior exposure to Node is required. Some of the major topics we'll cover include when Node may or may not be the right choice for your project, Node's event loop and asynchronous development model, Node's package manager, npm, and what a typical development toolset might look like. By the end of this course, you'll have a basic understanding of Node and hopefully a good idea of whether it's right for you.

Considering Node.js
Course Introduction
Hello, my name is Paul O'Fallon, and I'd like to welcome you to the course, Node.js : The Big Picture. If you're new to Node.js and wondering if it's right for you or your organization, you've come to the right place. This first module is entitled, Considering Node.js, and here we'll examine how you'll find Node commonly used, and we'll take a few minutes to reflect on the history and technology origins of the project. Before we wrap up this module, we'll also discuss a couple of reasons you might not want to use Node.js. Okay, enough of an introduction. Let's get started. I guess the best thing to start with is, well, what is Node.js anyway? This definition is from the project's home page, "an asynchronous event driven JavaScript runtime. " Makes sense, and of course that's true. I mean, it's their project. But Node.js is most commonly thought of as server-side JavaScript, although, as we'll see, it's used for much more than that. So, you're interested in learning more about Node.js and whether it's right for you. Great! I was in your shoes one point as well. We all got here from somewhere. In my experience, most folks investigating Node.js come from a couple of different backgrounds, either an existing front-end developer or a back-end developer. As a front-end developer, maybe you're looking to extend the applicability of your JavaScript experience to the server side. Being able to create both the front end and back end of a single page JavaScript app is the elusive goal of the full stack developer. Or maybe you just want to contribute to the tooling ecosystem enjoyed by so many front-end developers today. The vast majority of these tools are written in JavaScript and run via Node.js. Alternatively, maybe you're like me, a back-end developer, more familiar with traditional web development stacks like Java or .NET or even Ruby and Python. Node.js may be the opportunity to learn JavaScript in a server-side environment you're more comfortable with. And while the point is debatable, Node.js may just provide a more lightweight alternative to the application server style deployment model you've experienced building server-side applications thus far. Finally, the vibrant ecosystem and broad platform support can make Node.js an appealing runtime for building cloud native microservice-oriented solutions. This last point is a good segue into our next topic.

Where Is Node.js Commonly Found?
Let's take a look at the places where Node.js is commonly found. First is in this microservice and API space. One of the things that, to me, differentiates Node.js from many of its server-side predecessors is how close your code is to the action. And what I mean by this is that there's no server to deploy your code to. Your code is the server. Take a look at this example program from the Node.js About page. In fact, for a long time, an example similar to this was front and center on the Node.js home page. Notice that this code itself is listening for incoming requests on a specific host and port. That isn't work deferred to some underlying application server or framework. When an incoming request is received, the appropriate JavaScript function is invoked. These 11 lines of code, along with the Node.js runtime, are all it takes to create a very basic API or a microservice. This approach contributes to Node's reputation of being a very lightweight option for building these types of solutions. Sure, over the years frameworks have evolved to abstract away a bit of this, making it easier to build increasingly complex applications. However, you're never too far away from the concepts outlined in this very basic example. Another place Node.js has found favor is in the cloud, particularly with serverless functions. As a dynamic language runtime with a fast startup and low memory consumption, it's proven to be a very capable engine for this type of application. Here's the AWS web console with a lambda function written in Node.js. As you can see, they've managed to whittle it down to something even more basic than our earlier API example and made it editable directly in the browser to boot. This code truly is just a JavaScript function that takes an event parameter and returns a response. AWS isn't the only cloud provider to leverage Node.js this way. Azure also supports Node.js for their Azure functions. In their approach, it's still just a simple JavaScript function, this time being called with two parameters, the context and the request itself. If you were to visit Google's cloud, you'd find similar support for Node in their cloud functions. A third area where Node.js can be found is in the development of command-line applications. This one represents a departure from the server-side focus of the use cases we've discussed so far. However, this is one that will likely sound familiar to most front-end developers. Many of the tools used to build, package, and deploy complex browser-based web projects are themselves written in JavaScript and run in Node.js. This includes projects like webpack, gulp, ESLint, Yeoman, and many others. If you run a CLI tool as part of your front-end development process, chances are it's running in Node.js. Finally, one area you really might not expect to find Node.js, but an area where it's found a fair degree of success nonetheless, is in building desktop applications. This is almost entirely due to the popularity of the Electron framework, which first originated out of GitHub's Atom editor project. Electron makes it easy to build cross-platform desktop applications using standard web technologies, something it accomplishes in part by leveraging Node.js for the local back end of the desktop application. In fact, I wouldn't be surprised to find that you're already using a desktop app written with the help of Node.js. For example, these popular applications, and more, are based on Electron. Node.js in a GUI desktop app? It really does prove Jeff Atwood's law true. Anything that can be written in JavaScript will eventually be written in JavaScript.

What Makes up Node.js?
Let's take a peek under the hood and examine some of the components that make up Node.js. To kick things off, we're going to look at the Chrome web browser, which is made up of many separate components, including the Blink rendering engine and, most importantly to us, the V8 JavaScript engine. This same engine is where Node gets its JavaScript support. Another open-source project, libuv, is the source of Node's asynchronous I/O and its event loop, two topics we'll discuss in more detail in a later module. Node adds some custom code to these projects to round out its runtime offering. So, what are the implications of this approach? Well, for one thing, if you're coming to Node from a background in front-end development, your server-side JavaScript work is free of browser incompatibilities and the need for polyfills. Because your work is only targeting the V8 JavaScript engine, just like you'd find in a recent Chrome browser, there's no need to worry about whether this JavaScript code will work in Firefox, Safari, or some archaic version of IE. There's a great website, node.green, which maps the various ECMAScript features found in each of the major versions of Node. These features are, of course, determined by the release of V8 present in each Node version. It's a really great situation to be in. We get the benefit of the browser wars by gaining a JavaScript engine that's driven to grow and improve; however, we don't have the downside of having to support all of them, and that sounds like a better way to JavaScript to me. Well, there are still some things to consider. Even though we don't have multiple engines to support, Node itself will occasionally change its own API over time, albeit very slowly, and this may drive changes to your code. This is especially true if you're a library developer. You'll likely want to support a range of Node.js versions. Also, depending on how you're using Node, you may need to be aware of the operating system on which it's running and adjust to those differences as well. For example, when writing a CLI or a desktop application used across multiple operating systems, you'll have to deal with different path separators, forward versus backslash, as well as home directory locations and environment variable differences. Well, okay, so, maybe rather than being a better way to JavaScript, let's just call it a different way to JavaScript.

A Brief History / When Node May Not Be the Best Fit
Let's talk briefly about the history of Node.js. It debuted in 2009 with its creator Ryan Dahl demonstrating it on stage at the jsconf.eu conference. In 2010, Joyent became the sponsor for Node.js. At the time, the idea was that a corporate backer would help ramp up both the adoption and the evolution of the platform. That same year, the npm package manager was also released. We'll talk about npm more in a later module. No pun intended. The next year, Joyent and Microsoft collaborated on bringing Node.js to Windows. The results of this work included the introduction of libuv, which we saw earlier. Part of libuv's job was to provide an abstraction for the platform-dependent code used in supporting multiple operating systems. In 2014, things got a little interesting. Frustration with Joyent's governance caused a fork of Node to emerge, a project called io.js. This fork was committed to releasing more regularly in order to follow V8 updates more closely. With its first release in January of 2015, it immediately adopted a 1.0 version, which was a significant departure from existing Node versions which were hovering around 0.11. Fortunately, these parallel versions of Node and io.js were short lived. Later in 2015, a Node.js Foundation was formed, and they released their first version, Node 4.0, which represented a merging of the existing Node and io.js projects. Since then, the project has adopted a regular release cadence, which involves releasing a new major version of Node every six months. Because Node follows semantic versioning, this means an increment of the first number of Node's version. Alternating major versions, the even numbered ones to be exact, will be covered by long-term support. Currently, this support includes 18 months of active support plus an additional 12 months of maintenance. With this approach, there should never be more than two active releases in long-term support at the same time. Here's a screenshot of the current release cadence at the time of this course's creation. Node versions 10 and 12 will be the two even numbered versions under long-term support for the near future. Okay, so, we've spent some time covering places you may find Node.js, but what are some reasons Node may not be a good fit for your use case? First, and the most commonly cited example, is CPU-intensive tasks. As we'll discuss more in the next module, Node's event loop and asynchronous I/O allow Node to shine in I/O-intensive use cases. In fact, we saw it in the original definition from Node's About page, designed to build scalable network applications. Network implies I/O. In order for Node's event loop to function properly, your code can't spend too much time doing anything of its own. This includes work that requires heavy use of the CPU. Now, Node does have some coping strategies for addressing these types of use cases, but they're definitely not its sweet spot. Second, and this may seem obvious, but another reason may be that you just don't like JavaScript. I've occasionally talked to other server-side developers about the benefits of Node.js, and they just can't seem to get past the dynamically typed nature of JavaScript. Projects like TypeScript attempt to address some of these concerns, but Node may simply not be for you if you're not a fan of JavaScript. Stick around for the next module where we'll discuss Node's event loop and asynchronous I/O.

Thinking Asynchronously
Node's Event Loop
Hello, my name is Paul O'Fallon, and I'd like to welcome you to the next module of the course, Node.js : The Big Picture. This module is titled Thinking Asynchronously, and here we'll be diving into Node's event loop, its asynchronous development model, and how Node capitalizes on this approach in its own APIs. Let's get started. If you're coming from other server-side architectures, Node's approach to handling, say, incoming web requests may be a little different than what you're used to. I know it was for me. Some more familiar approaches include process-per-client, where each incoming request is allocated a process either from a pool of existing processes or a new one is forked in response to the request. Handling multiple requests simultaneously, as you might expect, means running many processes, each still handling just one request at a time. In this model, code running in each process is effectively isolated and unaware of the other requests being handled simultaneously. Something more prevalent nowadays is a multi-threaded approach. Here, a single process may have a pool of many threads, each responsible for handling incoming requests. In this case, concurrency is based on how many threads are available. Code running in this model, while isolated to a single thread, may be aware of other concurrent requests, especially if they're accessing shared memory. Node, on the other hand, uses an event loop in a single-threaded model. I put the term single-threaded in quotes because, under the hood, Node does run multiple threads to do its work; however, JavaScript code running in Node.js operates as though all incoming requests are being handled via a single thread in a single process. This "single-threaded" model means there is no simultaneous access to shared memory, but there is also no process or thread isolation between requests. They're all served by the same code running in the same process via the same event loop. Most everything in a Node.js application is centered around the events managed by this event loop. For example, the event loop will be notified when a request to open a file has been fulfilled by the operating system. Network activity, such as an incoming HTTP request, will also fire an event. Timers can even trigger events at some point in the future, which are also handled by the event loop. These events are typically surfaced in your application by registering callback functions, which Node will invoke when the specific events arise. A key takeaway here though is that, in a general sense, only one function in your code is running at any one time. That's why I mentioned single-threaded in quotes. If there are many incoming HTTP requests, each of which invoke your callback function, only one of those callback functions will be executing at a time. It's your job as a Node.js developer to spend as little time in your own code as possible, keeping it ready to receive the next event. If your code spends too much time on any one task, the event loop is effectively stalled and won't invoke any other callbacks until your work is complete. The Node.js documentation sums up this distinction quite well. In Node, the fair treatment of clients is the responsibility of your application. It is not handled by some underlying abstraction or framework. This can be a stumbling block to thinking asynchronously. I know it was to me. So let's dive a little deeper. One analogy I like to use, even if it's an imperfect one, is that of a diner. Imagine your code as a waitress. You serve customers incoming HTTP requests, for example, and in doing so you engage a cook to help fulfill your customer's orders, which you could think of as a database that you query in order to prepare an HTTP response. One waitress can serve many customers at once, and she does so by not waiting on anyone else to complete their work. Customers are looking at their menus deciding what to eat, and the cook is busy preparing orders. If the waitress though were to stop and simply wait for the cook to prepare one order for one customer, everything else would grind to a halt. No other orders would be fulfilled, and the customers would get angry. The diner only works when the waitress is able to juggle many requests simultaneously, not spending too much of her own time on any one of them.

Asynchronous Development
Let's look at a traditional programming model where this idea of concurrency is handled somewhere outside of the application code. In an example like this, we might have a function called serveCustomer, which performs a series of steps in sequence. The customer places their order, the cook prepares the food, and the customer eats and pays the bill, leaving a tip. In a traditional server-side scenario, each customer may be served in their own process or thread. Your application code may not even need to be aware of this. Adopting this model in Node.js, however, simply does not work. A function with a series of blocking actions will prevent anything else from happening. So, what would this look like written asynchronously? We must think of our application as a collection of functions that react to events, which invoke work that will cause future events to be fired and so on. The customer will let us know when he's ready to order, and the cook will let us know when an order has been prepared. Otherwise, we're happily waiting to take new customers. Here's the same function written with that approach in mind. Notice that our outermost function now takes two parameters, a customer and something we've called done. Done is our callback function. Our serveCustomer interface is essentially serve this customer, and when the customer is finished, invoke the function I'm passing you as the second parameter. A call to this serveCustomer function will return immediately, leaving us free to do other work. At some point in the future, the done function will be called, and we'll know that that customer has been served. Inside this function, we only make one function call. Remember, we want to spend as little time in our own code as possible. We call customer.placeOrder, but now that interface has changed as well. It takes two parameters, the menu and a function to be called when the customer is ready to order. Here we've defined an inline anonymous function we want to be called. This function takes two parameters, error and order. This signature for our callback function, the error as the first parameter and any other results afterwards, is a Node.js convention. If you're passing a function as a callback, you should expect it to be invoked with parameters in this order. Okay, so now we're inside the callback function passed to placeOrder. What do we want to do after the customer has placed their order? Well, we want the cook to prepare the food, so we'll call that function here. Notice that we're passing the customer's order as the first parameter and another anonymous function as a callback. Finally, once the food is ready, we want the customer to eat and pay. We're not doing any error checking in this example, so we can just use the same callback we were provided as the final parameter passed to eatAndPay. This means that when done gets invoked, it will be called with two parameters, an error, if any, and the tip left by the customer. If you've never seen this style of programming before, it can be disconcerting, but stick with me. It gets better, I promise. No pun intended. Now, if we were to run an example using this asynchronous serveCustomer function, it works like we would expect. All the customers are being served simultaneously. The work durations here are all made up and random, but you get the idea. One of the things you may have noticed in that last example is how quickly the code becomes very nested. When you're new to Node.js, this can be very difficult to follow, especially when the nested function calls are more spread out, as they would be in a real Node application. This characteristic of Node applications is called the Christmas tree problem. Well, that's the most polite description of it anyway. Over the years, JavaScript has introduced new paradigms, which have subsequently made their way into common usage within Node.js. Among other benefits, these help reduce the cognitive load when trying to comprehend these deeply nested function calls within an application. The first of these is promises. Promises are a JavaScript construct and are not specific to Node, so we're not going to discuss them in any great detail here. This is the same example function written using promises. One thing to note is that functions that expect callbacks are not inherently interchangeable with functions that return promises, although we'll see a solution to that shortly. Finally, with the advent of the async/await syntax, we're finally now able to represent our linear process as a sequence of statements while remaining true to the asynchronous nature of Node.js. This looks very similar to the anti-pattern we first demonstrated. However, this model still allows us to serve many customers simultaneously while keeping the code very easy to read and understand. This is my go-to approach for writing Node.js code whenever possible.

Node APIs and EventEmitters
This move from callbacks to promises and async/await is also making its way into the Node.js core APIs as well. Remember when I said that functions that take callbacks can't be used in cases that expect promises? Well, there's a utility in the Node.js API called promisify, which will take a function that expects a callback and convert it to the promise-based model. You can even use it on the other core node APIs and minimize your use of the original callback style. Beyond the promisify utility, some of the other core APIs are beginning to add native support for promises, such as the experimental support in the fs file system module. As Node extended the idea of JavaScript from the browser to the server, they also introduced some new concepts that build on the event loop and asynchronous programming concepts. The first is an event emitter. This is a simple JavaScript class that has a handful of methods, the most critical being emit and on. The idea is that you can instantiate an event emitter and then define multiple callbacks to be invoked when a particular named event is emitted. Here, when a data event is emitted, we want to log that data to the console. Now, elsewhere in our code, we can call this event emitter's emit function, passing in the type of event and the event payload. In this case, emitting Hello World will cause that text to be written to the console. For completeness sake, let's look at our serveCustomer example implemented with event emitters. Our overall function interface is the same. We're passed a customer and a callback function. Now though, our customer extends the event emitter class, so we're registering a callback for the decided event. That event payload includes the order, which is also an instance of event emitter, so we'll register another callback when the order emits a prepared event. Now that we know what we want to do when the order is prepared, we can ask the cook to prepare it. So far, we know what we want to do when the customer has decided on their order. Now we need to specify what to do when the customer emits the leaving event. That's when we'll call our done function with the tip. Finally, with all of our event callbacks registered, we can give the menu to the customer and wait for them to emit a decided event. We spend most of this code specifying what we want to do when and then finally kick off everything at the bottom by calling placeOrder. With the advent of promises and async/await, frankly, it's not as common to write code like this on your own; however, you will encounter higher-order constructs that are built on event emitters, and so it's important to at least know that they exist. In fact, our next topic is one of those higher-order constructs, streams.

Streams
A stream, true to its name, is a way for dealing with streaming data in Node.js while respecting the asynchronous programming model and the underlying event loop. You may never need to implement a stream of your own, but it's difficult to do much with Node.js without using streams in some way. Streams are broken down into three broad types: readable streams, something that you would read data from; writeable streams, something you would write data to; and transform or duplex streams that implement both of these read/write interfaces. Let's start with a readable stream. All streams extend event emitter and therefore emit certain events. They also have some methods and properties of their own. A readable stream emits events like readable, data, end, and error and contains methods like read, pause, resume, and destroy, as well as a handful of properties. You can check out the documentation for a complete description. Streams use this combination of events and methods to support back pressure. For example, as data arrives from an underlying resource, a file or network, for example, the appropriate events are emitted to let you know you can read from the stream. If you're reading from the stream more slowly than the underlying data is arriving, the stream will eventually reach a high-water mark, and the stream will signal to the underlying resource that it can no longer receive any data. Later, once enough information has been read from the stream, it will begin accepting data again while the reader continues to read from the stream. Writeable streams act in an identical fashion, just in the opposite direction. They have their own set of events, methods, and properties. In this case, if you're writing to a stream faster than the underlying resource can process it, the high-water mark will be reached, and you'll be prevented from writing to the stream until the appropriate event is emitted indicating that it's safe to do so. One of the best things about streams is piping. Because readable and writeable streams both have a consistent interface for applying and coping with back pressure, you can pipe a readable stream to a writeable stream, and the back pressure will be communicated from one stream back to the other. One example of this could be using a readable stream to read from disk and piping that stream to a writeable stream that is responding to a web request. In this case, it's likely that your readable stream will read data faster than the writeable stream can send that information across the network, so back pressure will kick in. The readable stream will slow down and won't overwhelm the writeable stream. Another example would be receiving an HTTP upload via a readable stream and writing that information to disk using a writeable stream. This streaming approach is really great when you don't need the entire contents and memory at once. You can process the information through this pipeline of streams one chunk at a time, all without interrupting the event loop. The fact that it's all handled asynchronously under the hood means that we can use this to handle many, many simultaneous requests. Streams show up in the Node.js API in several places. Files can be read from and written to the file system using fs.ReadStream and WriteStream, respectively. Node's HTTP support also relies heavily on streams for sending outbound client requests, as well as processing incoming requests and sending responses. We haven't really talked about duplex or transform streams, but one example is a transform stream you can write to as a writeable stream, and that will gzip the contents and make that compressed data accessible as a readable stream. Let's take a look at a simple example that uses all three stream types. Here we have a simple HTTP server that, for any request, will read a file on disk, lorem.txt, gzip compress it, and then send that compressed data as the HTTP response. Better yet, it does all of this using streams. First thing we do is set a couple of HTTP response headers with setHeader. Then we use the file system module to create a readable stream for the file lorem.txt. Next, we immediately pipe that stream to the stream returned by a call to createGzip. We then pipe the results of that to the HTTP response, which is a writeable stream. We could pipe as many of these together as we like, so long as we start with a readable stream and end with a writeable stream. One caveat I will provide is that event emitters and streams are built heavily on the legacy callback approach found in most of the Node.js APIs today. Using them with promises or async/await, as of this writing anyway, is not particularly straightforward. I fully expect this to improve over time, but you'd likely do well to not work too hard to try to combine them if you can avoid it.

Defining an Application and Managing Dependencies
Modularizing Your Application
Hello, my name is Paul O'Fallon, and I'd like to welcome you to the next module in the course, Node.js : The Big Picture. This module is titled Defining an Application and Managing Dependencies. Here, we'll take a look at how to modularize your own code in a Node.js application and how that same model applies to external application dependencies as well. Speaking of dependencies, we'll also introduce the npm command-line application and package repository. Let's get started. A Node project of almost any size will benefit from having its code organized into a collection of smaller files rather than just leaving everything in one big file. In fact, if you've spent any time in the GitHub repository for the examples in this course, you've already seen this. The code for the waitress, the cook, and the customer are in three separate files. In cook.js, we have a function called prepareFood and a line that begins with module.exports. Here, we are defining a JavaScript object with one property, prepareFood, which references our function. By assigning this object to module.exports, we have declared that this module, cook.js, exports one function named prepareFood. Anything else in this file, such as the ingredients constant, cannot be accessed outside of this module, effectively making it a private variable. Our customer.js file is similarly structured; however, in this case, our module.exports is just the Customer class itself. In order to use cook and customer in our waitress file, we need to bring them in via the require function. Now we can reference the prepareFood function via our new cook variable and instantiate our Customer class as well. You may be wondering why the cook variable here is lowercase, but Customer has a capital C. This is another convention. Since we're importing a Customer class, we name that variable with a capital, knowing that we'll likely end up calling new customer and assigning that to a lowercase variable. Cook, on the other hand, contains a function we'll be accessing directly, so we'll import that into a lowercase variable. This is just a convention, and doesn't have any effect on how the code actually functions. Another thing to notice is that we've prefixed each file name with its relative path. This. / tells Node to look within this project for a local file. The other use for require without a path element is to include a third-party module. You can see this in customer.js with require('moment'). This makes the third-party moment library accessible within that customer module. This moment library is made available to the project with the help of npm, Node's package manager.

npm and Application Dependencies
To most Node developers, npm is really two things, first a package repository where you can download and add third-party modules to your Node project. Npm is also a command-line application, installed alongside Node itself, which you can use to manage many aspects of your Node project. Let's take a look at a few common npm commands you might use in your Node project. A good first command to discuss npm init. Running this command is a good way to bootstrap your project and create a basic package.json file. This file is found in pretty much every Node project and is the central location for the project's metadata. This file gives your project its identity, and while you can create it by hand, npm init is a much easier way to make sure you get all the properties named correctly from the start. Arguably, the most common npm command is used to add dependencies to your project, like the moment example we saw earlier. Running npm install within your project will download a module from the npm package repository and install it alongside your project code in the node_modules directory. Adding the save parameter you see here will also update the package.json file to indicate that this module is now a dependency of the project. Npm accommodates more than one type of dependency. In the last slide, when we installed moment, we added it to package.json as a runtime dependency, meaning that the moment package is required for our application to run. Another type of dependency is a development dependency. These modules are used during a project's development phase. Examples of this could be modules for testing or packaging. Here, we are installing the Mocha package for writing tests and adding the save-dev parameter. Notice that this adds Mocha to a separate devDependencies section of our package.json. Having these dependencies recorded in package.json means that we don't need to check them into source control with our own code. When a new developer, or a build server for that matter, checks out our project with our package.json from source control, simply running npm install with no additional parameters will download a copy of all our project's third-party modules. Interestingly though, since modules may have dependencies of their own, we are really downloading the entire tree of dependencies necessary for our project. This is often like an iceberg. We may require very few modules ourselves, the part of the iceberg above the water, but the net effect is that we've introduced many, many dependencies to our project, the much larger part of the iceberg under the water. In our example here, just adding two dependencies to our project actually installed 180 modules alongside our code, totaling 16 MB. This isn't inherently wrong or bad, but is just something to be aware of. Your project may become, to a degree, dependent on many open-source projects with varying degrees of quality authored by many different people. This is not unique to Node, and to be honest, in my experience it works well the vast majority of the time. There have been some incidents though, most notably the left-pad incident, which have put this approach to the test. I encourage to read up on the causes and the improvements made to the npm package ecosystem as a result. Before we wrap up this module, I want to mention one more thing. For many years, npm has been the de facto CLI for managing your Node application dependencies, due in no small part to the fact that it's installed alongside the Node binary itself. In recent years, however, there have been other projects come up that seek to address this same need. A popular alternative to the npm CLI is Yarn. It leverages the same npm package repository for downloading and installing dependencies, but provides its own command-line interface for doing so. Over time, you may develop a preference for one of these over the other.

Assembling a Development Toolset
Installing Node / Testing Applications
Hello, my name is Paul O'Fallon, and I'd like to welcome you to the next module of the course, Node.js : The Big Picture. This module is titled Assembling a Development Toolset. Here, we'll start with how to install Node.js on a variety of operating systems. Then, we'll move on to a common approach for building web apps and APIs in Node. We'll wrap up with some options for testing and debugging Node.js applications. Let's get started. Installing Node is pretty easy. You can simply download it from the nodejs.org website if you like. It's also found in many package managers. For example, on a Mac, you can install it with Homebrew and on Windows with Chocolatey. My advice on Linux, however, is to use the NVM utility rather than the default package manager. I've run into permission problems trying to use Node installed via the operating system's default package manager. Also, NVM will let you move between versions of Node and potentially adopt newer versions of Node more quickly. Finally, there are also official Node Docker images available on Docker Hub. As we discussed way back in the beginning of this course, one of the most popular use cases for Node is building web applications and especially APIs. You can certainly do this with the default built-in HTTP and HTTPS modules that ship with Node, but a very popular abstraction framework for developing these types of applications is Express. Here is an example Express application written in a few lines of code. Express provides a very extensible middleware framework and has support for routing, as well as many popular template engines. There are other newer Node frameworks for solving these same problems, but Express remains my go-to framework for these use cases. There are a tremendous number of choices for testing Node applications. If you're a front-end developer and use Node-based tools for testing, it's very likely that those same tools can be used to test server-side Node projects as well. Here are a few of the testing tools that have served me well in my Node projects. First, we'll start with the test framework Mocha. You can install Mocha globally or as a development dependency in your project. In either case, you'll end up invoking the Mocha CLI to run your tests. All of the other testing tools you'll see here assume Mocha, but certainly do not require it. Here is an example of using Mocha's default BDD, or behavior-driven development, interface. We can group our tests using the provided describe and it functions, writing the test itself in the callback. This callback function is passed a function as a parameter, which we've called done here. We need to invoke this done function when our tests are complete. Note here too that Node has a built-in assert module for doing basic test assertions. However, this is where our next module comes in. Chai is an assertion library that provides a wide-ranging selection of assertions designed to make your tests more readable. Here's another Mocha test that uses a Chai assertion style called expect. You'll notice that Chai has a very readable, fluent API that makes the assertion itself very obvious. In this case, we expect the order to be one of the items in the menu array. The third tool in our testing suite is Sinon, which is a framework for spies, stubs, and mocks. This library helps us isolate the code we're testing by mocking the things we are not. Here's a test of the function that started it all, serveCustomer. In this case, we don't want to call the real cook, so we'll create a stub to replace it. Now, admittedly, our actual function didn't do a whole lot more than this, but if this was, say, a call to a database, mocking it here would make it much easier to test this function. Sinon also makes it easy to tell if the stub was called and, if so, how many times. Here's the calledOnce property, which we can assert using our fluent Chai assertions. The last tool in our test suite is Istanbul, which provides code coverage. The CLI for this tool is called nyc, and using that to measure the coverage of our tests is super simple. Here you can see that we have complete coverage over cook and customer, but not so much with waitress. We're given the exact line numbers that haven't been tested so we can go back and figure out why. These four open-source tools, when combined together, have been great tools in my testing tool belt.

Debugging / Conclusion
Now, if you're like me, your first debugging tool is printing to the console. I know it's not impressive, but it does get the job done. However, Node does have nice debugging capabilities, which are best leveraged in an editor or IDE. Here's a screenshot of Visual Studio Code in debug mode stopped at a console.log breakpoint. Most of what you see here is expected in any debugging experience, the step over and into options, the list of active variables, a place to add watches, the call stack, and a list of the loaded scripts. Admittedly, console.log is still my go-to debugger, but this IDE-based debugging capability has absolutely bailed me out when nothing else would. Okay, that was a whirlwind tour of Node.js : The Big Picture. In this course, we started with some common node use cases, including microservices, serverless functions, CLIs, and even desktop applications. We then looked at a brief history of Node, including its origins with Chrome's V8 engine, as well as the brief fork of io.js. We then looked at Node's asynchronous style of development, starting with callbacks, progressing to promises, and arriving at async/await. After that, we moved on to modularizing your applications with the require function, as well as leveraging third-party modules via npm. We wrapped up with a collection of testing tools and debugging strategies. I hope you found this course useful in your evaluation of Node.js. There's so much more to explore, and I encourage you to keep digging and learning. Good luck!
