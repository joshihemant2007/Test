Node.js is a perfect fit for a wide variety of applications. The HTTP module acts as a backbone for these applications by providing the functionality needed to create servers and interface with the web. In this course, HTTP with Node.js, you will gain the knowledge you need to implement Node’s powerful, built-in HTTP module in any application. First, you will learn about the request and response objects and how to work with streamed data. Next, you will explore URL handling, managing file uploads, working with authentication headers, and how to make requests to external APIs. Finally, you will delve into some common libraries offered by the community that help make development a breeze. By the end of this course, you will have the skills and knowledge needed to start building your own web applications with Node.js.

Course Overview
Course Overview
Hello. My name is Armen Avanesi. I'm a software developer and I'm very excited to present to you my course, HTTP with Node.js. Node is a perfect fit for a wide variety of web applications. The HTTP module is what acts as a backbone for these applications, providing the functionality needed to create servers and interface with the web. This course is a deep dive on the module, so we'll go as far as exploring the internals of it and its classes. Some of the major topics that we'll cover include creating a server, working with streams and event emitters, handling file uploads, and making API requests. We'll learn each of these topics by building an actual application together so you gain hands‑on practical knowledge. By the end of this course, you'll have a thorough understanding of when and how to use HTTP in applications of your own. Before beginning this course, you should be familiar with JavaScript, and more specifically how to use features present in ECMAScript 2015 and above. I hope you'll join me on this journey to learn all about Node's HTTP module in this course at Pluralsight.

Managing HTTP Requests
Introduction
Hi there, and welcome to my course on working with HTTP in Node.js. We're going to cover some really important concepts that are going to help your level up as a developer. Now, whenever I say HTTP, I'll actually be referring to Node's built‑in module with that name, which allows us to interact with the HTTP protocol. This module is very powerful and provides a flexible, robust API that allows us to do things like create a server, work with stream data, and make API calls. The best part about this module is that all this functionality is baked right into Node. We don't need to rely on a single external dependency. The plus side of this is that the knowledge around using this module will be relevant for a long time. On top of the core concepts that we're going to cover, I'll additionally touch on some topics like how to work with readable and writable streams, the EventEmitter interface, and some of the underlying classes that the module is composed of. This course is for you if you already have working knowledge of JavaScript and are comfortable doing things like requiring modules, using callbacks, and are familiar with general web server concepts, essentially the basics of working with Node. I've tried to make this course useful for as many people as possible. So, if you're already well versed in a particular section, feel free to skip it. To reinforce what we learn and give you some solid practical knowledge, we'll slowly build out our project application together. More specifically, we'll be building a collaboration tool for artists that will allow them to quickly upload their assets. The users will be able to use the app through the command line or browser and will consist of an API endpoint for creating users, the ability to upload and retrieve photos, and the ability to fetch a specific image's metadata. Following along is optional, but will definitely help make things stick. I've included a link to my GitHub with all of the source code that we'll be going through. To make things easy to follow, I've split up the code by module and by clip. I'll also be including a package JSON file with all of the course's dependencies. All you'll have to do is install them using the package manager of your choice, whether it's yarn, npm, etc. Now, as for what version of Node you should be using, the HTTP module has been in Node since its earliest days. So, feel free to use the one of your choice. If you're unsure, going for the long‑term support version is always a safe bet. At the moment, that version is 12.13.1, so it's what I'll be using. Lastly, things like how to save users in a database are outside the scope of this course. In reality, you'd implement something like a function or microservice that is responsible for this; however, I wanted to make this application feel as realistic as possible. So I've created simple services that we can call that don't actually have much logic in them. This will allow you to call the service just like you would in a real application. Great. Now that we know what we're going to learn and what we're going to build, let's get started.

Creating Your First Server
Let's get started by creating a very minimal web server in Node. Any time that we want to do this, we'll always start off by importing the http module, and then using the createServer function. Once we create the server, we create an anonymous function that will get fired, once for every request that comes in. For this reason, it's what's called the request handler. When Node calls this request handler function, it calls it with two parameters, request and response, oftentimes shortened as req and res. It's through these objects that we can analyze what comes into our server, for example HTTP headers and bodies, and craft a response that is going to be sent back to the client, for example a specific HTTP status code and a JSON payload. For now, we'll just use a log statement to see that we are receiving the request. Now, in order to actually serve requests, we need to call the listen method on our server, passing it the port number that we want our server to listen on. In this case, we'll use 8080. At this point in time, if we were to make a request to our server, for example by loading up localhost 8080 in our web browser, it would keep loading until it eventually timed out. This is because although we're receiving the request, we're not writing anything back to the clients. Although not completely finished, this is a great start for us. We've technically created our first server. We'll work on sending responses back to the client in the following sections, but first, let's take a deeper look at the components that make up the http module.

The Anatomy of the HTTP Module
In the short process of creating our server, we actually ended up working with a few of the HTTP module's core classes. It's a good time to take a look at those classes and the other components that make up the module. Top down, HTTP consists of some classes, some methods to do things like make API calls, and some constants that we can use during development. First off is the Server class, which is what's returned to us when we use the createServer function. This object can be looked at as a container for our logic and what will be the means of serving requests to clients. When source server receives a request, the Server class creates an incoming message and a server response and passes them to a request handler. So remember when you saw the request and response parameters from the last section? Know that these objects represent exactly what's coming into our server and what we want to respond with, so you can interact with them using any properties that these classes make available. These classes are used whenever we want to receive requests, but picture the other side of things when we want to use our server to make API calls, in other words, when we're acting as a client in the client server model. We have some classes that are used for that, the first being http.Agent. It is used for managing connection persistence and reused for HTTP clients. Essentially, it plays a key role in Node's networking functionality. You'll typically be working with this class a lot less than the others and only if you have some pretty specific requirements such as the need to modify a request's keep alive time. Lastly, a client request is created internally whenever we make an API call and represents an InProgress request. We also have a set of methods that allow us to do things like make those API calls and constants, which conveniently provide a collection of all HTTP status codes and a description of each one. For example, if we were to log out the module status code for 404, we would get the string, Not Found. These save us time from having to roll out our own constants, and in general, it is best practice to use them throughout our code. Since Node has an asynchronous event‑driven architecture, certain patterns like the use of streams and event emitters are very common. Now that might sound challenging if you're not familiar with those concepts. If it does, don't be overwhelmed. I'll provide a quick overview of what they are now, and we'll be working with them slowly. By the end of this course, you'll feel very comfortable using them. First off, let's discuss event emitters. Remember when we created our server in the last section and then used this syntax to create a request handler? What we actually did here was register a handler for this server's request event so that anytime it emits an event with that name, our listener function gets fired. Events can be emitted with parameters. In this case, our handler will be passed two. Hypothetically, if we wanted to, we could register multiple handler functions for a single event. In this case, any time a request came in, our server would emit the request event and all of our listening functions would fire. This is going to be the way we work with the HTTP module and its functions. The documentation will always let us know what events to listen for and at which point in time they will be emitted, for example, when a client disconnects, connects, etc. We'll then register the handlers to react to those events accordingly. Now, let's discuss streams. When working with Node, whether you're reading a file from a file system or making an API request, we'll often have access to data asynchronously piece by piece. These pieces are called buffers. So, for example, if a client were to send us a request with a JSON payload, we may receive it in several pieces. It would be our responsibility to collect all of it and then piece it together once we knew that the client was done sending data. We could then work with it from there. There's a dedicated section on why this is beneficial for applications, but it is handy to know this for now. Going back to our overview of the module, we are able to use the functionality that streams and event emitters provide because the module's classes extend the stream and event emitter interfaces. In particular, IncomingMessage extends the readable stream interface, ServerResponse and ClientRequest extend the writable stream interface, while all three of them plus Server extend the event emitter interface, meaning we can work with them using events. That covers the components that make up the entirety of the module. It's not mandatory to fully know these, but understanding what's going on under the hood will definitely help you as a Node developer. With that being said, I think we're in a great position to continue building our project.

Working with the HTTP Method, Headers, and URL
We now know that our request handler will receive two parameters, with one of them being the request object. Let's take a deeper look at it and grab some valuable information from it, like the HTTP method, URL, and headers. Generally, this information is going to be one of the first things we look at, and based off of this information, we're going to determine how to react to each request. As a starting point, if we log out the entire request object, you will notice that it is fairly complex, with a lot of valuable information on it. Right now, what we're interested in specifically is the request method and URL. Let's log these out and see what we get. As you can see, we are returned two strings, a slash and GET. Let's implement some very simple routing for our feature that will allow users to fetch an image's metadata based on its ID. When a user makes a request for this metadata, they'll be using a URL that includes a query parameter containing the ID, which will look something like this. To handle the parsing of this query parameter, we can use the url package, a native Node module that provides us with utilities for parsing and resolving URLs. First, we import it and then use the parse method, passing it the full URL, and true to make sure we parse the query string as well. If we log this out, you will notice that we get a full breakdown of our URL. What we need from this is the pathname and query. The pathname provides the URL free of any query parameters. We can use it in our routing to match all requests for metadata, regardless of the ID that was provided. We'll also make use of the query object, which contains key‑value pairs of the query parameters that were sent to us. In this case, we'll grab the ID and log it out to make sure it's what we want. If I save the changes and send the same request using curl, you'll notice the ID in the server's output. Next, let's use this ID in a function that returns us some metadata in JSON format. Let's log this out to make sure everything is working as expected. Okay, great, we're able to fetch our JSON. The next step in developing this feature will be to return this JSON to our users, which we'll do in the next module. Additionally, if we wanted to grab the headers, for example, to look at a user agent, we could also access them on our request object in the same way we access the method and URL. If I use curl to pass an authorization token in a request setter, you'll notice that it is neatly parsed in this headers object. Great, we've now covered how to get some key information from our requests and have implemented a very simple router without the use of any frameworks. Next, let's learn how to work with the request body.

Working with the Request Body
When working with POST or PUT requests, we'll generally want to work with the request body to access the payload that was sent to us. In the last section, we accessed the request object's URL, method, and headers as properties on the object. Working with the request body takes a little bit more work. We'll need to work with streams data. The part of our application that we'll work on now is the one that is responsible for creating a user from a JSON payload. First things first, let's add a handler for the event called data, which gets emitted every time we receive a new chunk of data or, in other words, a buffer. These buffers will get passed as a parameter to our event listener. I'm going to start off with an example of showing you how data arrives at our server into multiple pieces. We'll make a POST request to our server using a file that contains a large amount of users in JSON format. We'll use a large amount of users because if the payload is too small, that data might just come in as a single chunk. I've included this file in the clip's source code. Now, if we convert each buffer into a string and then log each one out, you'll notice that the data arrives at our server in several chunks. This is why we need to collect all of it and piece it together. To do so, we'll start by creating an array that will store each buffer. Next, we'll push each buffer into our array and then add an event handler for our end event, which gets emitted when our stream is closed. It's at this point in time that we'll know there won't be any more incoming data, so we can start processing our payload. Note that we can chain our handlers on the request object like so. In our handler, to get our parsedJSON, we'll use the Buffer.concat method to concatenate the individual buffers into a single one and then use the JSON.parse method, which can convert a buffer into a plain old JavaScript object. In our application, you want to be able to create users from JSON payloads that contain our username. Building on what we have so far, we can access a username that was sent to us. At this point in time, we could create a new user from it. In our case, we'll use one of our example services. Remember that we are able to listen to the request object event because it is actually an IncomingMessage, which extends two interfaces, ReadableStream and EventEmitter. This is the reason that we can read the incoming data as a stream by listening to the events that it emits. See how we gain a little bit of functionality from each class? So, to recap, our request headers, URL, and method are available to us immediately, but we need to fully receive a request body and wait for the connection to be closed before working with it. Buffers are like small packages of information that we can piece together, and we are able to receive streams of data because the request object is a ReadableStream, and we can listen to its events because it is also an EventEmitter. If you think that the work needed to get a request body is a little tedious, you're not alone. In the next section, we'll cover some packages that simplify this work and make development a little smoother.

The “body” Package
In the last section, we went through a fair amount of work in order to get our request body into a state that we could work with. Thankfully, there are packages in the Node ecosystem that make this process a breeze, one of them being a package called body. It's available on npm and is extremely popular. At the moment, it has over 400,000 weekly downloads and is a package that is recommended in the Node.js documentation. It is also easy to install and focuses on simplicity to help streamline development. Node.body is just one of the many packages that is used to parse request bodies. There are a few other options out there, too. For example, if you're using the Express framework, the body‑parser package is a very popular choice. Without further ado, let's see how we can use body. First, we'll need to install the package in our project. Make sure you've installed the dependencies I've included in the package JSON file with your package manager. Once that's done, body will be installed, so we'll need to import it. The way the package is designed is such that its functionality is split up into different modules. If I was to import all of these packages at once, this is what it would look like. As you can see, we have a specific module for working with requests that have text, JSON, and form data, and a module that will detect the type of content for you automatically. Since the end goal is for us to be able to create users from JSON data, we're going to use the JSON package. What we've imported here is a handy function that will provide us with the same functionality that we've implemented from scratch. This means we can get rid of this and drop it in here. This function accepts a few parameters. The first two are the request and response objects. So we'll pass in the ones that we've been working with so far, the same ones that our request handler receives. The next parameter is a callback that will be passed two things; err, which will represent any errors that occurred while parsing the request, and body, which will represent our successfully parsed JSON data. As a first step, let's simply log the data, assuming everything went well. If we fire off a curl request with the username, we can see the parsed payload. Now we could use this username to create a user just like we did in the last section. As you can see, body simplifies your workflow by enabling you to work with the request body in the same way that we worked with the requests, headers, and URL. Simply pull out the data you need from your ordinary JavaScript objects and work with them as needed. The concepts of buffers and streams are abstracted from you. We can also create a simple error handler here. Note that if we do have an error here, the body parameter will be undefined. Let's log out our error, and add a character to the payload we've been sending so that it becomes invalid JSON. If we use this curl request now, you'll notice that we get an error from attempting to parse our corrupt JSON. Of course, you'd want to handle this error properly in a production environment, depending on your needs. To recap, in order to use the body package, we import the function that pertains to the type of content that we will be parsing, using any if we are unsure of this. Next, instead of listening to the data event on our request object, we can use our imported function, passing in our requests and response objects, and a callback that will make our parsed request body available to us.

How About HTTPS?
We've gone pretty far with our basic server example. You might be wondering now, what if I want to use HTTPS on the server? Thankfully, Node provides a module with a similar API called HTTPS. To convert our server, we'll need to do a few things. First, we require the HTTPS module instead of HTTP. We can now use this new module when creating our server. Next, we need to pass an options object to the createServer function that we previously called without any arguments. Using this options object, we pass two things required to use an HTTPS connection, a key and a cert. These must correspond to two files on your system. They can be generated with a toolkit called OpenSSL. I've included a command that will allow you to generate them if you are using a UNIX‑based operating system. If we run this command, you'll notice that we have two new files in our directory, the key and our certificate. Next, we use a module in Node called fs that will allow us to synchronously read these files from our system. We'll be looking at responses in the next module, but in order to test this from our browser, we'll need to create a simple response to send back now. Let's do this with the end method, passing it a message. Lastly, we need to change the port that our server is listening on to the port that is used for HTTPS connections, 443. Okay, we've implemented the necessary logic. Let's run our server. On my system, I need to do this with the sudo command. Now let's access our server through our browser. Your browser may give you a warning since we generated and signed our own certificates. If this is the case, you should be able to accept the warning to visit the site anyway, which is fine, since we are just testing things out. Now, you'll notice that our server works, and you can see our message. That's all we need to do to allow our server to use HTTPS.

Summary
In this module, we started off by creating our server from scratch. We did this with the help of the createServer function, which allowed us to create a container for our server's logic. We then covered the different components that make up the HTTP module so that we know the classes it uses under the hood. Along the way, we implemented some routing for our metadata service so that incoming requests are directed to the appropriate logic. Also, our application now parses request bodies to access incoming payloads. We used this information to grab a username from it in order to create user accounts. In contrast, we implemented and configured the body package and saw how it can speed up our development. Lastly, we learned how to configure our server to serve HTTPS connections. In doing this, we saw how to use a key and cert derived from the OpenSSL package. These topics cover the fundamentals of how to manage incoming requests. Next, we'll look at how we can use the HTTP module to manage responses

Managing HTTP Responses
Working with Status Codes and Headers
We've been working with requests coming into our server, but as it is, these requests hang and eventually time out, because we're not sending any responses back. Let's change that. Any time we want to send a response back to our client, we'll need to do a few things. First, we'll set the status code and headers on our response, and then we'll write data to it. For example, we could write the contents of an HTML file that we want to serve or some JSON. Once this is all done, we close the connection. Let's get started by setting a status code on our first response. We currently have routes for creating a user and for fetching and image's metadata, but what if a user accesses a route that we haven't explicitly created? For example, what if they access /admin or even /hello? In these cases, we should respond with a 404 Not Found. We can do this by adding an else condition to our request handler, so that the logic we implement here runs for any requests that don't match our existing routes. We can set the status code property directly on the response object that is made available to us by our request handler. It's through this response object that we'll craft the exact response that we want to send back. To set headers, we can use the setHeader method, passing it the name and value of the headers that we want. This is the place you typically set headers used for caching or security, for example, to prevent cross‑site scripting attacks. Now we'll close the connection with the end method. This is because technically we're sending back a stream of data to our client, because our response here is a writable stream. Because of this, even though we're only sending back headers, we need to signify that we're done sending data. Here, to test things out, we'll set the X‑Powered‑By header to Node, and then use curl to send our request to a route that doesn't exist. As you can see, we are presented with a 404 status code and our X‑Powered‑By header. Note that we can call the setHeader method multiple times with no restriction on the values we set. So for example, we could even set a header such as Hello World. An alternative way of setting headers is by using the writeHead method where we pass in our desired status code and an object containing the headers that we'd like to set. It's a similar approach. The main difference is that when doing things this way, we manually write our headers to the response instead of counting on Node to do it at the correct time. To practice this on your own, try setting the Content‑Type header to applications/json to signify that response contains JSON data.

Responding with JSON
We are almost finished with the component of our application that is responsible for returning the metadata around the specific image. Let's complete the logic inside of this route so that we return this metadata in our response as JSON. We know that our FetchImageMetadata function takes an image id and returns a JavaScript object. We've stored this object in a variable. In order to return it in a response now, we need to do a few things. First off, it's important to let our client know the type of data that it will be receiving with the Content‑Type header. We'll do this with the setHeader method we used in the last section. We'll also set the statusCode property on our response to 200. Since our metadata is contained in a native JavaScript object, in order to send it back to our client, we'll need to serialize it, converting it into a string. We can do this by using the stringify method that is available on the global JSON object. Next, we need to write our data into the response stream that will be sent to our client. We do this using the write method, passing in our JSON. Note that as we discussed previously, a request object is a writable stream, meaning if we wanted to, we could write data into it multiple times, like so. Although in this case, it would no longer be valid JSON. Lastly, we close our stream with the end method. Let's give this a shot in our browser now and see what we get back. Great. As you can see, we have some serialized JSON data. We have now completed the image metadata feature of our sample application.

The Benefits of Streamed Data
We have now worked with readable and writable streams. We use the readable stream when we access the request body and the writable stream when we wrote our data to our response object. But what benefits do streams actually provide us? Let's clarify why they're so important and how they play a role in Node's reputation for being very performant. Overall, there are two main benefits when it comes to using streams. These are memory efficiency and time efficiency. Streams can make your applications more efficient because, as we've seen, we can operate on our data one piece at a time. This results in not having to load the entire contents of what we are working with into memory, there by using less resources at a single given moment. Let's use an example to illustrate this concept. We know that in our application, we want to allow artists to upload their files because we essentially host these creatives for them. Of course, at some point in time, they're also going to want to download these files. But what if we were hosting files of various sizes and they could be anywhere from a few megabytes up to a gigabyte? Traditionally, when serving a file you my load it entirely into memory and then serve it. This approach will generally hog a lot of the system's resources. Well, this is where streams come into play. Instead of loading an entire file into memory, we would be able to load just a piece of it and then pipe it into our response before loading the next piece into memory. Since we only need to use a small amount of memory for each file, regardless of its size, we are able to serve more files or requests more efficiently. One way of verifying this memory efficiency is by checking out how much our server process is using, for example, by using Activity Monitor on a Mac. Here, this is my server running in an idle state. I connected to it and served the 1 gigabyte file without streaming it. And this is what happened to the memory usage. As you can see, it had to load the entire file into memory. Now this is the same server serving the same file, but streaming it into a response. It uses just a fraction of the memory to accomplish the same task. Now, by not having to wait until a large file is loaded into memory, we gain on time efficiency as well. Instead, we start streaming this file to our client as soon as we have our first chunk of data available. This is why streams plays such a key role in Node.

Handling Errors
Let's quickly touch on errors and how to handle them. It is always recommended the handle errors, even if you're only logging them out. If you don't, the error will be thrown and will potentially crash your application. There are two places that we need to handle errors, on the request stream and the response stream. Remember how he added an event handler to listen for the data event on our request and response streams? Well, similarly, we can also listen for the error event, which will get fired with the error we've encountered as a parameter. This parameter will be of the error class itself, which allows us to access properties like its code, message, and stack. From here, there are several ways to handle them, depending on your needs, from using logging tools to making API calls to monitoring services. Regardless, you'll most likely want to let the client know that an error has occurred with the appropriate statusCode. In this case, we could set the statusCode variable like we've been doing so far, write a response back, such as an error message or full‑out error page, and then close the connection. So what are the reasons that an error might occur in the first place? Node's documentation has an exhaustive list of errors and their causes, but a common one is an ECONNRESET, caused by the connection to the server closing prematurely. Another common error to watch out for is ETIMEDOUT, caused by the connection that has stayed open for too long, eventually causing a timeout.

Summary
We've now worked firsthand with responses. We created the image metadata endpoint for our application, and in doing so, we learned a few things. First, we learned how to craft a response that includes headers and a status code. We saw how headers can be set in two different ways, with the setHeader method and the write method. Second, we learned how to work with the response stream to send back some JSON data. Remember, in order to be sent back to the client, the JSON data has to be serialized into a string. After this, we learned about the end method and how it is used to signal that we'd like to close our connection with the client. Along the way, we took a deeper look at the benefits of streams and how they allow our applications to be more memory efficient and time efficient. Lastly, we discussed the topic of error handling. We explored the details that the Error class makes available to us, and to tie things up, some common errors that are encountered when using the HTTP module. In the next module, we'll work on a new component of our application, learning how to handle file uploads.

Handling File Uploads with Formidable
What Is Formidable?
We've laid the groundwork for our application's main functionality by creating a server that will handle requests and responses. When a file actually arrives at our server, we're going to use a popular library called formidable to work with it. If you've never worked with formidable, it's used extensively in the JavaScript community and makes the handling of multi‑part uploads a breeze. Here are some of the main benefits from the documentation. It is fast, automatically writes file uploads to disk, it has a low memory footprint, and handles errors gracefully, and lastly, the package has a very high test coverage. You may be wondering if it's possible to handle file uploads without a third‑party library. It definitely is, but it's non‑trivial, very complex, and time consuming. It's not like working with a response body without the use of the body package like we did previously. Overall, I highly recommend this package, even in a production environment.

Handling File Uploads with a Callback
Getting started with formidable is easy. I've added it to the package.json that's in the source code for this video. Make sure you've installed that using the package manager of your choice. Once installed, we can require it in our project to make it available to us. Notice that I've modified our catch all route. Now, if a user goes to a route that we haven't created, they will be sent to an HTML page containing a form. You can run the server and load up localhost 8080 to see it. This is where users will upload their files or, in the case of our users, there creatives. The form element here has its method set to post, path set to upload, and encoding type set to multipart form‑data, since it will be used to submit files. Let's jump back to our server and create a matching route to handle these requests. Now let's implement the logic required to handle an uploaded file sent to us with the form. There are two ways to do so, with a callback or with events. In this clip, we'll be taking a look at the callback approach. To get started, let's create a new formidable incoming form and then call the parse method on it. The pars method takes a request object, so I'll pass it the one made available to our request handler, and a callback function. This callback function will be fired once the processing has finished with three parameters, error, fields, and files. The error parameter will be null if the form is processed successfully. The fields parameter will contain key value pairs representing the name and value of each of the forms fields. On our form, this will correspond to the description field. The files parameter will, of course, represent the files that we've uploaded. Let's log these out with some spacing to see the output firsthand. If I jump back to our browser, enter description, and select the file, and then hit Upload, you'll notice the output and the terminal. As expected, we have our description and its value, but additionally, we see the use of formidable's file class. This class represents a file and exposes valuable information about it, such as its size, location, name, and type. We can also get its last modified date, which will be a date object containing the last time the file was written to. For our application, this is exactly the type of information that we could store in a database so that it's accessible through a metadata retrieval endpoint. To complete this endpoint, we'll set a status code on our response and then close the connection. This serves as the basis for handling file uploads. We have one issue though, our file is uploaded to a temporary directory on our operating system. In the next section, we'll change that and specify where we'd like to have our file saved.

Configuring Formidable
Formidable offers a great API that allows us to configure our form handling functionality. Currently, when we upload a file, our form's default behavior is to save the file in a temporary folder assigned by our operating system. Let's change this so that the files are saved in this same directory as our server.js file. To modify the behavior of a form, we can pass our incoming form object a configurations object to specify its new behavior. In this case, let's set the uploadDir to the _dirname variable, which will place our files in the same folder as our server.js. Let's test this out. If we start the server and then resubmit our form, you'll notice that we can now see our uploaded file. You'll also notice that it doesn't have a file extension on it. This is another default behavior that we can change by setting the keepExtensions option to true. While we're doing this, let's also allow our form to accept multiple files and change the maxFileSize that's allowed from its default of 20 megabytes. This option takes the size of the file in bytes, so about the size of a 5 megabyte file. I'll also create a simple error handler. Now let's start testing this out. If I restart the server and resubmit the form, you'll notice that our file has kept its extension. Next, if I upload a file that is 10 megabytes, you'll notice that our form's error parameter is no longer null. It now contains a message describing the reason of failure. Next, if we use our form to submit multiple files, we'll notice that our files argument is now an array instead of a single file object. These are just a few of the configuration options available. We can also set things like the encoding and limit the number of form fields that will be decoded. Check out Formidable's documentation for the extensive list of options.

Handling File Uploads with Events
When we work with formidable by using a callback function like we did previously, we gain access to submitted forms, fields, and files only once it has been fully processed. But remember how we mentioned that streams and event emitters are used extensively throughout the Node community? Well, formidable is another instance of this. By hooking into events that are emitted, we can approach the handling of file uploads in an asynchronous, event‑driven way. We work with these events just like we worked with our request and response objects previously, by specifying each of the named events that we want to listen for, and then by specifying a function that we would like to have called when this event occurs, Let's go over formidable's events and what they're used for. Using the fileBegin event, we can detect when a file is added to the upload stream. The handler will be called with the name of the file and what has been received. Using this event, we could stream the file somewhere else as it came in, or change the directory that the file will be stored in. Once a field and file pair has been received, it will be available to use in the file event. This will be called with the same parameters as the fileBegin event. The field event is emitted each time a field value pair has been received. We can also listen for the progress event. This gets emitted after an incoming chunk of data has been received, with the bytesReceived and bytesExpected parameters. Being able to consistently access the amount of data that has arrived at our server is valuable and allows us to do things like create our own progress bar. If we were to do this, we could check the percentage of data received by dividing these two numbers and then communicate this number with our client. For example, with the technologies such as WebSockets. Similar to the error parameter using the callback approach, the error event is emitted any time there is an error while processing the form. Upon encountering the error, formidable automatically pauses the request. This means you'll have to manually call the resume method on the request object to continue the life cycle of it. The aborted event is emitted when the request was aborted by the user. This could, for example, be due to a timeout. An aborted upload will be considered a failed one so if this event is emitted, an error event will follow. Finally, the end event will be emitted when the entire request has been received. This is a good place to build and send a response back to the client, setting things like the appropriate status codes and headers. Putting it all together, this is what we have. Let's test out a case where we have a successful upload. If I upload a file and then check the server's output, you'll notice a few things. You can see where our file has started, the description that we entered in our form, and the constant updating of the amount of bytes that we've received compared to the amount that we're expecting to receive. Note that this process is happening asynchronously so the order of the events is not guaranteed. We can also see our file event fired once we eventually received the field and file pair, and finally, an event for when our request is done. Let's now see what happens when we cancel an upload. If I hit Upload, but quickly hit Esc to halt the request, you'll notice that our aborted event fires, which in turn causes the error event to fire. We have logged out our error, which, as you can see, describes exactly what happened. When compared to the callback approach, we have a lot more control over this entire flow. It's not always necessary to use the style of form handling, but knowing that is available is extremely useful.

Summary
We have now completed the portion of our application that will handle the uploading of files. In doing so, we learned about formidable and its features and benefits. Like I mentioned, packages like this play a huge role in the Node community because implementing this type of logic from scratch is tricky. We then saw firsthand how to work with the library in two different ways, with a callback and with events. We now know that events offer us the greatest flexibility when it comes to fine tuning our logic. Remember that when using formidable's API, you'll be working with two classes, File, which exposes information about a file, such as its size, location, and name, and IncomingForm. It's our incoming form object that we learned how to configure, passing in things like a custom upload directory, an option to keep file extensions, and how to specify a max file size. Lastly, we took a look at firm formidable's errors and how they provide us with information that conforms reason for failure. In the next module, we'll talk about Node's HTTP request capabilities.

Making External API Requests
Making a GET Request
When working with Node, you're often going to need to make an API request to another server. Thankfully, Node's HTTP module includes the functionality we need to make these requests. Getting started is simple. This is an entirely separate file from anything we've previously worked on. To make a GET request, we first need to import HTTP. Next, we'll create a variable and call the request method, passing it an object that specifies our configuration and a callback. In this object, we need to, at the very minimum, specify the URL that we'd like to make a request to as the hostname key. Our callback function will be passed the response that we get. Much like when we created a server and received requests, the statusCode and header has become available as properties, but we need to listen for a stream response through the data event. Again, much like receiving a request, our listener for the data event is passed a chunk or buffer each time it is called. If we want to test this out, we can log out each of these buffers as a string. We'll also add a handler for the error event, and then use the end method on our request to finish sending it. If we run our file now, our request will be made. At the top, you can see our statusCode of 200, a headers object, and then our response body. You'll notice that our response body was received in several chunks. Note that request is a method that gives us the most flexibility when it comes to making requests, but we can also use a convenience method called get to achieve the same result. If we take this approach, we'll pass a URL as our first parameter, and our end method will automatically be called, so this line is no longer needed. As you can see, we get the same output. Lastly, we can also make a call using the HTTPS protocol. To do so, all we need to do is import the https package instead. If we run our file again, you'll notice that we've made the same API call.

Making a POST/PUT/DELETE Request
We've now learned how to use the HTTP module to make a GET request. The way in which we make a POST, PUT, or DELETE request is very similar. To make this section as practical as possible, we'll work with two files, one that is going to make a request with the payload, and the same server that we've previously worked on. Let's first create a JSON payload that we want to send in our request. I'll use this stringify method here to serialize a regular JavaScript object into a string. This can be anything you'd like, as long as it's serialized. Next, we're going to build up the configurations object that is passed to our request method. I'll move it out into a separate variable to make it a little easier to read. In this object, we'll use localhost as the hostname since we'll be making a request to a server on the same machine. We'll add the port that our server will be running on, a path that we have a route for on that server, an HTTP method, and a headers object. Here, we'll specify the content type at length to indicate that we're making a request with the JSON payload. We're pretty much set, but we still have to send our payload. To do so, before we end the connection, we write our JSON data into the request stream. This is possible because a request object here is a writable stream. That's all of the logic necessary to create our request so let's jump over to our original server. Remember how we created our route for user creation? If we log the request body that we parsed with the body package, you'll notice our username. From here. making a PUT or DELETE request is simple. Going back to our other file, all we need to do is change the method used in our configuration object and reset.

Working with Auth Headers
Sometimes we need to make requests that are authenticated. One of the most common ways of doing this is with basic authentication, where we pass a header containing authorization as its key, and basic, plus a username/password combination as its value. Doing this is straightforward. Building up on our previous example where we created a POST request, we just need to add another key value pair in our headers objects. We'll add authorization as the key, but for the value, our credentials need to be converted into a Base64 string. Conveniently, buffers in Node can be easily converted into Base64 strings. So we'll create a new buffer, passing in our username and password, and then call the toString method on it, specifying the encoding of our choice, in this case, Base64. Lastly, it's best practice to handle the passing of credentials over HTTPS. So we import the HTTPS module instead, and specify that we'd like to use the standard HTTPS port of 443. Great! This is enough for us to get started making authenticated calls, passing encoded authorization headers.

The “axios” Package
Node's HTTP module provides a very flexible and verbose way of making API requests. Depending on your needs, though, you may want to experiment with other packages that are popular in the node community. Axios is one of those packages. At the moment, it has over seven million weekly downloads and is very well maintained. One of its features that I'm particularly fond of is its ability to work server side in node but also client side directly from a browser. This makes it a great fit for full‑stack applications built with JavaScript. On top of that, the package supports streaming, a modern promise‑based API, and the automatic transformation of your request and response data into JSON format. In our project, axios is already added to the package.json file, but if you ever need to install it manually, you can do so with yarn, npm, or the package manager of your choice. After that, we can require the package and call the get method on it, passing in a URL. We'll use google.com like our previous examples. Since axios has a promise‑based API, we'll chain two functions to our call, one that will be responsible for handling a successful response, which will be passed to the response from our call, and one that will handle any errors that might occur, which will be passed to the error in question. Since we're just testing this out, we're only going to log out the error here. If we log out the response, you'll notice that it contains details like its status code, method, and URL. Most importantly, though, the response body is made available to us through the data property, so we don't need to worry about streaming it. If we log it out, you'll notice that we get the exact same output as we did when we used the HTTP module, but we've only used a few lines of code. However, if we did want to stream the incoming data, we could. To do so, we'd start off by rewriting our request. The get method is a convenience method, but we need something a little more robust, so we'll call axios directly, passing in a configuration object. In this object, we'll specify we want to make a get request with the method key, set the URL, and then said the responseType to stream. Now in our callback, we can access the response's data just like we did the last time, but call the pipe method on it and pipe it into a writable stream. In this case, I'll pipe it to a file with the name I've chosen. Of course, In order for this to work, we'll need to import the fs module. Now, if we run this, you'll notice a file appears that contains the contents of the request. Another helpful use of the package is its ability to transform request data before it is sent to the server. Let's test this out with a post request to our existing server's user creation endpoint. To get started, we'll make a call to axios again but change the method to post. Next, we need to specify our payload with the data key. We'll pass in an object with an array of userNames. From here, In order to achieve our goal, we'll use the transformRequest key using an anonymous function as its value. This function will be passed two parameters that represent the things we can manipulate before sending our requests, data, which is our payload, and headers. Using our data parameter, we can map over our userNames and modify them in any way that we choose, for example, by adding a character to the end of each name. In reality, you may want to do something like the removal of unwanted or malicious characters, but how you transform the data is entirely up to you. As a last step, we need to make this function return a string, so we'll use the stringify method on our new data. Let's test this out. If we run our file and then look at our server's output, you will notice the array of userNames arrives as a payload. You'll also notice that our server's user creation endpoint isn't configured to accept an array of userNames. If you'd like to make this example work, you can modify our payload to be a string instead of an array and then transform the string. If we save this now and then jump back to our server, you'll notice that a user was created. On the flip side, after making a request, we can modify our response's data before passing it to the then or catch blocks using the transformResponse key in a similar fashion. One last feature that we'll look at is the ability to perform multiple concurrent requests. To do this, we can use the package's all method. This method accepts an array of axios requests like the ones we've been using and returns a single promise with all of our data. This means that in our callback we'll have access to an array of responses. Using this array, we could access the metadata from each response to, for example, log out the descriptions. Now, if we run this, you'll notice two lines of output corresponding to the data returned from each request. To finish things off we'll make this a little bit cleaner by moving these requests into their own functions. After they've been moved out, we can pass them into the all method, which makes things a little easier to read. Overall, the fact that node has a built‑in way of making API requests is great, because there's a certain level of reliability and stability that comes with it. However, sometimes it may speed up development to use certain tools and abstractions, and axios is a great one.

Summary
Here's a summary of what we learned in this module. First, we learned how to use the HTTP module to perform GET requests in two different ways, with the request method and the get method. Remember, get is just a convenience method for requests. We also learned how to perform POST, PUT, and DELETE requests. We also saw that we need to work with streamed data when receiving a response from a request, and when writing data into one to send a payload. We then discussed the format of basic authentication headers and how to implement them in these requests. We learned that when passing credentials this way, the value of the header needs to be encoded in Base64 format. Thankfully, this can be easily done in Node with the buffer class. Lastly, we discussed how packages in the Node.js community can help simplify development. Axios is extremely easy to use and does exactly that. We've come a long way, building each component of our project application from scratch. If you've made it this far, congratulations! I hope this was a great experience for you and hope you can carry this knowledge forward, applying it in your own line of work. Thanks for watching!
