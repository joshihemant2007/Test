Reading and writing files are powerful features in Node.js but used less frequently. In this course, Managing Files with Node.js, you’ll learn foundational knowledge to efficiently read and write files with Node.js in practice. First, you’ll explore reading files asynchronously and synchronously. Next, you’ll discover how to effectively write files. Finally, you’ll learn how and when to use streams for file manipulation. When you’re finished with this course, you’ll have the skills and knowledge of file manipulation needed to read and write files using Node.js.

Course Overview
Hi everyone, my name is Nate Taylor, and welcome to my course Managing Files with Node.js. I am a solutions architect at Aviture in Omaha, Nebraska. Node makes it easy to open a file, read its contents, and use that data in a way that makes sense to your application. In addition to reading contents, Node's file system API also simplifies adding data to a file, such as appending data to a log file. This course is designed to quickly get you up to speed on the file system API. You'll be able to manipulate files right out of the gate. Some of the major topics that we will cover include reading files, writing files, and handling streams. By the end of this course, you'll know how to open, read from, and write to a file, and you'll know when to use streams instead of file manipulation. Before beginning this course, you should be familiar with JavaScript, particularly Node.js. I hope you'll join me on this journey to learn file manipulation with the Managing Files with Node.js course at Pluralsight.

Reading an Entire File
Reading an Entire File
Unlike more common tasks, such as creating modules and functions, it's possible for developers to work in Node for quite a while without needing to manipulate individual files. Unfortunately, that means the skills to do that can often be rusty, or maybe even non-existent. That'll change with this course, as we'll quickly learn the essential skills to get you up and running with file manipulation. A common situation for line-of-business applications is the ability to read an entire file into the system. The reasons for importing the file vary from application to application. For example, one application might need to read a file into the system to import it into the database to take a delimited text file and convert it into meaningful records inside of the application. A second common reason that files are read into a system is to display the contents of that file. For example, a markdown editor or an IDE will need to open a file to display it to the user. What happens after the file is read is up to the application. For example, this module shows a demo of someone at Globomantics that has been tasked with analyzing data from fivethirtyeight's GitHub repo. Specifically, they've been tasked with analyzing the relationship between daily circulation and Pulitzer prize winning authors at newspapers. And while the context of the demos in this module fall under the task for Globomantics, the focus of this module will be on the mechanics of opening and reading a file. How the data gets analyzed is beyond the scope of this course. Here is an example of a Node script that will open up a CSV file. It then loads the contents into memory, converts the CSV rows into JavaScript objects, and finally it displays those records in the console, which you can see here. While this demo makes reading a file look trivial, you actually have several options available to read in an entire file. Sometimes the options can seem a little overwhelming, but don't worry, we'll start with the basics before showing you all of the various options. So what we'll cover next are the three parameters that you'll need to read a file.

Reading a File Asynchronously
Back in the old days of JavaScript, the only way that you could work with files or other blocking items was through a callback. That's because JavaScript is an asynchronous programming language. Over time, features such as promises and Async Await have been added to make it easier to handle the asynchronous code. However, at its core, JavaScript is still asynchronous, and so with that in mind we'll start by looking at how to read a file using the asynchronous readFile function in the fs API of Node.js. Start by cloning the repo that you see on the screen, taylonr/managing-files-with-nodejs, and once you've done that, open that folder in your IDE. I'll be using VS Code. Create a new file named asynchronous.read .js. Inside this file, the first thing to do is require the csv.parse file. This is a utility file that exists in the project that's going to convert our csv file to js objects. It's super rudimentary, but it will work for our purposes. It's abstracted out because we don't really care how to convert csv to js objects, that's not the focus of the course, we're just glad that we can do it. Next we need to require the file system API functions from Node.js. Specifically, since we want to read a file, we need to require the readFile function. The fs library holds all of the function calls that we'll use for reading and writing files in this course. The function that we're requiring here, readFile, will load the entire file into memory, which is what we want. The first parameter that this function takes is the name, so add a line, readFile, and give it the name. /data/pulitzer-circulation- data.csv. This is a file that's located in the data folder of our project. The second parameter that this function takes is the encoding. This tells Node how the file is encoded so it knows how to decode the file. For our csv file, it's simply utf-8, so we'll add that next. Remember, we started by saying that JavaScript is async by nature, and it needs callbacks to work, so that means this function also needs a callback, so let's add that now. And I'll shrink the file explorer a little bit so we can see the whole code. This callback is where we do our stuff, that is, this is where we take our data and do whatever it is that our application needs to do with that data. In our case, we're just going to convert the csv to js objects, and then put those in a console table. The convertCsv function is the function we required earlier, and it's going to return an array of js objects. Again, the course is going to focus mostly on the mechanics of opening a file and getting data from the file, and not the details of how to parse the csv, but if you wanted to you could dig into that csv.parse. Like I said, it's very rudimentary. But we now have an array that we can display on the console, and our function now has a filename, and encoding, and a callback. Next, go to your terminal. I'll be using the integrated terminal in VS Code. Run the command node. / asynchronous.read .js, and then hit Enter. As you scroll up to the top, you can see that here we have a table that shows the newspaper name, the circulation numbers from 2004, the circulation numbers from 2013, how many Pulitzer Prize finalists they had between 90 and 03, and how many they had between 04 and 14. If everything worked right, you'll see this table of results. It's the same table that we saw in the last clip. Of course, rarely does everything go just right. We'll see how to handle those cases next.

Handling Asynchronous Errors
When things fail with reading a file, that can be really frustrating, and the errors can be a little bit confusing, but after we walk through a few of these scenarios, you'll see they don't have to be perplexing. If you've been learning JavaScript recently, it's possible that you're not super familiar with callbacks. That can be forgiven. After all, these days a lot of teaching focuses either on promises or even Async Await. Since both of those are popular techniques, fewer people have experience with callbacks. One thing to know about the standard way Node handles callbacks is that the first parameter is the err, or error parameter. Look again at the code we added in the last clip. Here you can see the callback has err and data as the two parameters. This code works perfectly when there's no errors, but let's be honest, when does that ever actually happen. To see what happens if a nonexistent file is specified, change the filename from .csv to .cs, and then once again run the command node asynchronous.read .js. You're greeted with the error that's on the screen. It's everyone's favorite error, cannot read 'split' of undefined. Well that is an error, it's not really the error that started the whole thing. If you return to the code before the first line where vals is defined, add a new line. Here we're going to console.log out the error, and then we're going to return from the function. Now when you run the command again you get an error that says there was a problem with the file, and then it tells you that the problem was no such file or directory. This might not be any less frustrating, after all, why isn't the file there, but it is clearer than getting an error about the split function not existing. If you didn't get this error, and you still saw the stack trace, more than likely you forgot the return statement inside the if condition. Before continuing on, let's fix our filename so that we don't get that error again, so change it back to csv. Much like what you do after reading a file is up to the application, so to is what you do with the error information. In this example, we're just logging it out, but your application might want to retry or prompt the user for more information. Regardless of what you want to do, inside of an if block like this is where you would want to handle that. This error parameter in the callback will handle errors reading the file, however, there are a couple errors that you could experience that are not caught with this error parameter. The first one involves the encoding. In our function call, we specify that the encoding is utf8 to tell the function to expect a string, but if you delete that parameter so that the function is just a filename and a callback, and once again run the file, you'll see a new error. Specifically, that data.split is not a function. Since this didn't trip our error check, we know that Node found the file and was able to read it in, but still our code failed. If you were to add a temp line between the error check and the convertCsv to simply log out the data, and then run the command again, what you see is that the data that was read in is a buffer and not a string. If you don't specify the encoding for this function, it will read it into a buffer of bytes. Let me say that again. If you do not specify an encoding when reading a file, the system defaults to a buffer of individual bytes, and a buffer does not have a split function. This can be easily solved by always remembering to specify an encoding. So let's set that back to utf8 and delete our log of the data. There's one more error that is somewhat rare, but you still might see it. This time, instead of deleting the encoding, delete the callback so that the function call looks like this. And, again, run the command. This produces a pretty clear error. The solution is listed right there in the error. You must specify a callback, and it must be a function. Like I said, this will be a rare condition that you run into this error, but if you don't read files all that often and you tried something like const data = readFile, you will see this error. The solution is to make sure that you have a callback that takes both an err and a data property. Is the only way to fix that error to add a callback? What we'll cover next will show you how to read a file without a callback.

Reading a File Synchronously
Even though JavaScript is asynchronous at its core, there are still times that it's desirable to perform a synchronous action. That's particularly true when reading a file. A common use case is that you want to load data from a file and then do something with it, and until that data is loaded you do not want your application to continue. Thankfully, the file system library and node provides a way for that to happen. Start by adding a new file to the project named synchronous.read .js, and once again require the convertCsv function from the csv.parse file. Also, require a function from fs to read a file. This time, though, instead of readFile, require readFileSync. This illustrates the pattern that fs will use when naming functions. If a function is asynchronous, it will just simply be a name like readFile. If a function is synchronous, it will have a Sync appended at the end. The last clip finished trying to execute code that you see on the screen, and we received an error. That's because readFile is asynchronous, and we didn't specify a callback, but now we've required readFileSync, so change the function name from readFile to readFileSync, and then add a line to parse the data and log it out to a table. Now, go to your terminal and execute the command node. / synchronous.read .js. If you typed everything in correctly, you'll see the same table of Pulitzer data that you've seen throughout the course. However, if you typoed the filename so that it was .cs, you would see an error. This error might be frustrating, but it is pretty clear. There's no file with the name you specified. Your solution is to specify the name of a file that exists, but this is different than the asynchronous method earlier. That's because the asynchronous method called a callback with an error parameter, however, the synchronous methods in fs don't do that, instead they throw errors. In this case, the error would cause the system to crash. If you don't want your application to crash because of an error reading a file, you can wrap this code in a try catch block. When you try running this, you will still see an error description, but you won't see a stack trace on the console. That's because the catch handles the error, displays a message, and then allows the application to complete successfully. Once again, what you do with the error in the catch block is really up to you and your team, but I want you to know that you have options. At this point, you've seen how to asynchronously read files using a callback, and you've also seen how to avoid callbacks by reading files synchronously. However, this method will block all of your other code, so if the Pulitzer circulation data file was enormous, it's likely that your node application would sit at the readFileSync line for quite some time. Wouldn't it be great to have the best of both worlds that you could avoid callbacks while simultaneously not writing blocking JavaScript code? Up next I'll show you one simple way that you truly can have it all.

Read Asynchronously without Callbacks
Software development has a lot of trade-offs, and with the built-in fs functions, one trade-off you have is do you want asynchronous code, or do you want to avoid callbacks? Thanks to a helpful utility function, you can actually have both. Create a new file called promisified.read .js, and as with the other files, require the convertCsv function, and next require the fs module and assign it to a variable named fs. Finally, require the promisify function from the util library of Node.js. Note that if you run into errors with this function, make sure that you're using at least version 8.0 of Node, as that's when promisify was added. Now that you have those variables defined, the next step is to convert the readFile function to not require a callback, and you do this by passing that function to promisify. After this line, you have a function, readFile, that requires a promise instead of requiring a callback. There's a few things to note here. First of all, note that fs.readFile is not a string, it is the actual function from fs. Second, I chose the name readFile, but that might not be the best name, because in your code you probably don't want to confuse people as to why this particular readFile doesn't need a callback, but other ones in the code base do. And third, in order for promisify to work, the function that is getting promisified must have a callback that has the common err, then data parameters. If the order is different, you'll get unexpected results. Now it's time to use the new promisified function. As before, we're still going to pass in the filename, and the encoding, but we're not going to specify a callback. After all, that's what we're trying to avoid. What we will do then is add a .then function and a .catch function. The then function will handle the case in which the file is successfully read. The catch function will act like our if error condition from earlier. Head to the console once again and run node. / promisified.read .js. You will once again see the same familiar output of Pulitzer data, and now that the function is promisified, if you'd rather, you could use Async Await, running that one more time in the console, and you still get the same table data. This calls the readFile function inside of an async function, and awaits the results. So whether you'd rather use a callback, a promise, Async Await, or synchronously read a file, you now have options, and speaking of options, you don't always want to read an entire file. Sometimes you might just want part of a file. Coming up, we'll walk through the two functions you need to be able to read parts of a file.

Reading Parts of a File
Reading Parts of a File
Now that you've seen how to read an entire file, you can see it's pretty straightforward. You specify the path of the file that you want to read, and the encoding you want to use to decode it. You then receive the data of that file in memory, however, oftentimes you don't need to read an entire file. You might need to only read part of the file. Imagine a detailed log file of an application that's been running for quite some time. That log file would have a lot of data in it, thousands and thousands of lines detailing exactly what's going on with the application. Since that file is appended to, you don't need to open the entire file to know what's been going on recently. You might just need the last few lines. In fact, whether you're on Windows, Linux, or OS X, there's a command line tool that does just that. For example, on OS X and similar operating systems, there's a command named head that will show the first 10 lines of a file. Conversely, the command line named tail will show the last 10 lines of a file. And using Node you can create similar functionality. Once again we have our familiar demo of loading and parsing a CSV file of Pulitzer data related to newspaper circulation, but this time it's a little bit different. Rather than having dozens of rows, it only has four, and even then, the last row is not complete. You can see that the last two fields are undefined. Much like last time, this Node script opens a CSV file. It loads part of the contents into memory, in this case 200 bytes, and it then converts the CSV rows into JavaScript objects, and finally, it displays those records in the console, which you see here. In the previous module, there were a few options you needed to know to read an entire file, however, reading parts of a file is actually a little bit more complicated than reading an entire file. It requires at least two separate steps, and as many as five different parameters, and we'll cover both the steps and all the parameters next.

Opening a File to Read
When reading an entire file with readFile, Node takes care of opening a file and loading in the entire contents, however, when you want to read just part of a file, you have to break it down into multiple steps for Node. The flexibility of only reading parts of a file comes with increased complexity in your code. As in the past, start by creating a new file. Call this file asynchronous.read .partial .js. Since we're still reading in a csv, what's the first thing we need to require? That's right, the convertCsv function. Next we need to bring in two functions to manipulate the file. We'll pull those from the fs library. We need open and read. Now that we have our functions defined, the first thing we need to do is open the file. It's actually pretty similar to the API for readFile. The first parameter is the filename, and since this is an async function, the second parameter is a callback. Remember, standard callbacks for Node all take err as the first parameter, and this function is no different. The second parameter is the data returned from the process. In this case, fd is the common abbreviation used for file descriptor. When a file is opened on an operating system, the operating system tracks the open file and places an identifier to that file in a table of open files. What that identifier looks like varies from operating system to operating system. With OS X and similar operating systems, it's an integer. With Windows it's a 32-bit handle. Regardless of how it's stored in the kernel, the concept is the same in that there is an identifier that can be used to access the file. Since Node can run on multiple operating systems, the fs library abstracts away any of the differences and returns a numeric descriptor for each open file. Node then uses this file descriptor to look up the file from the operating system's file table, and the file table returns the desired file. Note that this is an oversimplified version of what actually happens between Node and the operating system, but it does illustrate the basics. Additionally, most operating systems have limits about the number of files that can be opened at any given time. So to wrap up the sidebar, a file descriptor can be thought of as a file identifier, and it typically is stored in a variable named fd, so if you see fd in the fs docs for Node, it's referring to a file descriptor. Returning to our code, we have an fd, it's the second parameter in our callback. The next step is to use that fd to read data from a file. You can probably guess the function we'll use. That's right, we'll use the read function which we required at the start of the file. The first parameter that read takes is the fd, so pass that in. Next, we need to give the read function a buffer that the file can be read into, so before that line where you're reading the file, create a new buffer. This function creates a new buffer and allocates it to be 200 bytes long. For this example, we're only going to read in 200 bytes at a time. If you wanted to do a different length, this would be the place that you would change. Now that the buffer is defined, let's add that to our read call. Next, the read function allows you to start inserting into the buffer at an offset. Note, this is not an offset for the file, but for the buffer. For example, if you set an offset of 5, the first byte read from the file would be stored in the fifth byte in the buffer, and bytes 1 through 4 would be 0. We don't want an offset, so we'll set that to 0. Now we need to tell read how many bytes we want to read from the file. For our demo, we want to fill up the entire buffer, so let's just use buffer.length. We still haven't told the function where to start reading in the file yet, and that's the next parameter. Let's have it start at the beginning of the file, so set that parameter to 0. So at this point we've told read what file to read from, where to store the data, where in our buffer to start storing that data, how much data to read in, and where in our file to start reading. The only thing missing is our actual data. Remember, from the last module, since this function is named read, and not readSync, it's asynchronous. That means the only thing missing at this point is a callback, and since it's a node callback the first parameter is an err parameter. After the err parameter, this callback takes two more parameters. The first is the count of the number of bytes that were read. Even though you specified 200 bytes using buffer.length, it's possible that the file doesn't even have 200 bytes, so it'll always tell you the number of bytes it actually read in the count parameter. The buff parameter is the data that was read. At this point, the only thing left to do is convert our csv to json, and log it out to a table. Jumping down to the terminal, you can run the command node. / asynchronous.read .partial .js. You'll see a familiar, albeit truncated, version of our Pulitzer data. This data represents the first 200 bytes of the file. There is one problem with this example. No matter what, it always starts at the first byte, and only reads 200 bytes. Up next, you'll see the changes you need to make to be able to chunk through a file.

Reading a Chunk at a Time
To be able to read through a file a chunk at a time, you need to know how big the file is so you know how many total bytes you'll need to read. To start, create a new file named asynchronous.read .chunk, and make sure you require the fs module. The first thing to do is to determine the size of the file. This can be done using the stat function from fs. First, initialize a variable totalSize and set it to 0. Then use fs.stat to get the total size. Note that there is a file change here. The csv file doesn't really lend itself to reading chunks at a time because the headers are only the first line, so instead, we'll be opening a log file. Also note that this is a dummy log file generated with random data, and not an actual file from an actual system. Now that we have our totalSize, the next thing we need to do is open the file the same as we did before. Now inside of the callback for your open function you need to read the file using fs.read, but this time put it in a loop that will read until all of the bytes are read in. And then make sure to log out the data. There are two points in particular to notice here. First, the second term in the loop is the totalSize divided by buffer.length. That is, it's the totalSize divided by the individual chunks that we're going to use. This gets us the total number of times that we need to loop through this routine. Second, the position parameter in read is now set to 200 bytes times the number of the iteration, so the first one will be 0, and then we'll start it by 200, and then we'll start it by 400, etc. If you go into your terminal and run the command node. / asynchronous.read .chunk .js, it'll provide you with a group of 200-byte chunks of data. There's two problems with this solution, though. First, it's not great that you have to keep track of how big your buffer is everywhere. You're probably managing more state than you really want to be. You're not just reading a file, but you're keeping track of where you are in the file. However, that's probably not the biggest problem. Look again at the output in your terminal, particularly the last couple of lines. Now, open the file data/ app.log and scroll all the way down to the bottom. Does the last line, line 1000, of your app.log file match the output of your terminal? If so, you're pretty lucky. As you can see, on my screen the last lines don't match. It cuts off the last line before reading it, and that's because we're executing read in a for loop, and read is asynchronous and there's no guarantee of the order that the functions will return. We'll fix both of these errors next.

Reading Parts Synchronously
By reading parts of a file synchronously, you'll be able to guarantee the data all comes back in the right order. As we've done so many times, create a new file and name this one synchronous.read .partial .js. Since we won't be parsing any CSV, we don't need the convertCsv function, but we do still need the fs library, so don't forget to add that. After you require the fs library, the first thing to do is open the file. Since we're talking about reading a file synchronously, let's open a file synchronously. That line opens the file. Can you take a guess at what it returns? If you said a file descriptor, you're correct. So let's assign that variable to the output of openSync. Now that we have our descriptor, we can pass that to the readSync function. A quick note here, you don't need to use readSync just because you used openSync. You could open a file in a synchronous manner but read it asynchronously, however, while it's possible, the best practice is to match either sync functions or async functions. And before getting to read, let's make sure to not forget to initialize a buffer to store the data. Let's use both the buffer and fd to read the data. Make sure here that you type readSync, and don't let your muscle memory kick in where you just type read. Note that this function, with the exception of the callback, takes the exact same parameters as the readSync function did, but since it doesn't take a callback, can you guess what it does return? You might think it was the file contents, and that would be a reasonable expectation, and while it's reasonable, unfortunately, it's wrong. If you remember back to read, its callback took two parameters besides the err parameter. It took a count and a buff parameter. ReadSync returns the first parameter or the number of bytes read. So how do we get access to the data that was read? The buffer that was created was updated through the process of readSync. So to get the data displayed on the screen, simply log out buffer.toString, jump down into your terminal and run node. / synchronous.read .partial .js, and we can see the expected results, the first 200 bytes of our log file. This solves the first problem from the previous clip, the file is now read synchronously, but how can we use this to chunk through an entire file? The first thing that we need to do is create a variable to hold the number of bytes read. Let's put that between the fd and buffer variables. Make sure to use let, not const here, because the count will be reassigned. In fact, let's go do that now by assigning count to whatever the output of readSync is. Then we're going to use a rarely used loop, and that's the do while loop. So above buffer add a statement do with a curly brace, and take the closing curly brace after the console.log. Then make one more change. On read, change the last parameter from 0 to null. There's two things to note here. First, the buffer is created inside the do statement. That's intentional. We want to re-initialize this buffer with all zeroes every single time we read it, otherwise when we get to the end there might be some leftover data. Second, the position parameter for readSync has changed from 0 to null. This is a small, but significant change. If the value is null, node will actually keep track of where it was in the file and pick up there the next time you try to read. Now for our while condition. Set it to loop as long as any number of bytes have been read. Go back down to your terminal and run the command again, and this time you'll see the contents of the entire file dumped out to the console. If you don't see that, and instead see the same section written over and over and over in a never-ending loop, it's most likely that you did not change the position from 0 to null, and so you just kept reading the first 200 bytes starting at position 0. But assuming it worked for you, you've now got a way to methodically chunk through a file. At this point, you've seen how to read parts of a file asynchronously and synchronously, and you've also learned how to travel chunk by chunk through the file. However, right now there's a ticking time bomb in your code, a bomb that'll go off at the worst opportune time. We'll cover that time bomb and how to stop it, coming up next.

Importance of Closing
Do you remember earlier in this module that we talked about file descriptors? In the sidebar conversation, we talked about how they are essentially pointers to a table of open files. Since then, we've been using fd to access data from a file, and our fd was rather innocuous. However, file descriptors have the ability to crash your application, but don't worry, the prevention is actually pretty simple. Let's start by looking at what the file descriptors can do. In your terminal, run the command node. / file.descriptor .error. This runs and then crashes. It tells you that it's attempting to open the files, and then it gives you an error, EMFILE: too many open files. What's causing that? Well, in your IDE if you open up the file, file.descriptor .error, you can see when you're looking at the code that inside of a loop the same file is opened 50000 times. This code eventually crashes and throws the error that you saw on the screen. But when does it crash? Let's modify it some to find out. Inside of the for loop after the file is opened, log out that fd. Now, go back to your terminal and run the command again. In this case, the terminal output the file descriptor each time a new file was opened before it eventually crashed. The number might be different depending on your OS and some other settings, but on my computer it crashed after the fd was assigned 24575. Back when we talked about file descriptors, I mentioned this line, most operating systems have limits about the number of files that can be open at any given time. This is the error that we're seeing. We're asking the OS to open more files than it allows. True, the code that you see is a contrived example. Why would you ever open 50000 files in a loop? But notice that the error isn't that the operating system limits the number of files that your process can open. Let me say that in a different way. The operating system is limiting the total number of open files, including files opened by other applications, so your code needs to be a good community member and not contribute needlessly to that number. Returning to the code, do you have any idea how we can prevent this error from happening? If you close this file, this error will not happen, so after the console.log statement, add a call to clsoeSync, and make sure you pass in the fd. Before running this, though, take a guess at what the last number will be displayed when the script runs. Do you have your number? Go back to the terminal and run the command again. Did you get the number right? The first time I ever tried this, I did not get that number right. I assumed it would be somewhere around 50000, but as you can see on my computer, it reused the same file descriptor. In my case, that's 22. When you close the file, it frees up that file descriptor from the table so it can be reused. In fact, if you were to use fs.close instead of fs.closeSync, you would see that fd fluctuates between at least two numbers and possibly more. So changing it from closeSync to close, passing in the fd, and then we're just going to provide an empty callback because we don't want to do anything with that, and we'll run it one more time, and here in my output you can see it's fluctuating between a descriptor of 22 and a descriptor of 23. Remember, any time that you use open or openSync, you need to call the corresponding close or closeSync. You might be asking, why didn't we talk about this when we looked at readFile or readFileSync? And you might be thinking that those functions automatically close the file after they're done reading, and that's not a bad thought, but it's only partially correct. If you remember back to the previous module, what was the first parameter that we passed to readFile? It was a string, or a path to the file that we wanted, for example, data.csv. In our case, it was the Pulitzer circulation file. In this situation, readFile will close the file for us because we're providing it a path to the file. However, if we tweak that code slightly by passing in a file descriptor so that it looks like what you see on the screen now, readFile will not close that file for us, because it will not close a file descriptor, and the code that's on the screen is a perfectly valid use of readFile, it doesn't have to take a string, it can take a file descriptor. So how do you keep all of that straight? Any time that you have a file descriptor, you are responsible for closing that file. If you don't, you're exposing yourself to the risk of opening too many files and crashing your application. By using openSync and readSync, you not only know how to read parts of a file, but you've actually got a head start on writing files. However, there's still some slight differences, and maybe even a couple gotchas. Next, we'll see how to make the correct decision so that you avoid those gotchas.

Writing to a File
Writing to a File
At this point, you've covered roughly half of what you can do with a file. You've seen how to read an entire file or just read parts of a file. The next step is to apply those concepts to writing a file. And since you're using Node, one situation you might run into is needing to export your source code so that other developers can use it. These exported libraries can be installed by others using npm, as you're no doubt familiar with, but before they can be installed by npm, they need to be created by you. In some cases, creating the library is as simple as exporting a single file to a package sharing a single file, but often it's much more difficult than that. On a recent project that I was on, we had modules nested several levels deep, and each of these modules had several different functions that could be accessed. In the end, we created a script that was part of our build process that would iterate through all of our folders and grab the correct files and functions and place them in a single index.js file that could be used by other projects. One thing that script had to do was write index.js, and if it didn't exist, it needed to be created. If it did exist, we want it to just overwrite what we had with our new version. Jumping over to the terminal, you can actually see this in action. The command on the screen, node. / create.index .js, this command runs a file named create.index .js, and as it runs it logs out to the console each file it's working on. For example, you can see Adding file: asynchronous.read .chunk .js and Adding file: asynchronous.read .js, but what does it mean when it says Adding file? Take a look at this index.js file that it created. This file is importing or requiring each of those files, and just like you saw in the terminal, you can see that it requires asynchronous.read .chunk. It then assigns that to module.exports with a name asynchronous.read .chunk. If someone were to install this package with npm, they would receive this index file and have access to all of the read functions that you wrote earlier in the course. Now granted, it's not a very useful npm package, if for no other reason than it only opens a couple specific files that you are referencing in the code, but this is an example of when writing a file using node can be useful. Before we get to this exercise, though, we need to cover a few key concepts. For example, how do you ensure that you only overwrite a file on purpose? We'll cover that first.

Writing an Entire File
Before we start creating an index.js file, let's take a minute or two to learn about how to write a file in general, and some of the gotchas that can happen. In your source code, add a new file and name this one asynchronous.write. Since we're not importing anything, and we won't be parsing CSV, we don't need to worry about that csv.parse module again for this module, but we do need to make sure that we require the correct functions from fs. Much like we did when we started reading files, let's start by writing an entire file, so require writeFile from fs. Let's start by adding to the log file that we saw in the last module, app.log. We do this with writeFile, and similar to its readFile counterpart, the first parameter is the filename. A quick sidebar, you've already seen in this course the difference between readFile and readFileSync. There is a sync counterpart to writeFile named writeFileSync, and it performs similarly to readFileSync. For this module, we'll be using writeFile, but know that if you prefer, there is a writeFileSync function that you could use as well. The next parameter that we need to pass in is the data that we want to write to the file. To get an example of the type of data that's already in that file, open up data/ app.log and scroll to the bottom. Let's copy this last line as an example, and then head back to asynchronous.write. Put the line that you just copied in as the second parameter. Let's make a quick change, just so we can be sure that it's our line that's getting written to the file. For example, let's change the URL it tried to access from /dot-com to /write-file-test. What would happen if we ran this file just as it is? It would actually error out. Remember, this is an async function, and all async functions in fs need to have a callback. If they don't, they'll throw that invalid callback error that we saw earlier in the course. So let's add our third parameter as a callback. This callback only has one parameter, and that's the err parameter, so our callback function will be pretty straightforward. If there's an error, let's display that on the screen, and if there's not an error, let's let the user know that the file was saved. We now have our writeFile with its three required parameters of name, data, and callback. We're ready to run the code. So go to your terminal and execute the command node. / asynchronous.write .js. You should see the message "file saved! " in your console. So let's open up our app.log and take a look to verify that our data actually was added. When you open the file, your line has been put into the file, but it's not exactly a successful run because it is the only line in that file. Before your script ran, there were 1, 000 items in the log file, but don't worry, since you cloned the repo with git, you can just check out the changes to that file and restore the log. Now, can we make sure that we add a line to that file and not replace the file with a new line? Sure, we'll cover that next.

Appending to a File
In order to append data to the file, instead of just overwriting the data, we'll need to make a small but subtle change. Start by creating a new file and naming it asynchronous.append .js. Just like last time, require the writeFile function. Since we're using the exact function that we used in the last clip, we know that the function itself is not the culprit for why the file was overwritten, so let's start with the code that we had from the last clip. We once again have our three required parameters. First we have the path to the file we want to write, second we have the contents of the data that we want to write, and third we have the callback. There is an optional fourth parameter that we've not used, and that is the options parameter. It comes between the data and the callback. The option we care about is the flag option. This is a string, and it tells the writeFile function how it needs to open the file. The default flag is w, which simply means overwrite the file, create it if it doesn't exist. We don't want the default, so we need to pass in an object as the third parameter. And inside this object, we need to specify the flag parameter and set it to the string 'a', for append. Go to your terminal and execute the command node. / asynchronous.append .js. Once again, when the terminal shows "file saved! ", open up that app.log file. This time, as you scroll to the bottom of the file you should see 1001 lines, and the last line should be the line that you just added. Returning to the .append .js file, you have another option for appending to this file. Back on line 1, update the function that is being required. Instead of writeFile, change the name to appendFile. Then on line 3 rename the function that's called from writeFile to appendFile. Finally, remove the options object. Much like the writeFile function defaults the flag to w, the append file defaults the flag to a, so if you run that command one more time, when you see the file saved in the console, you can again open the app.log file. This time you should see that line added a second time, which we see here. Even though appendFile exists, it's still important to know about the options object, because there are other flags that do not have corresponding functions defined. Additionally, there are other options available for when you write a file. If you wanted to know how to write a Base64 file but throw an exception if the file already exists, as an example, you can see how you could do that with the options object, next.

Utilizing Options
Up until this point in the course, the reads and writes have been pretty straightforward, and they haven't involved many options beyond the basic synchronous versus asynchronous cases, of course, and if all you're doing is overwriting an existing text file with another text file, that's probably fine, however, sometimes you need more power and flexibility. This time, instead of creating a new file, return to the asynchronous.write .js file. This file executed code that would overwrite our app.log file with our new log line. That's because it was using the default flag of w. So let's start by changing that. Add a third parameter here which can be our options object, and have that have the flag parameter that we want to set to wx. In Node, the x flag is the exclusive flag, and it can be added to other flags such as w, which will cause the system to throw an error if the file already exists. To see this at work, return to your terminal and execute node. / asynchronous.write .js. When you run this, you get the exception you see on the screen. It tells you the file already exists, and it tells you the path of the file that you are trying to create so you can verify that the path does contain that file. Using x like this can be quite helpful if you want to create a new file but you do not want to modify an existing file. Let's take a look at what options are available to you for the flag parameter. There's three main flags. These are the r flag for read mode, the w flag for write mode, and the a flag for append mode. There's also three flags that we can call the add-on flags that can be added to any of the main flags. X, as you've already seen, is for exclusive. The + is used to open a file in multiple modes, for example, r+ will open the file for both reading and writing. W+ will also open the file for both reading and writing. The difference is that if the file does not exist, r+ will throw an exception and w+ will create the file. A+, much like w+, will open a file for both reading and appending, and it'll create the file if it does not exist. S is for synchronous, for example, rs+ will open a file for reading and writing synchronously. Note in this case the s has to do with file I/O and not the JavaScript function. That is, including the s flag will not convert open to openSync. It's also important to note that not all combinations of these work. There's three allowed combinations for read. They are r, for read mode, r+ for read and write mode, and rs+ for read and write mode synchronously. There are four allowed combinations for write. W will open the file in write mode; wx, as you've seen, will open the file in a write exclusive mode, throwing an error if the file already exists; w+ will open the file in both red and write mode; and wx+ will open a file in both read and write mode, and throw an error if the file already exists. And the allowed combinations for append are a for append mode, ax for exclusive append mode, a+ for append and read mode, ax+ for an exclusive append and read mode, as for appending synchronously, and as+ for appending and reading synchronously. Let's go back to the IDE. You have more control than just how a file is opened. You can also set OS level permissions on the file. Staying in this asynchronous.write .js file, let's make a few adjustments. Start by adding the constants variable in the require statement. Next, change the name from app.log to newapp.log. Then get rid of the flag parameter inside the options object, and this time add a mode parameter with a value of constants.s_iwusr, the pipe, and constants.s_irusr. In your terminal, run the command node. / asynchronous.write .js one more time. This will create the file newapp.log with the permissions that only the owner can read or write the file. Depending on your operating system, there's various ways to show this. In OS X, which is what I'm using, I can simply execute the command ls -l. /data, and see that the newapp.log file has the permissions rw all the way to the left. That means that the owner, which is the set of permissions all the way on the left, has read and write, and nothing else has any permissions to that file. If you're familiar with the change mode command on Unix-style operating systems, you can use those values as well. That is, this command could be written with the value 0o600. This would accomplish the same thing. It would create a file with a a read and write permission for the user, and no permissions for either the other users or any users in the group. There are a handful of constants that you can use for these file permissions. This table shows that you can use either the fs constants on the left-hand side, or their equivalent octal values in the middle column. Regardless of which one you use, you have the power to establish what permissions your file needs. If you choose to not specify permissions at all, the default value will be that each user, group, and other will have read and write permissions. Let's hop back to the IDE for one more thing. In addition to options for the mode that the file is opened in like read or append and the permissions of the file which you just saw, you have the ability to specify the encoding that the write operation does as well. Let's stay with the asynchronous.write .js file. Let's delete the mode parameter from the options object, and add an encoding parameter and give it the value of base64. We can save that file and run the node. / asynchronous.write .js command again. And as we would expect, we see the "file saved! " message one more time. So if we open up the newapp.log file we see a lot of gibberish on the screen. That's the base64 encoded version of the string that you sent to the file. In addition to utf8, which is what we've been using for most of the course, and base64, which you see on the screen here, Node supports a few other encodings. It supports basic ASCII encoding, it supports utf16le or ucs2, which both refer to 2 or 4 byte little-endian encoded Unicode characters; it supports latin1 or binary, which are both 1 byte encoded strings as defined by the IANA in RFC 134; and it supports a hex encoding, which encodes each byte as two hexadecimal characters. With all of the writeFile work that we've been doing, we've had the entire new contents of the file loaded up in memory before we wrote them out to the file. Even though that's only been one line in our examples, we probably don't always want to do that. In fact, to create our index.js example, we need to not have that happen. So how do we write out the bits and pieces to a file? We'll see that next.

Writing Parts of a File
In previous modules, you've seen how you can read parts of a file. You often need to do the same thing for writing a file. Let's talk briefly about the requirements of what we're going to do when we build our index file. We'll open up a file index.js, and this'll be the file that we're writing to. Then we'll get a list of files that we want to add in as exports to this file. We want to iterate over our list of files and create an export statement that exports a function with the same name as the file. We want to write to that file, and then we want to close the file. Remember, whenever we open a file we need to make sure that we close it. On that list, we've already done most of that in the past. The new items will be number 2, getting a list of files, and number 4, writing to a file. Number 1 and 5 you've already seen in this course, and number 3 is just a normal loop or map statement that you've probably done thousands of times in your career, but when we put them all together, we'll have a feature that we can use to build an index file. In your IDE, create a new file named create.index .js. We need to require four separate functions from fs. Since we're opening and closing, we need to include those. I prefer the sync method, so make sure you have the openSync and closeSync functions. And since we're writing out we'll need to add writeSync, which is similar to readSync, which we'll use to write parts of a file. And finally, we will need the readdirSync to read all the files in the directory. Now that we have our fs functions included, let's start with what we actually know. We know how to open a file, so let's add that line. Notice the 'w' here. if we don't specify that, the file will be opened in read mode and will throw an exception when we try to write it. And remember, any time we open a file, well, we have to make sure that we close it, so let's add a close statement at the bottom of the file. We don't yet know how to get a list of files, but we're not going to let that slow us down, so for starters create an empty array named files. Next we want to iterate or map over those files and perform an action on each one. What action do we want to take here? Well, we want to write a line inside of index.js for each file that we have, so we need to add a call to writeSync and make sure to pass in the file descriptor indexFd as the first parameter. Now we need to actually write out the string that we want. In our case, we want our string to look something like module.exports .synchronousRead = require(' synchronous.read ') .read. So let's use a templated string for our second parameter. A couple of things to notice, when we did readSync we had to specify the position in the file that we wanted to read. We have that option on writeSync, that is, we could tell it to write at a specific byte location, but for our purposes we want it to be at the current location, which is at the end of the file. Second, notice the function named camelCase. That's a node library that'll convert dot and dash strings to camelCased strings. It's included in the project already, so if you haven't done so yet, make sure you run npm install. Then make sure that you require the library at the top of the file. Next, since we're referencing a variable, in this case, name, that doesn't yet exist, so before the writeSync line add this line. That will take the name of the file and strip off its extension. Then, so we know what file we're working with, let's log out a statement to tell us that. At this point, we have code that will open a file to write in, loop over a list of files, and add those files as export statements to our file, and then ultimately close our file. But we don't yet know how to get that list of files. Before we get to that, let's make two changes to our project. First, create a new folder named read, and move all of the asynchronous and synchronous read files into that folder. This will let us try out our index file on only the read files, and it helps keep out utility functions and other files like package.json. Second, we need to convert those files to have their own module exports. Right now it's just blocks of code, and if we tried to require those, then it would actually execute the code as we're requiring it, so for this one we need to cheat a bit, and we'll have each file export the same function name, read. So, let's open each of those six files that we just moved, and at the top of the file we'll add a line, module.exports .read =, and then we'll use a fat arrow function to wrap our code, and we'll come down and we'll close that with the closing curly brace. So we actually need to do this on each of those six files. Once that's done, return to the create.index .js file. The only function that we have required but haven't used yet is that readdirSync function, so it's a good bet that that's what we need to get the files out of a directory. ReaddirSync takes the directory name as a string of the directory that you want to include files from. So let's set the output of that function to our files variable. This will find all of the contents in that folder. Since our folder, read, only contains six JavaScript files, we don't need to worry about filtering the contents at all. If this was done at the root of the project, for example, we'd want to make sure that we didn't include directories or files like package.json or even our .gitignore file, so in that case we'd have to apply a filter to the files array, but in our case, we're all set. Pausing before we execute the code, let's look at what our file does. It opens a file for writing, it then gets a list of the files that we want to export, it loops over those files, it writes a new line for each file, and it closes the file. These are the exact same requirements that we started the clip with, so let's head to the terminal and run our command node. / create.index .js. If everything ran successfully, you'll see it output each filename to the console and add a new index.js file to the directory. Opening that file, you see that it exports a series of functions for reading files. If it did not go smoothly, you likely ran into one of three errors. One, you might not yet have run npm install, and so that camelCase function couldn't be found. Two, you might not have moved your files to the folder named read, and so the files couldn't be found with the code as it is. Or three, you might have had an error on the module.exports of your files. Hunt back a couple minutes in the clip and make sure those read files have properly defined module.exports. You've now got a script that can output all of your hard work on reading files to a single index.js file, and as great as that is, what happens if you find a typo in one of your read files, or what if you add a new file to that folder? Is there some way to have your index file auto-update? Well, I've got good news, there is, and the next clip will show you how simple that really is.

Watching Files
To make your create.index file truly versatile, it would be great to let that script run and detect changes to the directory and files underneath that directory. Thankfully, Node's fs module provides this exact functionality. Return to your create.index file. The first thing to do is bring in a new function from fs, so add the watch function to the list of functions that are being required from fs. Next, before the first line of code invoke that watch function. The first parameter that the function takes is the path that is to be watched. This path can be either a single file or a directory. In our case, we've specified the entire read directory. The second parameter is an event listener. I want you to note that technically this is not a callback, instead it's a listener or handler. It's called every time the watch event fires. One of the main differences is that there's no err parameter. This listener can take two parameters. The first one is the event type, which would be either a change or rename, and the second one is the filename that was changed or renamed. However, for what we're doing, we don't actually care about the event type, nor the filename that changed, because all we're going to do is rewrite our entire file, so we'll add our listener with no parameters. And what we want to do is wrap the current code that we wrote in that last clip inside the listener. With those changes, let's hop in the terminal and let's rerun our node. / create.index .js command. When you do, nothing is output to the window, but also note that the process hasn't ended either, it's just sitting there. So open up one of the read files and add an empty line and save the file again. You'll now see the list of files scroll by on the screen and the index has been updated. Similarly, if you rename asynchronous.read .js to asynchronous.read .bak .js, the process will run again, and as you look in the index.js file, you can now see that it's referring to the asynchronous.read .bak file instead of asynchronous.read .js. Or you can add a new file. Let's call this one simply temp.js, and let's make it essentially a no-op function, so we're going to have an empty fat arrow function. If you save this file again, you'll again see the list of files run by, and as you go back to your index.js, you can see that module.exports .temp now exists. Now that you can read and write files, you can handle a lot of various cases of importing data or saving data out to a file. But what if that file that you want to read is too large to contain in memory? Next up we'll look at a couple of functions that Node provides to allow you to stream data without needing to read it all at once.

Reading and Writing to Streams
Reading and Writing Streams
Reading small files does not have much of an impact on system resources. We've seen that throughout the course as we've been able to read and parse files with relative ease. However, the biggest file we've worked with has been around 90kB. What happens if we need to read in a considerably larger file? To find out how large files perform, I used an open source tool named Flog, which creates random log files. I created a log file with 10 million lines. Using code similar to what we've already seen, we can time how long it would take to read that file in synchronously. From the output, you can see that it took approximately 1 second to read the file. That might not be horrible, after all that file is 900mB, but keep in mind at this point there's no processing of the file. It took approximately 1 second to open the file and read in the contents before it could even start processing. For comparison, the same operation for our CSV file took 2ms, and the app.log file we worked with earlier in the course took 7ms. However, there might actually be another problem. If we try this same operation on a much larger file, we can get even worse results. This time, we don't see a time logged onto the console. Instead, we get an error, and the error is that it cannot create a string longer than a certain amount of characters. The error is telling us that Node tried to read in this file and assign it to a string, and in doing so it exceeded the max characters a string can have. In this case, that's roughly 1 million characters. I mentioned this was a much larger file. It's actually double the size, or 1.8GB, which is a pretty large file to be fair. What's going on with reading these files? Why are we seeing slow load times or even crashes with large files? Well remember that earlier in the course we mentioned that when we call readFile it's opening the file and loading all of its contents, and in our case we're assigning that to a variable. This means the entire contents of your file are being placed in memory before being accessible to your code. This is relatively quick, but it does eat up a lot of system resources. But there's good news. You only have to make a small step to apply what you already know about reading files to be able to read a stream. In fact, to get started you only need one function and one event, and I'll show you those next.

Reading Data from a Stream
Handling a stream is similar to reading a file, but there's at least one noticeable difference. With streams you don't read the stream with a function, instead, you create a stream and receive data via an event. As we've done so many times throughout the course, create a new file and name it read.stream. We're going to stream our log file, so we don't need to import our csv.parse file again, but we do need a function from fs. Import the createReadStream function. Now we want to use this function to create a new stream. As with a lot of the fs functions we've been dealing with like readFile, the first parameter to createReadStream is the path to the stream. In our case, that's. /data/ app.log. Whenever a stream receives more data, it fires off a data event. We need to make sure that we can handle that event. Go to your terminal and execute the following command, node. / read.stream .js. When you press Enter, you'll see two lines displayed on the console. It's important to note here that there are two separate buffers. The first one shows a buffer and states that there's an additional 65486 bytes left. The second buffer is similar, but it only has an additional 29428 bytes. Why are there two buffers? And why are their sizes different? The answer gives us some insight into how the stream works. Both buffers display 50 bytes on the screen. With the first buffer, if you add those 50 bytes to 65486, you see that the first buffer has 65536 bytes, or 64kB. This happens because the default size for a stream is 64kB. So what we're seeing on the screen is that our data event was actually called twice. The first time it had 64kB of data. The second time it had the remainder of the file, because the file that we're loading is less than 128kB. If our file had been, say, 129kB, the second buffer would have also been 64kB, and the last buffer would have only been 1kB. While 64kB is the default, you do have the ability to change that. Return to your code, specifically to line 3 where you create your stream. This function takes an optional second parameter. At this point, it's probably not surprising to know that that second parameter is an options object. Add that object to the code. Now we need to tell the stream the number of bytes to take at one time. This is called the highWaterMark. It's the maximum number of bytes that the stream will read at one point. For our demo, set that to 9550. Now go back to your terminal and run the command one more time. This time you should see 10 buffers displayed on your console, and the last one should have a remainder of 9014 bytes. Using the highWaterMark gives you control over the number of bytes you want to handle at one time. If you're fine-tuning your application, this is an area to investigate, but for the most part, 64kB will probably be what works for you. That said for demonstration purposes, let's leave it at 9550. The next thing we want to handle is actually seeing the content of our stream, and not just bytes. We can return again to the options object to solve this problem. A second option that we can specify is the encoding. Set that to utf8 to tell the stream that we want text, and not just a stream of bytes. Returning to the terminal one more time, run the script and you'll see the text of our log file. In fact, it looks very similar to seeing the output of files when we used readFile. It's great that we can display the file like this, but sometimes you need the stream to pause for a bit so that you can deal with the data that you're receiving. Thankfully, the API supports that exact functionality, and next we'll look at how we can work with chunks of data at one time.

Pausing a Stream
Streaming can be a great way to handle large files or events that send a lot of data. However, sometimes the stream of data can move faster than your code can handle. How can you manage the data in those situations? Return again to your read.stream file from the last clip. This file has the on data event that it's using to show the contents of the data that came back. In our case, that's the app.log file, and we saw in the last clip that our current configuration is going to fire this event 10 times. The first thing we want to do is tell the stream to temporarily stop sending data. We do that by adding a line before the console.log statement inside our stream.on function. We call this function first, much like you would normally call preventDefault on the client side before doing anything else. We want to make sure that while we're processing this event we're not receiving an additional event, so calling pause first solves this for that. From your terminal, execute the command node. / read.stream .js. As before, data streams across your screen, but this time you can tell it didn't send the entire file, as the last line printed is an IP address with no additional data, it's simply 183.131 .46 .199. However, the script just stops there and returns you back to your terminal. That's because the stream isn't producing data anymore, and there's no other code that the script needs to run, so simply pausing the data isn't enough, because that will only allow you to access the data up to the point that you pause. As you might imagine, there's also a resume function. To see how this works, add some code after the console.log statement. Here we're using stream.resume, but we're simulating that our code is doing something before we resume, so we're using setTimeout with a timeout of 2 seconds. This code will run every 2 seconds and resume the stream. Let's try this out, so go back to your terminal and rerun that command. When you hit Enter, you'll see the first block of 9550 bytes come across. You can even see that the last line is incomplete. Then after 2 seconds you can see the next block of code. If you let this run for about 20 seconds, you'll eventually see the entire file again. Part of the power of streams is that you can have input or readStreams, as well as output or writeStreams. This allows you the ability to bring in data from one method, process that data, and send it out somewhere else, and you can see how you'll do all of that with just two simple functions, next.

Writing to a Stream
Input streams are not the only way to work with streams. Sometimes you want to be able to stream data out, for example, to save it as a file. Start by creating a file in your IDE named write.stream. At the top of the file, import the createReadStream, as well as the createWriteStream functions from fs. Then define a stream similar to how you did in your read.stream file. Note the difference here. I changed the highWaterMark from 9550 to just 95. This allows us to simulate a longer running stream. In reality, you would probably want this to be the default 64k, but for demonstration purposes the smaller number is better here. Next, create a writeStream. Pass in a filename as the destination of the stream. We again need our on data event. As we did before, let's call stream.pause so that we get a better glimpse of how all this works, and the data doesn't just fly by. And don't forget to add your stream.resume so that the code will resume every 1 second. Now we need to do something inside this callback besides pause and resume our stream. Since we're focusing on writing, and not on reading, we are not going to log this out. Instead, let's log out a counter just so that we can see that it's still working. So before the line stream.on, create a counter, and then inside your on data event, log out the iteration like this after your stream.pause call. Finally, after this, let's write out our data to the destination stream. We can now go into the terminal and run the line node. / write.stream .js. As it runs, you'll start to see the numbers printed out to your console every second. If you open up the data/ output.log file, you can see some familiar log entries. Depending on your editor, it might auto-update this output.log file for you as the stream writes out to it. If, however, your editor does not auto-reload the file, you can close the file and reopen it, and you'll see the new changes. As with createReadStream, the createWriteStream function also takes some optional parameters. The two most common will still be the flags and the encoding, and these are the same as the options you used when you were calling writeFile, but other than that, there isn't actually a lot to writing out to a stream, or at least there's less to configure than when reading from a stream. In fact, throughout this entire course you've already been writing to a stream. Internally, console.log calls a stream to print data to the console. You can see that the process.stdout uses the familiar write function to send the data. That's because process.stdout is a stream. There's one challenge left though, and that is, what happens when your output stream is slower than your input stream? Up next, we'll look at problems that can cause and how Node provides a built-in method to handle the problem.

Handling Mismatched Streams
The last clip showed you how to take data from one stream and send it to another stream. In our case, that was reading in from one file and writing out to another. Sometimes that action can cause problems. So far our setup has been pretty straightforward. We've had an input stream, in our case a file that we were reading from, and we had an output stream, in our case a file that we were writing to. As long as the output stream could handle the data that the input stream is providing, everything is good. Data comes in, is manipulated or processed, and sent out without a serious burden on the system resources. Even if the output stream is faster than the input stream, there's still no problems. In that case, the output stream is idle waiting on the next chunk of data from the input stream. Unfortunately, that doesn't always happen. For example, writing a file is a slower operation than reading a file, so even in our examples, it's possible that the output stream would start to build up a backlog of data that it needs to write out, and the bigger the difference between the input and the processing output, the bigger that backlog is going to be. This problem is called back pressure. Essentially, it's a backup of data caused by streams being unable to process data before the next batch arrives. A quick sidebar here, the term back pressure comes from the world of plumbing. There it refers to a resistance or force opposing the desired flow of fluid through pipes. With fluids, it results in a pressure drop. There are some parallels to software. Instead of pipes, we have streams, and instead of fluids, we have data. If this isn't handled correctly, a memory issue can occur as the project's memory footprint grows bigger and bigger. Unfortunately, this was part of the problem that we wanted to solve with streams, we didn't want to keep increasing our memory footprint, so we have to figure out a way to not let that happen. Unlike previous examples, I've already written the code for this example, and it'll be using the large file I used at the start of the module. I did not include this file in the code, because I didn't want to force you to download a 2GB file just for one demo. Additionally, it's way too big to store on GitHub anyway. You can either create a large file of your own, or simply watch the demo in this case. To see the code, open the file mismatched.stream .js. Most of this code is fairly similar to what you just worked on. There are a few differences, though. First, the file that is being streamed is not app.log, but instead is a file named stream.log. Second, I removed the highWaterMark, which means it'll load 64kB in each chunk. Third, the data on event is not pausing the stream this time. We're just going to allow it to read as fast as it possibly can. And fourth, there's now a function called writeData. This new function writes data out to the stream on a timeout, in this case 60 seconds. Remember, most of the time you're not going to want to set a timeout in your write code. We're doing this only so we can simulate a write stream that is much slower than the read stream. When I go into the terminal, I'll run the file node. / mismatched.stream .js, and I'm going to expand the terminal so that we can see the output. The counter will go to around 20000, and then an exception is thrown. As you look at the exception, the cause is stated as JavaScript heap is out of memory. That's because it's loaded close to 1.8GB of data from the file. And more than that, while it's loaded in the data there hasn't been any data written out. If there had, it would have helped alleviate some of that back pressure. Thankfully, Node has anticipated this and has created a function to help. Returning to the code, we can replace everything below line 7 with one line. When I go back into the terminal and run the command one more time, we don't have anything output to the console, but the snippet runs quickly and returns to the shell. It was able to copy that nearly 2GB file without loading it all into memory, and without running into a memory exception.

Summary
And now, just like that, you're equipped to go out and tackle file manipulation in Node.js. Let's quickly review what you've learned. First, you learned how to read an entire file into memory and process the data as your application sees fit. Then you learned how to manage the entire lifecycle of a file by opening it, reading parts of it, and closing it. The extra steps give you more control than simply calling readFile. Then you learned how to write out to a file. It was in this module that you created a watcher to monitor files and update an index file if any of your files changed. And finally, you learned how you can read from and write out to streams, allowing you to process files that are too large to open in their entirety. And at least with reading and writing, you learned how to do all of that either asynchronously or synchronously. Finally, thank you for taking time to watch this course. I really enjoyed working through the fs module, and I hope that you found it helpful. I'd love to hear from you. You can reach out to me on Twitter at the address on the screen.
