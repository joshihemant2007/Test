Packaging, distributing, and running software applications in containers is no longer a pastime just for early adopters. Containers are mainstream, and with that comes a concern about the security and integrity of containers as an application delivery mechanism. In this course, Securing Docker Container Workloads, you'll learn how to secure your application workloads from the perspective of the container itself. First, you'll learn about the Linux security mechanisms that go together to create the abstract concept of the container, and how they work together to ensure that containers are good neighbors. Next, you'll explore the privileges that are available to container workloads, and how you can adopt and apply the principle of least privilege to reduce the risk of privilege escalation. Finally, you'll see how to minimize the attack surface available from within a container by limiting the access it has to the kernel and other system objects. By the end of this course, you'll be equipped with the knowledge and techniques necessary for securing your Docker container workloads.

Course Overview
Hi everybody. I'm Nigel Brown, and welcome to my course, Securing Docker Container Workloads. In some corners of the IT world, containers are viewed with some skepticism when it comes to security. This skepticism is unwarranted, and if you take some time to apply some standard security mechanisms, you can make your container workloads pretty hard to compromise. This course is all about applying Linux security mechanisms from the perspective of container application workloads. In this course, the main topics that we'll cover include the use of namespaces and control groups, which provide isolation, applying the principle of least privilege in order to better protect our containers, reducing the attack surface available to container workloads, and implementing controls to limit access to system objects. We're not going to turn you into a Linux security expert, but by the end of the course, you'll have a thorough understanding of the security mechanisms at your disposal for making your container application workloads more secure. Before beginning the course, you should have some practical experience of Linux and some familiarity with the Docker platform and its commandline interface. If you've ever wondered whether it's safe to run your applications in Docker containers, or what it takes to make them as secure as possible, then join me to discover how to secure your Docker container workloads.

Isolating Container Workloads with Linux Namespaces
Course Introduction
Hello, and welcome to this Pluralsight course, Securing Docker Container Workloads. My name is Nigel Brown, and I'm your instructor for this course. As you're here, no doubt you have an interest in Docker containers, and their use in packaging and distributing software applications. Being a relatively new paradigm in the cloud-native age of computing, when it comes to security, containers occasionally get a bad press. This mistrust, however, is unwarranted, and provided due consideration is given to certain aspects of security, then a containerized workload can be as secure as any other application delivery mechanism. With that in mind, this course aims to provide you with the insight, knowledge, and techniques for securing your Linux-based Docker container workloads. So let's take a look at what we're going to be covering in the course, and what it means for you as you seek to make your containerized software applications as secure as they can be. First up, we'll take a look at a Linux kernel primitive, the namespace, and how it's used to isolate a container's process. The Linux namespace is a fundamental building block for containers, and when it comes to securing container workloads, it's important to understand how they are used to isolate a container's process, and what the implications are when we relax some of that isolation. Another container primitive provided by the Linux kernel is the control group. In the next module, we'll see how the Docker engine uses control groups in order to constrain the access a given container workload has to the host's resources. We want to ensure our containers are good neighbors and don't compromise the Docker host to the detriment of peer containers. Elevated privileges make it much easier for systems to be compromised. It's in our interests, therefore, to make sure that unnecessary privileges in our containers are removed before they become problematical. We'll devote a module to learn how to minimize the privileges available to a container workload. An off-sighted concern related to, but required by containers is the direct access a container's process has to the host's kernel. In the penultimate module, we'll see how Docker minimizes this access, and how you can provide further limitations by filtering the system calls available to a container's process. Finally, we'll see how to make use of Linux security modules in order to implement fine-grained access control to system resources. By the end of the course, you'll have a thorough understanding of the inherent security features available for containerized workloads, and the tools and techniques for minimizing the attack surface available from within a Docker container workload. Let's get straight in with this first module, which is called Isolating Container Workloads with Linux Namespaces.

Introducing Namespaces
Sometimes people who are relatively new to containers get confused as to whether they are a form of virtualization or not. Let's take a moment to set the record straight. The problem stems from people's understanding of the ubiquitous virtual machine. Generally speaking, a virtual machine is an emulation of a computer that makes use of a hypervisor, which presents an obstructed hardware layer to a guest operating system running in the virtual machine. Multiple virtual machines can be co-tenanted on same physical host, with the hypervisor managing competition for the host's physical resources. Containers are similar in concept in that multiple co-tenanted containers share the host's resources. A container, on the other hand, is a form of virtualization that is implemented at the operating system level in the host. Rather than being isolated from the host operating system by a hypervisor, containers actually share the host's kernel. Of course, this doesn't preclude containers running on a host, which is itself a virtual machine. In fact, it's entirely possible to run a Linux-based container on a Windows platform using features of Microsoft's Hyper-V hypervisor. Let's explore the nature of container virtualization in a little more detail. The very first thing that we need to understand about a container is that it is not itself a kernel construct. A container is an abstraction of some features of the underlying host operating system kernel. For a Linux container, these features are namespaces, control groups, and an isolated file system. Together, these primitives enable the creation of the abstract container concept. In a forthcoming module, we'll take a look at control groups, but for the moment, we'll focus on namespaces. So what exactly is a namespace? A Linux namespace is a kernel construct, which allows for the isolation of an operating system resource from the perspective of a running process. If you'd like, a namespace fools a process into believing it uniquely has access to the resource or resources in question. In reality, however, other processes in their own namespaces also have access to similar resources in their isolated environments. All of these operating system resources actually belong to the host system. Let's see which operating system resources can be isolated in this manner. The first ever namespace introduced into the Linux kernel was the mount namespace. The mount namespace isolates the set of mount points observable by a process. This enables a process, and therefore a container, to have and to maintain its own private filesystem. The UTS namespace isolates a hostname and NIS domainname from the perspective of a process. This means each container running on a host can have a unique hostname. The network information service is a legacy technology, and NIS domainnames are effectively redundant, so unless you have a particular need to use it, you can ignore this aspect of the UTS namespace. The PID namespace provides a process with an isolated view of process ID numbers. Effectively, this means that containers get to see just those processes that reside within its own PID namespace and no others. The IPC namespace isolates system V IPC objects such as shared memory and also POSIX message queues. This enables containers to have their own private IPC objects. Another key namespace, the network namespace provides a process with a unique view of the network stack, including interfaces, ports, netfilter rules, and so on. This allows containers to have their own unique network stack. Having being one of the most difficult to implement, the user namespace took a number of years to complete, and even now, it's not turned on by default in some Linux distributions. It isolates the set of user and group IDs from the perspective of a process. Finally, the cgroup namespace was introduced in 2016 and provides a process with an isolated view of the root directories of its cgroup hierarchies. In a moment, we'll use a practical demonstration to implement some new namespaces for a process in order to consolidate our understanding of Linux namespaces. But first, let's see how to determine which namespaces a process belongs in.

Creating a Container with Namespaces
Every process belongs in a namespace for every namespace type that a host system's kernel supports. So when a system is first booted, every process belongs in the default namespace for each namespace type. Default namespaces can also return the root namespace or global namespace. Here we've interrogated the data structures of the host kernel to determine the namespaces that the process associated with the bash command shell is operating in. This is achieved by listing a directory in the proc fs filesystem. A subdirectory called ns, an abbreviation for namespace, exists for every process running on the host. Here self refers to the querying process. The query produces a list of files. The files are symbolic links named after a namespace type, which contain a string with the namespace type, and an inode number, which uniquely identifies the specific namespace instance. Hence if two processes belong in the same namespace, their symbolic link will point to the same inode. We've already mentioned that every process starts off in a default namespace for each namespace type, so how do we go about putting processes into different namespaces? The Linux kernel provides three system calls for placing processes in namespaces. The first of these system calls is clone. As its name suggests, the clone system call clones a new process from the calling process, and can optionally place that process in one or more new namespaces. The unshare system call enables the calling process to move from its existing namespace or namespaces into one or more new namespaces. Finally, the setns system call enables the calling process to move to one or more preexisting namespaces. In the demonstration we're about to see, we're going to make use of the clone system call in order to create a new child process within some new namespaces. The clone sys call takes several arguments, but one of these is an integer, which encodes flags, which determine the configuration of the child process. The flags can be combined in a bitwise manner, which is how we specify the namespaces that we want our cloned process to belong to. In this example, we've specified that the child is to be cloned into a new PID namespace, a new network namespace, and a new mount namespace. Note that the flag for a new mount namespace is CLONE_NEWNS, which doesn't follow the convention of the other flags, which includes the namespace type. This is because the mount namespace was the first to be implemented in the Linux kernel, and a convention had not been established. Once successfully cloned, the child process executes a function, which is passed as a pointer to the clone sys call, whilst the parent waits for the child to exit. Let's see how namespaces work in practice, and how they are utilized as one of the inherent secure features of Docker containers.

Demonstrating Process Isolation with Namespaces
Okay, to set the scene, we'll be creating processes and placing them in new namespaces, so that you can better understand the use of namespaces when it comes to Docker containers. To start with, we have a terminal, which we'll split into two panes, each running a bash shell, each being its own distinct process. The first thing we can do is demonstrate that both of these processes belong in the same namespaces. To do this, we can simply perform a directly listing of the processes namespace directory in the proc fs filesystem. We need to do this with both processes. Careful inspection of the output shows that the symbolic links point to the same inode numbers for both processes. Both processes, therefore, belong to the same set of namespaces, that is the set of default namespaces. We'll keep the directory listing of the default namespaces in the bottom pane for future reference. We'll be using a program written in the C programming language, which is available in the exercise files for you to experiment with at your leisure. The program enables us to clone a new process, placing that process into a selected number of different namespace types. We have the relevant development tools already installed on this host, so let's go ahead and compile the program. We'll need to link our program against the POSIX realtime library librt, and we'll call our executable ns. Just to make sure it's compiled correctly, let's invoke it with the help option. The output shows us how to use the program, and we can run the program with a set of flags, which specify the type or types of namespaces to clone a new process into, optionally gel the process from a filesystem perspective, specify a command to run in the child process, and benefit from verbose output, so that we know what's going on as the program executes. Let's go ahead and clone our process into a new PID and mount namespace using the -p and -m flags. We'll also set the verbose flag, and the command the child is executing is env with bash as the argument. This allows us to set the shell prompt for the new environment. Our attempt failed, and the reason for this is that it's necessary to be a privileged user to perform namespace operations. So let's try again as a privileged user. This time we're successful, and the output is interesting to observe. There are two messages from the parent stating its own PID and also that of the clone child, which is 2669. The messages from the child, however, suggest its PID is 1, and it also informs about the command that it's executing. So let's explain the different perspective that the parent and child have regarding the child's PID. There is only one process that exists after the clone operation, but in the default namespace as seen by the parent, the process ID is 2669, whereas from the perspective of the child in the new PID namespace, the process ID is 1. Let's be absolutely sure the parent and child are in different PID and mount namespaces by listing the namespaces associated with the child process and comparing them to those associated with a process in the default namespaces. The inode numbers for the child process are different for the mount and PID symbolic links, showing that the child resides in different mount and PID namespaces. Before we finish up, let's just do a process listing from the child's perspective, an see which processes it knows about. As we should expect of a process running in its own new namespace, it can only see itself and any processes it subsequently spawns. This is in contrast to the bash shell belonging in the default PID namespace, which lists every process running on the host. We could spend a long time playing with the namespaces, and I would encourage you to do this, but for the purposes of the course, we need to move on to see how namespaces are applied, used, and customized in Docker containers.

Understanding Docker’s Use of Namespaces
Now that we have a thorough understanding of namespaces and their key role in the construction of containers, let's see how Docker makes use of namespaces. By default, Docker uses the following namespaces when it creates containers, PID, network, mount, UTS, and IPC. The cgroup namespace is relatively new, and it's not currently used by Docker, whilst the user namespace needs to be applied on a host-wide level via the Docker daemon. Currently in Docker, the use of user namespaces is more of a platform-level consideration, rather than a specific container workload consideration. Given this default isolation scenario, on occasions, it may be relevant or necessary to circumvent this behavior. The Docker API provides a means of adapting the default scenario. Let's see how to do this, why you might want to do it, and what the implications might be. So how can we customize the namespace behavior of Docker containers? Generally speaking, you might want two containers to share a particular namespace, or you may even want a container to share one of the default namespaces of the host, rather than being placed into its own unique namespace. The Docker API doesn't give absolute freedom here, because the whole idea of containers is that their process or processes are isolated from the rest of the host's processes, including those of other containers. However, a container can be created sharing a PID, network, or IPC namespace with another container or with the host. A container can share the host's UTS namespace, but not another container's UTS namespace, whilst it's not possible for a container to share a mount namespace with either the host or another container. Data can be shared between containers and between a container in the host via volumes instead. As we've already discovered, instead of creating a unique PID namespace for a container, it's possible to use Docker's API to create a container belonging in either the default namespace of the host or a PID namespace associated with another container. To share the host default PID namespace, a container needs to be run using the --pid config option with the host argument. There may well be a legitimate use case for doing this, but care should be taken when doing so. The process running in the container will have access to the whole process tree, and have the privileges necessary to terminate any of those processes. Clearly this is not a secure configuration. A more likely scenario would be to allow two containers to share the same PID namespace. It might be desirable, for example, to debug an application's process running in a container without polluting the container with the debugging tools. The debugging tools could be run in a second container with access to the application's process by virtue of sharing the same PID namespace as the application container. To affect this, we have to use the --pid config option again, this time using the container keyword in conjunction with the container's ID or name. In the same way that a container can share the default PID namespace of the host or of another container, so too can it share the IPC namespace. This can be achieved using the --IPC config option, using the same argument syntax as discussed for the PID namespace. The same health warning comes with respect to sharing host default IPC namespace. It's good practice, however, to allow two microservices encapsulated in their own containers to use a shared IPC namespace for interprocess communication. The IPC objects they would use to communicate would be private to the participating containers. Implementing shared network namespaces with Docker behaves slightly differently to those we've already discussed. For historical reasons, the --network config option is used to specify which network a container should connect to. A network is a first-class object in the Docker API, and could be a software-defined network that spans more than one host. However, it is possible to use the --network config option to share the network stack of the host, as well as with a specific container. In each case, this amounts to sharing the relevant network namespace with a newly created container. The usual caveat will apply when sharing the host's network stack, and whilst there are legitimate use cases for container's sharing the host network namespace, it should be avoided if at all possible. As regards to containers sharing their network stack by virtue of network namespaces, a classic use case example is for the implementation of a logical computing entity, such as the Kubernetes pod. Finally, it's possible for a container to share the host's UTS namespace, but not another container's UTS namespace. This can be implemented using the --UTS config option with the host argument. Sharing the host UTS namespace with a container gives the container's process the ability to change the hostname of the host itself. Use with caution. Moving on, we're going to use a demonstration, so that you can see how to safely overwrite Docker's default namespace behavior in order to allow two containers to share the IPC namespaces.

Modifying Namespace Use for a Docker Container
The purpose of this demonstration is to see how it's possible to responsibly alter Docker's default namespace behavior in order to take advantage of the isolation that namespaces provide. We'll start a container whose sole purpose is to create a POSIX message queue, listen for incoming messages, and faithfully dump the message to the terminal. We'll see how access to this message queue is limited to any processes that share the same IPC namespace as the container. The listening container, which we'll call receiver, uses a basic C program called receiver. c. It's available in the exercise files, along with a corresponding Dockerfile that defines its image. Alongside the receiver program, we will also make use of another C program for sending messages to the message queue called, funnily enough, sender c. The source code for the sender and its Dockerfile are also available in the exercise files. Let's split our screen into two panes, and then start the receiver container in the top pane. In order to help us when we're referring to the receiver container, we'll give it the name receiver. Once the container is started, it gives us a prompt indicating it's ready to receive messages. We'll swap panes and attempt to run the sender program on the host rather than in a container. We get an error message, because the message queue resides in the IPC namespace of the container, and not in the default namespace associated with the send process, which ran on the host. So far so good, this is what we should expect. Let's split the bottom pane into two, and in the middle pane we invoke a container based on the sender Docker image sharing the receiver container's IPC namespace. We don't get an error this time, which means that we've connected to the message queue. We're ready to send messages to the queue, so let's send a couple. The messages are placed in the queue, and the receiver container reads the messages from the queue and faithfully reproduces them. This clearly shows the receiving and sending containers sharing the same IPC object within their own private IPC namespace. Two is a party and a three is a crowd apparently, but let's drop into the third pane, and let's see if we can start another sender container sharing the already shared IPC namespace. This looks okay, and if we send a message, it's read by the receiver container. So we now have three containers sharing a private namespace. If we stop the second sender container and replace it with another container, which doesn't share the IPC namespace, then we get the familiar error message telling us that the message queue can't be opened. This simple demonstration shows the power of namespace isolation in the context of Docker containers. We've made use of the shared IPC namespace for private interprocess communication, but we could've employed any of the other available namespaces to similar effect. Selective sharing of namespaces between Docker containers is a safe and secure option for those use cases that can't be met with the default isolation configuration.

Module Summary
That's it, we're done with namespaces. What have we uncovered in this module? We've seen that containers are an abstract concept built from Linux kernel primitives, which includes isolation of resources from a process perspective using namespaces. We've discussed the various namespace types and what form of isolation they provide. And we've gained an understanding of Docker's application of namespaces, and the reasons for and implications of deviating a way from the status quo. You may not have a specific use case for deviating away from Docker's default namespace configuration, and if that's you, then you'll still benefit from the knowledge that your Docker container workloads are safely isolated from one another courtesy of namespaces. What's really important, however, is that as and when you identify opportunities for namespace sharing for your container workloads, you now have the means to make informed decisions about the most appropriate configurations to apply in order to maintain a secure container workload. In the next module, we're going to explore the other significant Linux kernel feature that contributes to the abstract container construct, the control group.

Controlling Access to Resources Using Control Groups
Module Overview
Hi, and welcome back to Securing Docker Container Workloads with me, Nigel Brown. We've already taken a look at one container primitive, the Linux namespace. And now it's time to investigate another one, the control group. This module is called Controlling Access to Resources Using Control Groups. You're already familiar with isolating container processes, but what about ensuring that containers behave nicely when it comes to consuming resources? That's where control groups come in. In this module, we're going to cover the basics of control groups, what they are, what they do, and we'll see how they are applied in order to control access to the host's physical resources. We're interested in Docker container workloads, so we'll extend our working knowledge of control groups, and see how Docker makes use of them in order to implement limits and controls. We'll move on to discuss some reasons for wanting to apply limits and controls, along with the resources we're able to subject to those limits and controls. For good measure, we'll demonstrate how to apply those controls using the Docker commandline interface. By the end of this module, you'll have gained a thorough insight into a valuable tool that Docker uses as a lever to help maintain the integrity of the host and its container workloads. You'll also have the knowledge that will help you to start planning for the controls that you want to employ for securing your own Docker container workloads.

Introducing Control Groups
A control group is a Linux kernel construct, which allows for limiting access to and accounting for the usage of the physical resources of a host system by a group of processes. Control groups were originally implemented in the Linux kernel in 2007 by two Google employees. This original implementation, however, has been supplanted by a new version 2 implementation, starting from Linux kernel 4. 5. Version 2 sets out to fix some limitations associated with the version 1 implementation. Both cgroup implementations coexist, and currently Docker makes use of the version 1 implementation. Alongside namespaces, control groups are a fundamental primitive of the container abstraction. If we were to lift the lid on a control group or cgroup as they are generally called, what would we see? A cgroup associates a group of tasks with a set of configurable attributes for one or more subsystems. There's a lot of jargon in that statement, so let's rephrase it to use some language with which you might be more familiar. A cgroup associates a group of processes with a set of configurable attributes for one or more system resources. In cgroup's vernacular, a task is a process, and a subsystem is a module that performs an action upon a group of tasks, which typically will be a resource controller. Let's have a look at the system resources we're able to control access to. The term system resources can mean all kinds of things to different people, so what do we actually mean when we talk about them in the context of cgroups? Well there are numerous subsystems available to attach to cgroups, but not all of them are used by Docker, so we'll just look at those that are relevant to Docker containers. The blkio subsystem is used to control and monitor access to I/0 for specified blocked devices on the host system. Next, the device's subsystem controls which processes are capable of creating and reading from or writing to specified devices on the host. The use of memory, including kernel memory, physical memory, and swap space is controlled and monitored using the memory subsystem. In some situations, it may be important to limit the number of processes that a process or processes can fork, and the PID subsystem is designed just for this purpose. Given that systems are designed to function with multiple processes competing for access to the system's CPUs, it's essentially that the subsystems exist to control how processes access the compute capability provided by CPUs. Two cgroup subsystems exist just for this purpose. First of these, the CPU subsystem controls access to the system's CPUs via its scheduler. Whilst this system is not on the load, processes have as much share of the CPU as is required. When the system is underload, however, a combination of cgroups and the CPU resource controller work to prioritize certain tasks belonging to one cgroup ahead of a set of tasks belonging to another. Last but not least, the cpuset subsystem pins tasks belonging in cgroups to specific CPU cores or memory nodes. Docker makes use of other cgroup subsystems, especially for reporting on resource usage, but the subsystems detailed here are those that concern us when we need to ensure that containers are conducting themselves as good neighbors. In the original cgroup implementation, cgroups are placed into one or more hierarchies. Typically, a hierarchy exists for each subsystem, but not always. Let's have a look at an example. Here we have a hierarchy of cgroups. On our host, this hierarchy can be accessed and manipulated via a virtual filesystem, and we'll see how to do this in a moment. The CPU resource controller subsystem is attached to this particular hierarchy, which means that this hierarchy is about controlling access to the host's CPU cycles. Each cgroup in the hierarchy inherits any limits applied to its parents, which can be further tuned using its own set of configurable parameters, which relate to the CPU subsystem. For any given CPU hierarchy with an attached subsystem, every process running on the host will belong to one and only one cgroup. If a process is not assigned to a specific cgroup, it will belong in the root cgroup. If you're new to the concept of cgroups, this will may appear confusing, so let's take a look at an example.

Applying CPU Shares Limits to Processes
In this example depicted here, we're making use of a cgroup hierarchy located in the root cgroup, which has the CPU resource controller subsystem attached. The hierarchy is comprised of five cgroups and most have an associated task, which is indicated by the black dot within the circle. Our goal is to tune the CPU shares parameter for each cgroup in order to carve up the access that each set of tasks in each of the cgroups has to the CPU. The CPU shares parameter is a relative weight, the higher the weight, the greater share of CPU cycles the cgroup's tasks will have. Cgroup 1, which has a CPU shares weight of 1024, needs to share the CPU cycles with its direct cgroup descendents, cgroups 2 and 3. If we combine the weights of cgroups 1 to 3, then the proportion of CPU cycles afforded to the tasks in cgroup 1 is the ratio of its weight with the combined weight, which is approximately 28%. Similarly for cgroup 2, which has no cgroup descendents, its tasks have a proportion of CPU cycles equivalent to the ratio of its weight 2048 and the combined weight of cgroups 1 to 3. That's approximately 57%. Cgroup 3 has no tasks, it has no black dot. If it did, however, the proportion of CPU cycles its task would receive would be the ratio of the cgroup weight 512 with the combined weight. The share of CPU cycles derived for cgroup 3 then needs to be shared equally by the tasks associated with cgroups 4 and 5, which both have a CPU shares weight of 1024. The calculation of CPU shares for tasks in both of these cgroups translates to approximately 7% each. Now clearly, this is somewhat simplistic. On a real host system, the root cgroup has a bunch of tasks, which will be kernel threads, and cgroup 1 will have peer cgroups. This means any tasks in our example hierarchy share CPU cycles with all of these other tasks. However, if we can apply sufficient demand on the CPU, then the requirements of other tasks will be small in comparison, and we can approximate our desired behavior. We're going to implement this cgroup hierarchy on a Linux system, so let's keep the ratios we've computed in mind for each of the cgroups, so that we can verify the behavior of our hierarchy. If you want to recreate this demonstration, then you'll need a Linux system with a cgroup tools package installed. You'll also need the stress package, so that you can simulate an artificial load. Some notes are provided in the exercise files. The first thing we'll do is split our terminal into two panes, so that we can execute commands in one pane and view the results in the other. Let's set htop running, so that we can monitor the percentage CPU each of the processes we will create is consuming. Let's just remove the meters in one or two other bits, so we have more room to show our processes. We'll create the processes top down in the hierarchy, so the process ID, which is the smallest, will belong to CPU cgroup CG1, and the largest to CPU croup CG5. This will help us to identify which process is which, and which cgroup it belongs to. The first step in the exercise is to create a cgroup that pins the processes we create to a particular CPU core, so that the relative weights we've calculated can be demonstrated effectively. The cgroup is called all, and we're setting ownership of the files to our user for convenience. If we navigate to the cgroup virtual filesystem, and into the hierarchy attached to the cpuset subsystem, we can see our new cgroup. Let's set the parameters for the cgroup, so that the processes we create are pinned to the first CPU designated as 0. And then we can check to see that they've been set correctly. We're looking for a value in 0 in both of these files. All is well. We now have a custom cpuset cgroup configured the way we want it. Now let's create our first cgroup CG1 in the hierarchy associated with the CPU subsystem resource controller. If we take a look inside the cpu. shares file associated with the new cgroup, we can see that the cgroup has the default weight of 1024, which reflects the weight that we've already planned. Let's go ahead and start a stress process belonging to CPU cgroup CG1 and the cpuset cgroup all. If we look at the output shown by top in the lower pane, we can observe that our stress process quickly consumes almost 100% of the CPU. Next step, we'll create cgroups CG2 and CG3 as direct descendents of the cgroup CG1. Newly created, both cgroups will have a cpu. shares weight of 1024, but we want the weights to be 2048 and 512 for CG2 and CG3 respectively. So let's go ahead and set these accordingly. Our plan is to start a process belonging to cgroup CG2, but not one for cgroup CG3. Let's start another stress process, this time associated with CG2. Straight away, the 2 running stress processes consume the CPU in line with the relative weights their cgroups have been assigned, 33% for the process belonging to CG1 and 66% for the process belonging to CG2. If you remember from the scenario we've planned, once our hierarchy is fully implemented, we expect the process in CG1 to consume approximately 28. 5% of the CPU, the process in CG2 to consume 57%, and the processes in CG4 and CG5, 7% each. Let's create the final 2 cgroups, CG4 and CG5, both of which will have the default CPU shares weight of 1024. All we need to do now is assign a stress task to each of these cgroups, and watch what happens in the output of the top utility. Creating a stress process associated with CG4 has immediately altered the consumed CPU percentage. The percentage share of the CPU consumed by each of our stress processes works out to be pretty close to what we planned and expected. This demonstrate has helped to cement the theory of control groups, specifically how they can be used to limit the resources available to a group of processes. Whilst it's important to understand the concepts and operation of cgroups, fortunately when it comes to Docker containers, some of the complexities are hidden behind the Docker API. Let's move on, and have a look at how Docker makes use of cgroups in order to control the access container workloads have to a host's resources.

Understanding Docker's Use of Control Groups
So far, we've covered the basics of control groups, and by now, you should have a good grasp of their role in controlling access to system resources. As Docker containers are designed to be co-tenanted, it will come as no surprise to learn that Docker affords the user the means to impose system resource usage controls using cgroups. Docker is flexible, however, when it comes to applying cgroups to container workloads. So let's have a look at the options that are available. Normally, cgroups are handled by systemd, which is the init process on the majority of Linux distributions. Systemd takes care of managing a large number of operating system features, and while cgroups are one of those features, it's also possible to manage cgroups independently using tools or system calls which manipulate the cgroup filesystem directly. We saw how to do this when we created our CPU shares hierarchy. Why is it important to know this? It's important to know, because Docker manipulates cgroups using the cgroup filesystem, unless you tell it to behave differently. If you have the need to create your own cgroup hierarchy for a container or containers, which is entirely possible, then you need to be aware of which cgroup driver Docker is configured to use in order to influence where the container's cgroup is located. Let's have a look at the effect of using each driver. Irrespective of whether resource limits are applied to a container on invocation, when Docker uses the default cgroup fs driver, it creates a cgroup for the container in every cgroup hierarchy for each subsystem. Each hierarchy looks something like this. The root cgroup contains four sub cgroups, init. scope, system. slice, user. slice, and Docker's own Docker cgroup. Any containers that get created are placed in another new cgroup named after the container ID underneath the Docker cgroup. Here we have two cgroups for two different containers. If on the other hand, we wanted systemd to manage the cgroups associated with our container workloads, then we have to provide a config option when we start the Docker daemon. The config option is --exec-opts, and the argument would be the key value pair native. cgroupdriver=systemd. So it's an either or choice, and changing between the stars of cgroup management requires a Docker daemon restart. There are numerous ways to apply Docker daemon config changes, but the example here uses the daemon. json config file located in /etc/docker. The config option is pluralized here, as it can take multiple key value pairs. So if we're using the systemd cgroup driver, which cgroups do containers end up belonging to? The systemd cgroup driver makes use of the system. slice cgroup for Docker containers, and creates a cgroup called docker-ID. scope for each specific container. The ID is the container ID, and the. scope extension is a systemd convention indicating that the container processes were created externally to systemd via the Docker daemon in this case. When assessing your own requirements for controlling the access your container workloads have to system resources, you may well want to configure your own bespoke cgroup hierarchy. You might want to do this in order to provide a fine-grained allocation of resources for a container workload or workloads. We've already mentioned that this is possible using Docker, and it can be done on a system-wide basis or on a container-by-container basis. It's done by specifying a custom cgroup parent for containers. In order to apply a custom cgroup parent on a container-by-container basis, we need to make use of the Docker client. But if our requirement is to apply the cgroup parent to all new containers, it's more appropriate to apply the configuration to the Docker daemon itself. In order to be applied on a system-wide basis, the config option --cgroup-parent needs to be applied to the Docker daemon, which then needs to be recycled for the change to take effect. Again this can be performed using the Docker daemon's systemd unit file, or an override, or using the daemon. json config file. In the example depicted, our custom parent cgroup is located in the root cgroup, and it's called appserv. As we'll see in a moment, this implies we're using the cgroup fs cgroup driver. If your need is to selectively place containers in cgroups located in a custom location, then the exact same argument is available using the Docker container run commandline option or via the appropriate Docker API endpoint. In the example shown here, which implies cgroup fs usage again, the path to the cgroup parent is absolute, and therefore the created container's parent cgroup appserv is located at the root of the cgroup hierarchies for each subsystem. If the need is for a custom cgroup parent that doesn't require to be located at a specific location in the hierarchies, then a relative path can be used, and the parent cgroup will be located as a direct descendent of the Docker daemon cgroup under systemd's system. slick cgroup. The last configuration option available is to create a custom cgroup parent using the systemd cgroup driver for a specific container. The argument that accompanies the config option must end with. slice, whilst the beginning defines the location. The example shown here just uses appserv, which means the cgroup is located in the root of the hierarchy. If we wanted it located under the system. slice, then we would need to specify system-appserv. slice, and so on. We now have an awareness of the intricacies and some of nuances associated with Docker's use of cgroups. So now let's see what controls Docker provides through its API, and how and when we might want to apply them.

Defining the Resources Available to Control
So far, we've acquired some useful knowledge about Docker's use of control groups, and we definitely need to know some more. But before we visit the options available in the Docker CLI, let's ask ourselves why we might want to make use of control groups in the context of containers. There are likely to be as many reasons for using control groups as there are Docker deployments around the world. So let's just discuss some of the more obvious candidates. It will be important for you to assess your own situation and requirements, particularly when you deploy containers to a production environment. Containers are configured with a minimal set of device files necessary to function effectively. Sometimes it's necessary for a container to share a device file with the host, or to have the ability to create its own device inside the container. If you need to limit the operations that a container can perform with regard to specific devices, then cgroups come to the rescue. Some applications by their very nature consume a particular resource more readily than a more generalized peer application. For example, some scientific modeling and simulation tools can be very CPU intensive. In some circumstances, it may make sense to wait to the access of competing container workloads to specific host resources using cgroups. In may be appropriate to be prepared for either intentional or accidental depletion of the host resources through over consumption by container workloads. Either way, cgroups can help to ensure the continued operation of the host and peer container workloads with a judicious application of appropriate controls. And whether you make use of a container orchestration platform or simply run multiple co-hosted containers on one or more platforms, limiting the resources available to each container workload helps them to coexist and to be scheduled appropriately in order to optimize the consumption of system resources. Okay, we're going to take a peek at the cgroup specifics available via the Docker CLI. First up, Docker provides access to the cgroup hierarchy associated with the block I/O resource controller subsystem. Specifically, it enables throttling the I/O access containers have to blocked devices using weights, either generally or on a device-by-device basis. Due to a limitation in the implementation of cgroups version 1, this only applies to direct unbuffered I/O. Additionally, the Docker CLI allows for limiting the rate for reads from and/or writes to a specific device, which can be expressed in bits per second or as I/O operations per second. We've discussed the potential need for a container workload to access a device on the host. Docker provides a config option that enables a container to be provided access to a host device on startup. The type of operations it can perform on the device is controlled with a cgroup. A mechanism is also provided for defining a rule for operations on a device created after the invocation of a container. A number of controls are provided for limited and constraining a container's access to memory. This includes kernel memory, as well as physical memory and swap. Memory limits and constraints can be defined in terms of soft and hard parameters. A container, by default, can fork as many processes as are available from the maximum configured for the host's kernel. Unchecked, this is a potential avenue for exploitation as a denial of service attack, and a container should be limited to its required number of processes through the Docker CLI. As we've already demonstrated earlier, controlling access to CPU cycles is important. This is particularly true in a Docker container environment, lest the container consumes more than its fair share of CPU cycles to the detriment of other containers. The Docker CLI allows fine-grained configuration of the kernel's completely fair scheduler for a given container workload, as well as the realtime scheduler if it supports the use of cgroups. In fact, the Docker CLI does a good job of taking away some of the complexity associated with defining CPU access with a flag --cpu that helps to specify the fraction of total CPU access for a container. For example, it's possible to specify that a container workload can consume 1. 5 of the available 2 CPUs. Finally, we previously saw how cgroups come into play in order to pin processes to CPU cores and memory nodes. And this pinning is available for containers through the Docker CLI. Let's have a look at a couple of these limits and constraints in action.

Using the Docker CLI to Control Container Resource Usage
It's demo time again, and this time around we're going to have a look at limiting and constraining container workloads in terms of the resources they can consume. We have a simple Dockerfile at our disposal, which provides the stress utility we already used in the last demonstration. We're going to use this in order to simulate an extravagant container workload consuming CPU and memory. For good measure, we're also going to create a custom cgroup hierarchy for the container. The first thing we need to do is build our Docker image ready for use. Once it's built, we can create the stress utility inside a container by simply passing arguments at the end of a Docker container run command. We'll just wait a few moments for this to finish. Okay, our image has been built successfully, so let's move on to create our container. Our host has 2 CPU cores, 2 GB of physical memory, and a similar amount of swap configured. We'll specify that stress uses a single CPU hog and a worker that exercises 1. 5 GB of memory. The argument we specify on the commandline are appended directly to the container's entry point instruction. Let's take a look at how much of the system resources our container is making use of. Straight away, the container consumes approximately 75% of the physical memory, and all of the CPU cycles available. If we were to start a few more containers, we'd have some serious contention for the host's resources. This could lead to some poor performance for our container workloads, and may even invoke the kernel's OOM killer when demand for memory exceeds that which is available. Let's terminate our container. I'm going to try again, but this time we'll apply some limits in order to curtail the container's resource consumption. We'll capture the container's ID, we're going to need this in a few moments, and instead of running in the foreground, the container will run in detach mode. We'll also assign a customized cgroup parent called stress, which will be located in the root of the various cgroup hierarchies. The limits that we'll apply are the bandwidth associated with 0. 75 CPUs and 1 GB of memory. Okay, let's see what happens. The output from the top shows us that our container is now consuming approximately 75% of CPU cycles and 50% of physical memory. Let's also check to make sure the processes running inside our container have been placed in a control group under the cgroup parent we specified. Let's list the process IDs running in the container. Docker container top is the CLI command that provides us with the container's process IDs. Now all we need to do is list the contents of the tasks file for the cgroup associated with the container, which should be located in the stress cgroup parent we created. Every cgroup has a task file associated with it, which contains all of the processes that belong to that specific cgroup. There they are. Some cgroup controls can be updated on a live container, so let's do that for our container, and change the access to CPU cycles from 0. 75 to 0. 25. Htop shows us that CPU consumed by our container has fallen further to the desired 25%. Let's remove the container and exit htop. This simple scenario has demonstrated how easy it is to impose resource controls on Docker container workloads. If you plan to invoke controls on your container workloads, be sure to spend some time to determine the workload's optimum requirements. To finish up, let's create a container and impose a limit on the number of processes that can be created within the container. We'll limit it to 15. Docker's --pids-limit config option sets a limit in the container's PID cgroup. You may have guessed by the name of the container we specified, we're going to simulate a denial of service attack within the container by invoking a fork bomb. We'll also set up a monitor to display all of the processes running inside the container. At the moment, we just have one process, which is the bash shell. If you're recreating this demonstration, a special word of warning, don't try this on a system that you care about. If you inadvertently invoke the container without the PIDS limit, or your kernel doesn't support the PID cgroup subsystem, you'll render your system inoperable. Use a virtual machine or some other host that you don't mind having to turn off to regain control. Okay, warning over, let's invoke the bomb and see what happens. The fork bomb attempts to keep forking itself, but by the time that there are 15 processes running inside the container, thanks to the cgroup limit we've imposed, no further processes can be created. Docker container workloads are designed to run a single process. Occasionally in a few cases, a few more. The emphasis is definitely on less rather than more, so you will do no harm in limiting a container to as few processes as are necessary.

Module Summary
That marks the end of our foray into the world of cgroups and their particular application to Docker container workloads. We've seen that there can be a variety of reasons why it might be preferable to control the access a given container workload has to the available system resources. Assessing the requirements that individual container workloads have for system resources ensures that they operate effectively, but also ensures that they behave appropriately in a co-tenanted container environment. We've also seen how it's possible to create a bespoked cgroup hierarchy configuration for a container workload or workloads in order to tune how system resources are consumed. As a container primitive, cgroups represent an essential capability in the armory associated with securing Docker container workloads. Equipped with the knowledge of how Docker makes use of cgroups, you are now in a position to plan and implement the controls and limits that each of your Docker container workloads need.

Managing the Privileges Available to a Container Workload
Module Overview
Hello there, and welcome to this next module in this course, Securing Docker Container Workloads. My name is Nigel Brown, and in this module, we're going to take a look at managing the privileges available to a container workload. When we run container workloads, it's pretty crucial that we do all we can to protect the integrity of the workload to make sure that it isn't compromised in any way. A compromised container is bad news. Privilege plays a big part in securing our container workloads and having the means to effectively manage privileges is an essential tool in our security-focused toolbox. Let's get on and see what we're going to cover. You have a number of things at your disposal for managing container workload privileges. Just before we take a look at what they are, we should take a moment to consider the importance of managing privilege and the approach we should take in order to minimize our exposure to risk. We'll see how we can reduce the privileges available in a container workload deployment with the creation of a non-privileged user before we then move on to address some of the practical concerns with running our container as a non-privileged user. When we have good tools at our disposal which aid our endeavors, we should use them. We'll take a look at the Linux kernel's capabilities mechanism for managing the privileges available to a process before we finish up by exploring how Docker applies capabilities to manage the privileges available to a container workload. By the end of the module, you'll have a good sense of the relevance of managing privileges for container workloads and applications and the knowledge you need to plan the effective management of your own Docker container workloads.

Managing Privileges with a Non-privileged User
A privilege is a right granted to an individual, a program, or a process, a right, effectively, to perform an action upon an object of some kind. As such, the management of privileges always needs to be handled carefully in order to maintain a secure system. Extensive relaxation of privileges leads to a distribution of power that can't always, no pun intended, be contained. First of all, with the bestowment of power comes the choice of whether we want to be good citizens or bad citizens. We'd all hope that privilege is only granted to an agent we trust, and in so doing, we maintain the integrity of the systems we're responsible for. Some who are granted privileges might be less observant than others and may become unwittingly complicit in the exploitation of privilege and the power it confers. Handing out privileges is a bit like letting a genie out of the bottle. We can't always easily undo what has already been done. The more privileges we give away, the more we have the potential to lose control. In the case of Docker container workloads, just like any other paradigm in computing, we should always adopt the principle of least privilege. Least privilege means that a user or a program should get the absolute minimum set of privileges necessary in order to perform the task at hand, no more, no less. From a practical perspective then, what does least privilege mean for Docker container workloads? If you take a moment to consider all of the different possible use cases for containers, then trying to provide a standard system of least privilege to accommodate every requirement is nigh on impossible, in fact, impossible. As a consequence, Docker provides a container with a set of privileges that might be considered appropriate in many general use cases but allows for a user to adapt privileges to suit the purpose at hand. This default configuration means that a container's process runs as a privileged user with a user ID and group ID of 0. When Docker containers are invoked in the absence of a user namespace to isolate user and group IDs the privileged user in the container is the same user on the host. As we'll see later, all is not quite as it seems, and this default configuration doesn't provide open-ended privileges for the container. It's key to understand that if your container workload use case doesn't require additional privileges, then you should invoke your container as a non-privileged user. It's all too easy to run a container without regard to the required privileges and accept the enhanced default configuration. Please don't do this, especially in a production setting. If the use case allows, create a non-privileged user when authoring your Docker image, and set the container's user accordingly. We could just as easily specify a non-privileged user on the command line when we invoke a container, but we can't always rely on a third- party consumer of our image to do the same. If we want to enforce the restriction, it's better to bake the behavior in the image. Let's see how to add a non-privileged user to a Docker image. Often when we author Docker images for Linux containers, we'll make use of an image that's based on an operating system such as Ubuntu or Alpine Linux. Even if we make use of a base image that provides us with something higher in the stack, such as an image which provides programming language tools, the chances are that image in turn will be based on an OS image. Either way, it means we have access to the files and utilities for managing users, which enables us to create a new user just for the container and its process or processes. In order to add the user, we just execute the relevant OS commands using the RUN Dockerfile instruction. If your use case requires a deterministic user and group ID, you can specify a group ID and user ID in addition to the names of the user and groups. Often, our images require privileges for commands to be executed in order for image content to be created. For example, we couldn't execute the commands to add a user if we didn't have the necessary privileges. As a consequence, it generally doesn't make sense to set the container's user until privileged access is no longer required. To set the user, use the USER Dockerfile instruction. Any commands executed via a subsequent RUN instruction get executed as the specified user, and when the container's command or entrypoint is invoked, the resulting process is also executed as the specified user. With this approach, we can make use of privilege in order to establish the image and gain peace of mind when deriving a container with a non-privileged process running inside.

Advanced Management of a Container User
On occasions, some initialization, or setup, is required before a container's process is started. This always depends on the nature of the image in question, but might involve the initialization of a database or the establishment of required configuration items in lieu of the user specifying them. Whatever the requirement, the initialization is performed outside of the image itself because it's dependent on the specific circumstances related to the container being invoked. Remember, a Docker image is immutable and should not rely on any of the characteristics of any given scenario or Docker host. Instead, this initialization is usually performed courtesy of a script called an entrypoint script which is executed as the container's entrypoint. The script will perform the necessary initialization tasks before making use of the shell's exec built-in command in order to execute the actual program or command that the Docker image has been authored for. Hmm, what if the script requires privileges, but we've already dropped them by setting the container's user to an unprivileged user inside the Dockerfile? Basically, the script's attempt to execute the commands that require privileges will fail. Use of entrypoint scripts is not uncommon so we shouldn't just shrug our shoulders and say c'est la vie. What's needed is to defer setting the user into the entrypoint script until the point that the exec command is used to replace the shell with the container's program or command. At this point, however, we're outside the scope of the Dockerfile, so we can't make use of the user Dockerfile instruction. It's not available to us. This means we have to use a command within the entrypoint script itself to step down from a privileged to a non-privileged user. If we were discussing changing user at the same time as replacing what's being executed by a process, we'd normally think about using the sudo utility to achieve this. This is right and proper when our context is a system, but in the world of containers, sudo provides some challenges. Firstly, in the context of containers, it's a bit like using a sledgehammer to crack a nut. Linux systems are multipurpose, multiuser systems which need to cater for many different user cases whereas a container is an isolated environment with a single user and a limited purpose. Secondly, sudo forks the container's process and executes the program or command as a child process. For the purposes of signal handling, our container workload's process should be process ID 1 in the container, not a child of process ID 1. For these reasons, it's generally considered good practice to use a simpler alternative for switching user. Many Docker images, including the official Docker images, use a utility called gosu, which is based on the same code used by the Open Container Initiative's run c container executor. In effect, it's the same code that Docker uses when it sets the user for a container. Gosu can be found as a package in some Linux distributions. When the moment comes to replace the shell with a container's intended program, prepend the arguments with gosu and the desired user. A smaller alternative to gosu which is identical in purpose is the su-exec utility, which is available as a package in Alpine Linux. Both utilities are also available on GitHub, and details are available in the exercise files. Let's look at another hurdle we might encounter when we want to minimize the privileges a container workload enjoys by running as an unprivileged user. It relates to running a container workload with a minimal file system, so let's first look at why we might want to use a minimal file system. Many exploits come from unsolicited tampering of file system content, and, therefore, it's in our interest to ensure this doesn't happen, or at least take steps to minimize the possibility. If it's at all possible, then serious consideration should be given to starting container workloads with a read-only file system. This can be achieved using the --read-only config flag when a container is started with docker container run. Once up and running, no changes can be made to the container's file system content or its metadata. Of course, it's not always possible or desirable to run a container with a read-only file system, so we have to think about another approach to mitigating the risk associated with exploiting our container workload's file system. The leaner a file system is, the less parts there are that can be compromised. If you're able to, then removing all unnecessary content from the Docker image in order to produce a minimal file system for a derived container is worth the effort for the consequential gain. You don't have to use a base image that has a whole operating system, even if it has been pared back to a minimum of content. It's possible to base a Docker image on nothing, literally from scratch, by specifying the scratch keyword as an argument to the FROM instruction. The required content for the image can be copied from the build context, and this will represent the entire file system for a derived container. It will be very small and hopefully difficult to compromise. Here's the problem. With no password or group file and no operating system utilities for creating a user, we can't run our container's process as a non-privileged user unless we remember to do so via the command line. It seems that by employing one safety measure, we've rendered another one impossible to deploy. Thankfully, we have a way around this. Docker image builds can be implemented in stages. A single Dockerfile can have multiple FROM instructions, each of which identifies a build stage. We can, for example, use the first stage as the build stage and the second stage as the serving stage. All of the artifacts we require for the serving stage, including a password and group file containing a non-privileged user, can be copied from the build stage into the serving stage. In this example, the serving stage is based on scratch. And after we've copied the relevant files from the build stage, this is the only file system content in the image. Once the image is built, and we've derived a container based on the image, it will have a minimal file system as desired, and the container's process will be running as the non-privileged user specified. The use of a multistage build has enabled us to resolve the apparent contradiction we faced. It's time for us to take a look at the use of a non-privileged user in practice, so let's move on to a demo.

Running a Container Workload as a Non-privileged User
In this demo, we're going to see how to reduce the privileges available inside a container workload by running a container as a non-privileged user. Let's start off by taking a quick look at the application we want to encapsulate in a Docker image. It's a simple API server written in golang that enables the user to post to and query a database containing the rudimentary details of countries. The application is incidental for our discussion about container workload privileges, but we do need to be cognizant of the fact that the server listens for API requests on port 8000 and that we can make use of the API using the endpoints specified by the paths that are defined in the main function. Let's also take a look at the Dockerfile for the application. It's pretty straightforward. As the application is based on golang, we're using the Alpine Linux derivative of the official golang Docker image. A run instruction enables us to retrieve a package that we need for the application, as well as create a directory for the source code to reside in. We then set our work in context, copy the source from the build context into the image before building the executable, which we call apiserver. Finally, we expose the desired port and set the entrypoint to be our newly created executable binary. Pretty straightforward stuff. Our first task is to build the image itself, which we'll tag as apiserver. The build shouldn't take too long to complete, as our application is relatively trivial in nature. Okay, that's the build complete. Let's start a container based on the apiserver image, which we'll run in detached mode, and which we'll call apiserver. We must of course remember to publish the container's port so that we can interact with the API. With the container running, it's time to find out if the application is functioning as expected. To do this, we'll use the curl utility to post some data to the country's endpoint. The data simply contains the name of the country, Ireland, in this case, and its capital and content. We got a 201 Created status in return. So it all looks okay so far. Just to be 100%, we can try a query to make sure the data ended up in the data store. And we get a 200 status along with the content that we just posted. The application seems to be working fine. What we didn't do in our image is alter any privileges from the default set provided by Docker when a container is derived from the image. So if we check the process running in the container for our API server, it's running as the root user. We can also view the process from the host's perspective using the docker container top command. The process is running as the root user here as well. If this process can be exploited in some way, then root access inside the container is available to any would-be hacker. As a privileged user, there is more at his or her disposal for causing damage or for further exploiting the container or even the host itself. Let's remove the container and return to the image's Dockerfile in order to fix this. We'll make a copy of our Dockerfile, call it Dockerfile. user, and make the changes to this copy. The first change to make is to add a new system group, which we'll call api. It doesn't need to be system group, but it's best to follow convention when defining a user and group ID for an application. We'll also provide a deterministic group ID of 500. Similarly, let's create a new user. We'll use the same name, api again, and also specify that its group is the api group we just created. These commands will have created the user and group within the image content, but we still need to set the user for the container using the user Dockerfile instruction. Let's add a comment in the Dockerfile whilst we're about it. With the changes made, it's time to rebuild the image. In order to differentiate it from the first image we built, we'll tag the image apiserver:user, and we'll also need to specify the correct Dockerfile to use for the build. We made a change to the RUN instruction, so the build cache is invalidated at this point. And the build executes each of the Dockerfile instructions afresh, starting with the RUN instruction. Time to start a fresh container from the new image. With the container running based on the new image with the user tag, let's see what effect the changes have had on the owner of the container's process. Inside the container this time, the user associated with its process is the API user we specified in the Dockerfile. From the host's perspective, the user is no longer root; it's an unnamed user with an ID of 500. The reason it's unnamed is because the API user has been added within the container's file system, not the host's. The user ID of 500, however, is common to the container and the host. This change has made our container more secure, as it's running as a non-privileged user, but we can go further by removing unnecessary content from our image. The Docker image is 277 MB in size, which means a derived container gets a similarly large file system. It contains a lot of operating system content we don't need, as well as golang-related development tools which we don't need when we come to serve our API. Let's see if we can improve things and try to create a more minimal image for our container. Let's start by making a copy of the current Dockerfile that we're using, and we'll call the new Dockerfile, Dockerfile. multi, as we're going to use multiple build stages within it. After we invoke an editor, the first thing that we should do is add a useful identifying name to the first build stage by adding the keyword AS to the FROM instruction for the first stage. We've simply called it build. This will help us to reference artifacts later in the Dockerfile. The next thing we need to do is change the command that builds our apiserver binary. We're stripping everything we can from our image, which means we won't have any shared libraries available when we come to run the binary in the container. With this in mind, we'll need to build a statically linked binary. And whilst we're at it, we'll also remove any unnecessary symbols from the binary itself. (Typing) Now we need to move onto the second stage, which we'll commence by defining our base image as scratch. Remember, this means we're building our image from nothing. We're almost done with the changes. We just need to copy some things from the build stage of the image. The first thing is the statically linked apiserver binary. To do this, we just use the regular copy Dockerfile instruction, but we have to explicitly reference the build stage as the location of the source file. We also want to retain our non-privileged user in the new minimal image, so we have to copy the /etc/password and /etc/group files from the build stage also. That's it. Nothing else changes. Let's rebuild the image, and this time, we'll tag it apiserver:multi. And again, we must remember to use the correct Dockerfile for the build. Our Dockerfile has some amended Dockerfile instructions, so the build will take a few moments to complete. But when it's finished, we should have a drastically reduced image size. Let's see how big it is. (Typing) It's come down to just under 5 MB, quite a substantial reduction. This is perfect for when it comes to image distribution and container start times, but the real win for us is that we have significantly reduced the potential attack surface for our container workload and maintained the use of our non-privileged user. Let's just make sure our apiserver still works as we intended it to. We'll start a container based on the new multi image, and then post some data to the country's endpoint again. That looks pretty good. We can't use Docker exec to run the ps utility inside the container as we did before. It's not there because of the minimal file system, but we can view the process from the host's perspective, and we can see that's it's running as the user with ID 500. We'll remove the container before we look at one final alteration to our apiserver image. Let's make one last change. Instead of serving on port 8000, we're going to change this to port 80. This needs to be reflected in the source code, so we need to make a change to the argument of our HttpListener. We also need to make the same change to the EXPOSE instruction in our Dockerfile. As we've made some changes, we need to rebuild our image. Once again, we've made some material changes, so the build cache is invalidated, and the build takes a few moments to complete. Once it's finished building, we can start a new container, and we'll map port 80 from the container onto port 80 on the host. This time we're out of luck. We get a permission denied error message. This is because we're trying to bind our container process to a privileged port, and because we're running as a non-privileged user, it doesn't have permission to do so. Running a container workload as a non-privileged user is definitely our preferred option, but it's not always possible to do this. So how can we elevate privileges and simultaneously mitigate the risk associated with running a container as unprivileged user? Let's find out.

Introducing Linux Capabilities
The problem that we ran into in the demonstration is one that is all to familiar. We want our context to be a regular non-privileged user, but we want to perform a privileged action. Think about changing a password. A regular user needs the means to do this, but only in their specific context, which necessitates that the action must be privileged in order that unsolicited password changes are not made. In the early days of Linux and its Unix forebears, the user was either non-privileged or the all-encompassing authority of the superuser, or root user. Permission to perform privileged actions could be provided by making binary setuid, which gave the regular user temporary privileged access to objects they ordinarily wouldn't have access to. Setuid binaries, however, became a notorious ruse for exploitation. A compromised setuid process provides unlimited access as the root user. The modern Linux kernel provides a means of dissecting the privileges of the root user into a set of capabilities. This provides for a more fine-grained approach to the distribution of privileges. We can execute a process as the root user, but remove the privileges it doesn't need, or we can run the process as a non-privileged user and add the specific privileges it needs. If the process is compromised, the scope of the compromise is limited to just that defined by the relevant capability or capabilities provided to the process. The CAP_DAC_OVERRIDE capability, for example, allows a process to bypass discretionary access control permission checks. The capabilities shown here on this slide are just a small subset of the 37 capabilities supported in Linux kernel version 4. 4. Capabilities are bestowed when a program is executed. Which capabilities are provided depends on a number of factors. Capabilities are defined in a series of sets, and they're applied to running process thread and also to executable files. For simplicity, we'll refer to process threads as processes. So for processes, the effective set is the actual set of capabilities that the kernel makes use of in order to evaluate the fine-grained privileges that are available to a process. The inheritable set is the set of capabilities that can be bestowed by a parent process on a process that it subsequently forks. The permitted set is a superset of capabilities available to include in the effective and inheritable capability sets. A process can add capabilities from the permitted set into the effective and inheritable sets. The ambient set is the set of capabilities that are preserved into a child process after a fork when the executed file is non-privileged. A capability cannot be a member of the ambient set if it is not in both the effective and inheritable sets. The bounding set is a special limiting set of capabilities that access a security mechanism in relation to bestowed capabilities applied through a file's permitted capabilities set. The bounding set is passed on from parent to child. That's process thread capability sets. Let's turn our attention to file capability sets. File capabilities are stored as part of the file's extended attributes, and are therefore only supported by file systems that support extended attributes. File capabilities are used to give a process that executes a non-privileged file the ability to perform privileged actions. It's a bit like a set UID binary, but with finer-grained control. The file capability sets have similar names to the process thread capability sets, but behave differently. The effective set is more of a switch than a set of capabilities. If set, the process thread's set of effective capabilities are the same as its entire set of permitted capabilities. If not set, none of the permitted capabilities will reside in the effective set. The permitted set is the set of capabilities permitted for use in the new process, which override the inherited set, but which are limited by the bounding set. The inheritable set is employed by applying a bitwise and a mask to the process inheritable set to determine which capabilities exist in the permitted set after the file is executed. This all sounds dastardly confusing, but the process thread and file capabilities sets work in conjunction using a small number of simple rules. Let's have a look. What we have to remember is that apart from the init process, every process running on a Linux system has a parent, a parent from which is has been forked and from which it will inherit. When we want to determine what capabilities are bestowed upon a process, then we need to consider the parent's capability sets and the capability sets associated with the executable file of the new process. Capability sets are copied on a fork, and then transformed on program execution. Here's how. The first rule governs how a parent process passes on the capabilities in its ambient set. If the file executed by the child is privileged, its set of ambient capabilities are reset. By privileged, we mean a setuid, setgid file, or a file that has file capabilities. If the file is non-privileged, however, it retains the capabilities in the ambient set inherited from its parent. The second rule defines the new process's permitted capabilities as the bitwise OR of three components. The first operand is the bitwise AND of the parent's inheritable set and the file's inheritable set. The second operand is the bitwise AND of the file's permitted set and the bounding set, and the third operand is the process's ambient set. Remember, the permitted set is a superset of capabilities that the process may draw on for its effective and inheritable capability sets. The third rule determines the process's effective set of capabilities. If the file's effective bit is set, the new process's set of effective capabilities is its set of permitted capabilities. If it's not set, then its effective capabilities are its ambient set. The final rule is a simple one. The new process's inheritable set is the same as its parent's inheritable set. Now I know what you're thinking. This all seems very complicated, and it is, perhaps necessarily so. You need to know a few things about Linux capabilities in order to understand their purpose and their role in managing privileges within container workloads. Fortunately, again, the Docker user interface simplifies the application of the Linux kernel capability mechanism to container workloads. Let's see how.

Docker and Linux Capabilities
So how does Docker make use of Linux capabilities in order to mitigate the risks associated with privilege? We've already discovered that file capabilities are generally used in order to allow a file to be executed with privileges, but as a non-privileged user. File capabilities are stored in extended attributes, but these are removed from Docker images, so file capabilities are not a feature that is used by Docker. Of course, it's possible to get around this by applying file capabilities in an entrypoint script before the container's process is exec'd, but this isn't recommended. The upshot to this is that we can run Docker container workloads as a privileged user with reduced capabilities, but we can't run container workloads as an unprivileged user within enhanced capabilities. This behavior may change in the future if or when ambient capabilities are implemented in the Docker engine. Next up, Docker provides a whitelist of capabilities which constitute a reasonable configuration for many general use cases. What should or shouldn't be in the whitelist is very subjective, and the whitelist was arrived at after many iterations of discussion. As a consequence, a container's effective capability set is the capability whitelist, which can be amended by adding or removing capabilities with config options and is determined before the container is started. When you come to deploy your own container workloads or applications, don't blindly accept the whitelist. Take some time to evaluate the specific needs of your application, and always work to the principle of least privilege. This table describes the list of 14 whitelisted capabilities provided to the privileged user in a Docker container. Some of the capabilities look quite scary, but some of the more pernicious capabilities are missing from the list. The CAP_SYS_ADMIN capability, which is sometimes called the new root, is missing, as is CAP_NET_ADMIN, which has provided some vulnerabilities in the past. Unless you have a very specific use case, are completely aware of the potential damage of an associated compromise, steer clear of capabilities that provide excessive privileges. How do we go about managing container workload capabilities with the Docker CLI? Quite simply, we have two config options available, --cap-add and --cap-drop. We can use multiple options in one command, and we could also make use of the ALL argument to drop or add all capabilities to our workload. If we use the ALL argument, we can subsequently add or remove individual capabilities in the same command. When people talk about Linux capabilities, they generally use the full name, which is always prepended with CAP_. Docker doesn't use this convention, so CAP_SYS_MODULE is simply SYS_MODULE. Before we leave our discussion on Docker's use of capabilities and return to the problem we encountered in our earlier demonstration, let's just briefly touch on a configuration that the Docker CLI provides which gives super privileges to a container workload. The Docker CLI allows the use of the --privileged config option when starting a container, which not only gives the container's process all capabilities, but also access to all of the host's devices. It's as good as giving the container root access on the host itself. Its use should be avoided, but naturally, there are very specific cases which require its use. The classic example is running Docker inside a Docker container. Please use with caution. Now we had a problem earlier trying to run a container workload as a non-privileged user when we tried to bind to a privileged port. We've got some good knowledge about handling capabilities in a Docker environment now, so let's see if we can fix our problem.

Using Capabilities with a Container Workload
This is a continuation from the earlier demonstration in this module where we encountered a problem trying to bind to a privileged port as a non-privileged user. We're going to make use of capabilities to remedy our problem. Our starting point is to change the Dockerfile in order to revert back to a privileged user. This is going to involve removing the commands from the RUN instruction which add the user and group. Additionally, we must remove the USER Dockerfile instruction; otherwise, we'll still be attempting to run the container as a non-privileged user. Those are the only changes we need to make, but of course we need to ensure the changes are reflected in the apiserver image itself. This means a rebuild of the image, which will take a few seconds to complete. With the image rebuilt, we'll start a container based on the image running in detached mode and listening on privileged port 80. Now it would be interesting to see which capabilities our container has. As it's running as a privileged user, then we should expect it to have Docker's whitelisted set of capabilities. There's a useful utility called pscap that we can make use of which provides details of a running process's capabilities. If you're recreating this demo, then details of how to install this on various Linux distributions is provided in the exercise files. We can run this on our Docker host and filter the output it provides so that we just get the capabilities information for our container's process alone. As expected, the apiserver process has all of the capabilities in the whitelist and no more. This would do the job for us, but in actual fact, the container has more capabilities than it requires. Let's remove the container and see about dropping capabilities from the container's whitelist. When we issue the command to run the container this time, we'll use the --cap-drop config option along with the ALL argument, which will remove all capabilities from the container's process. We get the familiar permission error. Our container's process might be running as a privileged user, but it doesn't have the correct capabilities that will give it permission to bind to the privileged port. Let's remove our spent container once again. This time around, we'll use --cap-drop in order to drop all of the whitelisted capabilities, but we'll also use --cap-add in order to add back the CAP_NET_BIND_SERVICE capability, which should enable us to bind to the privileged port we're trying to use. The container started this time, so let's go ahead and use pscap to check its effective set of capabilities. Lo and behold, the single capability bestowed on the container's process is the CAP_NET_BIND_SERVICE capability. It would be handy to make sure that the apiserver is actually functioning and listening on the map port, so let's post some data to port 80 on localhost to check. And there we are, success this time. We've seen how Docker can apply capabilities through its user interface to manage the privileges available within a container workload. Our example is fairly trivial in nature, and when you come to manage the privileges of your own container workloads, things may be more complicated. If it's your own application, you should have a good understanding of its requirements. But if you're attempting to containerize someone else's application, things may not be so clear cut. Use trial and error in order to define an optimal set of capabilities, but be sure to do this with the principle of least privilege in mind.

Module Summary
We've come to the end of this module on managing privileges in Docker container workloads. We've covered quite a lot, so let's consider the key points. The principle of least privilege should be your guiding precept, which will help you to keep your container workloads safe from exploitation. In practice, this means running your containerized workloads or applications as an unprivileged user if you can. And if you can't, make use of the Linux kernel's capability mechanism to mitigate the risk. Take time to understand the capabilities required by your workload or application, and use Docker's whitelist as a starting point. But customize the whitelist to suit the purpose at hand. Whilst capabilities are not a panacea for the exploitation of privileges, their use, in conjunction with other security mechanisms, will significantly aid you in implementing secure Docker container workloads. In the next module, we're going to explore how to further minimize the attack surface from within a container by filtering the system calls available to the container's process.

Limiting the System Calls Available to Container Workloads
Module Overview
Hello again. My name's Nigel Brown, and you're watching a course entitled Securing Docker Container Workloads. In this next module in the course called Limiting the System Calls Available to Container Workloads, we're going to see how to make use of another Linux kernel mechanism in our quest to secure our Docker containers. There is no silver bullet when it comes to securing Docker containers. Our approach needs to involve minimizing the risk of compromise, which you can achieve through reducing the attack surface available to someone trying to exploit our container. We're going to see how we can achieve this using the Linux kernel's secure computing mode, or seccomp, for short. Let's get an overview of the content for the module. To start off with, we'll gain an understanding of the kernel's secure computing mode itself, which we'll follow with a brief demonstration of how it works in practice. We'll move on to see how Docker makes use of seccomp and what's available to you to enable you to customize its use to suit the purposes of your specific scenarios. In fact, a certain amount of investigation is required to match a seccomp configuration with the requirements of an application or workload. And we'll explore how we can then analyze those requirements for subsequent use. To finish off, we'll create a custom seccomp profile for a particular container workload and then apply the profile to a running instance of the workload. At the end of the module, you'll be able to make good use of another important security feature of the Linux kernel in order to better secure the Docker container workloads you're responsible for.

Introducing Secure Computing Mode
The Linux kernel's secure computing mode is a mechanism which allows us to limit the access a process has to the kernel itself by way of the system calls it's able to successfully invoke. Secure computing mode is usually abbreviated to seccomp, and as we'll see shortly, if we're using the extension that makes use of Berkeley Packet Filters, we use the term seccomp BPF. A system call is the means by which an application process communicates with the Linux kernel in order to access the various resources of the system on which it runs. The kernel is the heart of a Linux system and performs many functions in order to ensure everything runs optimally. It runs in the kernel space, which functions with the highest level of privilege, whilst user applications run in the user space, which is significantly less privileged. Whenever an application needs to write to a file, listen for connections on a socket, or clone a process, amongst many other things, it does so via the kernel. The kernel performs the various privileged operations on behalf of the user space process. User space applications employ the system call interface directly, or more usually, make use of a library of subroutines which provide a wrapper to the system calls provided in the interface. The Linux kernel has been designed to provide services, so why should we want to limit the system calls available to an application such as a Docker container workload? Well, if a process is compromised in some way, it may have user space privileges that might be used to make system calls it ordinarily wouldn't use. And if the system call itself can be exploited, it may further compromise the system. And there are over 300 system calls available in the Linux system call interface, most of which any given application workload doesn't require access to in order to perform its necessary tasks. So removing access to system calls that are not required reduces the attack surface available, thereby minimizing the risk of compromise. Currently, the Linux kernel supports two different modes for applying seccomp, strict mode, sometimes called mode 1, and filter mode, sometimes called mode 2. The original implementation of seccomp in the Linux kernel dates back to 2005 where a seccomp BPF, which enables the use of filter mode, was introduced into the kernel in 2012. They are very different, so let's see why. In essence, seccomp strict mode is very limited and inflexible, in terms of what it provides, and seccomp filter mode aims to remedy these limitations. First of all, strict mode only allows a small number of system calls to be made by the calling process. After some initialization, a process is limited to reading or writing to a previously opened file. That's it. In filter mode, however, we have the ability to allow or disallow the whole range of system calls provided via the system call interface. Further, the criteria used by seccomp filter mode in its decision making can include the values of the arguments to the system call in addition to the system call itself. Not so with seccomp strict mode. Finally, in seccomp strict mode, the action taken by the kernel when a call is made to any of the many disallowed system calls is to deliver a SIGKILL signal to the calling process. That's pretty drastic. As we'll see shortly, seccomp filter mode provides a few more options. The upshot to all of this is that seccomp strict mode is not widely used at all and had even been considered for removal from the kernel itself. Seccomp filter mode, however, is used in a variety of applications including Chrome, Firefox, OpenSSH, Tor, Flatpak, and, of course, Docker. Let's find out a little more about seccomp filter mode. When in filter mode, seccomp uses a Berkeley Packet Filter, or BPF, to filter the system calls. Berkeley Packet Filters have been reapplied from their original intended purpose, which was to filter network packets inside the Linux kernel. In fact, Berkeley Packet Filters have a number of applications besides seccomp system call filtering. So what exactly is a Berkeley Packet Filter? A Berkeley Packet Filter is a program written using a BPF-specific instruction set. It's a low-level language, a bit like an assembly language. The execution of the filter is carried out by a BPF interpreter, which is an in-kernel virtual machine. Because BPF filters are based on a low-level language, they present a level of complexity that may be undesirable to a lot of programmers who rightly are more interested in their application. For that reason, the libseccomp library was developed in order to provide a sensible level of abstraction for programmers to use. Seccomp filter mode provides us with more flexibility when choosing the course of action we want the kernel to take when a BPF evaluates an attempt to execute a system call. The BPF returns a 32-bit integer, with the most significant 16 bits containing an action value, and the least significant 16 bits containing potentially any associated data. The return value is determined from the definition of the filter specified by its author, which is you or me, and the evaluation of the rules it contains when an attempt to invoke a system call is made by an application. Let's see what the returned actions can be. The first action value, SECCOMP_RET_KILL, does as its name suggests. It immediately terminates the calling process without executing the system call. This is similar behavior to seccomp strict mode. The SECCOMP_RET_TRAP action value results in the dispatch of a SIGSYS signal to the calling process, which a SIGINFO structure populated with relevant data which can be used for debugging purposes. The system call is not executed. If a SECCOMP_RET_ERRNO action value is returned, the system call is not executed, but the value held in SECCOMP_RET_DATA is returned to the calling process as the errno value. This enables a graceful fail of the attempted system call, which can then be handled by the application. A ptrace tracer can be notified, if one has been configured appropriately, in response to a SECCOMP_RET_TRACE action value. Finally, the returned SECCOMP_RET_ALLOW action value will result in the kernel executing the system call. Now that we have a reasonable understanding of seccomp and Berkeley Packet Filters, let's take a look at a simple example.

Demonstrating the Use of a Basic Seccomp BPF Filter
This demo shows the two seccomp modes we've discussed applied to a simple example. It serves to validate our understanding of the purpose of the secure computing mode mechanism. If you want to follow along with the demonstration, you'll need to make sure your Linux host has the developer version of the libseccomp library installed. Take a look at the exercise files for more details. Let's look at seccomp strict mode first. We have a simple _____ program which will help us see the effects of seccomp strict mode. We'll make use of some system calls which are allowed in seccomp strict mode and one that isn't allowed, and we'll see what happens. Here's some other things to take note of. The program is punctuated with printf statements so that we can chart our progress. Before we enter seccomp strict mode, we'll open the /etc/passwd file for reading. We're unrestricted at this point so there should be no problem performing the open system call. Following the call to open, the program uses the prctl system call to set the seccomp mode for the calling process, which is set to SECCOMP_MODE_STRICT. At this point, we're in seccomp strict mode, which means we're essentially limited to using the read and write system calls only. We're using the write system call each time we use the printf function. The program also makes use of the read system call and reads the first line of the /etc/passwd file. So far, all should be well, as we haven't tried to use any disallowed system calls. But what if we now try to close the password file using the close system call? In theory, the process which invokes our program should get sent a SIGKILL signal by the kernel in response to the attempt to invoke a disallowed system call. If it doesn't, we've got a final printf function call, but we're not expecting to see that executed. Let's see if this works out in practice. First, let's compile the program, which we'll call strict. And now let's run it. No surprises. The program happily steps through each system call until it tries to close the file, when the kernel steps in and terminates it. You can already see how restrictive seccomp strict mode is, restrictive to the point of not being practicable. Let's remedy the problems with our program execution by using seccomp filter mode instead. This variation of the program is essentially the same, but for the fact that we're making use of the libseccomp library to initialize seccomp filter state and to add some rules to the filter before loading the filter for use by the kernel. Initially, we're allowing the use of the same system calls as those allowed by seccomp strict mode, which should give us the same outcome. Let's compile the program, and this time, we'll call it filter. And we'll also need to link the libseccomp library. And now let's run it to see what happens. We've experienced the exact same behavior as before. The program terminated when it tried to make use of the close system call. Because we're using seccomp in filter mode, we can do something about this. Let's add another rule, which allows the close system call in addition to the few system calls we've already allowed. Let's recompile the program and then see if our change has made a difference. This time, we've been able to close the password file, and the program has completed to its natural conclusion and exited with a successful state. This simple demonstration has shown how to apply seccomp in order to selectively limit the system calls an application can make during its execution. We need this same facility at our disposal when we deploy container workloads, so let's move on to see how Docker embraces seccomp when containers are deployed and how we can influence the system calls available to a container workload.

Understanding Docker's Use of Seccomp
If you're wondering how easy or difficult it is to apply seccomp to your Docker container workloads, the first thing you need to know is that you are possibly already using it unconsciously. If the host has seccomp enabled in the kernel and provides a version of the libseccomp library greater than version 2. 2. 1, then the prerequisites are met. The easiest way to find out if Docker is using seccomp is to run the Docker info command, and look for the section that relates to the security options. If it is using seccomp, then amongst the lines of output related to the security options, there'll be one the same as the one shown in this slide. It's important to note that some older versions of Linux which may still have support provision don't meet the prerequisites required by Docker for its use of seccomp. In order to provide a meaningful mechanism for reducing the kernel attack surface from within a container, Docker employs seccomp filter mode, which it implements using libseccomp. In fact, as Docker is written in the golang programming language, it makes use of a golang-based interface to libseccomp provided by the project that maintains libseccomp. Just as it does with capabilities, Docker provides a whitelisted set of allowed system calls, which cater for a wide variety of typical container workload use cases. This whitelist of system calls is applied to every container workload that runs under the jurisdiction of any given Docker daemon, unless a user overrides this default behavior. We'll see how to do this in a moment. So how is this system call whitelist defined? Well, Docker defines its seccomp configuration as a seccomp profile, which is a Docker-specific, JSON-based configuration file. In using a configuration file instead of baking the configuration into Docker's code, it provides us with the opportunity to alter the configuration to suit our purpose. The default whitelist is not going to fit every use case. Let's see how we can modify the application of the default whitelist. In a bid to provide as much flexibility as possible, the default seccomp configuration provided by Docker can be altered in a few different ways. Firstly, the default seccomp profile can be replaced by a custom profile if the need or desire is there. Next, on a container-by-container basis we can choose to run a container unconfined from any restrictions imposed by seccomp. This effectively disables seccomp for the container in question. Or, if relevant, we can override the default seccomp profile on a container-by-container basis. This means that different containers being managed by the same Docker daemon have different seccomp configurations. The location of a custom seccomp configuration profile needs to be provided as the argument to the --seccomp-profile config option for the Docker daemon, which needs to be restarted in order for the custom profile to take effect. This custom profile will be applied to all invoked containers. In this example shown here, we've used the Docker daemon's configuration file to specify the config option and its argument. As we've learned previously, there are other options available for applying permanent changes to the Docker daemon's configuration, so take care not to define conflicting configurations. When we want to affect the seccomp behavior on a per-container basis, whether it's to run one unconfined or to override the default profile configured on the Docker daemon for a specific container, we do this by specifying a config option and argument combination for the Docker container run command. The config option is --security-opt with the argument seccomp=, with either the word unconfined or the path to a valid seccomp profile, depending on what you're trying to achieve. The default seccomp profile commences with a default action for the kernel to take in the event that a container's process attempts to execute a system call. This default action passes error data back to the calling process, which results in a permission denied error message. From here forward, a large number of system calls are whitelisted with the allow action. These system calls are allowed unconditionally. Subsequently, a small number of system calls are given the allowed action, provided certain conditions are met, which are expressed in the values associated with the keys args, includes, and excludes. We'll see how these conditional rules work in a short while. Minority of the total system calls provided by the kernel system call interface are disallowed outright in the default profile. Those that are are disallowed for good reasons, and Docker's documentation provides an explanation in each case. Okay, we now know that Docker applies seccomp on our behalf, and that if we wish to, we can alter the behavior of the default configuration. But how do we define seccomp policy, and more to the point, how do we know what our application needs in terms of system call access in order to function correctly. Let's see if we can answer these questions.

Creating a Custom Seccomp Profile for a Container Workload
To get started with defining a seccomp filter in the context of Docker container workloads, let's have a look at the structure and syntax associated with the Docker seccomp profile. If you need to create a custom profile, then a good starting point is to make a copy of the default profile created by the Moby Project, which maintains Docker. A lot of effort has already been expended on our behalf in order to make this default profile, so we should reuse what we can. More importantly, we can also learn from its content. Our goal might simply be to add or remove system calls from the default whitelist, in which case, we can edit the list of system calls in the name array. This is a straightforward amendment to the default profile. Be aware that if you add capabilities to the default capabilities whitelist, you may end up with additional allowed system calls that you weren't expecting. This is because some capabilities require certain system calls in order to be effective. For example, if you add the CAP_SYS_TIME capability, then the clock_settime system call is added to the whitelist by Docker. A number of system calls are required by Docker to create a container workloads process. If you remove them it can't bootstrap the container. If you invoke a container using the no-new-privileges security config option, however, then the seccomp filter is applied later in the container's execution process, and will require fewer system calls. This config option sets an option in the kernel for the container's process, which simply ensures that when the container's process is started, it won't acquire any new privileges. This enables the application of the seccomp profile at a later stage with less privileges. Unless you have a very good reason not to, you should always start your container application workloads with this security option enabled. It's possible you might want to specify a rule which is based on the context at hand, rather than simply whitelisting a particular system call. You may want to whitelist a system call based on the arguments passed to it, for example. Not only does Docker provide the means to specify conditional rule matching based on the system call's arguments, but it also allows for rudimentary filtering based on set capabilities and the host's architecture. To understand how this works, let's have a look at an example from Docker's default seccomp profile. The rule shown here relates to the clone system call, which is specified in the name's array, which could contain more system calls if the rest of the rule format is applicable to them also. We have just the single clone system call, however. Next, provided the conditions and filters return a match, the action the kernel will take when the filter returns will be to allow the system call. The args attribute deals with matching the arguments for a specific call made by the container's process for the clone system call. Within it, the index is the argument position, with 0 being the first argument. This rule then will test the first argument of the clone system call, which is an integer which specifies the various flags used to inform the kernel how to clone the process. The value and valueTwo attributes are used in the conditional evaluation of the arguments, whilst the op attribute is the comparison operator that will be used to make the evaluation. Essentially, the conditional will be true if the application of the value attribute to the system calls argument as a mask yield what's specified by the valueTwo attribute, in other words, 0. The big number that starts 2080 and so on is the amalgamation of all the constants used for specifying new namespaces, for example, CLONE_NEWPID, CLONE_NEWNET, and so on. The evaluation then will only yield 0 if the clone system call doesn't attempt to create any new namespaces. This is deemed to be a prohibited use case for the clone system call from within a container. Let's have a look at the remaining attributes. A comment attribute is provided for clarity but is benign in nature. The includes and excludes attributes act as filters for the rule, which can relate to capabilities or host architecture. Here in this example, we have two excludes filters. The first specifies that the rule should be excluded if the CAP_SYS_ADMIN capability is set. This makes perfect sense. It's a capability with considerable power, and therefore it should be possible to create namespaces with this capability. Without this excludes filter, our rule would prohibit cloning new processes in new namespaces in all circumstances. The second excludes filter specifies that the rule be excluded if the host's architecture is one of s390 or s390x. This caters for the fact that the flags argument for the clone system call on these architectures is not the first argument, as with other architectures. Another rule caters for evaluating the same scenario on these two architectures. It's possible you may never need to make use of the conditions and filtering aspects of the Docker seccomp profile, but the facility is there should you need it. Let's move on to see how we can determine which system calls our application workloads need. Determining which system calls specific container workloads require is an iterative process rather than an exact science. Some effort is required on your behalf in order to arrive at an optimal configuration. There are a variety of tools that can help in this activity, perhaps the most obvious being the Linux utility strace. Strace monitors the boundary between user space and kernel space, in particular the system calls that are made by the process being traced. In theory, if we exercise a container workload thoroughly whilst its process is being traced, we should be able to determine the system calls it makes use of during its operation. In order to make this work effectively, we need to make a version of our Docker image just for the purposes of determining the container workload's use of system calls. This is because we need to add strace, or whichever tool we might be using, to the container's file system. During normal operation, we don't need or want the image to be polluted with unnecessary content, but we can't achieve our goal unless we have a non-intrusive tool at our disposal. To get the most accurate picture we can of the container workload's requirements, which should exercise the application with no constraints. By no constraints, we mean with all capabilities, no seccomp filtering, and no applied Linux security module profiles. The latter is the topic for the next module. But in order to disable an AppArmor profile or SELinux policy, just use the --security-opt config flag specifying unconfined for the security mechanism. We saw how to do this for seccomp earlier on. The process for creating a custom profile can take some time and needs some care in order to get it right. For the purposes of our demonstration, however, we can get a feel for what's involved by creating a custom seccomp profile for the simple golang-based apiserver we used earlier in the course.

Implementing a Custom Seccomp Profile for a Container Workload
In this demonstration, we'll make use of the strace utility in order to find out which system calls our simple apiserver makes use of. Once we have this information, we'll create a custom seccomp profile and run a container workload with the profile applied. First things first, let's make a copy of the apiserver image Dockerfile, which we'll call Dockerfile. strace, hopefully for obvious reason. We only need to make a couple of changes to the Dockerfile to achieve our goal. Remember, we have a very minimal file system for the image, and for the purposes of our task, we'll need to add some content back. The easiest option would be to change the scratch base image to an Alpine Linux base image instead. We also need to add the strace utility to the file system so that we can make use of it as we attempt to analyze the system calls used by the application. We're almost done. We just need to add the strace command to the image's entrypoint. And whilst there are a whole bunch of different config options we can make use of, we'll use the -c and -f options, which will give us a nice, neat summary of system call activity. Those are all the changes that we need to make to our original image, so we just need to save the file. And now we have to build the new image so that we can make use of it. We already have an image tagged as apiserver:multi, so to differentiate our tracing image, we'll tag it apiserver:strace. Now let's run a container based on this new image. The output that strace generates will be captured by Docker's internal logging mechanism, so we'll get to see this right at the end when we finish exercising the container workload. We should explain some of the config options we're using to run the container, so let's do this one at a time. The first line here should already be familiar. We're running Docker on a Ubuntu host, and it has an AppArmor profile applied, so we're going to disable this for the container by specifying that the container runs unconfined. Similarly, we'll also want to run the container unconfined from a seccomp perspective. We're looking to find out what system calls our application needs to make, and we don't want to take the risk that one of them has been disallowed by the default seccomp profile. And finally, in order to get a true understanding of the container's system call requirements, we'll also add ALL capabilities for the container's process. The container is completely unfettered and has every system call at its disposal. Now that's it's running in detached mode, we can exercise the API, whilst in the background strace is busy recording the activity. It's time to exercise our application. In the real world with a real application, this exercise would need to be carefully planned to make sure that we captured the entirety of the behavior possible for the application. We have a simple container workload, and for the sake of brevity, we'll just exercise a couple of the endpoints in order to gather some data to work with. First, let's post some data to the country's endpoint using the curl utility. And then we'll just query an endpoint just to make sure we get the data back that we expect to get. Quick and easy. Exercising our application workload has been relatively brief. You can expect a much more involved process when you can _____ this exercise in your own realm. Let's stop the container so that we can see the results of the trace. We can get at the trace's output by examining the container's logs using the docker container logs command. We can see straightaway that of the several hundred or so available system calls, our application has used just 29. If all is as we expect, then we should be able to limit the whitelist of system calls in our custom seccomp profile to just this limited required set. In so doing, we will have massively reduced the attack surface available via the system call interface. Let's remove the spent strace container and move on to creating our custom seccomp profile. Starting with a copy of the default seccomp profile, we can edit the configuration to suit our apiserver application. So that you don't have to laboriously watch me do this, I'm going to transport you in time to the finished article. And here it is. The custom seccomp profile is much simpler. Some of the architecture attributes have been removed for convenience, as this container workload is targeted at the x86_64 architecture alone. The system calls turned up by strace have been added to the whitelist array, and all other content has been removed, including the conditional rules, as they don't apply in our situation. All that remains is for us to test it. Now, we have a big old long command to execute this time, so we'll just recap on the security features that we're applying. In addition to the minimal file system associated with the apiserver:multi image, we also have all capabilities dropped via the CAP_NET_BIND_SERVICE capability, the no-new-privileges security option, and our small whitelist of allowed system calls applied via the custom seccomp profile. It looks as if the container has started okay, so let's make sure the application behaves the way we expect with all of the added security mechanisms that we've applied. That looks good. The apiserver has responded in the manner we'd expect, so we'll conclude that it's functioning correctly with the applied seccomp profile. Let's remove the container once again. We'll perform one last test by taking out one of the required system calls from our custom seccomp profile. This should show us that in the absence of all of the required system calls, our container workload will fail to function as we expect. We've removed the listen system call, so let's find out what happens when we start the container. It looks as if the container has started. The CLI command has returned with a container ID. Let's test it. Oh, that's not quite as good. Something has obviously gone wrong. In fact, the container has bombed. Yes, it did start, but very shortly afterward, it exited because it wasn't able to execute the listen system call, which it requires in order to function as it was designed to do. How do we know this? Well, we can examine the container's logs using the docker container logs command. It shows us that the container's attempt to call the listen system call was an action it's not permitted to perform. This brief demonstration has shown us how to go about generating a custom seccomp profile for a specific Docker container application workload, how to apply the profile to a running instance, and the importance of carrying out a thorough analysis of the application's system call requirements.

Module Summary
That marks the end of our investigation of the secure computing mode as applied to Docker container workloads. In addition to acquiring some general knowledge of the purpose of seccomp, you've also learned how you can apply this in a container context. Seccomp is a valuable tool at your disposal for reducing the attack surface available to anyone trying to compromise your container workloads. But we also know that it is a non-trivial exercise to author a seccomp profile for a specific application, whilst the effort to trace system calls by thoroughly exercising a container workload may be large and somewhat complex, the potential reward in terms of defense from attack, is substantial. This module has given you a good introduction to securing your Docker container workloads using the Linux kernel secure computing mode and will enable you to start reducing the potential attack surface of your individual containerized applications.

Implementing Access Control for Container Workloads
Module Overview
Okay, we're about to embark on the final module of this course, Securing Docker Container Workloads. In this final module, which is called Implementing Access Control for Container Workloads, we get to see another powerful Linux kernel mechanism at play. We're all familiar with the standard techniques at our disposal for controlling access to files located in a file system. But is this enough when we want to control the way container workloads access the objects within their environment? Well, we're going to see how Linux security modules can be applied to give us more strength and depth when it comes to securing our container workloads. Let's take a moment to see what we'll cover. To get us started, we'll take in an overview of the concept of Linux security modules and how they can help with access control. Then we'll explore the first of the security modules available to use in a Docker context. That's SELinux. Of course, while we're discussing it, we'll also take a look at how it can be applied to container workloads. Another popular Linux security module is App Armor, and again, we'll explore how it works and how it can be applied to container workloads. And then to finish up, we'll generate a simple AppArmor profile and apply it to a container workload in order to see how Linux security modules help us to control access. At the end of the module, you won't be an expert in SELinux or AppArmor, but you will have an understanding of the concepts behind these security mechanisms and what they can achieve on your behalf. More importantly, you'll have the means to get started with developing access control policy for your Docker container workloads. Let's get going.

Access Control with Linux Security Modules
Access control is all about managing and controlling who can access what on a system. We often refer to the who as the subject and the what as the object. So how does Linux handle access control? Well, Linux inherits many features from its ancestor operating system, which is Unix. One of these features is discretionary access control to file and directory objects using the familiar read, write, execute permissions for the classes, user, group, and other. Access granted to these object is at the discretion of the owner, or user, and is based on the subject's identity. Discretionary access control permissions are relatively course, with just three access modes, and is considered to be very limited in what it can provide by way of controlling access to objects. What if we want to enforce centralized policy-driven access control? What if we want to control access to the other system resources such as network interfaces, a message queue, or a network port? What if we want a more fine-grained approach to access control than is achievable with the Linux kernel's discretionary access control? These are not unreasonable expectations of an advanced operating system like Linux, and has been the driving force behind the development of security mechanisms which provide mandatory access control. Mandatory access control is policy driven, encompasses many more object types, and can be very granular, which makes it far more flexible for managing access control. Mandatory access control sounds great then, but inevitably, there is an associated cost which comes in the form of increased complexity. Given the apparent need for mandatory access control in the Linux kernel, several solutions have been provided for its implementation over a number of years. These solutions have been developed independently for a variety of reasons, but they seek to solve the same general problem. The very first solution, security enhanced Linux, or SELinux for short, was included in the mainline Linux kernel in 2003, while Smack and TOMOYO were included in 2008 and 2009, respectively. The other major solution for mandatory access control, AppArmor, made it into the mainline Linux kernel in 2010. SELinux and AppArmor are the mandatory access control solutions which prevail as the most popular in the Linux domain today. Whilst any of these mandatory access control solutions can be used on a platform where Docker containers are running, Docker CLI only provides support for finetuning the application of SELinux or AppArmor policy. Back around the time the discussion of including mandatory access control into the Linux kernel was first taking place, it was decided that no single solution would be favored ahead of another. And for that reason, mandatory access control mechanisms are implemented as modules via the Linux Security Module framework. When a subject wants to access an object, the kernel first checks whether the request can be granted according to discretionary access controls. If not, access is denied, and the Linux security module is not troubled. If the request passes discretionary access control checks, however, it's forwarded to the Linux security module for checking against the defined mandatory access control policy. A binary response is returned to the kernel, which either grants or denies access based on this response. The policy is defined in user space and then loaded into the kernel using tools specific to the Linux security module in question. So that was a bird's eye view of the Linux Security Module framework, but what about module choice? The first thing to say is that this whole topic of mandatory access control with Linux security modules is vast. It could be a whole course in its own right, and I know you can find specific Pluralsight courses on SELinux and securing Linux services in part with Linux security modules. As we're primarily interested in Docker's use of these modules, our brief discussion here is just limited to the characteristics of SELinux and AppArmor. The comparison doesn't seek to recommend one module over the other, but tries to point out some things you might want to think about when you're deciding how to approach mandatory access control for your container workloads. Here we go. The first thing we should note is that whilst each module is supported in the mainline kernel, different Linux distributions tend to favor one or the other. For example, Fedora-based distributions such as Red Hat, CentOS, and so on, favor SELinux, and this will be the default module for these distributions. Similarly, Ubuntu and the SUSE family of distributions favor AppArmor and ship it as the default module in their distributions. You may take the view that it's sensible to use the default module associated with the Linux distribution you use for you Docker host, and it may be too much effort to retrofit the alternative. SELinux was developed in part by the US National Security Agency, and beyond its basic use, has a comprehensive, multilevel security capability designed for the very highest security requirements. To that end, it is extremely comprehensive in terms of its capabilities, covering a large number of object types. On a system-wide basis, SELinux denies a subject access to an object by default, only granting access based on policy rules. AppArmor, perhaps, is less comprehensive, covering fewer object types. It provides mandatory access control on a per-application basis and, again, denies access to objects unless policy allows it. SELinux and AppArmor differ in the way they classify resources for the purposes of granting or denying access to objects. In SELinux, a security context or label is used to classify files and other objects whereas in AppArmor, resources are classified using path names. Now this is the subjective part. There's no getting away from it. SELinux is complex and difficult to administer whereas AppArmor is relatively straightforward and therefore, easier to learn and manage. So say some. Whether this is true or not, this may not be a factor in your adoption decision anyway. You may be guided by corporate policy, choice of distribution, or by some other criteria. Finally, the SELinux policy that relates to Docker containers is maintained by Project Atomic, the upstream project for Red Hat's OpenShift platform. The default AppArmor profile for Docker is maintained by its own upstream project, Moby. We're going to consider the use of both modules in the context of Docker container workloads, and we're going to start with SELinux.

Using SELinux to Implement Access Control
Alright, let's cover some basic SELinux terminology before we describe how it works when it mediates concerning mandatory access control. We've already talked about subjects, which are the things that want access to the objects that we want to protect. Most commonly, the subject in SELinux is the owner of a process which runs inside a domain. The domain can have multiple processes belonging to it, but each process can only reside in one domain. This domain has a security context associated with it, which is used to determine what the process is allowed to do in terms of accessing objects in the form of resources. The security context is a label which is stored on behalf of the process inside the kernel. That's why you'll often hear people talk about SELinux labels. The terms security context and label are often used interchangeably. Similarly, an object such as a file also has a security context. File objects have file contexts, or types. Again, this security context is used when SELinux determines whether the subject can perform an access action upon the object. The security context label for a file is stored as an extended attribute of the file in question. So the subject and the object have security contexts. How are they used by SELinux in its decision making? Let's see. Firstly, the security context labels for subjects and objects are very similar. They consist of three elements separated by colons. The first two elements are for the SELinux user and role, respectively. And by convention, these first two elements end with _u and _r. The SELinux user is an identity which can be associated with one or more roles. The identity is not necessarily a single Linux user as specified in the /etc/passwd file, but its more likely to be a collection of Linux users mapped to the identity. The system user, shown in this example, is an identity used for running Linux system services such as the Linux audit daemon. The role element is an abstraction of the various actions that an SELinux user can perform when it accesses objects. The actions might relate to those performed by a web administrator, for example. Roles are only meaningful when we're dealing with processes, but in any case, an object always has a role element called object in its label, which acts as a place filler. Again, the system role is for Linux system processes. Finally, and mostly importantly, the third element is the type, which allows SELinux to determine what the process, or domain of processes, is allowed to access in the context of the object. By convention, the type element of the label ends in _t. Access is allowed or disallowed based on the types associated with both the process and the object according to the defined policy and it is termed type enforcement. Let's have a look at an SELinux rule by way of example. Here we have an allow rule, which is being used to enable a process to access a particular resource. The source or domain type of the process is specified in the rule along with a target, or object type. The object type may be associated with numerous different classes of object, so the rule also specifies the object class, a file, in this case. Finally, we've specified the permissions associated with the object class that are granted. We just have the right permission, but there are 20 different permissions available for the file object class alone. That's a lot when you compare it with the three available standard discretionary access control. Through type enforcement then, when a process which belongs to a particular domain with a certain type attempts to access an object with a certain type, if a rule exists for the attempted operation, SELinux will grant access. Otherwise, access is denied. SELinux policy is defined by writing rules that inform the SELinux security engine in terms of what access is allowed or denied. It defines the types for various objects and the types or domains for processes. It defines roles that enable domains to be grouped together, and users that are able to attain those roles. Luckily for you and I, the SELinux community has already written a comprehensive policy that is applied system wide on distributions that use SELinux as its preferred method of mandatory access control. Does this apply to Docker containers? Yes, it does. Let's see how.

Applying SELinux to Container Workloads
For Docker to make use of SELinux for container workloads, it goes without saying that the Docker host needs to have SELinux configured and turned on in its kernel. But that's not all. Docker doesn't enable its SELinux support by default, and the Docker daemon needs to be started with the selinux-enabled config option set true. On this slide here, we're showing how the configuration can be made in the daemon's config file, but these changes can also be made to the daemon's systemd unit file. With these changes in situ, newly created containers will be subject to SELinux policy enforcement. If we haven't got this clear already, container processes tend to get treated a little differently to regular processes that run on a Linux host. It stands to reason then that from an access control perspective, their needs will also be different. This means that SELinux policy has been written specifically for Linux containers and gets installed alongside Docker on certain Linux distributions that favor SELinux. Non-privileged containers are created with a domain type svirt_lxc_net, and their file system content is labeled with svirt_sandbox_file. These type names are soon to become legacy, as they're in the process of being changed to container and container_file, respectively. The change is trickling down from the Fedora upstream project, but an alias will exist for backwards compatibility anyhow. If every container gets these types on creation, this means that every container runs with the same types. Is this what we want from a security perspective? Well, the labels for containers have an extra element at the end to implement multi-category support. This extra element, which is randomly generated by the Docker daemon when it starts a container serves to make each domain and type unique for each container. This enables them to remain fully isolated from each other in terms of access, or allowing them to be securely co-tenanted. Earlier on in the course, we briefly discussed how it might be useful to run containers with enhanced privileges, perhaps sharing some or all of the host's namespaces, for example. In an SELinux environment, these privileges would be unavailable, as the policy would confine any attempts to acquire privileged access in this way. Fortunately, the container-specific SELinux policy provides a super privileged container domain type called spc. It's designed for just those scenarios where you want a container to participate in some of the management functions of the host itself. These use cases are not everyday requirements, but they are valid nonetheless. You should also be aware that if you run the Docker daemon on an SELinux-enabled host with its SELinux-enabled flag set to force, then all containers run as super privileged containers from the perspective of SELinux. Now if we get brave and decide that we want to create our own custom SELinux policy for a particular container workload, then we need a means of applying this policy in place of the default policy. We can do this through the Docker CLI using the --security-opt config option for docker container run. We just need to provide a key value pair as the argument with the string label as the key and the component of the label and its new value separated by a colon as the key's value. If the Docker daemon has its SELinux-enabled flag set true, then all created containers are subject to access control governed by SELinux. If we want to run a container unconfined, however, we can make use of the same CLI config option specifying the word disable as the value to the label key. Simple. That's some of the theory behind SELinux. So let's take a brief look at its implementation in a demo.

Demonstrating SELinux Applied to Container Workloads
If we were to open up the topic of writing custom SELinux policy, we would quickly disappear down a rabbit hole, a rabbit hole we don't have time to explore in this course. Instead, in this demo, we're simply going to have a look at the effects of running SELinux-confined containers. We're operating on a CentOS host that is running with SELinux enabled. We can show this by running the sestatus command, which informs us that SELinux is enabled in enforcing mode, as opposed to passive mode, and that the SELinux policy is based on major version 28. Even though the host is running with SELinux, if we list the security options configured for the Docker daemon, SELinux is absent from the configuration, so we need to turn it on. Let's go ahead and do this by creating a Docker daemon configuration file in the /etc/docker directory. As we saw just now, we have to make use of the SELinux-enabled config option for the daemon, which is a Boolean and needs to be set to true. To apply the changes, we just need to restart the daemon, which will read the config file and apply the change. To make sure that the change has taken effect, running the docker info command again confirms that SELinux is enabled and that any created containers will be confined by policy. Now that we have SELinux enabled, let's go ahead and create two inconsequential containers so that we can check the SELinux types associated with the domain and file contexts for each. In order to check the types, we need the process IDs of each container process, which we can easily acquire using the docker container top command. The process ID for container c1 is 1533, and for container c2 it's 1582. So what exactly does this output show us? Each container gets the same type for its domain, which is svirt_lxc_net. However, the categories associated with the label for each container are different. This is what makes each container unique from an SELinux perspective and enables SELinux to enforce intracontainer isolation. This isolation also applies to the file system content associated with each container, which gets the type svirt_sandbox_file, but with container-specific categories. We also mentioned that we can run super privileged containers. That's super privileged from the perspective of SELinux. We can do this by disabling SELinux on the Docker daemon, or by invoking an unconfined container using the --security-opt config option, or simply by invoking a container which shares a namespace with the host. So if we run a container, which we'll call spc, and specify that it shares the PID namespace of the host using the --pid config option, the container should be configured with a label which has type spc. Again, we can retrieve the container's process ID using the docker container top command. And if we examine the process, this time, the type associated with the process domain is spc rather than svirt_lxc_net. A super privileged container also gets a file context type of container_share instead of svirt_sandbox_file. To conclude our discussion of SELinux in the context of Docker container workloads, let's highlight a problem which you might encounter that relates to SELinux. We sometimes need to mount a volume from the host into the container. This often happens when people are using Docker in their development workflow. How does this work out when SELinux is being enforced? Let's create a file in the directory on the host, which we'll use to mount inside a container. We'll put some random text in the file so that we know that it was created on the host. First, we'll check to see the SELinux type that's associated with the file. The type is user_home with a level s0. Now let's run a container and mount the foo directory into its file system at /foo. If you're not familiar with mounting a host directory as a volume, just use the --volume config option, or its shorthand denotation -v, and specify the source location on the host and the destination in the container. All good so far. So let's see if we can see the whereamifrom file inside the container. Well, despite being the root user, we aren't able to see the contents of the /foo directory. The reason is, of course, because the type associated with the container's domain is virt_lxc_net, which does not have access to objects labeled with the type user_home. Let's exit the container. This is easily remedied. We can use a volume flag appended to the volume argument to inform Docker to relabel the host directory so that it has the same type as those for the container itself. Uppercase Z is what's required, which makes the volume private to the container. A lowercase z would enable the volume to be shared with multiple containers. Now if we try to list the whereamifrom file, we're able to see it. We're even able to write to the file. This time, we'll overwrite the content with something specific to the container. If we detach from the container using the Ctrl+P, Ctrl+Q sequence, we can see the changed content of the file on the host. A word of warning on this, however, this relabeling of the host content is retained after the lifetime of the container. You should also consider that if host directories get relabeled in this way, there may be unexpected consequences. Relabeling content associated with SSH, for example, is not a good idea. If important directories from the host need to be shared with the container, you should consider using a super privileged container by turning off SELinux for the container in question using the label =typedisable argument with the --security_opt config option. We saw how to do this earlier. We're done with Docker container workloads in SELinux. If your working environment is SELinux-based, and you've mastered authoring policy for SELinux, you now know how to apply that policy to container workloads. Let's move on and take a look at AppArmor as a means of applying mandatory access control to container workloads.

AppArmor and Applying Profiles to Container Workloads
Let's turn our attention to AppArmor. Unlike SELinux, AppArmor applies mandatory access control on a per-application basis rather than on a system-wide basis. It's now surprise then that a library of AppArmor profiles have already been written for a number of common Linux applications and utilities. For example, when the CUPS printing system is installed, it's daemon has an AppArmor profile applied in order to limit the access it has to various objects. Additionally, AppArmor comes with a set up abstractions, or building blocks, that can be used within application profiles. For example, an authentication abstraction exists which can be used in profiles for applications that need to authenticate users. This saves reinventing the wheel each time a profile is written, although profile authors need to take care that they aren't unwittingly providing more access than is required. When it comes to authoring AppArmor profiles, as well as the many existing profiles that can be used as a reference, AppArmor provides a set of tools that can be used in the process of authoring an AppArmor profile. Developing an AppArmor profile is an iterative process. Even when it's completed to the required level of satisfaction, application changes can break the profile, which will require changes and further testing. The process starts with the creation of an initial profile for an application either by hand or with the appropriate AppArmor tools. The profile is then placed in complain, or learning mode, whereby AppArmor will detect any violation of the profile's rules. Remember, everything is denied unless there is a rule that allows access to the object in question. In complain mode, AppArmor allows access to the object, but also logs the violation. With the AppArmor profile primed, the next stage involves running the application and exercising it thoroughly. I can't stress enough how important it is to put the application through the entirety of its functions, taking in every edge case that's known. Using the full range of test suites employed to verify the robustness of the application, is a good place to start. As you do this, AppArmor quietly logs all activity. Once completed, the logs produced by AppArmor need to be analyzed, either manually or semiautomatically by one of the tools available for developing profiles. Next, a validation decision needs to be taken as to whether the profile has captured all of the access requirements of the application. Is the profile complete or not? If it's not complete, then rules need to be added or amended within the profile before the profile is updated for another iteration of testing. This cycle of development may take several iterations, but once it's complete, the profile can be set to enforce mode where logging continues, but access control violations are no longer permitted. We've talked quite a bit about AppArmor profiles, so what exactly is in them? Let's have a look at a partial AppArmor profile for the dhcp client. Remember, everything is disallowed except that which is defined in the profile. The first line here, which appears in pretty much every AppArmor profile, is an include statement. It's much like a C programming language include statement, and it provides a whole bunch of useful variables that can be used in the profile. The next line defines the profile name and some optional flags. Normally, a profile is applied to a program by its name, so the profile name must match the path of the program that's to be confined, as in this example here. As we'll see shortly, it's also possible to have unattached profiles that are not associated with any particular program, but are applied to programs using a named profile transition, or a changed profile rule. Let's not get hung up on the specifics just now. Next, we see a bunch of include statements which are inserting snippets of rules associated with particular abstractions. The base abstraction is a commonly used abstraction, and it's understandable that we want to use the nameservice and openssl abstractions with a dhcp client. Following this, there are several capabilities defined for use by the program, cap_net_bind_service, for example. Then we have some network rules which describe the allowed communication modes. The last two snippets define rules for access to specific files. The first two rules here define the program has read-only access to very specific locations in the procfs file system. The rule makes use of globbing, as well as the PROC variable. Finally, we have an access rule for files, which can be found in one of two different locations. The rule syntax enables a single rule to be defined, which covers both circumstances. Once again, we could spend much more time looking at the anatomy of AppArmor profiles, but we need to focus on how they are applied in the context of Docker container workloads, so let's move on and talk about this. Docker's use of AppArmor profiles is kind of similar to its use of SELinux policy, but with some important differences. We previously found out that container-specific SELinux policy is maintained by the open source project Atomic whereas container-specific policy for AppArmor is maintained in a profile by the Moby Project. Like the default seccomp profile, the default AppArmor profile is crafted to account for the general use case, and provides a good degree of access control protection. The default profile is called docker-default. The profile is generated as a file in tempfs when the Docker daemon starts, and is then loaded into the kernel for subsequent use. The file defining the profile is removed. Whereas, we have to turn on SELinux behavior for Docker container workloads, not so with AppArmor. If the host has AppArmor enabled, then the default AppArmor profile will be applied to all container workloads, unless the user specifies otherwise. In fact, it's not possible to turn AppArmor off with a Docker daemon flag, so to speak, as you're able to do for SELinux. What if we want to apply an alternative profile to a container workload or run a container unconfined? Well, of course, the Docker client CLI enables you to do this on a per-container basis. Finally, with similar behavior to that exhibited with SELinux, when a container is invoked in privileged mode using the --privileged flag, the container runs unconfined from an AppArmor perspective. We've seen the use of the --security-opt config option on several occasions already, and it would of be no surprise to learn that we use this option to control the application of AppArmor to specific container workloads. As an argument, we can either specify apparmor=unconfined to disable access control checks for a given container workload, or apparmor=, followed by the name of an AppArmor profile we want to apply, in place of the default profile. Okay, remember our simple apiserver from earlier in the course? Well, we're going to generate and apply a custom AppArmor profile tuned specifically for the application.

Generating a Custom AppArmor Profile for a Container Workload
In this demo, we're going to create a custom AppArmor profile for the simple apiserver we used earlier in the course. We could do this in a number of ways, but in this demo, we're going to use a utility called bane in order to generate the profile for us. As our apiserver is very straightforward, and we have a general idea of its access requirements, bane is just the tool we need. So let's get started. If you're following along with the exercises, then details of how to get hold of and install bane are provided in the exercise files. Bane is pretty straightforward to use. We need to author a simple TOML base config file specifying our high-level requirements, and bane does the rest, like generating the AppArmor profile and then getting it loaded into the kernel. The only config option you might want to use is the -profile_dir option, which customizes the location where the AppArmor profile is saved to. Its default location is a subdirectory called containers, which will reside in /etc/apparmor. d, where you'll also find the rest of the AppArmor policy, including the abstractions and tunables we discussed earlier. In the demo, we'll use the default location provided by bane. So what do we put inside this TOML-based config file? Here's a sample from the GitHub repository that hosts bane. The file commences with a name for the profile. AppArmor profiles for Docker containers aren't specified with a file system path, but with a name instead. It's sensible to name it after the purpose of the container's image. In this case, it's nginx-sample. Bane prepends the name with the word docker when the profile is created. After the name definition, there are some filesystem components, a set of ReadOnlyPaths, paths that will generate an audit message on write, paths it's possible to write to, and executables that are explicitly allowed or disallowed. To final sections allow for specifying the allowed or denied capabilities and the access a container has to the network. AppArmor policy can be, and often is, significantly more complex than this, but for the purposes of this demo, bane serves us well. Going to take a look at a basic TOML configuration for the simple apiserver we've been working with. The file is called apiserver. toml. We specified the name as apiserver, which means the AppArmor profile that bane will generate will be called docker-apiserver. The requirements of the apiserver application are pretty straightforward. We know that it requires the privileges associated with the CAP_NET_BIND_SERVICE capability, as it needs to make use of a privileged port. We're also going to allow the CAP_NET_RAW capability for the time being. In terms of networking, we know that we'll be using a TCP socket, so we'll specify this using the Protocols key. Again, for the time being, we will allow the ICMP protocol and explicitly allow raw sockets, in addition to stream sockets, which is implied with the definition of the TCP protocol. In the real world, of course, things are going to be more complicated than this. Writing an AppArmor profile for a complex application will require a comprehensive understanding of AppArmor policy definition and an intimate knowledge of the target application. The application would need to be exercised in all of its functional areas whilst being profiled using an AppArmor profiling tool. Even then, expect to do some handcrafting and finetuning of the profile. Okay, let's instruct bane to create the profile for us, which it will place in the directory /etc/apparmor. d/containers. Bane will also load the profile that's created in enforce mode. It would be a good idea to check this. The aa-status command gives us an overview of the state of AppArmor, including the profiles that are loaded, differentiating between those that are in enforce and complain modes. We can see that both the docker-default profile and our docker-apiserver profile are loaded in enforce mode. We can view the profile that bane created for us, which we'll find in the container's subdirectory in /etc/apparmor. d. It contains rules generated from the TOML config file. Near the top, we have two network rules which allow TCP and ICMP protocol traffic for the inet domain, followed by a rule which denies the use of packet sockets. Two general rules allow full access to files and the ability to unmount file systems. The explicit whitelisting of the two capabilities we require can be found a little further down the file, along with a set of rules that deny access to specific locations within the /proc and /sys file systems. Some of this additional content is deemed important for containers, and is largely borrowed from Docker's default AppArmor profile. Let's go ahead and test the profile by creating a container and applying the new AppArmor profile to it. Before we do this, we'll review a single change which has been made to the apiserver Dockerfile, just so that we can demonstrate the effect of running the container confined by the AppArmor profile we created. Instead of building the container from scratch, we've used the small Alpine Linux image, which gives us some tools and utilities, one of which we'll be making use of. If you're following along with the exercise files, don't forget to rebuild the image from the revised Dockerfile. We'll start a container with the default whitelist, the default seccomp policy, and our custom AppArmor profile. Remember, the capability whitelist excludes the CAP_NET_BIND_SERVICE capability which we need for the container to make use of the privileged port. In theory, the AppArmor profile has this covered, as we explicitly set this capability to be allowed. It looks as if the container started without any problems, so let's try and interact with it using the exposed socket. We'll just make a couple of API calls, which should be sufficient as a proof of successful operation. That seemed to work just fine. So generating and applying the AppArmor profile has allowed us to continue to use our application. But how has it helped in protecting the container workload? To find out, let's first remove the apiserver container. Now let's create a new container from the same image confined by the Docker apiserver AppArmor profile, but we'll override the container's entrypoint so that we run the ping utility instead of the apiserver binary. We'll ping one of Google's public DNS servers. Again, this works just fine. The ping utility needs the CAP_NET_RAW capability, access to the ICMP protocol, and an explicit AppArmor rule allowing access to raw sockets. If we take away any one of these, then an attempt to use the ping utility will fail. Let's test this out. We'll make a simple amendment to the configuration file and comment out the explicit network rule that allows the creation of raw sockets. With the configuration saved, let's get bane to regenerate our AppArmor profile and get it loaded into the kernel. With that done, let's try another test of our container with an overridden entrypoint with a new profile applied. This time, we get a Permission denied error message for creating the raw socket. It's AppArmor that has intervened, courtesy of the restriction we placed in the profile to deny access to raw network sockets. SELinux and AppArmor are two powerful security mechanisms that the Linux kernel provides to enhance the control of access to system objects. The power they provide, however, comes with a warning attached. To get the best from these comprehensive security features, you need to invest a significant amount of effort to first get to grips with the security modules themselves, and then to profile your container workloads to within an inch of their lives to ensure you cover every edge case possible. It's not for the fainthearted, but it could make the difference between retaining the integrity of your container workload or not.

Course Summary
Not only have we arrived at the end of this module, but we're also at the end of the course. Before we finish up, I want to leave you with a couple of important practical considerations. The first relates to defense in depth. It won't have escaped your attention that there is a degree of overlap between these various mechanisms for securing container workloads. For example, you now know that you can manipulate the capabilities available to a container workload directly, using the Docker CLI. But you also know that whitelisting certain system calls with secure computing mode may result in some capabilities being added to the whitelist. And both AppArmor and SELinux policy can, and probably will, define certain capabilities. So which security mechanism is the correct one to use? Is there an order of precedence that should be employed? The answer is to use them all in order to provide defense in depth. After all, these security mechanisms are implemented in software, and as we know, software is not infallible. Define your containerized applications' requirements of each security mechanism independently. If they overlap, it doesn't matter. If the container can be compromised by weakness in one of these mechanisms, hopefully it will be stopped in its tracks by one of the others. The second point of consideration concerns the application and implementation of the security mechanisms themselves. Our discussion has centered on the Docker engine, and we've discussed the application of security mechanisms by way of the Docker daemon or through the use of the Docker client CLI. But Docker hands off the management of the lifecycle of a container to a container runtime called containerd, which, in turn, makes use of a low-level container executor called runc. This means that the implementation of the various security mechanisms is performed by runc when it bootstraps any given container, and is not performed by the Docker engine itself. Why is this important? We'll see in a moment. When container application workloads are deployed in production, they're usually deployed across a cluster of nodes courtesy of an orchestrator such as swarm. While swarm is part of the Docker engine, it has a specific API, which is distinct from the one used to run individual containers. Swarm doesn't deal with containers, but with services instead, which are implemented as containers. Unfortunately at the moment, swarm doesn't have a life for like equivalence for the Docker client CLI security options although security configuration applied to the Docker daemon still applies. A Moby subproject is ongoing, whose aim is to provide a consistent, abstracted user interface to the various security mechanisms that we've discussed, which can be applied to swarm services. The picture gets further complicated when we consider the various configurations of the more prevalent orchestrator, Kubernetes. Again, it uses an abstraction called a pod, which is a collection of containers. The Kubernetes API allows for the definition of a pod security context, which allows for the specification of some of the various security mechanisms that we've discussed. Knowing how the various security mechanisms are implemented and how they can be applied for each platform means that you can still benefit from defining custom security policy for your container workloads, even if you're not working with the Docker engine directly. That's it. We're at the end. I'd really appreciate some feedback on the course, and if you can rate it, that would be great. If you have any questions at all, feel free to post them on the course discussion board, and I'll reply to them as soon as I'm able. And if you'd like to get in touch directly, you can visit my blog site at windsock. io, where you can find my contact details. Come find me on Twitter and LinkedIn. I hope that you found the course useful, and I'll see you again soon.
