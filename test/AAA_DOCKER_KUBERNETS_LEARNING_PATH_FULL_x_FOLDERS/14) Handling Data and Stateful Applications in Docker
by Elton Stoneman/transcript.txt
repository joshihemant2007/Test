Docker containers run your apps in lightweight units on cloud or on-prem, but how does data work inside containers? In this course, Handling Data and Stateful Applications in Docker, you will gain the understanding of how to manage application data in containers which can run anywhere. First, you will learn how Docker builds the container filesystem and how you can persist data outside of the container lifecycle. Next, you will discover Docker volumes, and the ability to use shared storage with volume plugins. Finally, you will explore how Docker itself uses data, and how you can make the best use of your storage by optimizing Docker images. When you are finished with this course, you will have the skills and knowledge of Docker storage that you need to run stateful applications in containers.

Course Overview
Course Overview
Hey, how you doing? My name's Elton, and this is Handling Data and Stateful Applications in Docker. I've been running containers in production for many years, and I've learned that stateful applications, even databases, run just fine in Docker. You get all the benefits you expect with containers; the agility to run anywhere and scale quickly, to have consistent deployments in every environment, and enhance security, and you also get the benefit of decoupling your app from the physical storage that it uses, which opens up new ways of designing and running your applications. You need to understand how storage works in containers if you're going to make full use of Docker for your application estate, so you can confidently run all your apps in containers, everything from stateless APIs to replicated SQL databases. This course gives you the understanding. I'll explain how Docker builds the container filesystem from different physical sources, and how you can use them to separate your data lifecycle from your application lifecycle. You'll learn how to run stateful apps in highly available, scalable configurations on Docker servers and Docker Swarm clusters, and how to build optimized Docker images to make the most effective use of your disk. I'll be using Linux and Windows containers running on local machines and in the cloud. So you'll see how storage works across a range of different setups, and I'll use a variety of demo apps; from simple web applications which use disk as a cache for data, to replicated SQL and NoSQL databases. By the end of the course, you'll understand everything you need to know about storage in containers, to use Docker effectively for stateful applications. So stick with me for the next two hours, and learn all about handling data and stateful applications in Docker.

Understanding Storage in Docker Images, Containers, and Volumes
Course Introduction
Docker Containers are meant to be short-lived. When you deploy a new version of your app, or apply an operating system patch, you don't connect to the container and run an update. Instead, you build a new container image with the latest software, kill the existing container, and replace it with a new one. So what about application state? If your app writes data inside the container, what happens to that data when the container gets replaced? Welcome to Handling Data and Stateful Applications in Docker. My name's Elton, thanks for joining me in this Pluralsight course. Over the next couple of hours, you'll learn all you need to know about data storage in Docker containers on Linux and Windows. In this module, I'll focus on the container file system. You'll learn that the dist inside your container is actually a virtual files system that Docker builds from different components, and you'll see how you can make use of that for stateful applications. Next, I'll look more closely at one of those file system components, Docker volumes. You'll learn how you can use volumes to get high availability for applications, even if they weren't designed to run in a dynamic container platform. Then I'll cover Volume Plugins, which give you shared storage across your cluster, and you'll see how that supports modern stateful applications, which are designed for dynamic environments. Lastly, I'll cover storage in Docker registries, and in Docker itself. You'll learn where the data for images and containers is physically stored on the server, and how you can configure and manage those storage locations and drivers. By the end of the course, you will have a thorough understanding of data storage in Docker, and how to make use of that in stateful applications. So let's get started. Next, I'm going to jump straight in and start looking at the container filesystem.

Understanding Image Layers and the Layer Cache
Docker containers provide a virtualized environment where your application runs. That environment has a file system with a single disk. A Windows container has a C drive, and a Linux container has a disk device mounted at SDA1. The file system is populated with the contents of the container image, so when you run a Linux container from the Nginx image, you'll find the default website content on the disk at /usr/share/nginx/html. When you run a Windows container from the IIS image, the default website content is at C:\inetpub\wwwroot. You package your own applications with a Docker file, which you build into an image that has all your application content. You might package a website by starting from the official Nginx or IIS image, and then copy it in your own website content. When you start a container from your website image, you'll see that the filesystem is a combination of the original image with the web server deployed, and your own website content. It appears as a single filesystem inside the container, but Docker actually creates that filesystem by joining several pieces together. A Docker image is logically one thing. It's a snapshot of a particular version of an application, and it's the portable unit that you share on a Docker registry so you can run your app in a container anywhere. But the image is physically stored in many small pieces called image layers. Each line in a Docker file produces a separate image layer, and when you run a container from the image, Docker joins all those layers together to present the file system. So the first part of that virtual filesystem that the container sees is the product of all the image layers. This may seem like a low level concern, but actually it's a fundamental thing that you need to understand if you're going to make full use of Docker. It's fundamental because the image layers are read-only. That means they can be shared between different images and different containers, which is one of the reasons Docker is so efficient with storage. Let's see that in a demo.

Demo: Image Layers in Linux Containers
In this demo, I'm going to look at Docker images for Linux containers. You'll see how the contents of the image become the container's filesystem, and how the image is actually stored as read-only layers, which can be shared. I'll start with a very simple Nginx container. I'm connected to a Linux Docker engine here, so I can start Nginx with docker container run -d -P nginx; -d puts the container in the background, and -P publishes random ports so I can browse to the web application in the container, and I'm going to repeat that command and start a second separate Nginx container. Now, docker container list gives me the running containers, and I can see the ports that Docker has assigned. I'll browse to each of these, so port 32770 was my first container, and port 32769 was my second container. They both show the same default Nginx website, and this is the HTML content that's inside the container image. I can connect to one of my containers using docker container exec, with the -it flag to make it interactive, and then run bash. So now I'm inside the container, and I'll list the contents of usr/share/nginx/html/, and here's the HTML file for the default website. Both my containers have exactly the same filesystem. I'll exit out of this container, and run the same command to connect to the second container, and in the new bash session, list the directory contents, and it's the same set of files. I have the Nginx image on this Docker engine, which is 109MB. That's the logical image size, and it's not necessarily the physical amount of storage that the image is using. We'll see that if I build a new image on top of Nginx. This is a very simple Docker file which is based on the Nginx image I already have. I'm just copying in a new HTML file to the web directory, and this will overwrite the default HTML file from Nginx; docker image build executes the Docker file, and now I have my own website image. That image is also 109MB, so it seems to be a copy of the Nginx image, but actually it's not, because the layers are shared between the Nginx image and my own web image. I can see now by inspecting the images, docker image inspect for the nginx image gives me a whole lot of data including the imaged layers and their unique sha256 IDs. These three layer IDs start with ef6, ad5, and ece. Now actually, the Nginx image is based on the Debian stretch image, and if I inspect that image, you'll see there's a single layer with the ID ef6. So the Debian image layer is the first layer in the Nginx image. Run docker image inspect on my own web image, and there are four layers. The first is the ef6 Debian operating system layer, then the next two match the IDs from the Nginx image, and the final layer is from the copy command with my own web content. So the logical image size is the combined size of all the image layers. In this case, it's 55MB from the OS layer, plus 54MB for the Nginx runtime layer, plus just a few bytes from my own application. If I have other images that use Debian or Nginx, then they will share the same base image layers too. That makes for very efficient use of disk and network bandwidth because Docker doesn't need to download and store image layers if it already has them from a different image. That's especially important with larger application images, which we'll see next, when I do the same demo with Windows containers.

Demo: Image Layers in Windows Containers
In this demo, I'll look at Docker images for Windows containers. Conceptually, it's the same as Linux containers, and you'll see that the file system comes from the image, and that image layers are shared. You'll also see why that's much more important for Windows images. So here I'm connected to a Windows 2019 server running Docker. The Docker API and the command line is the same for all the platforms Docker supports, and I can start a simple web server with docker container run -d -P, and then use the IIS image from Microsoft's container registry, mcr; and I'll run a second container from the same image. Those containers are mapping ports 50095 and 50093 on the host, and when I browse there, I'll see the default IIS homepage with the same content being hosted by each of my containers. I can execute a command inside the container, and connect to it interactively. This will open a PowerShell session in one of my IIS containers. The default HTML file is here in C:\inetpub\wwwroot, and that comes from the container image; and in the second container, I can execute into a PowerShell session and see that the same file location has the exact same files. I have a Docker file here that copies a new HTML file into the default location on the IIS image. So this is packaging a custom website to run in a Windows container. I build that in the usual way with docker image build, and that gives me my web application image. I'll list my IIS images, and you'll see that there is one tag. This image is based on windowsservercore, which is pretty much a full Windows Server deployment that can run pretty much any Windows app. The cost of that backwards compatibility is the image size. You see the IIS comes in at over 4GB, my own web application image is also 4+ GB, but almost all of that is shared from the layers in the IIS image; docker image inspect works in the same way for Windows images. The base windowsservercore operating system image has two layers, c4d and 4db. The IIS image starts with those two layers, and then adds three more for the web server setup. My image starts with the two windowsservercore image layers, then the three IIS image layers, and then my own application layer. Another web app based on IIS would also share those five image layers, and just have a different application layer on the very top. So if the container file system is built from shared image layers, and those layers are read-only, how can you write data inside containers? I'll walk through how that works, next.

Persisting Data in the Container Writeable Layer
Image layers are the first part of the container file system, and they're shared between multiple images and multiple containers. The second part is the writable container layer, and each container gets its own writable layer. If I run two containers from the Nginx image, they both start with the exact same contents in the file system. But if I write a new file in container one, that only lives in container one's file system. It doesn't affect the data in the image layers and it doesn't exist in the second container. That file is stored in the writable layer for container one, but Docker manages all those layers so it looks like a single file system inside the container, and that virtual file system uses copy-on- write logic to give each container dedicated storage, but still share as much as possible from the read-only image layers. Copy-on-write lets containers modify data from the container image without changing the image layers. So if container one updates the defaults TML file from the Nginx image, Docker actually copies that file into the container's writable layer, and stores the updated version in there. Docker builds the container's filesystem by joining all the layers together, and files in upper layers hide files in lower layers. So the updated file in the writable layer shows in container one, but the original file still shows in container two. I'll show that working in practice in the next demo.

Demo: New and Updated Data in the Writeable Layer
In this demo, I'll look at the container writable layer. You'll see how Docker stores files in the writable layer separately from the image layers, and how it uses copy-on-write to let the container edit data without changing the actual image layers. I'm using Docker on Linux for this demo, but the same principles apply to Windows containers. I'll start an Nginx container in the background. The container's running, and I'll connect to a bash session by running docker container exec -it. Now, I'm connected to the container, and I can see the contents of the filesystem root with ls. This is the filesystem from the image layers. I'll create a new file by echoing some text, and writing that out to a file in the root. That file's there now, and I can see that it's just a few bytes. I'll also edit an existing file from the underlying image. Here in usr/share/nginx/html is the default index page for the website, along with the default error page. I'll just overwrite that index HTML with some different content. The file looks like it's been replaced. So now I'll exit out of the container and run docker container inspect to see the details of that container. There's a lot of information here, and it's in JSON, so it's easy to automate working with the Docker command line and the API. The part I'm interested in is the GraphDriver section, which is the details of the filesystem for the container. I'll go into more detail on this in the module on managing storage, later in the course, but for now, I'm just interested in the upper dir value. This is the location of the container's writable layer on my Docker host. I need admin privileges to see it, but I can work with the container's filesystem from the host using this layer. When I list contents, I see the new txt file in the root, and I can read the same file contents that the container sees. There's also a directory path for the Nginx content that the container updated. There's only one file in here, the modified index.html, which Docker copied up to this layer when it updated it in the container. Here's the new HTML content, but inside the container, there are two files of this path; This modified index page from the writable layer, and the original error page which comes from the Nginx image layer. It's all transparent to the container, which just sees a normal writable file system, but it means Docker is hugely efficient at building, distributing, and storing applications packed into images. The major disadvantage in using the writable layer is that it has the same lifecycle as the container, so when you remove your container, you lose all of its data as well. Next, I'll look at avoiding that with the last data source you can use for the container's filesystem, Docker volumes.

Using External Storage with Docker Volumes
Volumes are the third source for data which get serviced as the container's filesystem. You have the read-only image layers, plus the writable container layer, plus 0 or more Docker volumes. Volumes are just units of storage. You can create a volume and attach it to a container when it starts. The volume is stored outside of the container, but it appears to be a directory path inside the container, just like the read-only image layers, and the writable container layer. There are two advantages to using volumes. First, the volume has a separate lifecycle to the container, so when you replace your container with a new container from an updated image to deploy an OS update or a new version of your app, you keep your application's data. You'll remove the old container, and start the new container attaching the same volume. So when the app starts, it has all the data written by the previous version of the app in the previous container. The second advantage is that Docker has a plugin system for volumes, so the actual data can be stored on any system that provides a plugin. That means you can have volumes physically stored on storage devices in the datacenter, or manage storage services in the cloud, and use them in the same way. I'll cover volumes and plugins in a lot more detail over the next couple of modules, but I'll start with a simple example to show you how volumes work.

Demo: Containers with Volume and Bind Mounts
In this demo, I'll look at the basics of Docker volumes. I'll create a container with a volume, and see how that looks on the host, and then I'll show you how you can map a directory on your Docker host to a separate directory inside your container. The volume features in Docker are the same for Linux and Windows containers, and I'll use both in this demo. To start with, I'll run a Windows container based on Nano Server. You can attach volumes to containers using the --volume flag or the --mount flag. The mount flag is the newer option, and it has a few more features. What I'm doing here is running a container and mounting a volume with a target path of C:\vol1, so that's how it will appear inside the container. I'll be running a lightweight Nano Server container, and that runs in a restricted user account by default, so I need to run with the elevated ContainerAdministrator account, which is already there in the image if I want to work with volumes. So now I'm inside the container, I can run a directory list, and see the volume is just a normal directory called vol1, and I'll change into that directory and create a file with echo. Now, I'll just check that file exists, here it is, and this is the content. I'll create a similar file in the root of the C drive so we can see the difference between the writable container layer and the volume. That file's there too, and it has some content, so now I'll exit the session, which stops the container. There are no containers running now, but if I list all containers, I see my Nano Server container still exists, it's in the exited state, but the filesystem for the container is still there. I'll inspect that container, and I get the same sort of output for a Windows container that we've already seen for a Linux container. Here in the GraphDriver section, I can see the path to the container's writable layer on the Windows Server host; and here in the Mounts section, I see the path to the volume mount, which is serviced inside the container as C:\\vol1. Now I'll remove that container, and if I look at the contents of the writable layer on the host, that directory doesn't exist. The file that I wrote in the C drive of the container is gone, because the writable layer gets deleted when the container gets removed. But if I check the volume mount on the host, that directory still exists, and the file I wrote in the container is still there. Because this is a Docker volume, it exists outside of any containers, even though I created this one when I started a container. Volumes are first-class Docker objects, and the path I can see is actually the volume ID. I can start a new container and mount that existing volume by specifying the source parameter as the volume ID, and I can map it to a different target location inside the new container. This time I'll use C:\vol2. This is a completely different container, but the vol2 directory is here, and inside it has the same contents as the vol1 directory in the first container. Volumes are really useful, and you can even share them between containers, but these volume IDs with complex paths on the host aren't so easy to use. An alternative option is to mount a known directory from the host into the container. I'll switch to Linux containers here, this script just points my local Docker CLI to a remote Linux Server using environment variables. The syntax and behavior is exactly the same on Windows and Linux, the run command is the same; I'll start an interactive container with a mount, but instead of using a Docker volume, I'll use a bind mount, which lets me mount a path from the host into the container without using an explicit volume object. There's a directory called bind-mount on the root of my Linux Server, which already has some data in it, and I'll service that as /mount inside the container, and I'll use the official ubuntu image. So here I can see the mount directory, and these are the files which already exist on the host. I can create a new file in here because the mount has read and write access by default, and I can see this file on the host too. But write access is configurable, so I can exit this container and start a new one with the same command, but adding the ro option to the mount flag, which gives me read-only access. Now, inside the container, I can see all the data in the bind mount. The original files from the host, and from the previous container, but if I try and create a new file, then I get an error, because this directory is read-only. So that's the basics of volumes and bind modes. They're really the foundation to making stateful applications work well in containers. Next, I'll summarize what we've learned before we move on to more detail.

Module Summary
In this module, we've learned that the file system in a container is a virtual file system, which Docker projects from two or more physical sources. Firstly, there are the image layers, which provide the whole filesystem structure from the Docker image. Those layers are read-only, which means they can be cached and shared between images and containers, which makes for very efficient use of storage. Then, every container has a writeable layer, which it uses to store new data or to modify data from the image layers. The writable layer users copy-on-write logic, so if a container updates a file from an image layer, the file gets copied into the writeable layer, and the edits happen there. The writable layer has the same lifecycle as the container, so the data disappears when the container gets removed. Containers can optionally have Docker volumes attached to get persistent storage outside of the lifecycle of individual containers. Volumes can be simple mounts, surfacing a directory on the Docker host inside the container, or they can be explicit Docker volume objects, which can use a variety of physical storage options. Coming up next, we'll learn how volumes can support stateful applications, which weren't designed to run in dynamic container platforms. I'll use a relational database running in a Docker cluster, to demonstrate how that works, and that's in Deploying Stateful Applications for High Availability with Docker Volumes; the next module in Handling Data and Stateful Applications in Docker.

Deploying Stateful Applications for High Availability with Docker Volumes
Module Introduction: Using Storage in Container Clusters
Container platforms provide a dynamic environment where modern apps can scale up and down almost instantly, and operate unattended because they are self-healing. Stateful apps with older and tier architectures weren't designed to run in that kind of environment. But with Docker, you can run existing stateful apps in containers on a cluster, and get a lot of the uptime and scale benefits that you get with your modern apps. My name's Elton, and this is Deploying Stateful Applications for High Availability with Docker Volumes; the next module in Pluralsight's Handling Data and Stateful Applications in Docker. In this module, I'm going to look at how Docker volumes give you high availability for stateful apps running in containers across a cluster of servers all running Docker. I'll be using Docker Swarm for my demos, but the same principles apply to other orchestrators like Kubernetes. There are two ways to use storage in a cluster. The first is to use local volumes on the nodes, which means you are using each server's local disk for volume storage. This is the simplest option, and it means your volume storage has the same IO characteristics as your server. If you're running with fast, solid state disks, you'll get fast storage for your volumes. The disadvantage is that there is no replication of data between volumes, so each container only sees the data that's on the volume from the server where the container is running. Docker Swarm and Kubernetes don't provide data replication. So if you need that, then it has to happen at the application level inside your containers. There are certain types of apps which have this functionality built in. Relational databases are a good example. The replication logic works just fine across containers, but they need to be deployed on your cluster in a certain way. That can limit the scalability of your app and also limit the elasticity of your cluster. But it does make it suitable for traditional applications which weren't designed to run in a dynamic environment. The second approach is to use a volume plugin, which gives you cluster-wide storage. So all the nodes in the cluster can access the same shared storage. That gives you more flexibility with modern apps that are designed to be dynamic, and I'll cover that later in the course. In this module, I'm going to use local volumes. I'll start by running a simple stateful app on my cluster using local storage for the volumes, and we'll see how that looks.

Demo: Running Stateful Apps with Local Volumes in Docker Swarm
In this demo, I'm going to run a stateful app which uses volumes. I'll start by running locally with Docker Compose, and then deploy the same app to a Docker Swarm cluster. I'll use local volumes on the node for storage, and you'll see what kind of availability that gives my app. The app is a very simple web application which uses the file system to cache some data that it receives from an API call. The homepage just shows the server name, the API response, and whether or not the response came from the cache. This is the first hit to the website, so the response isn't cached, but when I refresh, the response does come from the application's cache, and that cache is just a file stored in a known location. I'm running the app in a container on a standalone Docker server right now, this is the Docker Compose file that I used to deploy it. In the service, I have a mount, which uses a Docker volume as the source, and the target is the cache directory, which is the location where the app reads and writes that cache file. The volume is explicitly defined here in the Compose file too. Docker container ls shows me I have just the app container running, and docker volume ls shows me the named volume, which Docker Compose created when I deployed the application. This is a single server, so I have no failover, but if I remove the container with the -f flag to force removal, and then run docker-compose up -d again, this created a new container which will use the same volume from the previous container. Refresh the web page, and I'll have a new server name, that host name is actually the container ID, but the same API response, which has come from the cache. So on my dev machine, the same volume is being reused between containers. I can use the same Docker Compose file to deploy the app to a cluster. My cluster has one manager, and four Linux worker nodes. For this demo, I'm using a Linux app, but local volumes on Windows work in exactly the same way. I'm also using Docker Enterprise here, but that's built on top of the open source swarm, so you can do all this with Docker Community Edition too. In this session, I've connected my Docker client to the cluster; docker node ls shows me all the nodes in the swarm. This is the directory with my compose file, and it's the exact same file that I've used on my local Linux machine. I'll use docker stack deploy to run this on the cluster, specify the path to the compose file, and I'll call the stack api-consumer. Docker service list shows me that the app is running, and I have just one container across the whole cluster. I didn't specify a number of replicas in the compose file, so the default is to run in a single container. I can browse to any node in the cluster, and Docker will direct the request to my container. This address is an Azure load balancer across my cluster. This is the first call to the app, so the cache is empty, and the website loads the data from the API. Refresh the page, and I get the same response, but now the data is coming from the application cache stored in the volume. I'll go back to the UI and see what's running. The service is running on one container, which is on host worker-linux-2. Because I'm using local volumes, there's just one volume for the service, which is on worker-linux-2, where the container was created. So what does high availability look like here? I'll go back to the service and restart it, which will shut down the existing container and start a new one. I can see the original container is in the exited state, and the new container happens to have been started on worker-linux-2 as well. So this container will use the same volume as the original container. I'll refresh the website, and now you'll see there's a new server name, but the original API value, and this shows me that the new container is reading the cached value that was stored by the old container. If I restart the service again, now this time, the container is running on a different node, dtr-linux-1. Back to the website and refresh, I get a new server name and a new API response, which has not come from any cache. We can see what's happened here if I look again at the volumes. There's the original volume on worker-linux-2, which is where the first container ran, so in here there will be a file which the first container wrote, and the second container used that cache file because it started on the same node. There's a second volume on the DTR Linux node now, where the new container is running, and this is a separate piece of storage. The volumes have the same name, but they're local to each server, so they're actually different volumes with different data. Docker creates local volumes when they're needed by a container. There are no volumes for my API consumer on worker-linux-1 or worker-linux-3, because they haven't run any of those containers yet. I'll scale up my service to run some more containers. Here in the service configuration under scheduling, I'll set the scale to four. As the server scales up, now I do have containers running on worker-linux-1 and worker-linux-3. In the volumes, you can see that all these nodes now have volumes on them, but they're local to each node, and there's no data sync between them. The two original volumes have different data in the cache files, and the new volumes have no cache files at all. On the web app, if I refresh a few times, then I'll hit a different container. So here, this API response has changed, and it's not cached, so this comes from a new container with an empty volume. Refresh again, and this is now cached. It's very straightforward to use local volumes, you just define them in the compose file, and Docker creates them when they're needed. The container attaches to the node's local volume, and it has all the storage locations that it needs, but the contents can be different across the volumes in the cluster. Next, I'll talk about what this means for the storage profile of applications like this, and for more complex stateful apps.

Understanding Stateful Application Storage Profiles
Every node in a swarm can create local volumes, which are just directories on disk surfaced into containers. It works the same way in a cluster as it does on a standalone Docker server, with a couple of important things to know. Docker only creates a volume on a node if that node is about to run a container that needs the volume, and Docker doesn't replicate volume data between nodes. That means if you have three nodes in your cluster and you run a service with a single container, only the node running the container has a local volume. Scale up to two containers, and another node will run the new container, and it will create a volume for the container to use, but the volume will be empty. Different containers running the same image have their own separate state. Now if you over provision the service, so you run more containers than there are nodes, then some of the nodes will run multiple containers. In that case, all the containers on that node will share the same volume. Containers on the same node share state, but containers on different nodes may have different state. This might work for your app, but it very much depends on how your app uses storage. If it uses disk to cache the response from expensive network calls, then it's okay if the container starts with an empty cache, because it's on a node with a newly created volume. It just means that container will need to make the network calls directly until it fills its cache. So the first few users of the website will get a slower response. Having several containers share the same volume on a node should be fine for that application too because they'll all read the same cache data, and if two containers try to write data at the same time, they should both be setting the same value. But for most stateful apps, where you need high availability, using local volumes in this way isn't going to work. You typically want all the containers that are running the app to have the same state, and when containers get replaced or moved between hosts, you don't want that to affect the behavior of your application. I'll show you how to use standard features in your container cluster to give you that functionality in the next demo. But before I move onto that, we need to look a little bit more closely at the storage profile where this approach is going to work.

Running Replicated Stateful Apps in Containers
The first app I've deployed to my swarm has a very simple storage profile. It expects to write data to a specific path, and if that path exists, that's all the app needs. It doesn't matter if the storage location starts off empty, because the app will fit it, and it doesn't matter if several processes try to write to the same location. Some stateful apps will have a similar profile to this, but usually they're more complex. A typical example is an end-tier application with a presentation layer, 0 or more business logic layers, and a storage layer. The storage layer for a traditional app will be a relational database. So how would that look in a highly available deployment using containers. I'll use postgres for my application database, but all the major relational databases have similar functionality. For high availability, you need data replication across several database servers. In my deployment, each database server will be a container in the swarm. postgres syncs data with master slave replication so there will be one container for the master, and one or more containers for the slaves. The slaves sync data from the master, and they are postgres service too, but they can only be used for database reads. That's how postgres keeps the replication simple. All rights go to the master node, and all the slaves replicate from the master. That way, you don't get concurrency issues with multiple rights to different servers. This pattern is very common, and actually very useful because it allows you to scale out reads across multiple slave servers. Think of a web store which displays a product list. That's all read-only access. You can scale up to hundreds of web containers reading from dozens of database slave containers. The master could run in a single container, it doesn't need to support high load, because only the company's internal users can update the product list. Replicated databases like this have historically been deployed to fairly static clusters. Say you have one master server, and three slave servers, and you only scale the cluster out once a year. Because of that, the database makes some assumptions about the cluster setup, which we need to be aware of when we run it on a more elastic environment, like Docker Swarm. Firstly, each node expects exclusive access to its own disk, so they don't interfere with each other. In container terms, that means you can't share a volume between many containers on one node. Secondly, the startup time for a new slave could be very high because it needs to sync all the data from the master, and it won't be available to clients until it has synced. So you don't want containers moving between nodes unexpectedly. That basically means you need to make your dynamic container platform behave a bit more like a static cluster, and I'll show you how to do that in the next demo.

Demo: Deploying Postgres with High Availability in Docker Swarm
In this demo, I'm going to run a two tier application with high availability in containers on my cluster. I'll use postgres for the storage layer, and run it with master-slave replication. You'll see how to use labels and constraints to limit where containers run, and provide postgres with the sort of run time environment that it expects. So here's the demo app running on my Linux machine, which is a standalone Docker server. I have a web store which just lists a whole lot of products, and an API which can be used to read and update the product list. Here's the compose file I've deployed. I have two postgres services, db-master and db-slave. They use the same Docker image because my postgres image has all the logic built into it to run either as a master or as a slave. The environment variables for the master sets up the postgres user account on the storage location, which is going to be provided by a volume. The slave service has the same setup, but it also adds the REPLICATE_FROM value, which makes this a slave for the db-master container. I won't go into the details on how the postgres replication works, but the Docker file and the setup scripts are all in the source code for this module if you do want to check it out. Then I have the API, which needs read and write access to the data. So the connection string is set up to use the db-master service. The web app only needs read access, so it's configured to connect to db-slave. I'm running this now on one machine with Docker compose so I don't have high availability, it's just the single server. But I can smoke test the setup and verify that data gets replicated, and I just need a few extra options to make this compose file suitable for deploying to a cluster. I have those in this override file called docker-compose-prod. If you're not familiar with overrides, it's a nice way of keeping your compose files organized. So my core compose file has the generic settings for deployment. The build override file has the extra sections for the CI server to build the images, and the prod override file has the production cluster settings. The deploy section only applies when you deploy this compose file as a stack in Docker Swarm. The API and web services are set to run in multiple replicas so there will be two API containers and eight web containers running across the cluster. Those components are stateless so they can run anywhere. The database setup is more interesting. I want the single instance of the database master, so it has a replica level of one, but I also want to make sure that container always runs on the same server, so I have a constraint in here saying it should run on the server which has been set up with the label postgres, having a value of master. That's a custom label which I can apply to nodes in the swarm, and if I want to make sure the master always runs on the same server, I need to make sure only one node in the cluster has this label set. The slave service also uses a label to constrain where it gets run, but in this case, I want to allow multiple slave containers. So I have a constraint limiting this to nodes which have a label of postgres==slave, and the deployment mode is global. Global means the service runs across every node in the cluster. More specifically, a single container will run on each node that matches the constraints. So if I add the postgres==slave label to two of my cluster nodes, I'll have two slaves; one container running on each node. So this is actually a very simple way of carving out part of my Docker cluster, which could be hundreds of nodes, and making sure my postgres containers always run on a known subset of my servers. I'll deploy this now. I need to join together the two compose files, which I can do using docker-compose, specifying the two file paths, and then running the config command. That joins all the files and validates the output. I'll capture the output in a stack.yml file, and this is what I'll deploy. I'll take a look at the generated stack file, and it has all the specification from my prod override file added into my original compose file. I'm connected to my cluster here, but before I deploy the stack, I need to set up the labels. I'm going to choose worker-linux-1 as my database master. So I'll run docker node update to edit the node's configuration, label-add to add a new label, and specify the postgres key with a value of master. I'll start with a single postgres slave, which will run on worker-linux-2, where I'll add the postgres=slave label. Now it's just docker stack deploy pointing to the join stack file, and I'll call this store. For this demo, I'm just going to check everything's working as expected. I'll scale up and down, and break things later in the module. So here's postman, and I'll use the cluster's load balancer address to check the API. The GET lists all products. This is actually just random data which the API inserts when it first runs, and this is looking good. I'll switch to the web app, and I see the same list of products. Remember, the API uses the master and the website uses the slave. So replication seems to be working. Product ID one has the same data in both. I'll go back to postman and update that product, changing the name and price, and setting the stop stock to 0. This is a PUT request, which returns a 204 response, so write access to the master must be working. Now refresh the website, and I see the updated details for product one, so the data change in the master has been replicated to the slave. In case you're skeptical, I'll switch back to the Universal Control Plane UI, and confirm what's been deployed. My stack has four services, one db-master container, one db-slave container, two API containers, and eight web containers. The db-master is running on worker one, which is the only node matching the postgres=master label constraint. The db-slave container is running on worker two. Again, this is a global service, so I will get one container for each node which matches the postgres=slave label, which is just this node right now. I'll go to the API service, and check the environment configuration. The connection string is using db-master, and similarly, I'll go to the web service and check the config. In the environment setting here, the web app is connecting to db-slave. One last thing to check. I'll look at the volumes where the data is stored. There's a single postgres master volume on worker one, and a single postgres slave volume on worker two. So this is all looking good. Next, I'll talk about what this setup means for scale and failover.

Pinning Container Workloads with Labels and Constraints
My cluster has multiple nodes, but with the labels I've applied and the constraints in my stack deployment, only two nodes are going to run postgres containers. I've effectively pinned the postgres deployment to two of my servers. I have a dynamic container platform that I can scale to hundreds of nodes, and I'm running in the cloud, so I could do that in minutes, but my postgres deployment will only ever run on the two labeled nodes. This is how I get my dynamic environment to look like a static environment. So postgres can run in the way that it was designed. I have a single master and a single slave. The containers are using local volumes for storage, so I get the same IO profile, and the same level of availability than I would have if I was running across two VMs or two physical servers. I've got all the usual benefits of containers for my database now. They start in seconds, I can run the exact same software in any environment using the same images, and I manage all the parts of my app in containers, and I've got extra flexibility too. If I want to add a new postgres slave, all I do is add the postgres=slave label onto another node. Docker will see that a new node matches the constraints for the global db slave service, and it will start a new container. That container will use the local volume on the node, which will start empty. So it will sync the data from the master. The slave won't go online until the data sync completes, so Docker won't send any traffic to the new node until it's ready. Docker load balances a request across containers in a service, so the web app uses a single host name for the database in the connection string, and the requests get distributed equally across the slaves. This is much better than standard database replication, where the client needs to specify all the slaves in the connection string, and the database client library decides which slave to connect to. In that scenario, if you add a new slave, you need to update your client configuration. So I can scale up and scale down my slaves here just by adding or removing node labels. Docker makes sure that the web containers use all the available slaves; and I get failover. If a slave container fails for any reason, Docker will start a replacement container on the same node, because the global service requires a container on each eligible node. The new node will use the original container's volume, so the data will be in the same state. The replacement container will need to sync from the master, but the downtime will have been seconds, so the sync should be fast, and the new slave will be ready quickly. If the server goes down and is offline for a longer period, the new slave container will need to sync all the changes when it comes back up, but the container won't accept traffic until the sync is complete, so Docker won't use that container, and clients won't get stale data. There's one area where my setup here doesn't have high availability, and that's because I've got a single database master container. If the master container fails, Docker will replace it on the same node, so downtime will be minimal. If the server running the master fails, there is no other node where the master can run, so it will be offline. There are ways to manage that, but they're going to be specific to the application. You may be able to run multiple masters, or you may be able to promote a slave to a master, but I won't cover that here. In the next demo, I'll show you what all those scale and failover scenarios look like in practice.

Demo: Scale and Failover for Replicated Stateful Apps in Docker
In this demo, we're going to see what failover and scale looks like with replicated stateful apps in a Docker cluster. I'll scale up my postgres slave, and show you how I get failover if containers or servers fail. I'll also show you the limitations of this setup for availability of the postgres master. Here's my web application which is still running from the previous demo. First, I'll see what happens when the database slave container fails. There's a single container here, and I'll stop it running. The service level for the db-slave service requires one replica running. So Docker starts a replacement as soon as this one stops. There's only one node where the service can run because only worker-linux-2 has the postgres=slave label. So Docker starts a new container on the same server. This new container attaches to the local volume on the host, so it already has populated database files from the old container. The logs show that the new container loaded the data, checked the replication status with the master, and then entered the ready state so it can accept read=only clients. If I refresh the website, I get an error. This is because my app doesn't have any error handling, and the postgres client has cached the connection to the old container. It's a transient fault, so when I refresh the page, the app is working again. There should be proper error handling, so the app should retry the connection to the database, and users wouldn't see an error. Docker can't do much about the application code, but I could put a health check inside my web application Docker file which check database connectivity, and that would give me self-healing for this fault. But the app is working correctly, even with a simulated container failure. I don't have high availability yet though. Containers can fail and be replaced, but I only have one node which can run a postgres slave. Scaling up is super easy with this setup. I just need to add the postgres=slave label to more nodes. I'll pick worker-linux-3 as a new postgres slave, and update the node to add the label. The db slave service is configured as a global service, so it should run on every node that matches the constraints. There's a new matching node now, so Docker immediately schedules a new container. There are already two running out of a desired level of two. I can look at the service, and see there's a new container on worker-linux-3. I'll check the logs here, and the content is different from the last set of logs we saw, because this is a new slave. It's running on a node with an empty volume. So first of all, it needs to replicate from the master. This is a tiny database, so replication is very fast. But for a large database, it could take many hours to replicate, and this slave wouldn't come online until the replication completed. So it wouldn't get any traffic during the replication. My web app is still working correctly, but now the data could be coming from either of the postgres slave containers. They're using the same DNS name, db-slave, and Docker is load balancing traffic between them. Now I have much higher availability, because my Docker slave service can survive container failures and server failure. I'll simulate a server going down by going into my worker-linux-2 node, editing the setup, and changing the node's availability. Drain mode is what you'd use when you need to do maintenance on a server. It evicts any running containers, and doesn't schedule any new ones, so it's like temporarily removing the node from the swarm. Back in the service list, the desired replica count for the slave changes back to one, and there's only one active node in the cluster that meets the criteria. The worker-linux-2 containers are both exited, and only the new worker-linux-3 container is running, but the service level is correct for this global service at 1/1. I'll refresh the site, and again, we hit the client connection issue that should be logged as a defect, but refresh again, and this is all correct. The same data coming from a new container, which is just replicated state from the master populating a new volume. I'll put the slave back into HA configuration by going to my worker-linux-2 node, and setting availability back to active. This makes the node a full member of the swarm again, able to run containers, so there are two nodes which meet the postgres=slave constraint. Back to the service list, and sure enough, the db-slave service updates from 1/1 to 2/2. The containers are running on each of the eligible nodes, using the local volume on each node. Lastly, I'll look at high availability for the master database. Only worker-linux-1 meets the constraints for running a db master container, and that's set with a replica level of one, so even if a new node meets the constraint, it won't run a master container as long as this one is running. On Linux 1, we can see the database container, along with lots of other containers. I'll simulate server failure here by putting the node into drain mode, which is going to stop all these containers. I'll go back and check the service list, and all the services are running green except db-master. So Docker started new containers to replace the ones that worker one was running, and they're distributed around the swarm. But db-master needs a node with the postgres=master label, and now there are none. So this is a 0/1. There's just the exited container from the inactive node. The web app is using the slave database, so this is still working fine, but back in postman, try to call the API, which will be using the master, and I get a 500 error. So the API is not working anymore because it can't connect to the master database. Let's fix that. I'll go into the nodes, and back to worker one, edit the configuration, and set it back to active. Now there is a node which matches the criteria for the service, which is currently under provisioned. So Docker will start a new container on that node. We see that happening here, and in the service details, there's a new container in the list. I'll look at the logs, and this container is ready to accept connections. It's loaded the database files from the local volume, and this is the same node the previous container used so the database state will be the same. The web app is still working fine, of course, and now when I switch back to the API, the GET request is working again too. I can use this configuration to get scalability and full high availability for the database slaves. That service is resilient to container and server failure, and I can automatically scale up and down just by setting labels on nodes. I don't have scale for my database master, but I do have partial high availability, because the servers can survive container failure and server restarts. We did it a lot in this module, so I'll recap what we covered, before moving on.

Module Summary
In this module, I looked at running stateful applications with high availability in Docker Swarm mode clusters. I focused on the simplest approach, which works on any Docker cluster out of the box, using local volumes on the server disks. This works in the same way in Docker on Linux, and Windows, and with other orchestrators, like Kubernetes. You need to be aware of two things when you use local volumes. The first is that Docker doesn't synchronize data in volumes across the cluster. Many nodes can have containers running the same service, all using local volumes, and the contents of all the volumes could be different. The second thing is that containers running on the same node will share the one volume on that server. So if you're running at high scale, you need to understand how your app operates if multiple instances access the same piece of storage. I showed an app with a simple storage profile that can use local volumes with no restrictions. In this case, containers could run anywhere, and the app would work in the same way; and then I showed a more complex storage profile using postgres with master slave replication. I used node labels to isolate part of my swarm for postgres, so the master container would always run on one node, and the slave would run on a set of nodes that I explicitly selected. This is a great approach for running older apps in a container platform, giving them a fairly static set of servers to run on, which are actually part of a much larger dynamic platform. You need to design your deployment with your app's storage requirements in mind. But you can do that with simple local volumes and still get better scalability, availability, and density than you get by running a separate dedicated database. In the next module, I'll look at cluster- wide storage using volume plugins to physically store data outside of the cluster so that it's available to any container running on any node. This lets you run stateful apps in your dynamic container platform without any special deployment considerations, provided your apps can make use of the shared storage; and it means you can scale your apps to use the full capacity of your cluster. So join me in Using Volume Plugins for Scaling Dynamic Stateful Applications, the next module in Pluralsight's Handling Data and Stateful Applications in Docker.

Using Volume Plugins for Scaling Dynamic Stateful Applications
Module Introduction: Volume Plugins and Shared Storage
Volume plugins for Docker add shared storage to your cluster, and that means you can run an even wider range of stateful applications in containers with high availability and scale, leveraging the features of an external storage provider. My name's Elton, and this is Using Volume Plugins for Scaling Dynamic Stateful Applications, the next module in Pluralsight's Handling Data and Stateful Applications in Docker. In this module, I'm going to focus on shared storage for the container cluster using a volume plugin. There are plugins for all sorts of storage providers, from dedicated hardware storage devices, like HP's 3PAR, through software defined storage, like Hedvig and cloud storage services. You can find the plugins all listed on Docker hub, which you can filter by type. This short link will take you to the list. I'm running my cluster in Azure, so I'll be using the Cloudstor volume plugin from Docker. The plugin supports different clouds, and in Azure, it uses Azure files as the storage provider. So my apps will be writing to a location in the container file system, which is actually a Docker volume, which is actually being stored in Azure files. This is a really useful alternative to local volumes in the cluster. You can deploy your service so that any container on any node can access the same data from a single volume. Or you can run a service where each container gets its own volume, but the volumes are accessible from any node. So if one container gets replaced, the new container can attach to the correct volume, even if it starts up on a different node. I'll demonstrate both those approaches in this module, using the same demo apps that I've used earlier in the course, and you'll see how much more flexible the container cluster is when you have cluster- wide storage options. I'll start with a simple example, where multiple containers are using the same shared volume.

Demo: Sharing a Volume Between Multiple Containers
In this demo, I'm going to deploy a service with multiple containers sharing a single volume. The volume will be using the Cloudstor plugin, and we'll see how Docker actually structures the data, and how you can manage your cluster storage through Azure. To start with, I'm running the API consumer app from earlier in the course, using the original setup. It's running across three containers, which are each using local volumes. This is a fresh deployment of the app, so when I browse to the website, the first response isn't cached. Then the container saves that response in the local volume, and when I refresh, I get the cache data. I'll refresh a few more times, and now I've hit another container. The cache is empty for this container, so it fetches the data from the API. Actually, this is just a random value that the app generates if the cache is empty. This container stores that value in its own cache, so I'll refresh, and now I get the cached response, but each container has a different value cached, because they're not using shared storage. So eventually, all three containers have cached responses, but the actual responses are all different. I'm going to switch to shared storage now, and fix that. But first, I'll remove the existing stack. In this session, I'm connected to one of my cluster nodes, and I can show you the volume plugin that I've installed. This is using Cloudstor, which is part of Docker Community Edition, and you can install the plugin to get shared storage if you're running in the cloud. I'll inspect the plugin, and you'll see from the configuration that this is using an Azure storage account. When I install the plugin, which you do from the Docker command line, I added the storage account details, so the nodes are able to read and write data. I have the Azure portal open here, and I can show you the actual storage account. Here's the account that Cloudstor is using, and the Cloudstor plugin for Azure actually uses Azure files. So each volume you create will be a separate share. Right now, there's a single share here for Cloudstor's own metadata, which is empty because I haven't created any volumes yet. Using a volume plugin is super easy. Here's the original compose file that I used to deploy my app. It specifies a volume, but there are no options in here, so it gets created as a local volume, which is the default. In my production override file, I'm specifying options for the volume. I'm giving it a name so it won't clash with other local volumes that already exist, and I'm specifying the Cloudstor Azure volume driver. That's all I actually need, but I can specify some other options for the volume plugin. In this case, I've added an explicit name for the file share, so this will get used for the name in Azure files rather than the plugin generating a random name. I'm deploying this as a global service, so containers will run on every node, but every container will use the same storage from the Azure file share. Here are the compose files, and I'll use docker-compose to join the two together using the config command, and capture the joined output in a file called stack.yml. The file gets generated, and I'll check the contents, and here's the service and volume definitions with my production overrides in place. This is the same Docker image that I've used previously. There are no changes to the app here. I'm just swapping out the storage provider for the volume. No I'll deploy this to my cluster with docker stack deploy, specifying the stack.yml file, and calling it api-consumer. That's deployed, so I'll switch back to the UI, and refresh the service list. The API consumer service is green with 5/5 replicas running. I'll go into the metrics view and see the containers. They're all running on different nodes because this is a global service. Now in the volume list, I only have one cs-api-consumer-cache volume using the Cloudstor provider. All the containers are using that one volume. Compare that to the previous deployment with three local volumes, where each node had its own volume on disk. In the Azure portal, I can see what the Cloudstor plugin does when a new volume gets created. In the metadata share, there's a file now for the cs-api-consumer- cache volume. This records the setup of the share, and I can look at the contents, it's just a txt file in JSON format. The share name is here, and that's where the actual data for the volume gets stored. Back in the share list, I'll refresh, and my cache share is here, but right now it's empty. So I'll browse to the website. On the first hit, I get an API response from the container, and it's not cached, but then this container will save the response in a file on the volume. That volume is physically using the Azure file share, so I'll refresh, and here's the cache file. I can look at the contents, and this is just the API response data, b0a, which the first container stored. Refresh the page, and the response comes form the cache, so the container is reading this from the cache file in the volume. Every container is using the same shared volume now, so when I refresh enough, I'll hit a new container, and it will have the same data from the same cache. All the containers have the same state now, shared in the Cloudstor volume. There's no code change here. I'm using exactly the same application that I previously deployed. I've swapped out the storage layer, but it's all transparent to the application. The Cloudstor volume plugin gives me shared storage across the cluster, and it also means I get all the storage characteristics of Azure files. I could upgrade this account, and switch to premium storage on SSDs for better IO performance, and I could set up replication within the region, or between regions. So I get all the functionality of Azure files for free without having to change my application to explicitly use Azure files. Next, I'll look at what this setup means for a more complex stateful app using database replication.

Mixing Storage with Shared Volumes and Local Volumes
Volume plugins let you use your existing infrastructure to get shared storage across your Docker cluster. All the major hardware providers and cloud storage services have plugins. You actually install the plugin on each node in your cluster, and then you just specify the volume driver in your deployment. I'm running my cluster in Azure, so I've set up the Cloudstor plugin, which uses Azure files as the backing store. Each volume is created as a separate file share, and the volume can be used by 0 or more containers. My API consumer app works fine with many processes accessing the same file path, so I ran a service with many containers all using a single volume. A container can run on any node in the cluster, and reach the Cloudstor volume. That also enables a different scenario, where I can run a single container with a volume and get failover. The container or server could fail, and the new container can run on any node and reach the same volume. That means I can use a Cloudstor volume to get high availability for my postgres database master service. Earlier in the course, I used labels and constraints to pin the postgres master container to a single node to make sure the master always ran on the same server, and used the same local volume. That doesn't give me high availability in case of server failure. But if I use a Cloudstor volume instead, then I do get high availability. I'd run a service with a single replica using a Cloudstor volume, and I don't need the label constraints. That gives me failover if the container stops, or if the server goes down. The replacement container can start on any node, connect to the master database volume, load the data files, and run from the previous state. I may not get the read performance my database needs from Azure files, but my slave containers can continue to use local volumes. I already have a highly available setup for the slaves. I can mix the volumes to use shared storage for the master, and local storage for the slaves, which will be fine if my app is more read heavy than write heavy. That's the theory anyway. A couple of things you need to be aware of with volume plugins. The storage looks like part of the container, but it's really coming from the underlying storage provider, which means the IO performance and the filesystem feature support may be different from the rest of the container. That may not be a problem if the app in the container just does basic file reads and writes. But if the application assumes the file system is Unix compatible, and tries to use Unix specific features, the app will fail if the actual filesystem for the volume driver doesn't support the full Unix feature set. So with that note of caution, in the next demo, I'll go ahead and deploy my postgres master database using a Cloudstor volume.

Demo: Using Cloud Storage Volumes for Postgres Master Containers
In this demo, I'm going to use a mixed storage approach for my postgres deployment. I'll use local volumes for the slave containers, and I'll use a Cloudstor volume for the database master. That should give me high availability for my master, but I've already hinted that this might not work as expected. So here's my compose file. I've already joined together the production override, so this is the full deployment file. I'm using basically the same setup as the previous deployment for this app. But for the master, I don't have any constraints now, because this container can run anywhere. The slave containers are still restricted to slave nodes, and the API and web services are using the same configuration with the same images as before. The main change is in the volume section. The postgres-slave volume has no settings, so it will use local volumes on the nodes. The postgres master volume is using Cloudstor with some specific options. Postgres runs as a special user ID inside the container, and that user needs to have permissions to access the files in the Azure share. Having these options sets that up for me. It will service the volume into the container with all the files owned by user ID 999, and group ID 999, which is the postgres account. This setup is specific to the postgres Docker image, but you'll find it's fairly common with other Linux containers, so this is a useful option in Cloudstor and other plugins. Now I'll deploy this. I'm in the directory with the compose files, and I'll deploy a stack using the new postgres compose file, and calling it store. That's all created, and I'll switch back to the UI and see how it looks. All the store services are green, so this is looking good, although the db-master did have a previous error. Let's see what's happening in Azure files. I've got a new share here with a random name, and in the Cloudstor metadata share, I have a new file for the postgres-master volume. I'll look at the metadata file, and there's the user and group IDs that I set in the volume options. So that looks good. Back in the share, I have a pgdata folder, but it's empty. Now, that's where the database master file should be. I'll go over to the service and see what's happening. There are a few failed containers here, so I'll drill down into this first exited container, and take a look at the logs. There's a fatal log entry, which doesn't sound promising. What's actually happened here is that postgres tries to create a link for the database file, and it uses a hard link rather than a symbolic link. But this is an Azure file share, which means it's actually an SMB filesystem, which doesn't support hard links. So that initialization step fails, and postgres can't run. The container exits, and Docker starts a replacement, but this is a critical issue, there's no way to fix this, so every container will fail when it tries to create that file link. So what about the rest of the app? The db-slave service seems to be running okay. But when I drill into one of these containers and check out the logs, then we can see that it's just spinning, trying to connect to a master which is never going to come online. The API also looks fine, but that's because there's no intelligence in the container to check for dependencies on startup, and there's no health check. If I try and use the API, it will fail, but the container status doesn't tell me that. There's actually nothing that I can do to get this setup working, postgres just isn't compatible with Azure files, and you would get the exact same issue if you tried to run postgres in the data center using an SMB network share for the storage location. So I'll remove this stack to clean up, and later in the module, we'll see what we can do to get full high availability for the storage layer in this application.

Comparing Relational Databases with Modern Distributed SQL
Docker supports multiple volume providers in a single cluster, so I should be able to run my replicated database with shared storage for the master and local storage for the slaves, and get high availability and optimum read performance. But my current deployment running on the cloud with a Cloudstor plugin for Azure files doesn't work for postgres. The database server expects certain features from the filesystem, which it doesn't get from the SMB share that the volume actually uses. There are plenty of other plugins that I could try, which use other storage services like Azure disks, rather than file shares, and for other clouds and datacenter storage devices. But my goal here isn't to get postgres working really, it's to get my app working. I don't particularly care what technology the storage layer uses, and if I'm open to using different databases, then I can choose a more modern option, which was designed to run in dynamic environments, like Docker Swarm and Kubernetes. So I'm going to swap out postgres for an open source database called CockroachDB. Cockroach is a super scalable distributed SQL database. It's designed to run at scale across many nodes, and it doesn't have any of the restrictions of classic relational databases, it doesn't use a master slave model with read-only slaves. Any node in a cockroach cluster supports reads and writes, and it manages data intelligently, spitting large datasets into smaller chunks, which can be quickly replicated across many nodes. Cockroach is getting a lot of interest. It's built to run in Docker containers on clusters, and it supports the postgres protocol, so you can swap out a static postgres cluster with a dynamic cockroach cluster without changing your applications. Next, I'll try out CockroachDB on my cluster, and check that it works with Cloudstor volumes, and that it works with my application.

Demo: Running CockroachDB in Docker Swarm with Cloud Storage
In this demo, I'm going to switch to a modern distributed database running in containers. I'll use CockroachDB, which is a drop-in replacement for postgres. I'll start by deploying cockroach as services with Cloudstor, and checking everything is working with my application. First, I'll deploy cockroach manually from the command line so we can see how it looks. I'll create a network for the containers to work in, and this is an overlay network, so containers can communicate across any node in this form. Now, I'll create a service for a single cockroach instance, which I'll use to initialize the database cluster. All these commands are in the resources for the module. Here, I'm creating a service called cockroach-init using a volume mount for storage. The volume driver is my Cloudstor Azure plugin. The source will be the volume name, and the target is the cockroach data location inside the container. I'm using the cockroach team's official image from Docker hub, and passing some startup commands. Cockroach can encrypt traffic between database nodes, but I don't need that for this demo, so I'm using insecure mode, and the advertise address is how other cockroach nodes will join the cluster; and I'm using the Docker service name. This service is really just to bootstrap the cockroach cluster. It will run a single container with a known advertise address, which other cockroach containers can register with. It's also where I'll run the cockroach web UI, I'm publishing a port for that. This will start up with just one task, and the Docker CLI will wait to check that the container is running correctly. The service list in UCP shows me the status is green, and I have a single cockroach container running. Now I can browse to port 8082, and this is the database web UI. There's a single database node, and Cockroach stores data in small chunks that it calls ranges. There are 22 ranges right now, and because I only have a single database node, there's no data replication, so all 22 ranges are under replicated, which means the data isn't being redundantly stored. So now I'll scale up the database by creating a new service. This uses the same cockroach Docker image, but the start command says to join the existing cluster from my bootstrap service running at cockroach-init. The volume mount uses Cloudstor again, but this time I'm using a different way of specifying the source volume. This is a really nice feature of Docker Swarm which lets you use a template for fields inside the service definition. This is saying that Docker should create a volume for each container in the service, and generate the volume name by joining together the service name, cockroach, and the Task.Slot, which is the index number of the container. There are four replicas in this service, so I'll have volumes called cockroach1, cockroach2, cockroach3, and cockroach4. That's created, and those tasks are starting, so I'll switch back to the cockroach UI. Initially, there was one node with 22 under replicated ranges. Refresh this, and now there are five live nodes, and only eight under replicated ranges. So the new nodes have joined the cluster, and they're replicating data from the original node. Refresh again, and now the cluster is fully replicated with ranges stored on all the nodes. The cockroach UI gives you a lot of detail. In the replication view, I can see that originally all the replicas were on node one, then within a minute of joining, all the nodes had replica stored. I'll go back to Universal Control Plane, and we'll see the volumes where that data is actually stored. There are four volumes for the templated service, which end with a task number, and one for the original cockroach-init service, they're all using Cloudstor. So the data actually lives in Azure files. Here are the new shares for each volume. So it looks as though cockroach works fine when it's distributed on the cluster, using shared storage in Azure. I need to be sure that my app actually works with cockroach though. I'll create a service for the API. I'm using the same image as before, so this is the exact same set of application binaries. The only difference is the host name and port in the connection string. Cockroach supports the postgres protocol, so my app will just think that it's talking to postgres. My service is up, and I'll check that in the UI. All the services are showing green. So in postman now, I can try out my API. This first call would generate all the seed data and store it in the database, and I get a list of products back, so this is looking good. Now I'll run the web app. Again, it's the same image, and the same setup as before except for the connection string. Cockroach doesn't have masters and slaves. Any node in the cluster can read and write data, so I'm using the same connection string here as for the API. My tasks are starting up. I'm running across six containers for the web app, and I'll browse to the load balancer for my Docker cluster. The web app looks like it's working fine, and the data matches what I'm seeing from the API. It seems I can use cockroach as a direct replacement for postgres, and my app just works in the same way. I'll remove these services now, and in the next demo, I'll deploy my app as a stack using a compose file, and check out what this setup gives me for scale and failover.

Demo: Swapping out Postgres for CockroachDB in Stateful Apps
In this demo, I'll switch to CockroachDB for my two-tier application. I'll deploy the stack to my cluster using the Cloudstor volume plugin, check the replication is working across the nodes, and I'll see how this setup supports high availability and scale. Here's the Docker compose file for my new deployment. I've taken the manual deployment steps from the previous demo, and captured them here, so this file can live in source control and the deployment is repeatable. I have the same pattern with an init container for cockroach, which bootstraps the cluster, and hosts the web UI. In a production deployment, the UI wouldn't be public. I'd probably add security and authorization with an Nginx container sat in front of the UI. Then the DB service runs across four replicas with a startup command joining the init container. Both database services are using volumes, and there are no deployment constraints, so the containers can run anywhere. The API and web definitions are the same, using the same Docker images again, but with the CockroachDB service as the host name in the connection string. In the volume definitions, I'm using the Cloudstor plugin. For the init service, I've got a name specified for the share. For the DB service, I have that same template syntax from the service create command, which will create one volume for each task named after the service and task. I'm connected to the cluster here, and I'll deploy the stack in the usual way with my cockroach app definition, and I'll call it store. In the UCP UI, I've got my stack listed, and when I check, the services are all green. This is a new deployment, but the Cockroach UI is listing on the same port that I used before. So when I refresh the overview, I see my new five node cluster. All the nodes are live, all the ranges have been replicated, and the nodes have all been up for a few seconds. The database view shows me that only the default cockroach and postgres databases exist right now. My app deploys the database on first use, so I'll switch to postman and make a GET request to the API. This first call will initialize the database with random data, and then return it, and this looks okay. Back in cockroach, the refresh database list shows my new store database. It's tiny right now, just 7KB stored in one range, but it will still be replicated across the cluster. I can drill down into the database schema and here's my products table. I can also check out the database's usage from this UI. I can see all the insert and select statements, how long they took, and the latency. This information is coming from Cockroach, so this is a consolidated view from all client connections across the whole cluster. I can see the cluster is replicating 24 ranges now, the nodes are all healthy, and if I switch to the web app, this is working too. The details for the product match between the website and the API, even though they're probably using different database containers, so the replication is working fine. I'll switch to the API and update that first product to check that the change gets replicated. The PUT request completes successfully with a 204 response, and in the API when I fetch the products, this is the new data. Back in the website, when I refresh, this is the updated data too. My DB service is running across four containers now, but I have a deployment which lets me scale easily. Each task in the service has its own dedicated volume, so I can scale up from four to six tasks, and that will start two new containers. In the cockroach UI, the node count has increased to seven, and the two new nodes are online. To start with, they haven't replicated any of the data, but refresh, and now we see that these nodes have already replicated a set of ranges. I'll switch over to metrics and check out the replication view. Here's the point where the two new nodes joined, and when they start replicating, then the data gets spread evenly throughout the cluster in less than 30 seconds. Refresh the website, and this is still working fine. It could be talking to any one of the database containers now, they all have the same data available, and in the API, I'll make another change and set the product name to m3, and the stock to 1. The PUT request is successful again, and in the API I can see the new data. In the website, refresh, and the updated data is here too. So I can scale easily with this setup, and I've also got high availability. I'll go back to the Docker Enterprise UI, and into the DB service. I have six containers now, and I can take the majority of them offline by setting the scale down to two. Now in the cockroach UI, I can see three live nodes. The init container and the two DB containers, and four nodes are listed as suspect, so cockroach immediately reflects that those nodes are not available. The cluster is still functional though, and in my application, the website is still working correctly. When I scale back up again, we'll see why the templated volume names are so important. I'll scale back up to a level of six, which means four new containers get created. They could be running on any node, but each new container will have the same task ID as one of the previous containers, so it will attach to the existing volume. These containers have only been up for a few seconds, but they'll start with a populated dataset. In the volume list for the store DB, I have six volumes; one for each task. Even if the actual containers running the task get replaced with new ones, the task ID for the new container is the same, so it will use the same volume. Back in cockroach, the cluster is healthy again, and the replication is balanced. The website is still working correctly too. The last thing I want to show is that the db-init service that I used to bootstrap the cluster is not a single point of failure like the postgres master service was. I can go into the service, browse to the container, and stop it. The service has a replica level of one, so Docker starts a replacement container straight away. The new container happens to have been created on the same node as the previous container, but because it uses a shared volume, it could be running on any node, and the behavior would be the same. Refresh the cockroach UI, and the init container is showing that's only been up for a few seconds. It's running, but actually the node is still initializing, so the cluster is showing a 16 under-replicated ranges. The init node has just started. Some nodes are two minutes old, and some are eight minutes old, but the data is still intact. When I refresh now, the status is updated, and all the ranges are replicated. I have high availability for the cockroach nodes because I'm running multiple replicas, but even the init node has high availability because of the Cloudstor volume. The container is running on dtr1 in my cluster, and I'll simulate server failure by browsing to that node, editing the config, and taking it offline by setting the availability to drain. The node is in drain mode now, which means all the containers it was running have stopped, but the init service still has 1/1 task. The new container is on worker node three, but it's using the same volume as the previous two containers, so the data is still there, and it will be online again quickly. The UI is up, showing all seven nodes are live, and the web app is still refreshing correctly. I'll do one last update with the API to check the replication, and set a new stock and price for the M3 product. The PUT request succeeds, so the update has been saved. I'll run the GET request, and this has the new data, and then I'll refresh the website, which also shows the new data. Using a modern database like CockroachDB with a volume plugin means I can run across my container platform without constraints, and get scale and failover. The fact that cockroach is a drop-in replacement for postgres means I can update my tech stack without any application changes.

Module Summary
In this module, I looked at using volume plugins to provide shared storage in a Docker cluster. I used the Cloudstor plugin, which works with Azure files to store volume data in SMB file shares. There are some new storage scenarios you can support when you use shared storage in the cluster. First, you can share a single volume between many containers. I use this with my simple API consumer app, with multiple containers all reading the cache data from a single volume. Next, you can use a volume to give you high availability for a stateful app running in a single container. Any node can access that volume, so if the container or the server running it goes down, the replacement container can run on any node and still read the same data. I tried that for my postgres master database, and hit an issue where postgres expects to use a full Unix filesystem, and it doesn't run if you try to use the SMB file system that Azure Files provides through the Cloudstor plugin. There are a lot of volume plugins you can use to work with different storage services and data center storage devices, but you do need to be aware that the physical filesystem may not support all the features that the app inside the container expects. The other way to use shares storage is to have Docker create a volume for each task in a service, meaning each container gets dedicated storage. Your application needs to take care of data replication, but if it does support that, then you get high availability and scale across your whole cluster. I showed that with CockroachDB, which is a modern, distributed SQL database. I ran my cluster across many containers, and I could scale up and down just by changing the service level, and my apps carried on working even when I took nodes out of service in my Docker cluster. CockroachDB is just one example, which is a straight replacement for postgres. There are other technologies doing similar things. A database called TiDB has a similar feature set for running in containers, and it works as a direct replacement for MySQL. Modern distributed apps run very effectively in dynamic platforms like container clusters, and if you can't deploy your existing database technology to Docker with the service levels that you need, it could be worth looking at compatible alternatives. So far in the course, I've covered all the major storage scenarios for stateful applications, and shown you how to get them working well in Docker. In the next module, I'm going to look more closely at how Docker itself uses storage. I'll look at where Docker stores image and container data, and show you how to configure storage options for your Docker engine and your Docker registry. That's coming up in Managing Storage on Docker Servers and Registries, the next module in Handling Data and Stateful Applications in Docker.

Managing Storage on Docker Servers and Registries
Module Introduction: Why Docker Fills up Your Disk
Docker takes a conservative approach to your data. It caches everything that it can safely cache, but it doesn't delete any data unless you explicitly tell it to do so. That means you can use your disk space very efficiently with containers, but you need to understand how to do that, and you need to actively manage it. My name's Elton, and this is Managing Storage on Docker Servers and Registries, the next module in Pluralsight's Handling Data and Stateful Applications in Docker. In this module, I'm going to take a closer look at the practical side of storage in Docker, showing you how Docker uses your disk and why you need to spend time optimizing your Docker files to build effective Docker images. You'll also learn how to configure the storage for your Docker engine, how to keep your system clean, and how to manage storage for a Docker registry. You've seen in earlier modules how the container filesystem is constructed from multiple sources. Docker volumes can be external storage locations, separate to the machine, where the container is running. But the image layers and the container's writable layer are always stored on the filesystem of the machine where the container is running. If you are spinning up lots of containers, and building and pulling lots of images, you can quickly use up your disk. That's because Docker doesn't automatically delete any of those layers. If you pull a new version of an image, the layers that made up the old version are still saved on disk. If you remove containers, the writable layers still exist too; and there's another way Docker can fill up your disk, that's with the build cache that the engine uses when you're building images. All that data is stored on local disk two, and it will keep growing until you actively remove it. I'll show you how to do that in this module too. To start with, we'll look at where that data gets stored, how you can configure the storage location, and how to see just how much data your images, containers, and volumes are using.

Demo: Managing Docker's Disk Storage
In this demo, I'm going to look at where Docker stores all the data that it needs to have on the local disk. That's image layers, the build cache, and container writable layers. I'll show you the default storage locations, and how to configure Docker to use different locations on Linux and Windows. I'll also show you how to check the amount of storage that Docker is using. You get an overall view of Docker storage using docker system df. This breaks down storage into categories for images, containers, local volumes, and the build cache. The active column shows how many objects are in use. I have 65 images here, and only 6 are active, meaning I'm only using containers from those 6 images, the other 59 images aren't being used, but Docker doesn't clean them up. Images stay on the system until they're explicitly removed. The reclaimable column tells me how much data I can reclaim if I remove everything that's not currently in use. We'll see how to do that safely in a later demo. Run the same command with a verbose flag, and you get a very useful output that groups all the objects by category, and goes into detail on each one. For images, I can see the logical size, which is the size of all the layers in the image, and I can also see the unique size, which is the size of the layers which are unique to this image, and the shared size, which is the total size of all the layers that this image shares with one or more other images. For containers, the size column is the storage used by the writable layer for each container. These containers aren't running stateful apps, so the size is 0, and you can see from the status, that these containers are exited, but their storage layers stay on disk until I explicitly remove them. For volumes, I get the name or the ID if the volume doesn't have a friendly name, along with the amount of storage used and the links column, is the number of containers using the volume. Remember that one volume can be used by 0 or more containers. This is all useful stuff if you want to drill down and find out where all your disk space has gone. To see where the data's actually being stored, you can use the Docker system commands. I'm connected to a Windows machine running Docker here, but as usual, the Docker commands are the same on any platform. docker version gives me the basic details of the Docker CLI and the Docker engine, and docker system info gives me the next level of detail. The storage driver is the type of file system that Docker is using, there's only one option for Windows, which is the Windows filter driver from Microsoft, but there are more options for Linux to support different filesystems. Docker Root Dir is the root directory where all the Docker data is stored. The default location on Windows is C:\ProgramData\docker. I can look at that route and I'll see directories for containers, volumes, and images; which is where all the local data is stored. On a Linux machine, run the same command, and the default route directory is /var/lib/docker. You need route permissions to access it, and the structure is the same as on Windows. I see the same directories for containers, volumes, and images. You can navigate through the storage here and link it to what's running, because everything is in a directory named with the object ID or name. Volume directories are named from the volume, and container writable layers are named from the container ID. The default route directory is fine for most dev environments, but on any machine where you have multiple disk partitions or multiple disks, you'll want Docker to use the data drive rather than the system drive. You can configure Docker by specifying options in the service that starts the Docker engine, or using the config file. The configuration file is a better option because you can keep it in source control. The file is called daemon.json, and the format is the same on all platforms. This is the config file that I use for my Windows Server. On Windows, the configuration file is stored at C:\ProgramData\docker\config\ daemon.json. I'm using the D drive for the data route directory. That's a RAID array on my server, so I get failover and decent IO performance for all Docker storage. That's writable container layers and local volumes, as well as image layers. Also in here, I'm configuring certificates for secure remote access to this Docker engine, and I have a local registry I'm using, which is running on HTTP, not HTTPS, so I've listed that as an allowed insecure registry. On Linux, the config file lives in /etc/docker/ daemon.json. My Linux server uses /data/docker for the route directory, which is a filesystem mount to a RAID array on that machine. I have similar config settings here for TLS and for my local insecure registry. You need to restart the Docker engine when you change the config file, and setting the data route directory is a really easy way to get fast redundant storage for your containers in a server environment. Next, we'll look at how to use as little disk as possible for your images.

Understanding the Docker Layer Cache
Disk is cheap, and your data may be important, which is why Docker doesn't automatically delete image layers, container writable layers, or local volumes. That's very useful when you need to pull a file from a container which you accidentally removed, or when you need to run a container from an old image, but it will eventually swallow up all of your disk. There are some very simple to use housekeeping functions that Docker provides, which I'll show you in this module, but it's also important to understand how to minimize the amount of storage that Docker uses in the first place. Remember that a Docker image is physically stored as many layers, and layers can be shared between images. If you build your images effectively, then you can make good use of the cache by sharing layers, and minimizing the amount of disk that you use. This comes down to how you write your Docker files. The golden rules are to centralize on a few base images, and to optimize your instructions so the most frequently changing content is at the end of the Docker file. Centralizing base images is important because the operating system layers will have frequent updates, probably releasing new versions every month with the latest OS patch. Imagine you have four Linux apps all using different Linux distros, because each project team chooses a distro that they're comfortable with. You now have four base OS images to maintain. These apps will all have a nightly build, and if every OS has a monthly update, then at the end of 6 months, you'll have 24 base OS image versions on your build server, and maybe they'll be in production and test environments too. If you can centralize on Alpine and Debian, you'll have half as many OS layers over the same period. That's less disk, less network bandwidth, and less time spent cleaning up unused image layers. It gets more complicated when you have many application stacks, like Java, .NET, Node, Go, and so on. But all the official Docker hub images should have minimal variants that help you keep your base OS image list small. Optimizing your Docker files to make best use of the cache is even more important, because it doesn't just save disk and network use, it can drastically reduce build times, which speeds up the workflow for developers and for the build servers. Docker computes a hash for each Docker file instruction using the content of the instruction and a checksum of the content for any files that the instruction uses. Docker uses that hash to check for a match in the layer cache. When there's no match for the instruction hash, the cache gets busted, and Docker executes that instruction, and all the instructions that follow. By putting the most commonly changing instructions at the end of the Docker file, you should find regular builds only execute the last one or two instructions. Everything before that comes from the cache, and the build is quick. How well you can do that depends on your applications, but next, I'll cover some general guidance on optimizing Docker files with multistage builds.

Optimizing Docker Images with Multi-stage Builds
Optimizing your Docker files is about making the best use of Docker's layer cache and build cache to minimize use of other resources; time, disk space, and network traffic. The first thing you should do is use multistage builds for pretty much every Docker file to separate any compilation or installation steps from the final image build. Every stage gets its own build cache, so if you only have one stage, then you only get one cache. If you compile code and then package it in a single stage, the compile gets executed every time code changes, which busts the cache, so every packaging step that follows also gets executed. In a multistage build, the compilation doesn't break the cache for the packaging stage, so the compile stage uses the cache up until the actual compilation, and then the packaging stage uses its cache up until the final copy from the compile stage. Multistage builds also make it much easier to break up any processes which are expensive on time or disk space into separate steps. Retrieving application dependencies are a good candidate for this. Many application stacks use an online repository for dependencies, and a local file specifying which dependencies the app needs. Java has this with Maven, Node has NPM, Python has Pip, .NET has NuGet. You can make the dependency download a separate step with the COPY instruction that just copies in the dependency specification. That could be the Maven POM file, or the .NET package.config and csproj files. Then I have a RUN instruction which just installs the dependencies. Dependencies are usually time-consuming to download, and change much less frequently than application code. So there's a lot to be saved by caching the downloaded dependencies in a separate image layer. The last piece of guidance is to use the right base image for your Docker file stages. Multistage builds only produce a single image with the contents of the final stage. You can use the most minimal image in the FROM instruction for your final stage, so your application image only has what it needs for the app to run. Any extra tooling that you need to compile, package, or download components can go into other stages. So those bits don't make it into the final image. That means you can use whatever tools you like to make your Docker files efficient, but the final application image has the smallest possible service area, which is a big security plus point. Many companies go one step further and build their own golden base images. So instead of using the open JDK or .NET images directly in your apps, you have a Docker file for your own base image, which starts from the official Docker hub image. You can use your Docker file for whatever customizations you need that are common across all your apps, and you can define your own naming scheme and update policy for your base images. You store the golden images in your own registry, and that makes it much easier to manage the base images used across your application state, and to make best use of Docker's image cache. Next, I'll show you some of these techniques for optimizing your Docker files in action.

Demo: Optimized Multi-stage Dockerfiles
In this demo, I'll show you the impact you can have when you optimize your Docker files. I'll cover the speed, size, and reuse benefits for maximizing the use of Docker's cache. The demo will use a Java application packaged into a Linux Docker image, and I'll also show you how the same approaches work with Windows Docker files. Here's my simple Docker file for a Java app to run in a Linux container. It starts from the official Java image from Docker hub, using the JDK variant, which has the tool chain to build Java apps. Then it sets up Maven to use for packaging the app. Next, it copies in all the source code files in one go using the ADD instruction, and then it runs the whole Maven build process, resolving dependencies, verifying them, and compiling and packaging the app. Lastly, it sets up the application settings, exposing your port that the app uses, and specifying your startup command for Docker to run Java, loading the jar file, which is the output of the Maven build. This may look okay, after all, it's only seven instructions, but actually it's all wrong, and I'll show you why. I'll start by building the image in the usual way, giving it a tag of worker-java. I've already pulled the Java base image to save on time. You see, the first step is to download and install Maven, and then the dependency resolution starts, which means downloading all the Java packages that the app uses, and there are a lot, so this will take some time. I'll skip forward while this happens. Now the build is done and the application step's run, exposing the port and setting the command to run the jar file. In all, it took 36 seconds. That's not too long, and it's necessary for the first build to do all the steps. All images have an ID, as well as a tag, and this one starts with 24c. If I repeat this command now, it finishes in under a second, nothing has changed since the last build, so every layer has a match in the cache. The image is still 24c, the same as the last build. Now I'll change some code. Don't worry about the details of the app, but the this is the example voting app, which you can find on the dockersamples organization on GitHub. I'm just changing the text of this log line, and now I'll build again. The first step to download Maven comes from the cache, but when we come to the add line, one file in the input content has changed, so now the cache is busted, and every instruction that follows is going to be executed. The dependencies all get downloaded again, even though the dependency list hasn't changed. I'll fast-forward again, and the whole build took 34 seconds, about the same as it took to build the image first time around, and now I have a different image ID, a3e. I'll list my local images and scroll up to the most recent. Here you'll see both images still exist in my local Docker cache. The most recent image I've built has the worker-java tag, and the previous image I can see by its ID, but it has no name. The repository and the tag are both shown as none. This is called a dangling image, because it has no name, I can't run a container from the image name, but the image still exists, and all the layers still exist in the cache, so I can run a container using the image ID. Again, only one file has changed between these two images, but I've got two sets of image layers here because of the structure of the Docker file. I'll run docker image history on the first image ID, and here are the layer IDs with the input command that generated the layer, and the layer size. In this Window, I'll run the same command on the second image ID, and I'll line up the two windows so we can see here where the image history start to diverge. The earlier layers come from the Java base image, the Maven install and the working directory setup is the same for both images, so they share the same layer ID. But the ADD command builds different layers because the source code file had changed, and now there are two 24MB image layers with the source and package content, and above that, different layers again for the application setup. There's no content for these layers, but if there was some extra data in here, it would be duplicated too, because busting the cache means you get different layers, even if the content would be the same. Every time I build this image with any changes in the source code, it's going to take 35 seconds and generate a new image, replacing the previous one, which becomes a dangling image. Each of those images will have a unique 24MB build layer, which stays in the cache using up disk space. So here's the improved version. The first thing to see is that there are two FROM lines, so this is a multistage build. The first stage uses the official Maven build image, so I don't need to manage the Maven install myself, this has the JDK and all the other tools that I need. Then I'm using COPY instead of ADD, which is the recommendation because the ADD instruction can be behave unexpectedly; but I'm only copying a single file instead of the whole directory, just the pom.xml file, which lists all the application dependencies. Now, I run the dependency resolve and verify commands. So after the first build, these commands only execute if the POM file changes, Docker won't download all the dependencies every time. Onto the second stage, I'm starting from the openjdk image using the Java runtime jre variant, based on Alpine Linux. This is the final stage which would generate the application image, so it doesn't need the Java development kit, and I'm using the smallest base image, which gives me everything I need to run my app. Next, I do the application setup with the expose and command instructions. The jar file doesn't exist at this point, but it doesn't need to. The Java command only runs when you start a container, it doesn't execute to this part of the image build. Lastly, I copy in the jar file from the build stage. The from argument specifies a named stage in a multistage Docker file, or you can also use stage numbers or an image name so you can copy files out of another image. So I'll build this as the worker Java image giving it the tag optimized, and specifying the new Dockerfile. The output is going to be pretty similar for the first build, where there are no cache matches, so the dependencies get downloaded, and all of the other instructions will be executed. I'll skip to the completed build, and this actually took 40 seconds, so it was a couple of seconds longer, and there is a minor overhead using multistage builds. I'll list the images for worker-java, and now I have two images with different tags. The optimize one is 88MB, compared to 179MB, so that's a 50% saving in size, because the final image doesn't have the JDK or Maven, or any other tools in there. There's nearly 100MB of content in the original image that I don't need to run my app, and it's all third-party binaries, and if any of them have security vulnerabilities, then so does my app. Repeat the build without any changes, and this is under a second again. Now, here's the real saving; when I go back to source code and make that one line change again. Now when I repeat the build, the first few instructions all come from the cache, because the POM file hasn't changed, all the downloaded dependencies come from the cache. The compilation still takes a few seconds, but I'm quickly into the second stage. This stage has its own cache, and all these instructions have a match in the layer cache, including the application setup, because I moved that earlier in the Docker file. Only the final command actually executes, because the jar file in the build stage has changed. In all, that took 10 seconds. So my optimized Docker file produces a much smaller, much safer image in much less time, and I'll compare the layers by checking the history of the two optimized image IDs. This is the first image that I've built with the optimized Docker file, a8e, and this is the second image ID with the changed source file, 2f0. I'll juggle the scrolling of the Windows here, so all the top layers line up. You can see that the layers match all the way up to the final layer, which contains the application binary, and that's the only difference in the images, so that's the only layer that should be different. Just by structuring the Docker file efficiently, you get all these benefits, and it's the same for Windows images too. I won't repeat the whole demo, but I have an inefficient Docker file here to build a .NET out to run in a Windows container. This uses the SDK image for .NET, copies in all the source code, and does the package restore and build in a single command, so it has all the same issues as my original Java Docker file. The optimized version uses the same approach. Now it's a multistage Docker file, which starts from the SDK image, and copies in just the project file, which lists the dependencies, and it does the dependency restore on its own. Then the rest of the source is copied in, and the project is built and packaged with the publish command. The final stage uses the much smaller .NET runtime image as the base. It sets up the application, and copies in the packaged app from the builder stage. If you build these images, you'll see the optimized version is faster, smaller, and uses more cached layers just like in the Java Linux example. Next, I'll show you how to clean up your Docker images and caches.

Demo: Cleaning up with Docker Prune
In this demo, I'll show you how to clean up your Docker environment with the prune commands. You can prune individual object types, images, volumes, and networks, and you can also prune the whole system. Docker doesn't delete old images when you build or pull new ones which use the same tag as an image that you already have. If you don't realize that, then you'll soon end up with an image list like this with dozens or hundreds of dangling images with none listed in the repository and tag fields. This happens when a new image replaces an existing image with the same tag. The tag gets assigned to the new image, and the old image has no tag, but it still exists on disk, and you can work with it using the image ID. Docker image prune is one command that will save you hundreds of gigabytes of disk space over the years. You can run it as is, and it removes all the dangling images. Or if you're doing a bigger cleanup, you can use the all flag to remove every image except those which are currently being used by your containers. The same goes for Docker volumes. Containers which have volumes attached don't delete the volume when you remove the container. That's the case whether you explicitly mount a volume when you run the container, and if the container image has a volume specified in the Docker file. Docker volume prune will remove any unused local volumes. That's where no containers exist which are using the volume, so it's not attached to any containers in any state. Instead of individually pruning different Docker objects, docker system prune is a combined command which clears the Docker build caches, Docker networks, and dangling images, and you can also use the all flag to remove unused images, and the volumes flag to remove unattached volumes. Docker system prune is something you can run regularly to clean up your system, and it's safe, meaning it won't break any containers that are currently running. It does remove data permanently though. It will warn you for confirmation, but only run it if you're sure you won't need that old dangling image or that unused volume. On a build server or a dev machine, it should be safe to run the all and volumes flags, and this is where you'll see the most benefit, because the non-production boxes are the ones which tend to have the most churn of containers and images. The prune command can take a while, and this is a very full box, so I'll skip past the wait. Now it's done, and it shows you how much space it reclaimed, which in my case was 239GB. Docker system df confirms that. Now I have no local volumes, and three images which are all active. So these are the images for the three containers that I have running. That's it for storage on the Docker engine. I'll cover our first module by looking at the storage in Docker registries, which is important, because if you're serious about using Docker, you'll want to run your own private registry, as well as using Docker hub.

Understanding the Docker Registry Layer Cache
The Docker image format is an open standard and so is the Docker Registry API. The most popular registry is Docker hub, where you'll find official images for all the most popular open source software, and you can use it with a free Docker ID to publish your own images. It's easy to run your own private registry too, which is useful if you want to minimize internet traffic, or you want to share images within your organization and own the storage, or you want to run a registry with extra features. That could be as simple as running a registry server in a container using Docker's open source registry image. Or you could use an external registry service, like Azure Container Registry, or you could run a product you manage yourself, like Docker Trusted Registry. You need to be mindful of storage for all those options. The managed registry services take care of the disk for you, but they probably charge per gigabyte, so you need to use them efficiently. The self-hosted options give you more flexibility in how you pay for storage, but you need to manage it yourself. The key thing to know is that the Docker Registry API works at the image level, so you push and pull images, but it's actually implemented at the image layer level, so the registry uses the same layer caching approach, and the same image layer IDs that the local Docker engines use. That means when you push an image to the registry, Docker sends the list of layer IDs in the image, and the registry checks them against its own stored layers. If any layers match, then they don't get pushed because they're already available in the registry. It doesn't matter whether the layer is there from a previous version of the image that you're pushing, or from an image in a different repository, or even an image from a different user; if there's an exact match, then the layer can be shared with all the images that use it, and the registry is smart about how it shares those layers. Say I push an image from my app which has a layer with some common binaries in it, another user pushes an image for a different app that happens to use the same binaries. So the registry uses its cache, and that layer doesn't need to get pushed. If I then delete my image, any layers which are shared don't get deleted, so you won't find a situation where the shared layers are no longer available. The best way to keep your registry disk usage to a minimum is to optimize your Docker images. So every build of your app shares as many layers from the previous build as possible. We've already seen how to do that in this module. When it comes to managing the storage for the registry, that depends on which registry you're using. In the next demo, I'll run some of those registry servers, and show you how it looks to manage the storage.

Demo: Managing Storage in Private Registries
In this demo, I'll show you how to manage storage in your own Docker registry. I'll use three registry servers, running a registry in a container on my own network, using Azure Container Registry, and using Docker Trusted Registry. For each of those, I'll cover how you use them, and the storage concerns that you need to be aware of. The Docker Registry Server is an open source application written in Go, which means it can run cross platform. There are official images on Docker hub for Linux containers, and you can build your own for Windows. I maintain a Windows variant as a public image on my sixeyed account on Docker hub This is the compose file I use to run my own registry in a Windows container. The image uses a volume for storage, and I'm mounting it from a local path here. This is actually for a portable registry that I take with me on a USB drive. The registry stores everything in this root path. In the repositories directory is the metadata for all the image repos that it stores, and in the blobs directory of the actual image layers, split into folders using the first characters of the image's SHA hash. I just start the registry with docker-composed up, and I can access it from the network. This is a plain HTTP registry, so there's no encryption, and the registry host needs to be listed as an insecure registry in the daemon.json config file for any Docker engines that use it. The open source registry doesn't have a UI like Docker hub, but it does have a REST API. So you can use the catalog endpoint to list all the repositories, and the tags endpoint to drill down into the list of images in one repository. This registry is just a container using a volume for storage, so I can manage the disk using any of the techniques that I've already covered in this course, like bind mounts and volume plugins. On the other end of the spectrum, there are managed container registries. This is one I have running in Azure. ACR is just a private Docker registry. It runs on HTTPs, and it uses Azure authentication, but you can set it up with an admin user that lets you access it outside of Azure using the Docker login command. Sixeyed.azurecr .io is my registry hostname for logging in, and I use the admin user's credentials here, and now I'm logged in. To push an image to a registry other than the default Docker hub, you need to give it a tag which includes the registry hostname. So I'll use docker image tag for my worker-dotnet optimized image, and add a new tag which starts with my ACR hostname. That one image ID now has two tags, and I can push it to my registry. My command refers to the image, but Docker sends up the list of layers to the registry. This is the first push for this image, so everything gets uploaded. But I've also built a new version of my app, and because it's an optimized Docker file, only the top layer of the image is different from the previous version. So I'll tag the v2 optimized image with the acr hostname, and now push that v2 image. All the matching layers from the v1 image already exist in the registry, so only the new layer gets pushed, meaning only 6MB of data needs to be moved to push a whole new version of my application image. There's a basic UI with ACR, so I can navigate and see the repositories that I have stored on the image tags in the repository. ACR has different service levels with a monthly cost tied to the amount of storage that you use, and the features that you need. At the premium level, you can get automatic replication of your registry storage to a separate geographic location, and although ACR does not have its own security scanning features, you can add those in with a third-party integration like Aqua. For a registry with built-in security scanning and image signing, you can use Docker trusted registry. I'm also running this in Azure, but Docker Enterprise runs in any cloud and on-prem. DTR uses the authentication from Docker Enterprise, which could be individually managed accounts, or it can link to LDAP or Active Directory. In the system section under storage, you can configure the storage backend. If you're running on-prem, you can use any of the Docker container options; mounting a network share, a local disk, or a volume. In the cloud, you can use a storage service. So I have my DTR configured to write data to Azure blob storage. DTR also has a garbage collection feature, so you can have it periodically remove dangling images, and you can configure how often it runs and how long it should run for. This is a nice feature for keeping your registry disk space under control. Optimized images are also important here to speed up security scanning. I've pushed a version of my optimized .NET worker image, and then repository is triggered to scan on every push. Scanning is done for each image layer, so if a layer already exists in DTR, it doesn't get scanned again when you push a new image. Only the new layers need to be scanned. When I push version 2, only the new application binary layer actually gets pushed, and back in DTR when I refresh, the scan of v2 is already complete because only the new small layer needed to be scanned. I can drill down here and see there are no issues in my application layer. Although there are some issues in components from the .NET layer, which I might want to look at. Those issues affect both my images because they're actually in the base image. My Docker trusted registry is using Azure blob storage, and I can navigate to the storage account, and see the data. The core registry server is the same as the open source registry, so the file structure is the same, and I can see the blobs and repositories listed here. So how you manage the storage depends on the registry server that you're using, but efficient use is all about optimizing your Docker images using the techniques that you've already learned in this module. Next, I'll wrap up with a quick summary of the course.

Course Summary
Thanks for joining me on this Pluralsight course, and I hope that I've covered container storage at the right level for you. In this module, I focused on the practical side of storage in Docker. You've seen how to configure where Docker stores its data, learned why Docker's storage footprint grows and grows, and how to clean up using the Docker system prune command. You've also learned that you need to optimize your Docker images to make the best use of the cache, and that minimizes build time, disk usage, and network bandwidth. As a minimum, you should be using multistage builds, ensuring that the base image in your final stage is the minimal image which will run your app, and structuring your Docker file so the most commonly changing parts are at the end. There's a great resource from Docker's documentation called Best practices for writing Dockerfiles. Here's a short link that will take you there. That covers the techniques that I've shown you here, and has lots of extra guidance for building better images. I also looked at storage for Docker registries, and showed a couple of options for running your own private registry server. If you're not doing that right now, then it's something you should consider. It gives you a single place to store all the versions of all your apps, no matter what technology stack they use. If the registry does security scanning and image signing too, then it's a great way to secure all your apps in the same way, whether they're new Node.js apps running in Linux containers, or 15-year-old .NET apps running in Windows containers; and here again, optimized Docker images are the best way to make efficient use of your registry storage and scanning. In the course, I've shown you that containers are meant to be short-lived, but the state of your application can live beyond individual containers. Docker volumes and volume plugins are the enablers for stateful scalable applications in containers, and the approaches I've shown you should give you the confidence to run your own stateful apps in Docker; and with that, it's the end of the course, and I hope you found it useful. Please do leave a rating, and feel free to get in touch with me if you have any feedback. I've got plenty of other Pluralsight courses covering Docker, Azure, Linux, and .NET that you might want to check out too. My name's Elton, and thanks again for joining me in Handling Data and Stateful Applications in Docker.


