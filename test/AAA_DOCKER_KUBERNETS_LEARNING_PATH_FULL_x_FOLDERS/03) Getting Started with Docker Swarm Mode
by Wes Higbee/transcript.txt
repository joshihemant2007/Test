The docker engine on a single node revolutionized how we run applications. But in production environments, you need more capacity and reliability than a single node can offer. In this course, Getting Started with Docker Swarm Mode, you'll learn how to control a cluster of nodes as easily as a managing a single node. First, you'll discover how to set up a swarm, add nodes, and launch services that describe containers. Next, you'll explore how to route traffic to the cluster and within the cluster, ensure containers recover from failures. Then, you'll learn how to roll out updates and deploy multi application stacks. Finally, you'll cover how to leverage integrated health checking and protect sensitive information with secrets. By the time you're done with this course, you'll have the knowledge to deploy your own swarm.

Course Overview
Course Overview
Hi. My name is Wes Higbee. Welcome to my course, Getting Started with Docker Swarm Mode. Just this morning, I was doing a TeamCity demo that requires a server to be up and running, that has a website, also multiple agents to perform work, and an external database as well. Now for demo purposes, I was able to spin this up quickly on a single machine with a Docker engine. But in production environments, infrastructures like this can oftentimes need to span multiple nodes so that we have the capacity for the agents to quickly get your work done. It'd be nice to keep the benefits of a single node when we move to multiple nodes, and that's what we're going to talk about in this course with Docker Swarm Mode. We'll start out by looking at how we can set up a swarm, how we can add multiple nodes to that swarm, and how we can schedule a service that spans across those nodes, a service being something like a website. We'll take a look at how we can scale that website up. We'll take a look at how we can access that website externally, so how we can route into that website to be able to find it on whatever node it's running on, and we'll also talk about how that website might use some internal database or maybe an internal API if it needs further information, and how that website can find that internal database or API if they're running on the swarm as well. We'll take a look at look at the power of reconciling a desire state to make sure your application is always running. We'll look at rolling updates, container to container networking. We'll take a look at using stacks to simplify how we deploy multiple applications with a single Docker compose file. We'll also see health checking. And then we'll wrap up with protecting sensitive information using secrets. By the time we're done with this course, you'll know how to set up your own swarm that can handle any work that you throw at it. Let's get started.

Why Multiple Container Hosts?
The Magic of Docker Should Work on Multiple Machines Too
Still to this day, Docker blows my mind with how easy it is to spin up rather complex infrastructure and applications even on a single machine. As I mentioned in the introduction to this course, just the other day I was doing a demo of some software that required multiple moving parts, software that I hadn't set up in any demo environments for probably about six months now, so I didn't have a demo environment available to me. I had to get something set up and running for the demonstration, and it only took me a matter of seconds to grab a docker-compose file, a compose file that you might have seen if followed along with some of my other Docker courses. I can just take this file here. I can save it to disk. I can then hop over to the command line here and cat out the contents of the file that I just saved to disk here in this TeamCity demo folder. And then I can simply run docker-compose and then up, and that's all I have to do to spin up a rather complex set of infrastructure here, turn on both the TeamCity server and an agent, so to separate applications. And the agent piece I could scale as well. In the past, it would have taken me hours to set up a demo environment even if I knew exactly what I was doing, even if I had some VM images that I wanted to restore somewhere, that would still take quite a bit of time, certainly not as fast as this process right here, and I'm running this, and you can see here I'm pulling these images down for the TeamCity server and TeamCity agent so I don't even have the latest version of these on my machine. But even with about a minute download time, I'm already up and running here. And if I switch over to my browser, you can see that the TeamCity server is up and running. And off I can go to set up my demo environment. It's just that easy to get up and running. And I've been doing this type of work with multiple applications on a single computer for quite a while now. In the consulting work I do, I can spin up hundreds and thousands of containers on my local computer so long as they're not so resource intensive. But at some point in time, the things that we want to do will require more resources than a single machine can handle. So we could have a single machine as a development machine, or maybe we have a single machine in some sort of staging or test environment. But when we get to production, we need a lot of resources. And at that point in time, we need something more than a single machine can give us because even the biggest of machines oftentimes can't handle the load that we need to throw at it. And at that point in time, it'd be nice to have something that is just as easy as what we've seen here to be able to spin up containers that can all talk to each other but across multiple nodes, across multiple machines instead of just one machine. And that's where Docker Swarm comes in, and that's what we're going to take a look at with the new swarm mode that's built into the Docker engine as of version 1. 12. And we'll also see the changes that came along in 1. 13 and some of the things that are being worked on right now. I shouldn't have to change anything about my process to get an environment like this up and running. I should have the niceties of the command line here if I want to open up a new tab. And let's just say I want an nginx server as well. Well I could do docker run, and then I could say something like nginx, and then also I need to specify a port here. We'll map that to port 8088 on my host, and we'll map that into the nginx server that's running inside of this container. Boom! Up and running, nginx is being pulled down right now. And just like that, I have an nginx server up and running. Or I come over to the command line, and I'll open up a new tab here, and maybe I want docker run and this time port 8089 we'll use. We'll map that to port 80, and we'll use Apache instead of nginx. The image is started up. I already had this on my machine so it didn't take quite as long. And then if I hop over to the browser, open up a new tab, and just like that I have an Apache server up and running. I should have this same easy workflow when I move to multiple nodes, and that's what I want to help you with in this course.

What Are Some of the Concerns When Moving to Multiple Machines?
Let's take a minute and step back and talk about what's going on here aside from the commands that I fired off and aside from the browser windows that I opened up to show the software running. What is going on? Let's take a minute to do a quick refresh. So behind the scenes, I have a single machine here, that's it. You could think of this as a development machine. It's just a single end user that's running software. Remember that containers are just processes that are running your software. It's just a new way to run your software. And if you're not familiar with that, I encourage you to check out my course called Containers and Images: The Big Picture where I dive into what exactly a container is, what an image is, and why we need all this. I even touch a little bit on the subject that we're talking about in this course when it comes to clustering nodes together to be able to run lots of containers. But nonetheless at the end of the day, we're running processes on a single machine here. So when I fired off the docker-compose command, that fired up a TeamCity server. It also started a TeamCity agent. And I could scale that to multiple agents if I wanted to all from the comfort of that docker-compose command. And then I also used docker run to just one-off run both an nginx server and an Apache server. And I did this all within a matter of seconds. Now in the past setting this up, you wouldn't have done it that fast even if you had installation scripts to install that software. It's just not as pleasant as a workflow. Docker really simplifies things for us for launching just about any piece of software. But at the end of the day, these apps that are running as containers are just running on our single machine, and we can continue to add more of these containers, and it doesn't really matter what's running inside of them. But at some point, we're going to hit a wall. We're going to run out of resources on this single machine. And, of course, we can expand that machine to try to make it bigger. We can buy a machine with more RAM, more CPU cores in it, maybe a faster network card, maybe more storage. But at some point, we're just not going to be able to cram enough resources into a single machine to be able to run all the software that we want. And while that's probably not true for a development scenario, so my local machine here is probably fine, it's really beefy, it's going to be able to run whatever I need, and typically I only run one piece of software at a time. In production, that's just not the case. In production environments, we'll want to spread this work out over a series of machines for a number of reasons. Number one of which one of these might actually take up a lot of compute capacity or RAM and so there might not be enough for the other applications. But beyond that, we might want fault tolerance. If we lose a machine, the only machine that's running all the software we have, we lose everything. If we have multiple machines, we can lose one and it's not a big deal. But one of the things that we don't want in moving to this architecture where we have multiple machines, we don't want to have to manage each of these individually as if they're separate machines. It'd be really nice if we could just treat these as one large machine that we could just throw work at it like we have been my local machine here with Docker. It's be nice if I could just not worry about the fact that I'm using resources across multiple nodes. If we could do that, then theoretically we could have a machine that's as big as we could possibly desire because we could just keep adding more and more nodes to the mix as we need more and more resources. And that's where technology like Docker Swarm comes in. And so that's what we're going to be talking about in this course so that you don't have to manage those nodes individually because as you can imagine, that would become a nightmare as the number of those grows. So we'll see how we can use the Docker engine, which now has SwarmKit embedded in it to provide Docker Swarm mode, we'll see how that allows us to abstract away the node so it just looks like a node at the end of the day. And then we'll also see how we can then run containers on top of any of those nodes almost as if we had one giant machine to work with. As we work through this course, I'll help you understand how we can separate out these two prominent concepts, that of the node and that of the containers that are running on top of those nodes. I'll show you how separating these two pieces and managing these two pieces separately creates a nice division of responsibility where one group of people within your organization can manage all the nodes that are hooked up to your cluster, and then each individual team can decide what it is they want to run on top of this cluster without worrying too much about the underlying node topology. So in many ways, I'll show you how you can have a giant computer that could grow endlessly that your entire organization could use or a subset of your organization could use with multiple teams just throwing software at it, much like I was just throwing software here at my local machine. We'll see how Docker Swarm takes care of everything for us so we don't have to worry about where something runs. We just say, Hey, run this software for me, and it'll figure out where to put that software. It'll figure out where to start up the container at to best utilize our cluster resources. You focus on what you want to run, not on how it runs. Docker Swarm will also take care of considerations like resource utilization. You can tell Docker Swarm about the resources that your application needs, and then it'll figure out where best to put your app based on how many resources you ask for. It'll find the right spot for your application. And, of course, it's not just about putting an application somewhere. Docker Swarm will also take care of wiring applications together. So in a typical setup here, let's say we have a payroll website with an employee API behind the scenes and also a payroll database behind the scenes. Let's say we have this application that we're deploying here with multiple tiers to it. Well, Docker Swarm will make sure that the website that users go to can talk to the API that it needs to get employee information and also can talk to the payroll database. Even if these things are running on different nodes, Docker Swarm can take care of making sure they're networked together so that it's almost as if the software is all running on a single computer, or it's as easy as it is when you're running software on a single computer. So we'll see how the payroll website could ask for the location of the employee API, how an embedded DNS server will provide the IP address to the employee API, and how communication can happen over an overlay network that's created to connect together containers automatically so that you don't have to worry about any of that. And, again, nothing more complicated than the simple docker run commands that we've been using or the docker-compose commands that we've been using thus far. These are just a few of the topics we'll talk about. I wanted to give you a taste of what are some of the concerns that come up when we're running software on multiple nodes instead of a single node. But at the same time, I want you to know that you don't have to do much to account for all of this. The Docker Swarm is built to take the pain out of all of this for you. We'll dive into each of the topics I just touched on and many more throughout this course to help you get your own swarm cluster up and running.

What You Should Already Know
It's probably obvious already, but I assume that you have some working knowledge of Docker coming into this course. So long as those commands that I was running just a moment ago--docker run and docker-compose--so long as those don't scare you to run on your own machine and tinker with the resultant containers, I think you're prepared to take a course like this on Docker Swarm. However, if you'd like a list of prerequisites, I would encourage that you have taken some sort of course on the basics of Docker. And I have an example of this, my course Getting Started with Docker on Windows. There are other examples of getting started courses as well. You could take those. And then, also, you may not know that this is available, and it's not really a prerequisite, but it'll be really helpful if you've watched this course because there are a lot of concepts I've covered in here. It's my Containers and Images: The Big Picture course. In this course, I get into what exactly a container is. I show you how it's just a process with a high degree of isolation. I show you what an image is as well, how it provides a file system and a template for running an application. And I even get into a bit about clustering machines together towards the end of this course. So this is a conceptual course that you might also want to have under your belt before you get into a course like this where we're going to actually get to work building out a cluster with Docker Swarm mode. So to summarize all of this, are you comfortable starting up containers with docker run and also docker-compose on a single machine. If you are, then you're ready for a course like this so long as you're also willing or already are comfortable with starting up multiple virtual machines because we'll need multiple machines to work within this course. It doesn't really matter how you start up multiple virtual machines. I'll show you how to do it with Vagrant and Docker Machine. But at the end of the day, you'll need multiple machines to use for the lab environments in this course, at least if you want to follow along. So if you're comfortable with that, then you're also prepared for this course.

What if a Single Container Isn't Enough?
Let's take a look at examples of some of the issues that we can run into when we are working with a single node that might lead us to want to take advantage of having multiple nodes. And I'll start out with just a simple scenario where I have a single computer, and on that computer, I'm running a single container that contains a customer API. So as I run through the scenarios through the rest of this module, I'm trying to level set where you should be at right now to be able to follow along with this course. And I'm also trying to introduce some of the things that we can currently do and show how we want to ensure that we can continue to do these things when we move from a single node to multiple nodes. Now if you feel that you have a really good understanding of the issues when moving from a single to multiple nodes, you are free to skip the rest of this module and get right to work setting up a swarm in the next module. So we're going to fire up a container here that contains that customer API. And I'm going to map port 3000 out of that container so that I can access the customer API. Don't worry about following along yet. I'm just going to walk through a few examples. In the next module, I'll show you what you need to do to follow along with this course. So I'm going to start up this container, so I have one container running here. And then I'm going to load up this customer website, so we can see we get just a simple response back here from a customer API asking for customer 1, we get back a JSON customer object. Now with the single container running, we might wonder, What's the performance of this customer API? And so I can hop over to the command line, and I have a tool called ab that comes from the Apache project. And let me clear out the screen here. So this tool, I can pass to it a number of requests that I'd like to perform. So let's do 100 web requests on our customer API. And then I just to pass the location of the customer API. So it's on my local host port 3000, and then I'll go access that customer 1 record again. And I'll run this now, and I can see the results of making 100 web requests, and it looks like we have approximately 80 web requests per second that we can handle. Obviously, that's probably fine in development where I'm working on my sole computer here, and I'm not going to be making many requests to that API. But in a production environment, we'll probably need to handle more than 80 requests per second. Now just to prove that this isn't a fluke and that the requests are approximately 80 per second, I can run 1000 of these, and I'll pause the recording while these are running. And there we go, we still have approximately 80 per second even with 1000 requests. Of course right now I'm only making one request at a time, so I could bump up the concurrency here. Let's try making four requests at a time and see if that has an impact. And there you go. With four requests at a time, we can get about 92 per second. I've played around with some other settings for concurrency and have realized that we can get about 90 requests per second. So I'll leave this setting here of making four requests in parallel and then running through 1000 requests. Now at this point in time with a single container then, we're capped out at 90 requests. Do you have any ideas of what we can do to increase the number of requests? Well certainly we'll need to run more containers, or we might need to optimize our API. Now let's just assume that we can't optimize our API, that it is as fast as it's going to be. And instead we need to run more instances of our application to get a higher throughput. Do you have any ideas for how we could run multiple instances of our application? Join me in the next video for that.

Scaling Capacity by Scaling Containers
So one thing we could do, we could add another container on the same machine if we suspect that we have enough resources to be running two instances of our application without them just fighting for the resources on the same machine. So, basically, if we have another CPU that we could use, for example, that's unused or underutilized right now with a single container. So let's try this out. Now one thing we need to be concerned with, we already used port 3000 for the first API, so we need to bind to a separate port for the second instance of our application, which means requests will need to be routed separately to these two instances. But let's ignore that load balancing concern for a minute, and let's just test this out and see what happens. So right now we have a single container running, and I can go ahead and start up a second container. And this time bind to port 3001 instead of 3000 on the host. Run that then. docker ps, and you can see that we have two separate containers running, one on 3000 and one on 3001. So I can clear out the screen here, and I can run my ab command again. This time let's just test out 3001 and make sure it responds. And we get 91 requests per second. So now I'd like to run both requests at the same time, send 1000 requests to 3000 and 1000 requests to 3001 at the same time to see what my resource utilization looks like. I have a command prepared to do this. Let me walk through this. So, basically, I have two URLs I'm going to hit. One is port 3000, and then the other is port 3001. I'm echoing these out on separate lines. There is a line separator here. I'm just doing that so I can pass both of these to this parallel command to run these two commands in parallel. I'm specifying that I want to run two things at a time here since I have two URLs. I could have more than just the two URLs in this list here. And then the command I want to run is ab -n for 1000, and then this is a placeholder for the URL. So, basically, I'll run this ab command twice at the end of the day. Let me fire this off, and this will take a moment. I'll pause the recording, and then when this is done, we'll take a look at the output of each of these separate requests that are made. Both of those have run. Now let's take a look at the results. For port 3001, we are getting about 80 requests per second. And then scroll up here, and for port 3000, our original container, we are also getting about 80. So it looks like we can scale things up here to about 160 requests per second if we run two containers. So if we come back to our slide here, we can continue to deploy additional instances of our application until we saturate a single node's resources. And, of course, one of the concerns we're going to have is how we route requests to each of these instances. How do we load balance the incoming traffic so that we spread it out across multiple instances of our application? And, of course, that load balancing could add some overhead, which could further reduce the increase to 160 that we saw. Maybe we'll be at something like 140, 150.

What About Balancing Load Across Containers on Different Nodes?
And, of course, at some point we might run out of resources on a single node and need to move to multiple nodes. And, of course, we could start up a container over there with another instance of our application and maybe this time also bind to port 3000. But we're stuck with the same issue. We need to consider how we're going to load balance requests then to that separate node. After all, we don't want our customers to have to decide which instance of the app they want to use by trying out each of the different URLs to each of the different ports until they find one that's responsive enough for their needs.

What Happens When a Container Fails?
Another issue we could be confronted with if our application dies inside of our container when we make some invalid request or some request that has a critical failure, well, we just lost an instance of our application. So, for example, this application that I've deployed here has a route called critical. And if I hit this route, you'll see we get a critical failure, and the app is shutting down. And if I go look at the command line, above are the two containers that I was running. And now if I run docker ps again, you can see we only have one container running. One of them has stopped now because we had a critical failure. So we only have one instance of our app up and running. Of course, this could cause some serious problems. And in a development environment, it might just bring up the docker run command again and restart this container on port 3001. Actually, it looks like port 3000 is the one that died here. So I'll restart port 3000. And now if I clear out the screen and do a docker ps again, you can see we now have two containers up and running. So we fixed the problem. And in a development environment, this is fine. But in production, we need to be able to automatically react to this. So we're starting to amass quite a few concerns here. Let's make a quick list. So we had scaling the capacity of our application, and we are doing that with multiple containers. And then we need to worry about load balancing to those multiple containers. A secondary concern here then is container failure. What happens when a container fails? And, of course, there are many things we could do. We could set a restart policy on the container so that if it dies, we just relaunch it. And, of course, we have that with a single Docker engine as well. We can hop over to the command line. And when we do our docker run here, we can add a flag, --restart, to set a restart policy. And we can set a value of always restart, do not restart, on a failure go ahead and restart, and then go ahead and restart unless it's stopped. And this, of course, applies to rebooting our machine as well. So now if I launch one of these containers on port 3002 with unless-stopped as the restart policy, and in this case, I need to drop off the --rm. Now if I do a docker ps and clear out the screen, you can see we have three containers up and running, the third of which on port 3002 will recover from failures. So now if I go to my browser and make a request to 3002 on the critical port, you can see I can continue to make this request. It takes a second for the container to come back up, but I can continue to use that container. So this is a concern we'll have regardless if we're working with a single node or multiple nodes.

What Happens When a Node Fails?
Closely related to container failure is node failure. So in the case of a container failure, well, we can just relaunch a container, and we can do this over and over and over again any time our container fails. But what happens when we lose an entire node because that means that all of the containers go with it? How do we react to this? And, again, in development, this is something where we would reboot our machine, not a big deal, maybe even in a test environment, and then we would be back up and running if our containers just restart perhaps because we had that restart policy set. But in production when we lose a node, we need to bring up additional capacity somewhere else and also deal with how we load balance traffic to those new instances or new containers that are running so we can keep our application serving our users while we go ahead and fix the node that failed and perhaps bring the instances back up on that node or bring up some new node to replace the node that failed. So this is a scenario that we can't even approach unless we have some means of deploying our containers to multiple nodes. So when we have a node failure, we need to redistribute our containers and then fix the node that failed or replace it somehow. And redistribution is the tricky part. Right now with single Docker instances on each of the nodes that we have in a cluster, we'd have to manually go into each node and to bring up the containers maybe with something like docker run. And, of course, maybe we can automate that process with something like Ansible. Nonetheless, we have some orchestration concerns. We have to decide where the work is going to be placed, where we have capacity at, and then we have to get that work scheduled in that location. So loosely speaking, placement is another concern. Closely related to node failure is node maintenance. What if I want to take a node down to work on it, and it has containers on it that are in my production environment fulfilling requests. How do I get those containers off of it and onto a different node? So much like node failure, node maintenance requires us to think about the same set of concerns as node failure.

What About Internal Communication?
The one last area I'd like to look at is internal communication. So perhaps we have a web app that needs to talk to a database, or we have a web app that needs to talk to an API. Let's take a look at a quick demo of this. Let's talk about an example where we have our customer API that we've just been using, and that's running on port 3000. And we'll scale back to just one instance of this. And then let's say that we also have some sort of balance API or website that uses the customer API to produce a balance report. And let's say that this is going to run on port 4000. Again, all on the same node here talking about a situation where we're running perhaps in a development environment, and we want to have all of these services running locally. And, of course, the challenge here then is how do we get the balance API to talk to the customer API that it depends upon. And that customer API could be a database. It could be one of many different things when you have internal communication where one of your applications needs to talk to another internal application to fulfil a given request that it's processing. So to see this in action, first off, I will start the customer API on port 3000. Now just to avoid any confusion, at some point you'll be following along. I have all of these APIs built into a single image for simplicity. That's why I'm naming this customer API, and it's going to run on port 3000. So I can start that up then. And then I need a second call to docker run. This time on port 4000, I'll start up a separate container for the balance API. And then I'll add an extra flag here to set an environment variable that says, Hey, the customer API that you're looking for (we have that dependency), the customer API, you'll find it here. And I've given an IP address and a port. And, of course, this is not a very reliable way to set the destination for the customer API, but it works for now so that we can see the two pieces working together so I can go ahead and launch this then. And now we have two containers running. We have our balance API on port 4000, and we have on 3000 our customer API exactly like we had here in the diagram. Let's go test this out. So the customer API you can see is working on port 3000. And then the balance API is working here on port 4000, and behind the scenes this balance request that we're making from the browser here is actually being routed over to the customer API. Now that might not be so obvious, so let's do this. Let's go ahead and stop the customer API, so Docker and stop, and then I'll say customer-api. So now we only have the balance API up and running. So now if I flip over to the browser, and then when I reload, you'll see we get a connection failure trying to connect to port 3000. And that's because our customer API is now down. Now you might be wondering why did I use this IP address? It's the IP address of my machine, the same machine here that localhost is hitting. I can do this because right now, I'm binding each of these APIs to a port on my host machine so I can use the host IP address to make them communicate together. This is a simple setup. In a minute, we'll take a look at a different setup where we allow each container just to have its own IP address. Let's go back now and turn our customer API back on. So port 3000, customer-api, and run this. Now if I come back over to the web browser and refresh, you can see our balance request is going through now, which is making a web request behind the scenes to that customer API that we just brought back up.

User Defined Networks to Connect Containers on a Single Node
So when it comes to having multiple applications that depend upon each other, and you're running containers with each of these applications, networking can get tricky. And I just showed you a simple example where we bound port 4000 to our balance API and port 3000 to our customer API. So we have those bound to our node, and we could even access them externally as we saw by hitting localhost to access both of these services. And then when I set up the balance API, I use the IP address of the node as well. So really what we had going on here, we had the balance API talking to the node's network interface and connecting on port 3000 to be able to connect back to the customer API, and that's because the customer API exposed itself by mapping to port 3000 on the host. Of course, this is going to be tricky to scale, and it defeats many of the purposes of Docker because we're back to sharing a single address space on the node to connect our apps together both internally and then to get external traffic into our applications. It's like we've lost the benefits of a network namespace even though we're using containers. So what we really want is to not use that and use something else. And that's why Docker introduced a concept of a network, a custom network. So we can create a virtual network then, and we can wire together these two containers so that they can talk to each other. And then we don't even need to expose the customer API because the customer API is just needed behind the scenes by the balance API. We can then go to the balance API on port 4000 via the node so we can get in from the outside world to the balance API. And then behind the scenes, the balance API can connect to this private network to get access to the customer API that it needs. And then we never need to expose a port for the customer API unless for some reason we need that accessible via the outside world as well. The nice thing is now each container here has its own IP address that we're using to connect together and talk to each other on this private network. So let's see what this looks like by switching over to the command line and running through another scenario. So I'm back here on the command line, and I've stopped all of the containers that I had in the last example. I got rid of those. And I'm going to list out the networks that I have here. There are a series of networks. We're not going to talk about what's in here. What I want to show, though, is that I can create a new network for the purposes I demonstrated in that diagram a moment ago, so I can create a new network by docker network and then create. I need to specify a type of network, and in this case, I'm going to use a bridge network. It's a network I can use locally to connect containers together on a single node. And then I just need a name for this. So how about we call this the company network. I can run that create statement then, and you can see an Id is printed out. If I run docker network ls, though, you can see we now have a new company network. You also see the TeamCity network down below that was created by the example we worked through earlier in the course. Now I'll go ahead and launch the same two containers, except I'll add an additional flag that specifies to use that company network. So let's start out with the customer API container first. So we have a docker run here, and then I have -d for detached in the background. We've got port 3000. Actually, we don't need that port to be exposed anymore now. We just need this internally. So I'll get rid of that because we don't need to access the API from the outside world. I'll then specify the name of the container as customer-api just like before. And then I've got my image name here. And by the way, I went ahead and built a separate image just to avoid confusion here. I now have a customer image that we can use with just a customer API. So this is pretty much what we had before minus the port being bound to the host on port 3000. Now the one difference here, I need to connect up to that private network. Do you know how I do that? What flag do I need to pass here? Well in this case, I need to specify the network, and then I just need to set that to company. So that will attach this container to the company network that I created. When I run that then, you can see it's created Docker and ps, you can see that that's running with the one caveat now that we can't access this because I didn't map a port to the host. It is still running in the background though, and it is still accessible on that private network because the server is still up and running inside of that container. If we were to go directly to that container, we could access the server that's running inside. So that's the customer API. Let's do another docker run here in the background. We'll give this one the name of balance-api. This one we will also attach to the network company. And in this case, this is swarmgs, and then I have a balance image. Now I need to specify a few extra things here. First off, I need to specify the port. Quick quiz: Do you remember how to do that to bind this to port 4000 on the host? In this case, -p and then 4000 as the port we want on the host and 3000 is the port that the API is listening on inside the container. And then I also need to specify an environment variable again to connect this back to the customer API. So it needs to know how to connect to that customer API. And in this case, the environment variable again is MYWEB_, and then CUSTOMER_API. And you need to set that equal to something. Now before I did the 192. 168. 0. 102, and then I did port 3000. But we are not opening that port up again on our host so we can't use this. So another quick quiz: If you've worked with private networks before with Docker, do you know what I need to replace this part with? Well in this case, we leave the port alone. We're still listening on 3000 for the customer API. We just get rid of using the host IP address of my Docker host. And instead I use the name of the customer API container that I created. And that's because the beautiful thing with these networks that we can create, these custom networks, they have service discovery enabled for us. And service discovery is set up based on the name of the container. So in this case, balance-api or customer-api. These are resolvable via DNS with these container names. So I can say ping customer-api, and I'll get back an IP address that is specific to the customer API container. Or I could say ping and balance-api, and I will get back an IP address specific to the balance API container. And, again, that comes with these networks that we've created, user-defined networks come with an embedded DNS server for service discovery. And so this is why I can use customer API, the container name with the customer API, when I'm passing in the environment variable to tell the balance API how to find the customer API. So I'll go ahead and run this now, docker ps, and you can see both of these containers are running now, and we still have port 4000 for our balance API exposed. So if I come over to the web browser, and I refresh here, you can see that our balance API is still working. However, we can't access the customer API because we didn't map that back to my Docker host. And just to validate things, if I do a docker and I do a stop on the customer API, stop the customer API, docker ps just to show that we are only running the balance API, if I go back over to the web browser and refresh now, now you can see that our balance API cannot find customer API on port 3000. So then if, of course, I go back to Docker and run, and I go back to the statement to re-create the customer API again attached to the company network, everything runs fine now. If I hadn't attached to the company network though, this wouldn't work out. So it's this private network that I created that facilitated all of this communication. So we have this nice means of creating these custom networks on a single node to be able to wire our containers together. It's all automatic. We don't have to do any of this by hand. It's a really neat feature of Docker on a single machine. This is another piece of the puzzle that we'll want to take a look at when we move to multiple nodes, and we'll see how an overlay network can make things just as easy when we're dealing with networking across multiple nodes.

docker-compose Simplifies Complex Containers and Networking
Now while I was running all of these commands here to create a user-defined network and then to launch containers attached to it, we have a lot of parameters and a lot of commands going on here, and this can get very confusing. So I quickly want to show you an alternative way to create both of the containers and the user-defined network without nearly as much hassle, and we can do that with docker-compose. So docker ps here, you can see the two containers running. I'll go ahead and stop both of these. We have a clean slate. I'll clear out the screen here. And then I'm going to change directories to where I have this example at. I'll move into a company folder. So we're thinking of creating our company infrastructure. And inside of here, I have a company. yml file for our docker-compose setup. And you can see this is not so convoluted. We have the customer API pointing to the customer image. We have the balance API pointing to the balance image. So we'll create two containers here for the services. And then port-wise, we expose port 4000 for the balance API. And then under the environment section, I specify where the customer API is at. And, again, I can use the same format here, customer-api via service discovery on a private network back to port 3000. That maps back to the service I have here or the container I have here called customer-api. And I don't need to specify anything about the network because that's all automatic by default with docker-compose. A private network will be created. We'll see that in a moment here. So this replaces the need for all the docker run calls and the call to create a Docker network. Let's take this for a spin. First off, docker network and ls. You can see what networks exist. We have the company network that we created before. I'll leave that there because a different one will be created. And then I can run docker-compose. The file here is company. yml. And then I can say up. And I'll do that detached in the background. Now you'll see in the output here that a network was created called company_default. If I do a network ls again, you'll see that that network is listed here in addition to the company network we created. So this is the nice thing about docker-compose, it creates a network specific to a compose file so that the containers are automatically attached to their own private network, which means that they have service discovery to talk to each other. And you can see that we created both of those containers for the customer API and for the balance API. So if I do a docker ps here, we have the same two containers up and running. And you can see that port 4000 is exposed for our balance API, so let's hop over to the browser, refresh here, and you can see our balance API is still working. If I go back to the command line, now if you've used docker-compose before, do you know what I can do here to turn off the customer API? Well in this case with docker-compose, instead of the up command, I can do stop, and then I can specify the service that I want to stop. And in this case, I want to stop the customer API. So I'll put that in. When I do that, we shut down the customer API. If I go back to the browser and refresh, you can see that we don't get our request to go through for the balance request because we can't access the customer API. If I then go back and start up the customer API, I can now make requests again. So docker-compose really simplifies things when it comes to spinning up more complex container setups and networks with Docker. And I expect that you're somewhat familiar with this coming into this course because I want to talk about how docker-compose can be used with a swarm instead of a single node because this is a very nice way to spin things up as opposed to running all those commands that I was just showing you a moment ago. Really at the end of the day all we have is this compose file with about 10 lines inside of it to spin up a rather complicated infrastructure. And that's a beautiful thing. But we want to make sure that this comes over to the Docker Swarm world. We don't want to lose this declarative style of specifying what we would like to have created and then having something else create it for us. It's a nice separation of concerns. And, of course, we want that networking capability to create these networks to connect our containers together automatically. We want to see that come to a multi-node setup as well. Let's talk about what that might look like next, and let's talk about some of the issues that we'll run into when we have container to container dependencies for internal communications and we're spanning multiple nodes.

What About Scaling Internal Application Dependencies?
When we get to production and we have application dependencies like this, we'll have many of the same considerations we had with a single application. How can we scale each of these different applications? How can we run instances across different nodes so that we can have more resources but also fault tolerance and high availability for our application if we need it? So we'll have a load balancer for incoming requests just like we had back with a single application. And this time it's a load balancer for our balance API. Now right now it's just pointing to a single instance of a container running our balance API. And then that balance API needs to talk to our customer API. Now instead of doing that directly, we really want some sort of load balancer for that as well so that we can scale the customer API. And then that load balancer will then route us to an instance of our application for our customer API when a balance API needs to make a web request. Once we have these load balancers in place, then if we need to add another node or just start up more containers on a single node, it's not a problem to then spin up another container. And in this case with the customer API, we could just point at that one as well from our customer load balancer. We could also start up more instances of our balance API. We could start two more of those, and then we could hook those up to our balance load balancer so we could route requests to those as well, and then those need the customer API so they all point at the customer API load balancer. And now we can scale each of these components separately. So if we need 10 or 20 customer APIs running, and we need 30 or 40 balance APIs running, we can have that, and we can flexibly scale the capacity. Now on a single node, if you're using docker-compose, you might have in the past used the capability to scale an application up and down. You could do all of this without much of a hassle because you're running on a single node, and you can have a user-defined network on that node that connects all of these pieces together. And it's all pretty much transparent to you. But when you move to multiple nodes if you want to have this same ability to scale different dimensions of your application, especially when internal communication is involved, then obviously there are going to be different considerations. And hopefully things can be just as easy when we're working with multiple nodes. And we will see that in this course. So keep this in mind. We can have these interdependencies in our application, and that can really make things complex. But I'll show you how Docker Swarm makes things really simple so you don't have to think much about these different load balancers and how you scale different components and how to worry about wiring all these pieces together because I'm sure you can imagine if you had to do all of this by hand in terms of placement and then load balancing, this could easily become a nightmare. So we now have a laundry list of some of the things that we might want to worry about as we move to a production environment with multiple nodes. These are just a few of the things that we can think about. I wanted to give you some demonstrations of some practical things. Now let's turn our attention to how we can use Docker Swarm to easily build out and schedule work across multiple nodes. And to do that, the first thing we're going to take a look at in the next module is just how we can get Docker Swarm set up and running on a single node and what it looks like to start up containers on that single node via Docker Swarm.

Creating a Swarm and Running a Service
Preparing a Single Node to Follow Along
In this module, we're going to switch gears and take a look at how we can set up Docker Swarm on a single node and how we can run a container via Docker Swarm on that single node. So the whole focus of this module is just to get familiar with how we launch containers with Docker Swarm and how that differs from launching containers as we've seen with the Docker engine via the docker run command or docker-compose. Now we'll see how we can use what's called a service to start up a container and also how we can scale from a single container to multiple containers with a service on a single node. And then once we've seen how to do that and once we've learned some of the basic terms around Docker Swarm, we will turn our attention then in the next module to how we can add in multiple nodes and see how the service that we create can be used to spawn containers across those multiple nodes. So the good thing is in this module you can focus on just a single computer. This really makes things easy for you to follow along because you can use the same environment you've probably been using thus far with Docker on your local development computer. And you can follow along with just about any edition of Docker. And by that I mean you could use Docker for Mac on a Mac. You could use Docker for Windows on a Windows machine. If you are going to use Docker for Windows, make sure you enable Linux containers mode instead of running Windows containers. I'll be working with a Linux container example. And, of course, you could use Docker on a Linux distribution. Whatever environment is comfortable for you to start in, all you need is to have Docker installed on a single machine to follow along in this module. And then version-wise, at the time of recording 1. 13. 1 is the newest version. However, there is a new pre-release version that I'll be using in this course. If you come out to the Docker repository, and you take a look at the Releases tab, there is a v17. 03. 0-ce-rc1. Docker recently changed its versioning scheme, so in the past we had 1. 13 and 1. 12 before that. Now we will have a version that matches the year, so 2017 except it's just the 17 part, and then dot, and then the month, so 03 now for March. So I'm going to actually be using this 2017 March release. So as long as you're using that or something newer, you'll be fine to follow along with the examples in this course. If you're using Mac or Windows, and you're curious about what version you have, on a Mac go up to the menu here and click on the little Docker icon. In Windows, this is in the lower right in the tray, you'll find an icon, and there will be an About Docker option. Click on that, and you can see the version. In here I'm using 17. 03 and then ce-rc1. Now just to be safe, I am going to come into the Preferences here, I'm going to go to the Reset tab, and I'm going to reset to factory defaults so I know I'm starting from the same place that you are. Now that that's reset, I'll run docker version just to confirm the numbers. And then in the output, take note of the experimental field. I'll talk about this next, but I want to enable this for this course to have a few extra features of Docker Swarm mode.

Enabling Experimental Features
In this course, I am going to enable the experimental mode. The Docker Swarm mode features that are embedded into the Docker engine are actively being developed. Now much of what we're going to use is stable. But there are a few nice features that are being developed that are in experimental mode right now. For example, the docker service logs subcommand. Now when you're following along with this course, you might want to check the release notes for Docker and look for the word experimental. You'll have a newer version likely. And at that point in time, there'll be newer features that are experimental. You can check to see if there's anything that you want to enable. So if you're following along especially in early 2017, you'll probably want to turn experimental on as well. And the neat thing is as of version 1. 13, if you want to use experimental features for Docker, all you have to do is set the configuration flag. It used to be that you needed a separate build of the Docker engine. Now you don't need that. Now on a Mac here and on Windows it's going to be similar, come up to the icon or down in the tray on Windows, and come to Preferences. Go to the Daemon tab. And make sure the box here is ticked for experimental features. You could also come over to the Advanced tab and set this up explicitly inside of the configuration. And all that is is experimental, and then set the value to true or false depending on which way you want that to be configured. In fact if you want, you can uncheck and then check this box, and I believe it'll add the configuration in here. Yeah, experimental: true now. If I uncheck that, it shows you the modified config file that's set to false now. So make sure that's enabled--experimental: true. And then if you're enabling that for the first time, go ahead and Apply and Restart your Docker engine. On a Linux machine, you just need to put experimental true in the configuration file for the Docker daemon and restart your Docker daemon. Docker's back up and running. And just to confirm again, run the docker version command, and you should have experimental: true here on the server side.

Enabling Swarm Mode by Initializing a New Swarm
So you now have a single node with the Docker engine installed on it. And you also have the Docker client here for the CLI. And you can make requests to the Docker engine, for example, to list the containers that are running. And I have none right now. Of course we could look at images as well. Now at the fresh install of the Docker engine, if we take a look at the docker info command, we can take a look at information, and if you scroll up here, at some point you'll see swarm and then colon, and then you should see inactive if you have a fresh install. And that's because the swarm mode of the Docker daemon is not enabled by default. That's why it's inactive. And, by the way, swarm and then the word mode, you might be wondering about that versus the old Docker Swarm. Well swarm mode just means you're enabling the swarm mode of the Docker engine. That's where that name comes from. Right now that mode is disabled. So if you want to enable it here with your single node, you'll use one of a series of commands that are available now. If I take a look at the help for the Docker command, there is a subcommand, actually a whole suite of subcommands that have sub-subcommands. So there're all these new management commands as they're referred to. And one of those is the swarm management command. And so each of these management commands up here has a series of subcommands. For example, there is now a management command for containers and images to organize the growing number of commands that were piling up via the Docker CLI. This is getting kind of unruly, and it's hard to know which of these relate to containers versus images versus other aspects. So the Docker CLI has now been reorganized. So, for example, we now have docker and image and then ls. This replaces what you're used to with docker images. We also have docker and container and ls. This replaces what you're used to with docker ps. That's not really what we're looking at. We're looking at swarm mode. There's a subcommand for that. And if you get the help for that, you can see there is a series of sub-subcommands as I refer to them. And one of these is called init that will help us initialize a swarm. And so if we have a node that doesn't have swarm mode enabled, we need to enable it. And the way we do that is by creating a new swarm. And to do that, well, a couple of things. We could get some help for the init command, and there are some options here, but we'll largely just ignore those, and we'll run docker swarm and init. That's all you have to do on a single node to initialize a brand-new swarm. And now if I run the docker info command again, what do you think we will see for the swarm section of this information output? Well in this case if we scroll up now, we have a bunch of information below the swarm section, quite a few lines here. And most importantly, the swarm mode is now active. So congratulations! That's all it takes to set up a new swarm. It is a swarm of a single node. Nonetheless, you have swarm mode enabled, and we can now begin to work with the features of Docker Swarm.

Listing and Inspecting Nodes
If you run the help again for the Docker command and scroll up to those management commands, we now have a few other management commands that have been enabled for us. So we just looked at the swarm sub-subcommands. We also have node, and we have service. Stack is also related to Docker Swarm mode, and so is secret. We'll get to those later. For now, let's take a look at docker node and get some help on that. So this subcommand that's a management command helps us manage the nodes that are a part of a swarm, and a swarm is just a cluster, like a swarm of bees chasing after you. You need something to manage those bees, to add bees to the swarm if you need more of them, or remove bees from the swarm if there are too many that are chasing after you. Anyway, to get status information, we could run something like docker and then node, and then we could do ls to list out the nodes that are a part of this swarm. And right now, we just have one, and we will only have one in this module. And for that one, we have a hostname, which is moby, and that's because I'm using Docker for Mac and so this is a Moby Linux VM. You'll have something similar on Windows. And if you're running in a Linux environment, you'll have the hostname of your machine. So in Docker for Mac and Windows, keep in mind there's a VM behind the scenes, where a Docker daemon is running in a Linux environment for you. There's also an Id for this node. The little asterisk indicates that this is the node that you're on right now, and since there's only one, obviously that will be the one that's asterisked. We have a status of Ready so our node is ready to perform work. It's Active. And in this case, it is the Leader node. So it's in charge of everything. We'll get into more of these status fields as we add other nodes later on. In addition to docker node ls, we have a few other commands, one of which is to inspect the node. So we can do docker node and then inspect. And if we don't pass anything, the command is going to yell at us. One of the convenient features we can pass, though, is the word self to get information about ourselves, so the node that we're on right now. And when I do that, then we get a whole bunch of information about this node that is part of the cluster more than we get in that simple list with docker node ls. So just like everything in Docker, you can dig into an individual node and get some more information. And there are a couple of ways you can do that. You can do the docker and then node and then inspect. Of course there's also the docker inspect subcommand, which you can still use for now. And you can pass the name of the node to this as well and get the same information. So moby was the name, the hostname for this node. The other way is with docker node ls, and you can grab part of the Id, so 301 is the start of this Id. So we can do docker inspect as well and put 301 in. And as long as that's unique enough in terms of all the things that Docker identifies, then we can go ahead and run that and get the same information back.

Creating an NGINX Service
Now for the good part. We want to run a container on our node just like we have been with the Docker engine, via docker run, or with docker-compose. But now we want to do it through swarm. And now that we've enabled swarm mode, we have some additional functionality. First off, we can run our containers just like we have in the past with docker run. However, they will only be a part of the single node. They won't be a part of a cluster anymore. If we want our containers to be a part of the cluster, we use the new subcommand called service that I just pointed out. And if you get help for this, you'll see there are a series of sub-subcommands, one of which is called create. We also can list services. We can inspect services. We can get logs, so there are a lot of commands that you're probably familiar with when working with containers that are available here. And over time, this list is going to grow. So what you're likely to see actually is that the list of subcommands for service is similar to the list of subcommands with regards to managing a container. Now, obviously, there are quite a few more for managing containers. But we will see this list grow up here for services. Create on a service, though not the same, is somewhat similar to what we're used to with run when it comes to a container. I'm just saying somewhat similar because it's the same entry point to getting a container running, but they are not the exact same thing. So do not confuse the two, just know that they're somewhat similar, and you're going to see a lot of similar flags and arguments passed to the docker service. And then create subcommand, you'll see similar arguments that you would normally be passing to docker run or maybe arguments that you'd be typing up inside of a docker-compose. yml file. So if you look at docker service and create and get the help for that, you're going to see a lot of arguments that look familiar. And this list is also going to grow over time. So docker service and create is how we get to a running container. So let's go ahead and create our first container here. So docker service and then create. And, by the way, once we've created and seen this container running, I'm going to break down how this works and show you what exactly a service is and what a task is. But, first, let's get one up and running before we worry about all these new concepts and terms and how things actually work behind the scenes. So docker service and then create, and then we're going to give this a name. And in this case, let's just call this web. And then I need to specify a container image, so let's go ahead and use nginx since that's available publicly, the official nginx image. And, of course, nginx is a web server, so I want to publish a port. So we can use -p for that or the more verbose --publish flag. And then we specify the port on our host in this case, 8080. And I'll map that into 80, which is what nginx is running on inside the container. So just like that, I can run that command then. You'll see an identifier is returned back. This is an identifier for the service that we created. If I go ahead and open up localhost and port 8080, you'll see that we have nginx up and running on our local computer here, almost as if we'd used docker run to create the container. In fact, if we go back to the command line and do a docker ps, we can see that we do have a container running with the nginx command inside of it based on the nginx image. So that looks quite familiar. Let's dissect what's going on behind the scenes. Join me in the next video for that.

A Service Is a Definition of an Application
So a service is how we create containers with Docker Swarm mode. So what exactly is a service? A service is simply a definition. It's a definition describing what you would like to run. So your application. It could be a definition to run a website like we did with nginx here. It could be a definition to run a database. It could be a definition to run a customer API like we saw in the last module. So it describes an application that you would like to run. But it's not the actual running application inside of a container. So in our example here of nginx, we have a service definition that says, Hey, we'd like to use the official nginx image, and then we'd also like to publish the port 80 inside of the container onto the host as port 8080. We'll get into the specifics of how that works later on in the networking module of this course. We can also set things like the command to run, the working directory to start in when that container started up, or even environment variables. So these will control the running application. But we can also set things like memory and CPU limits. Or we can say what networks that we want our application to be able to connect to. And then, lastly, we can set things like replicas, and this allows us to specify that we would like more than one container to run if we set this to 2. So as you can see, this is a definition of what containers we'd ultimately like to run. And, really, you should think of a service as a definition for your application. A service is agnostic of the fact that we're using containers for our application. As far as the service is concerned, we could actually have a VM behind the scenes that's running our application. And even if we had a VM, well, we could use the same bits of information to describe what VM we want to run. We could have a VM image. We could have ports that we want to expose from the application running inside that VM. We could have the command for the app we want to run nginx again. We can specify the working directory, the environment variables, limits on that VM. We could say what networks we want that VM to attach to. And we could even say how many VMs we'd like to spin up. Now this isn't possible right now, but things are architected this way so that in the future Docker Swarm might be able to run more than just a container. For now, though, we'll go back to just containers.

Services Lead to Tasks
So a service is a definition. It describes a desired state for your application that you'd like two instances of your application to be running, but that's not directly mapped into running containers. There's another notion in between, and that's called a task. So a service leads to one or more tasks that you can think of as slots that are eventually used to run a single container. So if you ask for two replicas when you're defining a service, then there will be two tasks that lead to two separate containers. A quick quiz for you: How do you think I can print out the services that are part of my swarm? In this case, docker service, I can get some help here, and you can see there is an ls command. If I do that, docker service and then ls, we can see that we have the web service that we created before. There's the identifier that was printed out when we created this, which by the way I have over in another tab here. I have a docker service create statement, and here is the Id that comes back, so sugvw, sugvw over here, and docker service ls. So we have an Id. We have a name, which is the name we gave it. Remember, we gave a name here with --name and then web. And then we have replicated as the mode. There are two types of services. One is replicated where we can create multiple instances. And those instances will be spread out across the cluster as the scheduling component of Docker Swarm sees fit. There's another mode right now called global that will essentially run only one instance of our application on each node in the cluster. We'll get into this more a little bit later on. We then have the number of replicas. In this case, we have 1 of 1. And this means that we have one that we asked for, and that one is up and running. It's operational, hence the 1 of 1. We could have 0 of 1 if things weren't up and running yet. And then we have the image in this case that we asked for--nginx and then the tag latest. Another quick quiz: How can I get more information about this service than what's available here from service ls? Well in this case, we can do a docker and then service, and then there's an inspect sub-subcommand to which we can pass either the service name, web, or the Id. And that will give us all the inner details of this service. And there's quite a bit of information here. You have the name of web just like we had up above here. But we also have a section that's called TaskTemplate. And this is underneath the specification for this service, which is a long object with many different configuration elements on it. So the definition that we provide turns into this specification and this TaskTemplate that is then used to create the tasks. For example, we have a mode here of replicated and specifically one replica. So that means there will be one task then. And that task will have this configuration. It'll have this particular image, the nginx image. And the specification here is a little more verbose providing the digest of the specific image that we'll be using. We also have other things like resource limits if we had provided that information when creating the service. So a service is a definition, a recipe for tasks. And then a task ultimately turns into a running container. It's a slot though. It's not the actual running container itself. It's another declarative concept that describes the container that we would like to be running. And if we hop over to the command line and clear things out here, docker and then service, get the help for this again, you can see there is a ps. This lists the tasks of a service. So docker service ls lists our service, and then ps breaks down the tasks. So in our case, docker service and then ps, and then this command takes the name of the service. So I'll put in web here. And then what do you think it's going to print out when I run this? This is going to be a lot like docker ps. We get a list of tasks back and, remember, a task maps to a container. Now you might be wondering why there are four on my screen and there's probably only one on yours. Well I actually started this demo yesterday and picked it up today, so my computer probably went to sleep, and that probably killed off the container that I had running. And so the container was restarted when my computer came back up. So ignore these three lines here and just focus on this very first one that says Running on it. So this is the one task that corresponds to the service that I created. And this one task then corresponds to a single container. Now because I've got this history here that might be confusing, join me in the next video where I show you how to remove a service, and then we'll add it back so we have a nice clean working slate, and my output looks a little bit more like yours.

Removing a Service
So in addition to creating a service somewhat like creating a container, we can also remove a service. So we can get rid of this web service. All I have to do is docker and then service and then remove (rm). And then I need to give a name here, in this case web. Once that's done, then docker service and ls, and you'll see that the service is now gone. And if I do a docker and service and ps, put in the name web, that doesn't exist anymore, so there are no tasks for us to look at. And then, of course, if I want, docker service and then I can use create to re-create the service. And do you remember how I can do that for the nginx example? Take a minute and pause the video and see if you can create your service after removing it. So here's the command I used. So --name of web --publish, and then a publish port 80 again. It has port 8080 on the swarm, and then the nginx image. When I run this then, that will re-create the service, docker service, and then what do I run to see that new service? I'll use ls. And there you go, we have the service again with the identifier that was just returned to us after we created it. And then if I want to see the tasks associated with this service, how do I do that? Docker service ps and then web in this case. And now you can see I just have one task. The task history is gone there because I've removed and re-created this service, and it has a new identifier now. Now in a development environment, removing a service and adding it back is probably a typical thing to do as you're experimenting and learning. But in a production environment, you don't usually remove a service. You usually update a service. Removing a service would be akin to getting rid of an application. If you simply want to tweak some settings for the application, maybe deploy more instances of it, for example, maybe change the command paths, an additional argument, or maybe roll out a new version. In all of those cases, you would just be updating a service. Let's take a look at that next.

Updating a Service to Scale the Number of Containers
So right now I only have one task, which means I only have one container for this service, for my web service. If I wanted to have two tasks, I need to change the definition. And that's where the command docker service, and then you tell me, which of these commands do you think I use to make a modification to the definition of my service? Well, that last command there, update, this allows us to change the definition. So let me clear out the screen. I'll bring back the ps so we can see we have one task right now. I'll do docker service update, and then I can just specify which service, web in this case, and then there are arguments I can pass to this. There are quite a few of them. I'll go ahead and print all these out, and we can cycle through some of these. So, for example, if I wanted to increase the number of tasks, you can see here --replicas is an argument. And I can set this with an equal sign to 2 to say I'd like two replicas for my service. And now if I run ps again, what do you think will show? Well there you go. We have two tasks now, each of them running our nginx image. So we have two separate containers. How could I go about validating those containers? Well how about we do a docker ps. This is something you're used to. Let me shrink the font a little bit. So we have two tasks here that map to two separate containers. And if I want to move to three containers, then I can just run the update again with three replicas. And now when I do a ps, what do you think will show? And I'll do the service ps first. We have three tasks. Now scaling is a common operation, and you might have seen this already. But the service command has a scale subcommand to do exactly what we were doing. It's a little bit less verbose than the update and the replicas flag. We can simply say, Hey, scale the web service, and let's scale to 4 here. This does the same thing at the end of the day. And we do a service ps. We now have four tasks up and running, which means we have four containers, which I can see with docker ps here. There you go, we've got four containers. So if you think about this going back to docker run, if we wanted four separate containers, we'd have to fire off docker run four separate times. We didn't have any notion of scaling to be able to replicate the number of a given container that we have. So that's part of the reason why we have a definition with the service. It's declarative. It describes what we want. It isn't the actual instance of our running containers. So the swarm then keeps this desired state that's a part of this service, this service definition, keeps that desired state in mind. And it's constantly monitoring to make sure that we always fulfil the constraints that we've described inside of that service. Let's take a look at that in the next video.

Swarm Managers Ensure the Desired State Is Maintained
So a service represents the desired state for your application. It's a description. It is not the actual running containers. The service definition is something that we pass off to a swarm manager when we run docker service create, and then we pass off modifications to that definition with the docker service update command. Ultimately, we have a recipe for what we would like the future to look like. And that's what we're describing. And it could take some time to get to that point. And it's also possible that at some point in the future, one of our containers could die. And at that point, we would no longer be fulfilling the definition of the service. So if we have a situation where we have two tasks for two containers, and one of those containers dies, then we're no longer fulfilling our definition, and something has to start up a new tasks to be able to create a new container to get us back to our desired state. And that's the big difference with services. And that's a big benefit of services as opposed to a traditional container. The swarm managers are responsible to make sure that this desired state is always enforced. So it continually monitors the desired state of the cluster, and it reconciles any disparities. So let's go see what this looks like. So I'm over at the command line right now. We have four tasks up and running. Let's confirm that with a ps, and we have four containers specifically running. Let's be malicious though. Let's come in and take this container Id, which by the way corresponds to web. 4, which means it's the 4th task replica that this corresponds to up here, so web. 4. Let's do a docker and then let's do just a stop here. We'll pass in the container Id of 22 and 35 should be enough here to match this one. So we'll kill off that container, and then we'll do another ps. So web. 4 looks a little bit different now. It looks more like what I just showed you a moment ago when I had four tasks with a couple of them dead from my computer restarting. So web. 4 now has two task entries. One of these tasks is marked Shutdown, and you can see the idea affiliated with it is owg. And then look up at the top here, owg. That was the one that was running before. Now we have a new task for web. 4 that's running, and it has a new Id. So not only do we enforce a desired state, so if I do a docker ps here just to look at the containers we have running, there is now a new container, 2055, for web. 4. So there are now four containers, that's important, which is what we described that we would like with our service definition. But not only that, once a task is dead, it's gone. It's a slot. It's used one time. Once anything goes wrong even if we just stop the container that backs that task, that task is killed off, removed, and then a new task is created. This might be the most polite of analogies, but let's say you're at the DMV, and there are four clerks at the DMV like we have four web workers here. If one of those clerks dies, then a new person is brought in to fulfil the work that that 4th clerk needs to perform. So, likewise, we have a new task once the container for a previous task dies. So we have this nice little history here for the tasks that are now dead. And if we were to kill off the container, the new one that was created, 2055 for web. 4, so you can see that in the name, the naming convention is the service, and then the task slot number of. 4 or. 3, for example, and then the task Id goes into the container name as well, so this matches up with the new one that was created, the new task and the new container. Let's go ahead and stop this one as well, docker and then stop, and then 2055. Now what do you think will happen when I run docker service ps again? What will show up in the output? Let's run the ps again. Is that what you expected? We now have killed off this particular task, the 4th one, twice now, so we have a new task once again. And if I do a docker ps, what do you think will show up? Well in this case, we have yet a new container with a brand-new Id that corresponds to our new task. So we could continue to kill off this particular task, and we'll keep a nice little history of that in the docker service ps output. So if I'm mean here and do a docker stop here again and ba4, the new container for the tasks that we've been really mean to, and if I do a ps, you can see there are now three dead tasks, and there's a new one yet again. And if I do another docker ps and I'm mean again, docker stop and then 3fd, and again that corresponds to web. 4, that's dead now, docker service ps and web. There are now four dead tasks. So we keep this nice running history. Now one thing I will point out actually, I didn't see this at first, but if you look up here, at one point in time when we grabbed a snapshot of things, this new task that was created that we just killed actually was only ready. It wasn't yet running. So there is some time between creating a new task and then actually having a container up and running. So you'll notice as you're experimenting, it takes some time to bring the cluster back into your desired state. There's some time to reconcile things, and that makes sense. In life our applications don't just start up instantly when they die off and we create a new version. It takes some time to enforce that desired state. Most of the time that's fast enough that you won't notice it. But at times don't be thrown off if you see something like Ready or Pending in different places to indicate that a given task is not yet up and running. And also I'll point out and this kind of gets _____, but this desired state column here that says Ready is a desired state for the task, so the task itself is a slot that's eventually filled by a container. And there can be a disparity between the container's state and the task's desired state. The container eventually gets into a state to match that of the desired state of the task, but there can be some disparity there as well. So just watch out for that. A task is declarative as well. It describes the container that it eventually will be running to fulfil the definition of that task.

The Scheduling Process
If you'd like to learn a little bit more about tasks and how scheduling takes place with the swarm cluster, you can come up to the docs and look for Tasks and scheduling underneath How services work in the docs, so How services work, and then Tasks and scheduling. And there's a nice diagram here that explains the process. So you run docker service create just like we did. That's sent off to the swarm manager API. And right now we just have one node, and it is a swarm manager because we initialized a swarm on it. So this API accepts the command that we send it to create a service. Then we have the orchestration component that is performing that reconciliation to make sure that the desired state is enforced. And since this is a new service, this has to then create some tasks to represent that service. So a set of tasks in a way is an instance of a service. Services lead to tasks. There's then an allocator that will allocate IP addresses to tasks for networking purposes. We'll need to talk to those tasks once they are assigned to a given location on our cluster. We'll get into that later on though. And then those tasks are dispatched so that they are assigned to run on a given node. And right now we just have one node. So all of our tasks are run on that single node, which is a great way to start out learning because you can always look at the containers that are running on your single node. And then it's the scheduler that actually tells a worker node, so there's a notion of a worker node, and that is separate from the manager node. So right now our single node performs both of these roles. Anyway, the scheduler instructs a given node to run a task. And that's based on the assignment that was done before by the dispatcher. So in our case, we tell the exact same node, the single node we have, that it needs to run a given task. If you come down a little bit lower, then you can see the rest of the process here. On the worker node, this node checks for any tasks that it needs to be running, and it creates a container for each individual task. And it monitors the state of that container. And if anything goes wrong with that container, information is passed back to the manager node so that the task for that container can be removed and a new task can be created, scheduled, and dispatched in the same manner. So this is a great diagram to explain some of the inner workings. I encourage you to refer to this as you work more and more with Docker Swarm.

Creating a Second Service for Our Customer API
Let's wrap up this module by doing one more thing. We had that customer API, and we talked about how the throughput of it could benefit from multiple containers. Let's see how we can do that with a swarm now. We saw how we could do it with one-offing containers. We also saw how we could do it with docker-compose and the scaling features of docker-compose. But that was all for a single Docker engine. Let's take a look now at how we do this with swarm. Can you take a guess at what I need to do to run the customer API? Well, first off, we need to create a new service--so docker service create. We'll give it a name of customer and the api. We'll go ahead and publish a port so that we can access this. So this will be just the customer API. I'll make this accessible on port 3000, and that maps internally to port 3000. And then I just need to specify the image name, and in this case it's swarmgs, and that's for swarm getting started. This is out on Docker hub. I'll make this a little bit bigger so you can read this. And then /customer. And then that's it. We can just fire off this service creation statement. While that's running, I'll do a docker service, and then we'll do an ls. We can see the new customer API. You can see 0 of 1 replicas. I'll do a ps here and quickly see if I can find out the state of our tasks. We have one task right now. The desired state for that task is Running, so this is really interesting here. Notice we have desired state versus current state. The current state is preparing. We're probably pulling down the image right now, and that'll take a little bit of time. And once that's done, then the current state should converge to the desired state of Running. So let's run this again now and see where we're at. It looks like we've been running now for about 48 seconds. So this is that desired state reconciliation on the level of a task now. These are the tasks down here. Up above we have our service. The desired state of the service says we'd like one replica, so here's the desired state after the slash. The actual state was 0 at that point in time. If I run the service ls now, you can see we have our 1 of 1 replicas operational. So two levels of desired state reconciliation. Long story short, we have our single container here as far as our customer API is concerned. Notice that everything else stays in place for the other service that we have. So we have two independent services, one for the web and one for the customer API, and they're two independent concepts, two independent definitions that are maintained separately. So each of the applications that you have that you would want to launch on to your swarm, you'd create separate service definitions for each of them. This means I can do things like docker service, and then I could scale, and I could just scale the Web API down to one task. And now if I do a docker ps, pretty quick things converge back to just one task for our web service, docker service and ls. You can see we just have 1 of 1 now on the web service as well. Let me clear this screen out now. So we've got our service deployed now, and we published port 8080. Let's flip over to a browser and take a look at this. So over in my browser here, port 3000/customer/1, and you can see there we have our response back, our customer API is up and running.

The Swarm Mode Routing Mesh
Now how in the world is it possible, when we move to multiple nodes especially, how in the world are we accessing this port 3000 on our local computer that is a part of the swarm? How does that work? Well it turns out that just like when we publish a port on a single Docker engine on the Docker host, we publish that port and then we can access the container externally, the same concept applies to a swarm except a port is published on every node. So let's just say we are publishing port 8080 here in this example out in the docs. So on every single node that is a part of our cluster, port 8080 will be published and mapped to what is known as the swarm load balancer as part of the swarm. And when traffic comes in then to port 8080 from the outside, it's then routed to one of the containers that we have running with the actual application. And in this case, it's routed to one of these two, and specifically on port 80 then. And this is all taken care of for you automatically. It's a part of what's known as the swarm mode routing mesh. You can even send requests to a node that doesn't have a container running for a given service, and it will figure out how to route that traffic to one of the other nodes that does have a container running. So swarm takes care of all of this routing for us. It's really neat. Long story short, it's a lot like publishing a port in the past except the port is published onto every single computer, and you can really forget about the implementation details as you're getting started and just think of it as something you've had in the past except automagically you'd get it on every single node in your cluster. This is truly impressive and really simplifies the process of getting an application up and running and then accessing it because you have an automatic load balancer put in front of all of the instances of your applications. So that load balancing concern that we had when we were talking about multiple instances of our app, that load balancing concern is taken care of for us by the swarm. So we should just be able to scale our application willy-nilly, add more containers, and get increased throughput without actually having to do anything but say, Hey, I want two containers running. Let's take a look at that next.

Testing Throughput on a Scaled Service
So right now we have a single container running for our customer API. So if I use that ab command again and make 1000 requests, use a level of concurrency of 4, and then hit local host on port 3000, which is where this is at right now, we can test out the performance of the API running on the swarm. So we are going across that swarm routing mesh that we just talked about. We're making 1000 requests that get routed to that single container that we have up and running. And it looks like we still have the 91 requests a second that we were used to when we were running this with the Docker engine locally. So we have the same throughput with a single container. Now what do I need to do if I want to have two containers? Well I need to do docker service scale, and then I set customer-api=2. And just like that if I do a ps, you can see we have a desired state of Running on our second task, and it is starting less than a second ago. And if I do this again, you can see it's been running for 12 seconds now. So we now have a second container up and running. We can confirm with docker ps. Yes, we do have two customer APIs up and running now. And automagically both of those will be accessible via this port 3000 that's published. The routing mesh will take care of routing the traffic to the appropriate instance. So I just fire off the request again, and we see what our throughput is now with two containers both load balanced via that swarm routing mesh. And, hey, take a look at that. We actually have 180 requests per second now. So we've doubled the throughput of a single container. And if I wanted to scale this up further, let's clear things out, run the scale command again, and let's just crank this up to four containers now--docker ps, or actually let's do docker service ps, and I need to specify the customer API, and this will probably be up and running by the time I can actually fat finger the command for this. We've got four of these running. Let's now do our ab command again. Let's take a look at the throughput this time. We have 300 requests per second now. So we're not quite getting four times the throughput. That would be 360. But, hey, we at least have 310 here. That's pretty awesome! Let's run that again and see if that number comes out the same. 300 this time. So we are doing really good with these four containers. We could continue to scale up although I don't know if it's going to matter much. I only have so much CPU capacity on this machine of mine. But let's try eight now, which is the number of cores that I have on my computer. And we're at 315. So we're not really improving much beyond having maybe three or four of these containers up and running on this single node. And part of the reason for this I just remembered, if I come to Preferences for Docker for Mac and take a look at the Advanced tab, I only have 4 CPUs allocated to the moby Linux VM, and that's why I'm probably capping out at about the throughput of 3X or 4X times the throughput of a single container. If I crank this up, I could probably get higher throughput. Nonetheless, really easy for us to scale up and to be able to route all of our traffic in transparently and have it mapped over to those containers, and we don't really care where they're at, how they're running, everything's taken care by Docker Swarm so that we don't have to think about it.

Adding Nodes
Moving to Multiple Nodes
Now that we've seen a single node, and we're familiar with how we launch a container, and we can relate that back to what we've done in the past with docker run and docker-compose, we now have docker service create, and even though things aren't exactly the same with the service, they're pretty similar. And at the end of the day, we get a container. Now that we understand that, let's move to multiple nodes and see how a service can scale across nodes. So we'll move from our single node setup with a couple of containers for a service into a model where we have multiple nodes, and we'll see how we can launch containers for various different services across all those different nodes. And the beautiful thing is there's not much to do other than add in the nodes, and then everything else is going to take care of itself with all the commands that we've been using thus far. If we have multiple nodes, then the swarm managers will just go ahead and transparently dispatch the work across multiple nodes instead of just a single node.

Destroying the Single Node Swarm
I'm going to get rid of the swarm that I have here that I created on my individual machine here and a stand-alone mode. I'm going to get rid of that and re-create a new swarm on some VMs. Now if you want to continue to follow along with this course but you're a little bit concerned about creating multiple machines, you can continue with your single node. That's fine. You just won't be able to run through some of the scenarios with multiple nodes. But everything will still work on a single node. If, however, you're familiar with VMs, I would encourage you to go ahead and get rid of the swarm that you created locally and create a new swarm on some VMs, which is what I'm going to do. So let's get rid of the swarm that we have. So one route you could go is to come up to your Docker for Mac or Docker for Windows tray icon, go to Preferences, and you could reset this environment, Reset to factory defaults to just wipe out the machine, thus effectively getting rid of the swarm. If you don't want to do a factory reset, a second option would be to leave the swarm. Now this isn't normal. It's not something you'd do in a production environment with a manager node, which is what we have here, but I can show you how to do it so you can get rid of a swarm in a local environment without doing a factory reset. So in that case, we'll do a docker ps and see all the containers that are running. Let me clear this out. Now if I do a docker and swarm and give a list here of the help, docker swarm and then there is a leave command. Let's get the help for that. So if we use this, the current node should leave the swarm, and it's going to warn us that we are leaving the swarm on a node that is participating as a manager. If we remove the last manager, we will erase the current state of the swarm. And if we want to do that, we just need to pass the --force flag. That will take a moment, and the node has left the swarm. And now if I clear out the screen and do a docker ps, you can see nothing's running. Obviously commands like docker and then service ls are not going to work because we're no longer part of a swarm. And this specifically is a node that is not a swarm manager, which is what you need to be able to list services. And it's asking us to go ahead and run docker swarm init or swarm join to go ahead and create a swarm here. But we don't want to do that. We want to create a swarm on a new set of VMs. Now one thing when we destroy the swarm, we still have the images available locally, so you could clean those up if you wanted to as well. I'll just leave these. They're not going to cause any harm. Those are the images that were pulled down to run the containers that we had as a part of our services.

Creating and Managing VMs with Docker Machine
There are two ways that you can go about creating VMs. Well there are probably more than two ways, but there are two ways that I will show you, number one of which you can use docker and machine to go ahead and quickly spin up machines or VMs that have the Docker engine running on them. And then the other approach is to use Vagrant. Now I'll primarily use Vagrant in this course. It's just my personal preference, but it doesn't matter what you do. If you want to create VMs, use the tool that you like to use and then just adapt to what I'm showing here to work with the tool that you prefer. Adapting your tool versus the tools that I like helps you fill in the gaps and learn a little bit more than you would learn if you were just blindly copying what I am doing here. So I will use Vagrant, but let me first show you how you can use Docker Machine to create the VMs. First off, Docker Machine comes with Docker for Windows, Docker for Mac, or you can pull it down stand-alone. It also came with the old Docker Toolbox. There are many ways to get this onto your machine. Once you have that installed then, you can come over to the command line and run the docker-machine command. I have the auto-completion set up so it helps me out a little bit here with my shell. There is a create subcommand. There are actually other subcommands just like the Docker CLI, so create here is what we do to create a VM. We need to give it a driver which specifies what environment we want to create this in, and I will be using VirtualBox. It's cross-platform and easy to work with, but as you can see here, many other choices if you want to spin up a VM somewhere else. Again, pick your preference. Once I specify that, then I need to give a name to this VM, so I might do something like m1, and m is short for manager. I like short names because I can type them at the command line pretty quickly. And that's all I have to do. Of course there are other options I can provide. You'll have to go through this list and decide what you might like. You could, for example, add some engine options if you want, and I'll show you some that I'm going to set in a moment here with my Vagrantfile. You might want to pass these in here. So these would be passed to the Docker daemon when it starts up. You'll want to steer clear at least right now of these swarm flags because these have to do with the old Docker Swarm, the stand-alone version of Docker Swarm. We are now using Docker Swarm mode, so steer clear of those or see if the Docker Machine has been updated to work with the newer Docker Swarm mode. We will be using the new Docker Swarm CLI to set up our swarm. So not these options. And then down below you can see when you pick VirtualBox, for example, you have a bunch of VirtualBox options that you can then tap into. And don't forget you can always go over to VirtualBox or whatever your hypervisor is, and you can use the options inside of that tool to further configure the VM that you've created. For example, I could set the memory, or I could set the number of CPUs to use for a given VM. I won't do any of that though. I'll simply create this VM called m1 specifying the driver alone. You'll see here that this is pulling down version 17. 03. It is now March 2, 2017, so the 17. 03 is now released. It is no longer a pre-release edition. Once that's up and running then, we have the traditional command. If you've ever worked with this before to set up our environment, to point at that particular Docker engine, so I can specify that there to set up my environment. This will show me how to then evaluate this little script that can set up my environment, to set up environment variables, basically to point at that Docker engine that's now installed on the VM. I could paste this into setup and point at that new Docker engine. And this one won't have any images if I list those. And then, of course, you can continue to create multiple of these by calling that command multiple times, and then you'll have multiple VMs up and running. Now there are some other commands here, docker-machine and ls to list out the machines that you have. Looks like I actually have an old one that's lingering around. You can use docker-machine and ssh to get into the machine. You'll need to specify the name of the machine though, m1 in this case, and now I have prompt I'm inside of that VM. I'm SSHed in. Docker ps here, you can see I can still talk to the engine inside of this VM. I can exit out then. And when I'm done with the VM or if something goes wrong in the setup process, just do docker-machine rm, and you can specify m4 and m1 in this case. I will wipe out both of the VMs that I have here. And just like that, I no longer have any VMs via Docker Machine. I'd strongly encourage you if you are not quite familiar with Docker Machine and you want to use that, refer to the docs here, the reference for the create subcommand. For example, if you look through here at engine-opt, you'll see a nice example of how you can pass some options to the engine. For example, you could set experimental and =true to turn on the experimental features, which is one of the options that I will specify. So you can add this engine-opt several times here to specify multiple configuration settings for your Docker engine that's going to be started up.

Accessing my Lab Setup Vagrantfile
I prefer Vagrant, and one of the reasons why is I can give you Vagrantfile to help you set things up. So I've already set this up to be able to create six different machines. You can actually tweak the range here, so I can create up to three what I will call manager nodes and then three worker nodes. Managers do just that. They manage the state of the cluster, so the state of the swarm, and workers, they do the work. So they carry out tasks, and they're not involved in managing the state of the cluster. Also by default, managers perform the worker role as well unless you turn off that feature. So in my setup, I have a total of six VMs we could create. Again, you could tweak the number here if you wanted to create more VMs. This is a simple Ruby script here, and this is configuring things largely to point at a provisioning shell script, so node. sh. And if you look in the repo here in the provisioning folder, there is a node. sh shell script. This is going to run inside of the VM, so don't worry if you're on Windows. This runs inside of the Linux VM. Vagrant will take care of that for you. And it's going to first off install the test version, so a pre-released version of the Docker engine, the latest pre-released version. And then it's going to add the Vagrant user to be able to execute Docker commands, so it adds that to the Docker group. So it has access to the Docker socket. I then create a configuration file for the daemon so I can do things like turning on the experimental mode. I also set debug mode to true. I set up the engine to listen both on the local UNIX socket and also on a TCP port of 2375 so I can connect both locally via socket or remotely via TCP. If you're curious about the contents of the rest of this file, just read through the comments. So this node provision script sets up Docker on that VM after Vagrant creates a VM. That's how this all works. So if you want to follow along, you can clone this Git repository that I've set up. You'll have this on your local machine then. You'll see we have the Vagrantfile and then the provisioning script. And this'll work just fine on Windows as well. Keep in mind this Bash shell script runs inside of VM. All I have to do then with Vagrant is run a vagrant up. Join me in the next video for that.

Launching 3 VMs with vagrant up
So I'm sitting here at the command line, and to bring up a given VM, all I have to do is specify the name. I could do all of them by not providing a name. But for right now, I just want to bring up one of the manager nodes, and then I'll also bring up two of the worker nodes. So I'll bring up three nodes in total just by calling vagrant up. It'll take a few moments for that to happen. If you don't have the box, for example, that I'm using, which is boxcutter/ubuntu1610, that will take a little while to download. Once that's downloaded though, you can start up multiple VMs off of it in a rather short period of time. I'll pause the recording until this is done. Once those are up and running, we can run vagrant and status just to check things out. Now you will see a few things in here called not created. These are VMs that were defined in my Vagrantfile, but I have not yet created. There'll also be some that are running, for example, m1 here and then w1 and w2. You can see the name of the VM over here on the left-hand side. So I've got three VMs up and running. Now I could create more if I wanted to. I just have to call vagrant up when the time is right to spin those up. But I won't do that right now.

Accessing the Docker Engine Inside the Vagrant VMs
With these VMs I can then SSH into them just like with docker machine. I can specify m1 in this case. I'm dropped into my Ubuntu VM, and I have docker ps here for the Docker engine. Again, this is all happening via that UNIX socket because I'm SSHed into this Linux VM. If I exit out, though, if I do a docker ps on the host, I get nothing back, and that might be slightly deceptive. If I do a docker image ls, you can see I have the images from before, so I'm still pointed at my Docker for Mac VM. If I want, I can point at the Vagrant VM that I just created by exporting an environment variable here to set DOCKER_HOST, and then I can set that equal to, and then 192. 168. 99. 201 is the IP, and I know that because I set these up. That's defined in the Vagrantfile if you want to see that. That points me at m1. Once I do that, then docker and then image ls. You'll see that there are no images. So I'm clearly not pointing at my Docker for Mac VM. I'm pointed now at my new Vagrant VM. If I really want to know what I'm pointed at, docker and then info, you'll see here the name is m1. And in this case, you can see that our swarm is Inactive. And, of course, if I want to point at one of the worker VMs, I can set this to 211. That's just again how I define things in the Vagrantfile. I can cat that out and show you if you'd like. So the worker nodes are all defined as 192. 168. 99, actually that's the prefix for the network for all of these VMs, and then it's. 21 and then the number. So worker1 will be 211, worker2 will be 212, worker3 will be 213. And then for the manager nodes, those are all 20 and 1, 202, 203. I've got this loop here on Ruby that's doing that and injecting the number into the IP address that's being set then via Vagrant. So that's how that's all happening. Long story short--because I ran the export statement up above to change that environment variable, which, by the way, with PowerShell, you can do $env and then colon (:) and then DOCKER_HOST and then you can do = and then put quotes around and then type in whatever IP you want to access the respective node. But I'm pointed at 211 now, which is worker1. If I do a docker and an info, you can see the name here is w1. So changing the environment variable for the Docker host is effectively the same thing as docker-machine where you do the env and you specify the name, and it prints out the little script that you run. This is the same thing; you're just changing an environment variable to point at the Docker engine that's listening on that TCP port, which is why I changed that in the configuration so you could access these Docker engines remotely. Now one difference from Docker Machine versus the Vagrant VMs that I'm using here--with the Vagrant VMs, I didn't set up tls, so these are unencrypted communications to these individual Docker engines, whereas Docker Machine sets up encryption and sets up the certificates to automatically work for you. So that's one difference. It's not a big deal though in a development environment to just tinker around with unencrypted communications to your Docker engines.

docker swarm init --advertise-addr
So now let's get to work. Do you remember what we do to create a new swarm? Well we need to run that Docker Swarm and then init. And in this case, I want to run that on the manager node that I've created and that's because wherever you initialize your swarm, that node automatically becomes a manager node. So on my export statement, so I'll look through these here, I want 201 for m1. I can always do a docker info and grep for Name just to be safe here, and you can see that that's m1. Now that I've done that, if I also grep for swarm here, you'll see the swarm is Inactive. And if I do a docker swarm and then init, I do have a tiny bit of a problem, and that's okay. I left this so that we could talk about this. Now, this depends on the VM that you've created. So if you're using another tool, you might not have this issue. But the VMs that are being spun up have two separate interfaces, one that's bound to 10. 0. 2. 15, this is a NATed interface in VirtualBox, and then the other one is the IP that I assigned here, 99. 201. This is a private static assignment. So what's going on here is the Docker Swarm mode doesn't know which of these IP addresses that we want to bind to. Specifically, which one we want to advertise as the IP address that we want other nodes to communicate with us. This IP address is then used for access to the swarm manager API and also for overlay networking. And so we need to specify an advertise address to avoid the ambiguity. So anytime you have multiple interfaces with multiple IPs, you're going to be asked to provide the IP that you want that will be used to communicate with the rest of your cluster. So I'll use the 168. 99. 201 here. Once I've done that, then things work successfully. And before I look at the output here, I'm going to go back up to my info command and grep for swarm again. So I'm just looking at the output here. You can see we now have swarm mode as Active. We get this little snippet back that has a token inside of it. And this token is what we can use to go to one of our other nodes and have it join up to the cluster. So quite a few things happen when we run swarm init. The current node is switched over to swarm mode. The current node is set to be a leader node. That means it's in charge of the cluster. At this point in time, it is the only manager node. And by default, it's also a worker node so it can begin to accept work for services that are deployed. Because it's a manager node, it uses that advertise address to make available the swarm mode API. This can be used to submit a service as we saw in the last module. This node also initializes a distributed datastore to keep track of the state of the cluster. You could think of this as a distributed database to hold things like the services and tasks that are a part of a cluster. As we add manager nodes, this datastore that's distributed will be replicated across the manager nodes, but it will not be sent or replicated over to the work nodes. And then a self-signed root certificate authority is generated for the swarm, which is then used to generate the tokens that we see here in the output that we can use to connect up additional nodes. Let's take a look at that next. So I just initialized my very first node, and it has

Joining Worker Nodes to the Swarm
So I just initialized my very first node, and it has become a manager here. And it's been given an Id as well. And that's in addition to its name of m1. And then I've got this output here that says to add a worker to this swarm, I can run the following command. So I can literally just copy this, and then I can connect up to one of my other nodes, one of the worker nodes. And to do that, I will change my environment variable here to point at 211, that's worker1. If I want to check on that, I can do docker info and grep for Name again just to be safe. And, yes, we've got worker1 here. Now if I just paste in that command that was printed out here, we get output that says this node was joined as a worker node, not a manager node, but a worker node. Now on the subject of tokens, let's break this down a bit. So it starts off with the prefix of SWMTKN for swarm token. And then the next component is a version for the token. And then the last two components are the important pieces. We have the digest of the root CA, and then a randomly generated secret. The digest is used to validate the root CA of the manager that the new node is communicating with. And then the secret is used to make sure that the given node that's trying to connect up to the cluster is actually approved to do that. In a moment here, we'll see how this is different between the worker and the manager token thus delineating if a node is connecting as a worker or a manager node. So in production environments, be very careful with what you do with these tokens. Fortunately, if something goes wrong and one of these tokens gets out, you can use this --rotate flag that's a part of the docker swarm join-token subcommand. You can use a --rotate flag here to quickly rotate things, and you won't have to do anything to reconnect existing nodes. And then a new pair of tokens will be generated to connect up new nodes. Enough about tokens. Let's come back to connecting up worker1. If I had wanted to joint his as a manager node, I could have used a different token. And to do that, I would just run this command docker swarm join-token and manager. There's a separate token that's used to join up a node as a manager. So if I run this here, you can see an error that you'll see quite often when you're connected up to a worker node and forget that you can only run management commands on a manager node. I need to go back to m1, that's 201. So I'll change my Docker host to 201. I'm now on m1 again. Now I can run docker swarm join-token. This will get the join token now. I could use this now to join up as a manager. And you'll notice this token is different than the token we have up above for a worker node. Specifically, you're going to see that this last part is what differs. And I did a little search here just to show you that these yellow parts are all matching. It's this last part that's different. So let me clear all that out because that's a lot going on here, which also reminds me, if you lose that token, you just have to connect up to the manager, and, again, if I do an info here, you'll see that I'm connected to m1, and in addition to docker swarm and then join-token for a manager, I can ask for a worker instead, and here's this join token again, the same one I was just using. I'll copy this, and then in a moment, I'll add in the second node. But before I do this, before I add that second worker node, I want to take a look at the nodes that I already have. Do you remember what command we use to do that? Well in this case, it's docker and node. That's the management command. And then ls is the subcommand to list out the nodes. And you can see we have m1 and w1 and, oh yeah, this is a great way also (I forgot about the little asterisk here), this is another nice way to see which node we're connected to. So we have two nodes. If I connect up to worker2, do you remember how I do that? Well I set the DOCKER_HOST environment variable. So look for host again, go to the end here, 212 for worker2, and then I will copy and paste to join that up. Now when I run docker node ls, what do you think will show up here? This is a trick question. You can't list nodes from a worker node. You can only list nodes from a manager node. So I need to connect back up to my manager. So I'll go back to 201. Don't worry. I'm switching between nodes a lot here. This is only to get things initialized. After I get things initialized, I'll primarily stay on a manager node unless I'm joining up a new node. Anyway, so I'm back on the manager now, and now if I do docker node ls, I can see all three nodes that I have connected, m1, w1, and w2. And you can see that m1 is marked as a Leader for the Manager Status. And there's nothing really telling you that w1 and w2 are workers. I just know that because I added them as workers. You can confirm this though with the Manager Status field. If it's empty, that means worker. Now I wonder in the future if we'll see a Status indication of a worker. Maybe we will and we'll have a nice clean output here that says Worker. And maybe this changes to Role Status instead of Manager Status. Then we could have Leader again as the manager, and it is the manager that is in charge. It's like the CEO of a company. There will be other managers potentially, and they will just be called Reachable under the Manager Status. We have our nodes connected up. Next, let's take a look at running some services.

Creating a Service to Visualize Our Cluster State
So as much as I like creating these slides for you, it's hard to make them dynamic to reflect the current state of the lab environment. And, fortunately, I don't have to because there's a nice little tool that we can pull down inside of a container that will do this for us and show us what our cluster looks like. So let's take a look at what it takes to set that up. That'll be the first service that we create on our new cluster. You can find this tool out on Docker hub. You can also find the source code for it out on GitHub. It's called this docker-swarm-visualizer built for Docker Swarm mode. And this is loosely speaking what it looks like. We have each of our machines, and we can see the various tasks that are assigned to each of our machines. It's a nice way to visualize what's going on. So I need to come back over to Docker hub and grab this slug here for the image I'd like to use. And then I can come back over to the command line, and let's get this bad boy up and running. So a couple of things here, docker and then service and then create, and we are off to the races. Now, we have a few options that we need to pass to this. Let me walk you through this. First off, let's just give this a name of viz, so our application that I want to describe as a service definition is a monitoring or a visualization service that lets us look at the state of the cluster. And there are going to be several options here. So I'm going to wrap a line. Next up, this visualization service will have a port. So I'll use --publish. And I'll run 8090 on the cluster mapped into 8080 inside of the container. So it's a nice way to think about how we are publishing ports now. Think of publishing a port to the cluster. It's going to be available everywhere on the cluster now instead of on just a single node. And we're mapping it into a port that's running inside of the container. And who cares about the IP addresses? I don't, so we don't even need to think about those. We just need to think about how we map these ports around. I'll wrap another line here. The next thing I need to do, this service needs access to the Docker engine on one of the manager nodes. So assuming that the service has a task that's running a container on one of our manager nodes, I can then access that Docker socket, and I need to map that in. Now with docker run, we would have used a volume switch here. With a service, the syntax is a bit more verbose, and that's because there are quite a few options you can pass here. There is now a --mount flag. And instead of typing all this out, out on the visualizer Docker hub page, there's actually this entire service create command. So we can just copy this. So we have equal (=), and then we copy all this information to basically map in the Docker socket. I'll paste that in now, and then I'll break a line. So all we're doing is giving the service that's inside of the container, we're giving it access to the Docker socket. And I also said we had to be on the manager node. You can also grab this part. This is known as a constraint that says, Hey, we are only going to run this service if a given node's role is equal to manager. So I'll paste that in as well. So constraints are a new thing. They allow us to constrain where our application is going to run. So it's another part of the definition of our application, just like we're defining this --mount that we need to have available for our application to run. And then the last part is the image, and you can grab that from Docker hub as well. Just like that, I get an Id back from the service. Do you know what I do to look at the status of the service? I can do docker and then service ls. You can see our viz service here, the Id's match here and here. It's replicated for the mode, that's the default mode. And, hey, it's not quite up and running yet. We still have 0 of 1. So let's run a ps on this, ps and viz. Darn it! It's already up and running. It's too darn fast! I should have run that a little bit earlier so I could show you that difference again between current state and desired state. Now I am connected up to the only manager node right now. You can see that up above, the asterisk on m1. So this container for the task that's a part of this service, so this task right here of dj-whatever, this should be running on the manager node, which I'm connected to, and indeed you can see there's the container that is affiliated with the task, so container, task, and then service to form the hierarchy. So we should have our application up and running now. And, again, I published on port 8090 on the cluster, so on the swarm. So I should be able to access that. Now do you know how I will go about accessing this application I published on port 8090? I'll go ahead and open up, and then the IP is 192. 168. 99. 201 for the manager node, and then I'm going to access port 8090. I can't go to localhost because, remember, we created VMs, and it's inside of these VMs where our applications are running. So when I run this, boom!, I just love this interface. There're so many things about running software that I've never used before that are so simple now with Docker. And now I have this really neat visualization up. And I'm going to shrink the font a little bit so more of it shows on the screen here. I have this awesome visualization to monitor the current state of the cluster. And I don't have to make so many slides now. And you get real-time updated views here as I tweak things. So I can put this on the right-hand side, for example, and then over on the left, let's split the screen here and clear all this out. Join me in the next video where we start up another service, and we can see it come to life over on the right-hand side.

What Happens When a Node Is Shut Down
Now that we have this live visualization, let's tinker around with our cluster. First off, I can do a docker node ls, and we can see the three nodes that we have up and running. And we can see each of those are green on the right-hand side. If, however, I bring one of the nodes down, which I could do with vagrant and then halt, and then I could say w2, so I'll bring down w2, shut it down, you'll see over on the right-hand side once the VM shuts down, you can see it's gone red over on the right-hand side. So we have a nice visualization of our cluster here--vagrant and status and you'll see here w2 is powered off. If I do a docker node and then ls, now what do you think you'll see in the output here? Let's run that. You can see now w2 is marked as Down. And that's clearly because we have shut it down. It's not pingable, so it's obviously not in a Ready state. Something's wrong with this node. Now Availability, it might be a bit confusing here. This does say Active. Availability has to do with scheduling, so can the scheduler assign tasks to this node? And, yes, theoretically it could so long as this node's not actually down. So Down is indicating that there is a problem because this node should normally be available for scheduling. And then if I clear out the screen here and do a vagrant and then up passing w2 to that, this will bring the node back online. And then watch over on the right-hand side, our red icon goes green when it's available. And it looks like things are back up and running. I'll do a node ls, and we now have our w2 node back to Ready. One more quick demonstration, if I wanted to, I could remove one of the nodes using the docker node rm command. So rm -h for that. And then you can see I just need to specify the name of the node I want to get rid of. So I'll go ahead and remove one of the worker nodes. So I'll specify worker2 here. And I get back an error that I can't do that because the node is still ready for work. I really need to go into that node, and in a production environment, I go into the node and do docker swarm and leave, and then I could remove it from the cluster. In this case, I'll go ahead and pass -f to force removal of w2, give it a second over on the right-hand side. Over on the left-hand side here, I'll do a docker node ls. You can see we have two nodes left. And over on the right-hand side, that should update and get rid of things. I'll refresh this. Sometimes it doesn't remove nodes when those are removed. So just keep an eye out for that individualization. And then if I go ahead and add that node back, do you remember how to do that? Well I need to get the token first--docker swarm join-token, and then I need a worker token in this case. And now I need to connect up to m2. So I'll run my export to change my environment variable, and I'll change to 212 for worker2. If I want, I can do a docker info and take a look at the name. I'm okay now. And then I can grab this token and add this back to the cluster. And it still thinks it's a part of the cluster, so we'll do a docker swarm leave here, and now we can paste this in and join back up. And there you go. Over on the right-hand side, when you add nodes back in, the visualization seems to automatically update. Sometimes they don't drop out though. So just keep an eye out for that. Now join me in the next video where we create that service that I promised.

Creating and Scaling a Second Service
So I'm back here on my manager node connected up to that Docker engine, docker node ls. We still have our three nodes, and we have our visualization on the right-hand side. Let's go ahead and create one of our services, so docker and then service and then create, and I'll bring back one of the commands I used earlier. So let's bring back our customer API in this case, so the name is customer-api, and then we're publishing port 3000 on the cluster into 3000 inside the container. And then we're using the swarmgs/customer image. Now when I run this, keep an eye on the right-hand side. And look at that, we've got a customer-api, it's marked red. It's not quite up and running yet. docker service ps, and we'll grab customer-api. Yes, we have an intermittent state here. We are preparing right now, 9 seconds ago, and the desired state is to be Running, so we're still in the preparation process. And if I run that again, not quite there yet either. We're still preparing. So you can see that there is some time lapse here between the desired state that we specified (and we've now come back online here) and converging to that desired state. It takes a bit of time and in this case to pull an image down to worker1. And notice that we were dispatched here to worker1. Now let's go ahead and scale this service now and see what happens. Do you remember how to do that? How do I scale this customer-api service? docker and then service, and then we pick the scale subcommand, and then we grab the customer-api, and then we'll scale it up. Let's do two this time to just add one more task, which will add one more container. Now before I run this, I want you to take a guess. Where do you think this next customer-api task is going to be placed? Let's run it a find out. We've scaled to two. And take a look at that. It went on to worker2. It's red right now, so we're pulling down that image as well. And we are now up and running. docker service ps and the customer-api, you can see we have our two tasks running. They're both in the desired state of Running, and both of them are running. So their actual state of the container is Running in both of these cases. Now let me clear this out. And where do you think if I were to scale up again, where do you think a third task or third container would end up at? Just take a guess. Is that where you expected? Let's talk about this in the next video.

The Spread Strategy and Testing Throughput of the Scaled Service
We now have three tasks or three containers for our customer-api service. And they've been evenly distributed across the machines that we have. We have three VMs here. And the reason they are rather evenly distributed and all the work isn't just sitting on one node is because by default, and the only strategy that Docker Swarm mode employs right now, is to spread out the work across all the nodes. So as we continue to scale up, for example, if I go to five here to add two more, they're going to be put on w1 and w2 because that's the strategy that's in play. And look how fast those came up now that we have the image downloaded. Anyway, the strategy is to evenly distribute these. If I go to up eight, so I add three more, boom! We've got eight up and running now. And, again, we added one to each of the nodes that we have. And that would be the case if we continue to scale up. And take a look at that. It is so remarkable how fast we can scale up and down across multiple nodes. This is what I was talking about in the introduction of this course. I wanted a way that was as easy as docker run to be able to throw work at a cluster, and I just showed you how to do that. And don't forget, if I make a new tab here and make this big, I can access that service now. Do you remember how I do that? So all I have to do is go to any of the nodes that are a part of the cluster to port 3000 and there we go. I've got the API available. I could change this. I could go to 211 for worker1, and I still have the API available. Same thing for 212, for worker2. It's available on each of those nodes because I've published that port, port 3000, onto the cluster. Now technically speaking, there are other modes of publishing ports onto the cluster where those ports don't have to be published onto every node. I'll talk a little bit about that later on, but I won't touch on it much in this course because that's really an advanced topic. But know that there are other ways, and there are two right now. And in the future, there will be other strategies for how ports are published. But the easiest of these is the one that just publishes a port on every node and uses that load balancer to redirect to a given container. Now before I wrap up here, let's split the screen and go back. I'll pull up the visualizer. And let's scale our service down. Now how do I do that? Well in this case, I just drop the number. Let's drop down to three. And take a look at how fast that was. We're now back to just three instances of our customer-api. And if I want to, I can clear out this screen here, pull back my ab command, 1000 requests, four levels of concurrency, this time instead of localhost, I'll put in the IP for the manager1 node, go to port 3000, and run this request. Now hitting my swarm cluster with three nodes in it instead of just a single node. And take a look at that. With three containers up and running, I'm getting 200 and some requests per second, which is about the same boost I had on a single node. If I wanted, I could scale back up and see what happens if I go maybe up to four containers. Now when you're doing this, make sure that the containers are all up and running. Look for the little green over here on the right-hand side. Once they're up and running, then they can serve requests. Now let's fire this off with four containers instead of three across our cluster. And we're getting about 310 requests per second. So the same load we had with a single machine. Just keep in mind this is spread across multiple machines now. So instead of four containers on one machine, I have three machines with one container on each. I have two of them actually, and I've got two on one of the nodes. So this is the beauty of using these services on a cluster. Once I add in more nodes, I can just start up instances at the containers on those additional nodes. And if I actually have more capacity beyond a virtual machine here, I could start cranking up my requests per second higher and higher.

Inspecting Nodes and Clustering Gotchas
Let's look at a few more things with regards to nodes. In addition to docker node ls, we can inspect any of these nodes. So if we'd like to know some more information perhaps about docker and then node, and then we can do inspect and pass w2 this time. We can get the IP address, for example, if we're curious about that. We can look at the plugins that are set up on this node. We can see the architecture and the OS. And, by the way, these aspects of a given node can be used as placement constraints if, for example, we need to say, Hey, this particular container is a Linux container. It has to run on a Linux node. Those are some constraints we can place based on the platform of a given node that is sniffed out for us. We can see the resources available in terms of CPUs and memory. We can even see the version of the engine that we're running. So that's one way to inspect a node. Another way or another thing to think about, we have four containers running right now. Let's scale down a little bit just to avoid any confusion. So I'm going to scale here, and I'll just have two tasks as a part of my customer-api service. That drops two off of the middle node. Let me scale back to one actually. I don't even want one on our m1 manager node. So let's just go to this setup right here. m1 has the viz task, w2 has the customer-api task. Now I'm sitting here on m1 right now. That's why I can do docker node ls. I can't do docker node ls on a worker node. Anyway, I'm sitting on m1. So when I run docker and then ps, what's going to show here? Let's run it and find out. Let me make the screen big. Is that what you expected? We only have one container on our manager node. Sometimes that's counterintuitive to remember. I guess we do have this nice visualization right now that's pointing it out. But when you're banging around on the command line, and you're trying to look at the actual containers, maybe you want to exec into this container and poke around, just keep in mind that you have to be on the node that the container is running on to be able to get into that particular container. Now right now that's the case when it comes to something like exec. I think we'll see more commands in the future, though, that allows us to exec into a container that's a part of a service. We'll be able to do that from any node. But right now when you want to do things like docker ps, the old school commands for just manipulating the containers directly, just keep in mind that you can't get to these for containers not running on this node. And that applies to some other things as well that can be confusing at times. We haven't really talked much about networking. I want to briefly point out that there's an ingress network. This is used as a part of that routing mesh that I talked about. So when we publish a port, this network is used for the communication that goes back to the actual container. So our containers are hooked up to this ingress network. And one thing that can get really confusing at least right now is if I do a docker network and inspect on ingress, let's say I want to see what the IP addresses of the containers are, I can do that, but then if I scroll up, even though this ingress network is used on all the nodes, I only have access to the containers when I inspect this network. I only see the containers on the current node I'm on. So you can see viz right here because I'm on the manager node which has the viz container. There is nothing of this customer-api even though it's on the ingress network as well. This sandbox is something else that I don't want to get into. If I come down, though, and I switch over to worker2, so 212, now if I do a docker network and let's just start out with an ls, you can see we have this ingress network as well. However, if I do an inspect, now when I run this and I look at the containers that are supposedly part of this network, I can see the customer-api container, but I can't see the viz container. So this is one that can trip you up. When you're poking around and inspecting things, keep in mind that only some things can you see if you're on the node that has the actual container running on it.

Listing Tasks per Node
Now if you don't have that nice visualization for looking at what's running where, you can come over to the docker node command, the management command, and there is a ps command that lets you list out tasks that are running on a node. Actually one or more nodes. Let's look at the help for this. I really never pay attention to this output here. It keeps telling me that -h is deprecated. I wish that it'd just leave it in actually. I need to be using --help, but this seems to work for now. Anyway, docker node ps and then I can specify the node or nodes I'd like to look at. So here's where I could do something like m1, except I need to be on a manager node to use this like many other commands because it relies upon the cluster state, and that's something that only manager nodes hold onto. So I need to change over to 201 for the manager node. And now I can run node ps m1, and I can do w1, and I can do w2. So this is another way from the command line I can see what's running where based on a node. I can clear out the screen, and I should be able to actually just run node ps, and that'll show what's running on this node, and then I can pass multiple nodes if I wanted to.

Promoting a Worker to a Manager and Demoting Too
There are a couple of other commands called promote and demote. These allow you to toggle whether or not a node is a worker node or a manager node. So I'm sitting on m1 right now, docker node ls. You can see we have two workers for w1 and w2. If I wanted to make one of the workers, like w1, if I wanted to make that a manager, I could come in and do promote and then w1, and you'll see here that node 1 has been promoted to a manager. If I list out the nodes, you'll see it's not called Reachable, which is a status for a manager that is not the Leader. And then if I want, I can do docker node and demote to bring w1 back down to just a worker node. It's been demoted, so docker node ls. Now it's just a worker node. It has no manager status. This could be useful if for some reason you need to bring a new manager node online quickly, maybe you have a worker node that you'd just like to promote instead of bringing up an entire new machine and joining it as a manager node. So in the event that something goes wrong, you might use something like this to promote to a manager. It's also great when you're testing things out.

Draining a Node to Perform Maintenance
One last node subcommand is the update command. This is a more general purpose tool to modify a given node. For example, we could switch the role by passing either worker or manager, a little more verbose way of doing things instead of using promote and demote. We could add some labels to a node, and labels can be used then to constrain the placement of a service, so your own ad hoc labelling scheme. Maybe, for example, you want to label one node as a beefy machine versus another nodes as a not-so-beefy machine, or maybe you have some spot instances on EC2 versus some on-demand instances versus some reserved instances, and you want to make sure that if you place a given service onto the cluster, if it's a really important service, you might not want to put it onto a spot instance. So you can use labels to do that. And then you can also set the availability, which let's take a look at that. We can change that availability column, and that's again for scheduling. We can set it to active, pause, or drain. Let me clear out the screen here. And I'm going to split the screen and go back to the visualization on the right-hand side. Right now we've got w2, and we've got the customer-api on it. Let's say we need to do some work on w2, something's gone wrong with that machine. We can come over and do a docker node, and then we could update. We could set the availability of it to drain, and that means get the work off of it. Now I've got a quick question for you. What do you think will happen when I run this? Well let's run this, and I want you to keep an eye on the visualization over on the right-hand side. So I'm going to drain node w2. And it would help if I actually specified node w2. It's drained now. And do you see that? The customer-api moved from w2 to w1. Why is that? Well, remember, our service definition specifies what we would like for the desired state of our cluster, and that definition includes one instance of our customer-api, so that's what we have. And we've drained node w2 now so it can't be there anymore, so it has to be somewhere else, and it was moved over to w1. And if I were to come over and drain w1 as well, what do you think would happen? Is that what you expected? It moved it right over to m1. Now if I drain m1 as well, we're going to be in trouble because we have nowhere to put anything, so I won't do that. But now you know how you can take work off of a node, and this is great again for maintenance purposes if you need to do some work on a node. Go ahead and drain it so long as there's okay to do that. And then when you're done doing the maintenance work, you can just come back and set your availability back to Active. So it's active, pause, or drain. Pause will allow tasks that are already on a node to continue to run, but it won't allow new tasks to be assigned. So it's somewhere in between active and drain. Active then brings a node back, w1 in this case. It also brings back w2. So I've brought both of those back. Now take note that the container didn't move anywhere. This is expected behavior at this point in time. Now in the future, we might have different behavior. But the scheduler we're using right now isn't going to move any work around unless it has to. So it's not going to move our containers unless it's forced to. And in this case, just because we brought a couple of nodes back online, we made them available again by setting them to Active, that doesn't necessitate rebalancing the work and moving any work back onto those nodes. It's easy to get tripped up by this to think that, well, we should go ahead and rebalance the work. But that may not be and quite often isn't a desirable thing to do. You don't want to move your application unless you absolutely have to so there's nothing wrong leaving it where it's at unless there's some reason to move it. If I wanted to, though, I can come clear the screen on the left-hand side. And now if I wanted to rebalance the work, can you take a stab at what I might do? Well in this case, I could always scale the service up. So maybe I'll go up to two here. It's not going to hurt anything. And then I could go back down one. And I've now effectively moved the work over to another node though. Keep in mind based on the strategy that is scheduling the work, you might not get things to move around so easily, but if you scale your service, that's going to force the scheduler to take a look at where things are at. And as you add or remove, it's going to try to rebalance things based on that spread strategy or whatever strategy you're using, though at the time of recording there's only strategy for replicated services, and that's to spread them out evenly.

One Container per Node with Global Services
Let's wrap up taking a look at one more aspect of a service that we can set when we create it. So there is a mode that I was talking about, and you can set this to global or replicated. Replicated is the default. It's what we've been looking at throughout this course thus far. We also have global though, and this is just a different distribution algorithm. In the case of global, we will put one instance of a task (and thus one container) onto each of the nodes in the cluster. So let's take a look at this. And to do this, I'm going to start up a new service. I'll call this one cAdvisor for the container advisor. So this is going to be based on an image that's built by Google that produces metrics and a dashboard for an individual container host. It's called cAdvisor (Container Advisor). This is out on Docker hub. And there are a lot of things that you need to give it access to from the file system. So there are a lot of mounts. So I'm going to cheat and just copy from this article here because I don't want to type all of this out by hand. There are four different bind mounts that you need to add so that it has access to various different things to be able to produce metrics about your Docker host. So the important part here is I'm setting mode to global, and then the name is cadvisor. And now I'm going to split the screen again here. In fact, I'm going to wrap a line here first, and then I'm going to split the screen. I'm going to come back to the visualizer so you can see this. And I'm going to paste in those four lines from mounts that I copied. So we have the four different binds, and then the last line has the google/cadvisor image, return, our service is created. And do you see how we put an instance on each of the nodes? And then I'll do a docker service (gosh, it's faster than I am), ps and cadvisor, and we can see that there are indeed three tasks that are created. So we have three containers up and running. And that's true because we're going to have one per node. So if we added a new node to our cluster, it would get an instance of this container advisor. Now you might be wondering what in the world would you use this for? Why wouldn't you just use the replicated mode? Well, I don't know. You tell me. What can you think of that this might be good for? Well there are a lot of services that we want to run on each of the nodes in our cluster that could be helpful in terms of monitoring those nodes, extracting information, and then maybe sending that information somewhere. And in those cases, we might need a service to run on each node, and we can use Docker to do that now instead of needing to manually deploy this software that needs to be on every node. We can just let Docker take care of that for us with a global service. We will revisit this container and this service and take a look at the interface it provides later on when we get back to networking. For now I just want to show you that this creates an instance on each of the nodes, and there's no way to then scale this particular service. So if I do docker and service and scale, I can pick cadvisor and I can say something like five, and you can see here the scale is only used with replicated mode. Docker service and ls here, you can see our cadvisor service. I am going to just bring that down though so it's not cluttering up our interface. So docker service and then rm and cadvisor. And just like that on the right-hand side, you can see everything's gone now. So that's another thing about services--they provide a single item that you can remove to get rid of everything that has to do with the service, a nice way to just clean everything up in one single command. If you're doing this with docker run and creating multiple containers across multiple nodes, you'd have to go into each node and remove each of the containers that you had created. This is a nice way to just say, Hey, get rid of all these containers on the cluster.

Swarm Mode Is Incredibly Easy to Setup
As we wrap up this module, I hope you can see how easy it is to use swarm now, especially if you've used swarm stand-alone in the past, the older version of swarm that ran in a container, now that we have this embedded, it's really easy to set up. A single command docker swarm init, and we're up and running. We've turned on the swarm mode, which behind the scenes is just this SwarmKit which you can look at out on GitHub, you can take a look at the source code for SwarmKit, this is now just embedded inside of a Docker engine itself. So as we've seen, this embedded version of SwarmKit is referred to as swarm mode. this docs page has a nice list of the benefits of swarm mode, benefits that I think you're starting to see through the demoes we've done in creating a multi-node cluster and launching services on it. For example, it's secure by default. We saw how tokens are used to encrypt communications and validate that nodes are approved to join the cluster. This is made possible because certificates are generated automatically so that you don't have to deal with that. Certificate rotation is also built in automatically, which is yet another convoluted concern if you had to manage that on your own. Now security by default and the cluster management or SwarmKit being embedded inside of the Docker engine itself makes it incredibly easy to get a swarm up and running. If you've worked with other tools for orchestration, you'll know that there really isn't anything that's this easy to set up. There are still plenty of other orchestration tools that I love to use that have great features, but I haven't seen anything as easy as swarm mode to get up and running. Now some people are complaining that too many features are being put into the Docker engine. But I, on the other hand, look at this from the perspective of needing all of these components in a platform. I need something to manage a cluster. I need something to manage secrets as we'll see later on in this course. I need something to help me with networking. I need something to help me scale my application, scale it both up and down. I need something to manage the state of my running application, and this declarative service model is a great way to do that. Docker Swarm mode enforces the desired state for my application so that I don't have to monitor and react to problems. And as far as updating my app, well it'd be nice to have something to make it easy to roll out new updates. I need something to help me with load balancing and service discovery and routing traffic. And I would really prefer now to have to install all of these pieces separately. I'd really prefer something simple for the installation process because when it comes time to set up a cluster, all of these components have to be installed side by side. Now at the end of the day, the complaint that all of these things are glued together becoming a monolith, that's somewhat invalidated by the fact that each of these are actual separate components that are actual separate projects like SwarmKit here, so they can be developed independently. They're just glued together into a single package, the Docker engine, when it comes time to install all these different components.

Ingress Routing and Publishing Ports
Routing External Traffic to Cluster Services
We've already seen how we can access services, for example, a website or a Web API. We've seen how we can access these when we have deployed them to our cluster, and we have even multiple instances of a Web API up and running. We can see how we can access these via a published port. In this module, though, I want to step behind the scenes and talk a little bit more about how this works and give you a good understanding of how you get traffic into a swarm because that's a major consideration, how you're going to bring in outside traffic and allow that outside traffic to access the applications or services that you have running on your cluster.

Published Ports Provide External Access to Services
Let's step back and take a look at the topology of our swarm. So right now we have three nodes. We have a manager and two worker nodes. And each of these has an IP address that is a part of a single network that connects these nodes together, and this is much like a physical network, though in this case, this is actually a virtual network since I have virtual machines. Nonetheless, it's just like a physical network where we have a switch that we connect together all these various different nodes so that they share a single underlay network that connects out to the internet in this case. I'll get to why this is called an underlay network in a minute. Now it is possible with Docker Swarm mode you can have modes that are on separate networks that are routed together. I don't have that in this case though, so I will simply represent a single network here with a switch. And then on these nodes we have some services running, and specifically that means we have containers on these nodes. And this doesn't 100% mirror the last example we left off with, but it's pretty close to it. We have the visualization service, for example, that has a single container, and this is on our manager node because it needs access to that manager to be able to get information about the swarm state. And that visualization service is a website that is listening on port 8080 inside of the container. We've also published that port to make it accessible on the swarm, so we've published it to the swarm, which means that each node has a port 8090 that is available for us to access that visualization service. So we could go to any of these nodes and get to that visualization service on port 8090. So 192. 168. 99. 201 and then port 8090 or 99. 211 and port 8090 or 99. 212 and port 8090. It doesn't matter which node in the swarm we go to, we can access that service. And then we also have our customer API, and we've scaled it up and down. Right now we only have one instance, but you could imagine that we have three of those. Let's just say that one is running on worker1 and two of them are running on worker2. And, of course, these are all websites as well, it's a Web API. So in this case, our Web API is listening on port 3000 inside of the container. This API happens to be a Node. js application. It's pretty typical to start Node. js apps on port 3000, and that's why that example runs on port 3000 inside of the container as opposed to 8080 for the visualization service. It doesn't really matter though. At the end of the day, we have a website up and running and the container has to listen on some port. Now this one we also published to the swarm, so that means there is a port 3000 (we use the same port number), there's a port 3000 on each node in the swarm that we can go to if we want to access this customer API. So, again, it doesn't matter what node we go to. We can get to the customer API via port 3000 on any of those nodes. So just to confirm that, let's go take a look. So here we go. We have the manager1 here on 201, and we've got our customer API working, 211 and 212 all have this up and running. Again, they're all on port 3000. We also have our visualization service. It's published on port 8090 so you can see here on 201, 211, and 212, we can access this visualization service. So that's what I mean by publish to the swarm. That's a beautiful thing. It's quite a bit like we had in the past when we published a port to a single node with the old Docker engine stand-alone. Now we have the same thing across our entire swarm. That's very convenient. But how does this happen? Well that's where that swarm load balancer comes into play. And this is just a generic concept to refer to the fact that traffic that comes in on one of these ports on the nodes somehow gets routed to one of the running containers when we publish a port to the swarm. Let's talk about how that happens next.

The Ingress Overlay Network
So what we haven't looked at yet is that somehow we have to connect that swarm load balancer to that port 3000 that's published, and then that swarm load balancer somehow has to get the traffic back to our actual containers. And, remember, the traffic might come in to a node that doesn't have one of the containers for that service running on it, so it has to route that then to another node to then finally get into a container. So what's missing behind the scenes here is that there is another network that the swarm load balancer is connected to, and this is called the ingress network. It's an overlay-type network so it can span multiple hosts instead of just a single host. And it's created by default when we enable Docker Swarm mode. If I hop over and do a docker network ls, you can see we have this ingress network. It's of type overlay, so that's the driver, and the scope is swarm. So this is available on the swarm. If I hop back to another tab here and connect up to my Docker for Mac, docker VM, which again I removed the swarm from, if I do a docker and a network and an ls here, you won't see the ingress network. So, again, that's created when we initialize our swarm. It's a special network that's meant for routing published ports back to containers that actually provide the given service. And if we inspect it, so docker network and then inspect, and then we pass an ingress, and if we scroll up here, we'll see a lot of details, part of which is the configuration of the network itself. You can see the subnet, for example, is 10. 255, and it's a /16 network, so we have a lot of potential IP addresses here. Now this is all created by default, and right now I don't believe there are any settings for you to override anything about this ingress network. I wouldn't be surprised to see some options though in the future to allow you to control maybe the subnet, for example, maybe even to control the size of the subnet. So what this means then is that when we send a request into one of these nodes, so from somewhere outside, maybe the internet or maybe another network internal to our organization, when we send a request into a node on that port 3000, it's routed then to one of the containers that's a part of the service that the port is published for. So 3000 is always going to be routed to one of our customer API containers. Keep in mind, though, we could send a request into one of the nodes that doesn't have a container running. And in that case, the traffic's going to be redirected to another node via this ingress overlay network so that we hit a container that has a service that we're requesting for the given port. So there's a lot of stuff going on behind the scenes with this ingress overlay networking. We'll talk a little more about networking later in this course. But for now, understand that when you publish a port, this is how the magic happens. We have this network behind the scenes that's called an overlay network. It is a virtual network that is running on top of the underlay network. So the physical or virtual underlay network that actually connects our nodes together, it has really no knowledge of this overlay network. The overlay network is just being routed through that network, encapsulated inside of packets transparently such that at the end of the day, we have this virtual network that we can think of that's connecting our containers together up to the various different load balancers to route incoming traffic for published ports. So overlay means running on top of the underlay network. That's the relationship between those two network types. And this overlay networking technology that's been recently added to the Docker engine is a pretty remarkable thing. In fact, if you think about containerization as having to do with compute capacity, so creating these highly isolated compute environments, that's what a container is, that was really revolutionary, well these overlay networks, to have these just work out of the box, to not have to do anything to set this up, in a way, that's just as remarkable as a container itself. So once again, we're seeing how the people that are behind the development of Docker are really trying to make our lives easy so we don't have to think about these complex concerns, like networking together, containers are running across multiple nodes. We can almost treat it as if we had one giant node that we can scale to whatever size we need by just simply adding new nodes behind the scenes.

Options for Routing External Traffic to Nodes
It is really convenient that we have published ports on every node in the cluster. It makes it really easy for us to access one of our services. We just go to one of the nodes, and then we hit the respective published port. But that is not going to work so well for the outside world because we can't expect people to know these IP addresses to figure out what node they need to go to. So while the port is somewhat taken care of for us because it doesn't really matter which node we go to, we still have to know which node we need to go to. And, of course, we could set up a DNS server that points at one of these single nodes. And then all the traffic from outside would be routed to that node and everything will work fine. But that's not going to be so reliable if that one node goes down. We still have other nodes we could be using, but we wouldn't be using them. Also we'd be sending all that traffic to one node and perhaps overwhelming it. We'd really like something with DNS where we could send requests to multiple nodes and maybe use something like round-robin DNS or serve up multiple IP addresses for a given domain name. And then the client has a choice of which IP address it's going to connect to, so which node it goes to. And then the client in failure situations could try another IP address in the list that it retrieved. However, that makes it the client's responsibility, so web browser, makes it the web browser's responsibility or any other client to deal with failure, where as we could actually deal with the failure if we had a load balancer instead. So it's pretty typical then than you'll have an external load balancer outside of your swarm, which means you have one IP address for that load balancer. And then that load balancer will route off to the various different nodes that are available. And that way you could bring new nodes online, and you don't have to do anything but update your external load balancer and point out the new node. And, of course, if a node goes down, the load balancer here could be smart about that. It could have health checking integrated, HAProxy is an example of a load balancer that has integrated health checks, so it could make sure that a node is live before it sends a request to it. So the load balancer can take care of all of these failure scenarios for your customers so that they just hit a single IP address. I just want to make this obvious if you're considering how you'll route external traffic into your swarm. You'll probably want to use some sort of load balancer. It provides the best end user experience. Now just to give an example with something like AWS, you could use the ELBs. In fact, Docker for AWS will take care of that for you.

Host Publishing Mode Instead of Ingress Mode
Thus far, we've been talking about publishing a port to the swarm so it's available on every single node in our cluster, which really makes it easy to get to things from the outside world. And then when we hit one of those ports, our traffic is routed across that ingress network to an instance of a container somewhere. It doesn't matter where it's at. And that traffic is load balanced across all the different containers that are running. So there's no guarantee of which container's going to fulfil a given request when we hit one of those ports on a given node. Now that's desirable in a lot of cases when we deploy an application and we don't really care which instance of the application we read our data from. But there are going to be situations where we want to control which container the request is routed to. And it specifically has to do with that global deployment mode that we just talked about. So when we deploy one container on each node, for example with the container advisor image that we looked at, each of these containers is collecting information about the node that it's on. And so if we want to access the metrics for a given node, then we need a way to predictably go to the container that is assigned to that node and not to a random container because it might have metrics for a different node. So in this case when we publish, we want to do something different than use that ingress network to load balance the requests. And this is something you're probably much more familiar with with a single Docker engine on a single node. We can map a port just like before except each of these ports on a given node is mapped directly to the container on that node. It's not mapped over that ingress network, and it's not load balanced across the containers that we have running. Port 8080 in this case that we would publish for the container advisor using this secondary mode where we publish to the host would then map directly to the container on that host. Let's take a look at an example of this.

Ingress Publish Mode Routes to a Random Container
First off, let's just try publishing using the ingress mode that we've been using throughout this course, so we'll use that ingress network to randomly select a container then to fulfil a given request. So in this case, I will publish 8080, and I'll map that to 8080 inside of the container advisor container. And then when I run that, we create the container advisor service again, docker service and ls. You can see we have our container advisor. It is global so there are three replicas, one on each node. And then if I come over to the browser and I open up that website, let's all go to the manager node, which is 201 on port 8080, the port I just published onto the swarm. And, remember, we're using the ingress mode here. If I scroll down and choose this Docker Containers section, you'll see there is a hostname here, and this is listed as m1. So I guess I got lucky this time. If I refresh this enough times though, look at that. We now have w1 as the hostname. So this has randomly routed to a running container. So if I wanted metrics for the manager node, I wouldn't have a predictable way to go about doing that. I might have to try connecting multiple times before I get metrics back for the manager node, and that's not exactly what I want. I'd like to be able to go to a given node here and get the metrics for that node every single time. So I want a 1 to 1 mapping between that global container that I'm running and the node that it's running on. So now let's switch the PublishMode from ingress over to host, and we'll do that in the next video.

Removing a Published Port on an Existing Service
So, first off, let me inspect the service for the container advisor. And inside of here you can see we have ports mapped. And one of the ports is 8080, and you can see the PublishMode here is ingress. So let's change that, and let's start out by getting rid of the published port that we already have. So docker and service, and do you remember what we do to remove a published port? I guess we haven't actually done this yet, but can you take a stab at what we might do here? Well we need to update our service definition. Remember, a definition is the desired state, so if we remove a published port, so publish-rm, which takes the target port, in this case 8080, we can remove that then from our container advisor service. And now if I do an inspect, if I scroll up a bit here, you'll see that we still see ingress here. And that's because this is the previous specification. Now the new specification, the current Spec here, if we look through here, doesn't contain any published ports. So let me clear this out, and join me in the next video when we add back the published port we want with the new mode.

Adding a Host Mode Published Port
So now how do I go about adding a new published port? Well I can do docker service update, and this time it's --publish-add. So you'll see that many of the options that we have when it comes to updating a service have both an add and a remove. So as you scroll through this list, you'll see duplicates of most options because you could either add or remove the option. So I'll put this back to --publish-add, and now we need to specify that we would like to publish a port, not in ingress mode but in host mode. And to do that, we'll use a bit more verbose syntax. We'll do mode is equal to and then host. We need the published port, which we will set to 8080. And then the target port, which is the port that's running inside the container. That's also 8080. So this is a bit more verbose, but it gets the job done, and it changes the mode of publishing a port. And when I do that now if I do an inspect on our container advisor service, and then you'll see here in the Endpoint specification, we have 8080 and 8080. And then now we have PublishMode host instead of ingress. So now things are different. Now we have a 1 to 1 mapping on each node that we access. We will then have our request fulfilled by the container running on that node that has the metrics for that node. So if that's truly the case now, I should be able to go to the browser and refresh on 201 here, that's manager1, so the hostname should be m1, and it is indeed m1. If I refresh a whole bunch of times, this is never going to change over to anything else because we are now using that PublishMode of host. If I open up a new tab, and this time I go to 211, which is worker1 on port 80, and then if I click on Docker Containers, scroll down, the hostname is set to w1 here. No matter how many times I refresh this, this is always w1. And then the last node, let's copy this and open up a new tab, change this to 212, go to Docker Containers, and you can see we have worker2 here. And no matter how many times I refresh, this is always going to be worker2. So now we have a 1 to 1 mapping with that published port. The published port is still on every single node in our cluster because we have a global service with one container on each node. Now if you use this PublishMode of host, you have to be careful to make sure you don't run multiple containers on a single node for a given service. Otherwise, you will have a port collision, and that won't work out. So typically when you use this PublishMode of host, it's probably going to be in the case where you have a global service with one container on each node, or perhaps if you have a container that's just running on a single node somewhere or a subset of your nodes, but you're never going to use this PublishMode of host if you have multiple containers running on a given node because they will collide. In addition to ingress and host modes, there will be new modes that come about in the future, so keep an eye out for those, and just watch the documentation for how a given mode works, and then try to match up the best mode that you need for your given scenario. Most of the time, you'll probably be using the ingress mode though, especially when you're deploying your applications.

Publishing a Random Port
When you publish a port, you don't have to specify the published port. That can be assigned automatically for you. So if you just specify the target and not the published port, a random generated port will be published to the cluster. Let's take a look at this. Let's hop over to the command line, and we'll create a new service here. I'll call this one random. I will publish a port here, and in this case, I will use the nginx image, so that's a website running on port 80, so that is the target port. If you want, you can be explicit and say target= and then 80. And then nginx is the image. So if I create this service called random running on a target port of 80, I'm publishing that port, but I'm not specifying the published port number. So I can run that, create that in the background. I can do a docker service ps and take a look at the random service. And you can see we have a task up and running. How do you think I can go about finding out what port that service was published on? Well it sounds like a case of using docker service inspect and passing in our random service. This is a good case here to use the --pretty option I believe. Let me clear off the screen so I can run this. And down below, you can see Ports and PublishedPort, and we are starting at port 30000 because that is the default start of the random published port range. If I were to create another random service called random2, again, target port of 80 but not specifying which port. If I then inspect random2, you'll see it's published on port 30001. So I can take either of these, and then I can open up in a browser, point at my manager IP address of 201, and point at 30001 for the port. And there we go. We've got nginx. And if I go back here and open up 30000, you see we also have nginx. So you don't have to specify the published port if you don't want to. You could instead generate it, and then you could sniff out the value with docker service inspect. One use case for this is if you're running some monitoring software, something that you're just manually going to navigate to when the time is right. Then in that case, you could just publish whatever port is fine, and then you'll go look up the port when you need it. Now as for using random ports with your applications, you'll need to discover those somehow so this might not be so helpful, at least not right now, though it looks like some work is being done to implement SRV records with DNS to integrate with Docker's DNS-based service discovery, which we'll see later on in this course, at which point in time, you could use this DNS-based SRV request to figure out which port your application has been published to. And then you could be using random ports because it wouldn't matter.

Reconciling a Desired State
Reconciling a Desired State
Maintaining a desired state is a big part of the value proposition when moving to Docker Swarm. So in addition to being able to manage multiple nodes and being able to throw work at those nodes, we get to manage that work from the perspective of reconciling a desired state. So instead of explicitly specifying what it is we want to run, we describe in a declarative style. It's kind of like the captain of a ship that sets a course to go from maybe New York City to London. The captain sets sail and points at London and then periodically checks the direction of the ship. If the ship has turned slightly, then the captain can readjust the course of the ship periodically because from time to time the ship will get out of sync with the direction that it should be headed. But, as long as we correct course frequently enough, we can make sure we are always close enough to the desired state to get where we need to go. And when things get off, we can quickly fix them. In this module, I want to look at more examples to make sure that we drive home the point of what exactly it means to reconcile a desired state.

Quiz - What Happens When We Add a Node to the Swarm?
I'm going to test your knowledge of this notion of reconciling a desired state. So I've got our cluster right now with three nodes on it. If I were to add another node, what do you think would happen to that node? What do you think might or might not run on that node? And to be specific, let's say that this additional node is a worker node. So let's bring up worker3. What do you think will happen if we bring up the third worker node? Well let's bring this up and find out. So that's another question for you. How do I bring up that third worker node? What's the first step? Well in this case, it's vagrant and up worker3 (w3). We need to create the VM. Make sure you do that from the folder with the vagrantfile. Once that's created then, we can go about the process of connecting this up to our swarm. So let me clear out the screen on the left-hand side. Now before we go further, I want to be explicit and ask you to write down what it is you think that will be running on this third worker node? So just write down, scribble on a piece of paper what you think our visualization on the right-hand side here will look like for the third worker node. Now that you have that, let's go about the process of connecting this up, which leads me to another question for you: How do I get that third worker node connected up to the third cluster? Well, if I have a worker, I need to get a token to be able to join that worker up to the node. Let me make the screen a little bit big here, and let me do a docker, and now what do I type in to get a token? In this case, it's swarm and then join-token, and then I specify worker as the type of join-token. I can then copy that part. And now what do I do with this join-token? Well I need to connect up to that third node. I can do that with the export. I'll change my environment variable here and point at 213 for the third worker node. How can I validate that I have the proper node here? That I'm pointing at the right Docker engine? Well I can do a docker info, and I can grep for and then put Name in here. And you can see we are on worker3. So let me clear out the screen here. And then all I have to do is paste in the join statement here with the worker token, and the node is now joined up as a worker. Now for the surprise reveal. When I switch back to the browser, and we take a look at our visualization, what do you think's going to be there? Is that what you expected? Do you remember that we started up that container advisor global service? And there's one of those per node. So if we bring on a new node, it's going to have that container advisor global service because we need one of these on each node. And so when we bring on a new node, our desired state is not met for this service and so a new task is allocated to that new node. And then a new container is then started to get back to our desired state that we specified for this container advisor service.

Creating a Pending Service and Inspecting Task Status
How about another quiz? We've seen how we can use constraints to make sure that a service is running on a particular node or a subset of our nodes. Do you remember which service we used constraints on? Well it was our visualization service, so docker service inspect viz. And then if we look here in the output, if we scroll up, we can see that we have a placement constraint set. The node. role should be equal to manager. And if I just make sure that, well, this is a PreviousSpec, if I scroll up, though, you can see that this is on the current Spec as well. So here's the current Spec. So we can use constraints to ensure our app runs on the proper nodes. What do you think will happen, though, if we set up a constraint such that no nodes can fulfil that constraint? So think about that, and the hold that thought because I want to show you where you can go to figure out what types of constraints you can put onto a service. And the only place I've really found good docs for this is out on the SwarmKit repo itself out on GitHub, so docker/swarmkit. Remember, SwarmKit is embedded into the Docker engine now, and that's what we're referring to as swarm mode. So if you look through the ReadMe here, and if you scroll down, there's a lot of really good information. But there's a section down here for constraints. And this is at least a partial table of some of the constraints that you can put onto a service. So, for example, we can use something like a hostname that's not available. So let me copy that. And then I'll come over and create a new service at the command line with docker service create. I'll give this a name of on, and we'll call this w4. We don't have a 4th worker node, but we could add one. And then on this node, let's just go ahead and run the nginx image, the official image. And then I'll also publish a port for nginx, and we'll go with something like 9000 mapped into I believe 80 inside of the container. Now for the part where we add a constraint. So --constraint here, and we just need to set this so that we do node and then. hostname==, and we'll do w4. Obviously, that's a constraint that won't be met. I will run this now. Now what do you think will happen when I do docker service ls? Let's run it and find out. So we've got our onw4 service, 0 of 1 replicas available, and we're just going to sit here forever because we don't have a node with the hostname of w4, so this service just can't run. Let's clear this out and look at this though. So let's run a docker service and ps on our w4 service. And we can see that we have a single task assigned here. The task's desired state is to be Running, but the current state is Pending as of about a minute ago. So our service is in a Pending state because there's just nowhere for it to run, and it's just going to sit here forever. Now another situation where you could run into a pending service is if perhaps you had just launched a service, just had created one, and it had not yet had a chance to actually start up. So if you had run this command fast enough, you might have seen this even if there is a node that's available. But more likely than not, you'll see Pending when you don't have a node that can run your service. It can't run the task or one of the tasks associated with your service. And while we're at it, how about we inspect and take a look at that task. So 4vskcc should be enough here. We can get some more information about this particular task if we want to drill into what's going on here. And if we scroll up here, we've got a status block that shows us a little more information. The state is specifically pending, so tasks have a status that has a state. We have our desired state as well that's running. Sometimes this view is a little bit easier to look at than that view up above here when you get a bunch of tasks. Sometimes this information wraps around. This view is somewhat nice here with the JSON output. We even have an explanation here. This is really nice. We have no suitable node that can run our task. So our constraints specifically were not satisfied by all four nodes that are a part of our cluster. And maybe we should contrast this to one of the tasks that's already running. So let's take a look at the tasks that are affiliated with our viz service. So 39a this part. Let's do docker inspect on that. And if I go up to the status section here, this one's running. It has a message of started, so the container is actually executing, versus the desired state. So here you can see the desired state matches the state of the task, and that is running. And because this is running, we have container information as well, the container Id, and I guess the PID.

Joining a New Node to Fulfill a Pending Service
So now let's see what happens when we bring up a node that meets the constraint that we placed on our service. So I'll come over to our visualization here on the right-hand side, and this will wrap around. I'm going to shrink this a little bit actually. So what I want you to do is I want you to scribble down what you think's going to happen when I bring up worker node 4. Do you think that we'll have a container running on that node for our service when we bring that up and join it up to the cluster? Well let's find out. So in this case, my Vagrantfile only supports up to three worker nodes. So all I have to do is come in here and go down to the 3 and replace that with a 4, so I just made our loop 1 to 4 for the worker nodes here. You can make that as big as you want. Up to 9 will comply with the template I have here. And then save and close that. And then I can do a vagrant up and then w4 in this case. That'll take a moment to create the VM. Once that's up and running then, I can go ahead and connect to that node. It'll be 214 for worker4, docker info. I'll grep the name, yup, that's w4. And before I join this up, I'm going to do the docker service, and I'll get the tasks again for this onw4 and accept. That's something that I have to do from the manager node, so let's go back to that real quick. I always forget that. Now I'll come back to 214, so I'm back on worker4 now. I just want you to see that we have this task Id, and I want you to see what happens when we bring up this additional node when we connect it up to our cluster. So for that, I need that worker token again. I do believe I have this in my command history here. And then when I fire that off, keep an eye on the right-hand side, look at that. Boom! Up and running. Is that what you expected? So not only do we have our container advisor started up on worker4, we also have onw4. So our service that we created that was pending is no longer pending now because we have met the constraints that we specified when we created that service. So this is that really important aspect of swarm. This is one of the really valuable features of it. It's constantly monitoring the cluster. And if something that we desired is not possible, well, as soon as it is possible, then swarm will enforce that desired state. And now one last thing here. Let's go back over to 201, which is the manager node, and let me take a look at the tasks for onw4. What do you think's going to come back in the output here? Do you think we'll have the same task Id? Well take a look at that. We do have the same task Id. So that task just sits there in that pending state until it's possible to assign it to a given node. And you can see here now we have the desired state of Running, and we have the current state of Running. Now that's wrapping around, so as I said, sometimes it's nice to inspect, so docker inspect, and then we can pass 4vs, that should be enough there, run that, and that gives us JSON output. But one thing I will show you, if you put --pretty here, it truncates some of the information in the inspect, but it puts it into this nice table. Except it doesn't look like that works with the docker inspect command itself. That does work with things like docker service inspect. Unfortunately, I can't inspect the task from here. But I could inspect the onw4. If you do --pretty, you can see we have this nice human readable format instead of the JSON. A little bit less information, though, and it does cut out some of the information as well. Anyway, I guess we can't get that for what I was trying to refer to, which is the status of this particular task up here for onw4, but let's scroll up then and get that in the JSON, so we have status, running, it started now instead of that issue we had before where all of our constraints were violated. We have our container Id, and we have the desired state is matched at running.

What Happens to a Service When We Lose the Only Compatible Node?
What do you think will happen if we shut down that worker4 node? Actually, let's go so far as to just destroy it. So vagrant destroy, force this on worker4. What's going to happen to the onw4 service? And the task and, well, the running container that's on worker4 right now? Let's see what happens here. The VM is down. Over on the right-hand side, it doesn't look like our UI has realized this yet. Let's take a look at the service here from the perspective of our manager node. So it looks like onw4 has already reacted here. The status is already updated to show that we have no replicas running, and if I were to drill in and take a look at the tasks, is that what you expected to see? And, by the way, over on the right-hand side, we can see that worker4 now is just turned off. It's down. That's why we have the red dot. If I do a docker node ls here, you'll see that we have the word Down on worker4. But let's come back up here to the tasks for that service for onw4. Our old task here, 4vs, that one's desired state is marked as Shutdown now. It says the current state is Running. That's the last time that this task was updated from the current state of the container, remember, we just yanked out the VM and destroyed it. But the desired state of that task is now shut down, and we've created a new task with a desired state of running that is now pending. Can you remember why we have a new task? Well, remember, tasks are a one-way concept. Once the container fails for a task for any number of reasons, that task is then terminated. It's shut down, and a new task is created based on the definition of our service. Since our service defines one task or replica in this case, we have a new task that was added, and now this task can't be assigned anywhere because we don't have a node with the hostname of w4 up and running and attached to our cluster. And that's why this one is marked as Pending. And if we drill into that task, docker inspect, and we do i3j, if we go up to the status block, you can see that there is no suitable node once again. We have a little more information this time though. We have one node is not available for new tasks, and then we have our scheduling constraints are not satisfied on four nodes. So there is one node that would work, but it's just not available for new tasks. So if you have trouble starting up a service, and you can't get all the tasks to start, come here to the inspect information and you should have a little more help to figure out what's going wrong.

Cleaning up Nodes That Have Failed
Another dimension of reconciling a desired state is just dealing with our list of nodes here. We destroyed worker4 so there is no way for us to reconnect that. But normally when a machine goes down, the machine will come back up and then once again be a part of the cluster. So our list of nodes here in some way represents a desired state for the nodes that are a part of our cluster. And when something's not available, then we can see that discrepancy here in the status information. So we'd like this node to be up and running so the status should be ready, but when it's not, it's going to be marked down instead. And so if we want to get rid of a node permanently, one thing you will have to do is forcibly remove that node if it's gone. Otherwise, it's going to sit here in the list and be marked as down. And do you want to take a stab at how we can do that? Well we do docker node and in this case, we can use the rm command. And to this we just need to pass the node, in this case worker4 is what we'd like to remove. And when I run that, you'll see we get w4 back here, and then I can do a docker node ls, and you'll see it's gone out of the list. Now there's one thing special about removing w4 because it was down. Let's try to remove w3, except I need to put docker node rm in. Let me clear out the screen here. So in this case, I can't remove this node because it's not down as it says here in the output. So one thing special about w4 there, I was able to remove it simply because it was down. If a node's not down, then you need to go over to that node, and you need to have it leave the swarm. That's the safest way to remove a node is to do the docker swarm leave on the node and then remove the node thereafter with docker node rm. And it gives us w4, though, is down, which represents a real-life situation where that node is never coming back up because we deleted the VM. And so there is no way for us to bring it back up to do the docker swarm leave, in which case we can just skip that step and do docker node rm w4, and we're done with that node. Now our visualization hasn't updated, but if I refresh there, the node that is gone now is dropped out of the list. So the nodes themselves, you should think of them as a desired state. Once they join up, they're supposed to always be a part of the cluster. So if you need to get rid of one or if one fails and you need to take it out, well, you'll need to remove it then and take it out of what is the desired list there with the docker node rm command.

Remove Vestigial Services
While we're on the subject of cleaning up the list of nodes, don't forget if you create a service that can't be run anywhere that it's going to sit out there and wait to be run. And, of course, that could catch you off guard if you forget about a service that you put out that can't run anywhere and then later on somebody brings up a node that can fulfil the constraints of that service, and then all of the sudden it starts to run. So if you aren't going to use a service, make sure that you remove it. You explicitly need to remove the definition of the service. Don't rely on the fact that it's just not running and that it can't run right now.

If Your App Fails Then the Corresponding Task Will Be Shutdown
And a quick quiz for you: If we have an application running in a container, and let's just say that that application has an uncaught exception and it terminates, what's going to happen for the task that's associated with that container? Will the container simply restart and the application be up and running? Well let's find out. So let's say we are going to create a new service. And I've got one prepared here. I'm calling it exploder. I'm going to run port 9001 on the cluster and map that into port 3000 inside of the container. And I'm using this image called swarmgs/nodewebstress. And I'm doing this because it has an endpoint in the Web API that will terminate the application much like an uncaught exception would bring down an application. So let's run this. We've created our service--docker service and ls. You can see it's not up and running yet, so let's get the exploder status here for tasks. And it looks like we are still in a preparation state. You can see exploder here is red. It's not quite ready yet. It's up and running now. If you come over to a web browser, and if you go to the critical route, this will kill the application. But before I do that, let's do our docker service ps and go to exploder and make sure we have our list of tasks here. I'll run that now. And now I'm going to run ps again. This is your last moment to take a guess at what's going to happen to our list of tasks here now that the application died. Is that what you expected? So if the app dies, that means the container that's associated with it dies. And just like anything else, when the container goes down, the task is then terminated. It's marked as a desired state of Shutdown. That is complete as of 10 seconds ago. And the most important thing is we create a new task that has a desired state of Running. In this case, we're still preparing here. It probably is now up and running now. And notice in this case that we switched from node w1 to w3 after we killed off the task that was a part of the first container that was running. So we switched nodes here. So keep that in mind. There's no guarantee that the work will get scheduled onto the same node. So in this case, you can see exploder has moved over to w3 now. So let's try to do a couple of things here. Let's kill off exploder again, and let's get our ps here. It looks like it's moved back to w1. Let's see if we can get it to move to one of the other nodes. It looks like it keeps going back to w1, and that's probably because we have two tasks running on each of our other two nodes so I can't get it to go over to one of those. It's probably just going to ping-pong back and forth between w1 and w3 here. Anyway, so you can see it did go back to w1. And if I do the ps again, it went back to w1. And I'm sure if I do this enough times, we'll go back to w3 eventually. There we go, we're back on w3 now. So just keep in mind if anything goes wrong with the container that's running your application, the task is gone. So don't get too comfortable with the task Id's. Those aren't going to last very long.

Scale a Service to Zero to Stop It Without Removing It
When you're done with a service, if you don't want to get rid of it (remember just a moment ago I said it's not a good idea to put a constraint on it because somebody could later fulfil that constraint), if you don't want a service anymore, one thing you could do is scale it down to 0. So, for example, our exploder service--I could do docker service scale here and put exploder down to 0. This way no matter what happens, if somebody changes any of the nodes in our cluster that might fulfil the constraint that I put on the exploder service, well in this case because there are no replicas, there is no way for the service to accidentally run. So this is a good way to pause a service if you want to keep it defined, but you don't want it to actually execute.

Desired State Reconciliation Affords SRP
One of the key takeaways when it comes to reconciling a desired state is that a service is a definition. And anybody could define what that service should run. And what you'll find is that you'll have individual teams of people that specify the applications and services that they would like to have running on a cluster. And those individual teams can be separate then from the team that maintains that cluster. There's a nice separation between the nodes and then whatever it is that's running on top of those nodes. And organizationally speaking, you'll find that you can have one team of people take care of making sure you have enough node capacity in terms of scaling the number of nodes or scaling back the number of nodes, and the people that want to run applications on those nodes don't really need to think much about those nodes. And it's all thanks to the fact that we have this desired state reconciliation. If you had to explicitly specify where an application runs at, so if you had to explicitly SSH in and run your container on all the nodes, obviously you wouldn't have this capability to segment responsibilities. And, instead, the people that run applications would have to worry a lot more about the nodes themselves. So take advantage of this.

Rolling Updates
How Do We Update an Application?
A natural extension of the topic of reconciling a desired state is to answer the question, How do we update our applications? So if we have a service that's deployed running version 5 of our application, and we want to deploy version 5. 1 or version 6, how do we go about doing that? And before I tell you, do you want to take a guess?

Updates Seem to Happen All at Once
Let's say we have an empty cluster, and we create a new service. And we specify that we'd like one replica for that service, and let's say this is a new payroll service. So we submit this service definition, it goes off to the manager node, and the manager node creates a task that it then assigns to a given worker node to be able to run the container for our payroll application. And in this case, it also launches specifically version 1 of that payroll application. And then let's say that we go ahead and change the service definition to scale up to three replicas. Well in this case, the manager node will create two new tasks and assign those to the various worker nodes. And then eventually we'll have two more containers up and running. This type of update where we are scaling happens to be non-destructive to the tasks that we have. So we don't kill off that first task and re-create it. We leave it alone, and we just add as many as we need to scale up to the point that we ask for. Or vice versa, if we scale down, we'll just get rid of the ones that we don't need and will leave the existing ones in place. But if we do something like change the version of our application, so we will change the image that we use to go to version 2 of our payroll application, well in this case, we have containers that are running version 1, and we can't just update those existing containers to use version 2. We have to get rid of those containers and re-create new ones. So naturally as we've seen thus far in the course, the tasks will be shut down for the existing containers, and then the containers will follow. And new tasks will be created. And eventually we'll have new containers up and running with the new version of our application by using the new container image that we specified with the tag of version 2. And while this is the final destination, we have three new tasks with three new containers, in reality, tasks aren't removed all at once and then added all at once. Now it can be hard to see this because the defaults for how updates to a service definition are applied, the defaults roll things out pretty quickly. Unless maybe you have a large container image to pull down for the next version of your application, it's been pretty hard to run any of the inspect commands (like the docker service ps command to see what tasks are running), it's been pretty hard to run that fast enough to catch the update in action. But the update is actually rolled out incrementally, and that's what we'll take a look at now. So let's step behind the scenes and take a look at this update process, and let's slow it down so that we can see what's actually going on.

Updates Are Incremental
So let's step back here to the previous point where we had three containers running version 1 of our payroll application. And then we roll out the change to our service definition on the right-hand side to bump up to tag 2 for version 2 of our payroll application. In that case what really happens is one of the tasks is shut down, and then the container follows suit. Then a new task is created so that a new container with version 2 will be up and running. And technically that new task is created while the first task is being shut down, but the important point is the new task won't be set to a desired state of running until the first task and its container are actually shut down. So a small technicality there, tasks are replaced with a new task, and that's actually done simultaneously, but only one is going to be running at a given time. We'll see that in a minute with a demo. By default, once the first task is updated successfully, then a subsequent tasks will be updated, which means that that next task is killed off. Its container will follow suit. And a new task will be created that will eventually have a new container running with payroll version 2. Assuming that was successful, then we can move on to the next task to update. So the last task here is shut down. Its container follows suit. And then we have a new task created, and then a container follows suit for it as well. Now I had this happen one at a time in each node. That's not necessarily how things will happen in reality. A task could be shut down on a given node, and then a new task could be started on a different node just depending on that spread strategy that's at work or whatever strategy you're using in the future if there are new placement strategies. The important point is that we have a rolling update going on. And by default, only one task is updated at a time. And that's a good thing because you wouldn't want to shut your application entirely down and not be able to fulfil requests while you're in the process of an upgrade. So this rolling update gives you the ability to incrementally push out a new version of your application with zero downtime. Now, of course, that assumes that you can run two versions of your app simultaneously. And as long as you can do that then you can take advantage of zero downtime deployments. So let's take a look at this in action.

Running a Docker Command on Every Node in the Cluster
In preparing for demoes, I oftentimes try things out before I demonstrate them. And as a result of that, sometimes my cluster has state that I don't want it to have. For example, I have some images right now for this upcoming payroll example. I have those images already on my cluster because I was playing around. And so I thought I would just share a little script I made here to remove the images from all the nodes because you might want to use this for your own learning purposes. So basically right now, I have this script called run-on-all-nodes, and it's just going to iterate through (and let me show you this), it's just going to iterate through our IP addresses that I have VMs for right now, and it's going to run whatever command I pass into it. You could modify this for PowerShell. Basically it's a loop over the IP addresses. I build up the DOCKER_HOST. I export that DOCKER_HOST environment variable so I'm pointing at that given host. And then I can evaluate whatever Docker command I want. So I can pass in any Docker command here. For example, I can do run-on-all-nodes, and I can do docker image and ls. And just like that I've gone through my list of nodes, and I can see what images are on each node. And you can see, for example, I have my payroll image disseminated already, and I want to get rid of that, so I'll want to run a command on each of these nodes to get rid of the two images. So I have two different versions here. Now notice that there is no tag on any of these. So I'm not going to be able to refer to the image by tag. This has to do with how we specify a tag and how that's resolved to an actual digest of a specific image. So most of the time, when you specify a tag for an image or if you don't, it uses latest, and then the manager node is going to resolve that tag for you. And it's going to resolve it to a digest. Remember, a digest is a pointer to a specific image, whereas a tag can change, a digest can't change because it's content addressable. This difference is a lot like the difference between a branch or a tag in Git versus a commit SHA in Git. The commit SHA can't change because it's based on the content of the commit, whereas the tag and the branch, they're just like pointers that could be moved around to point at different commit SHAs. Remember, a tag in a way is just an alias for us human beings that can't remember big long digest blurbs. So a lot of the time, the worker node where the actual container is going to run, where the task is sent off to, that worker node is only going to see the digest and not the tag itself, and that's when in this case, when I took a look at the images on all of my nodes, I don't have tags on those various different images that were pulled because they were pulled by digest. It is important to know that there are some caveats to this. For example, if you're using content trust, then the client is going to resolve the image's tag to a digest before even contacting the swarm manager. Of course, in this case, you will only see a digest on all of your nodes as well. This topic is referred to as pinning. I cannot get into all the nuances of this. I'd encourage you to check out the docs for this. It's important to understand when you move into production. But in a Getting Started course, I don't want to get into all these minor details that you can simply refer to these docs for. Back to cleaning up these images, I have to use part of this image Id, so I can now do my run-on-all-nodes, and I can do docker image rm and paste in both of the identifiers that I'd like to get rid of here for both of the versions of this container image. Now I can run through that. And in a moment if I clear out the screen here and do my ls again, now you can see that the payroll image is gone everywhere. So just a little tidbit to help you out. As you're tinkering, you can use this to run a Docker command on all of your nodes.

Specifying an Image Tag When Creating a Service
Just a head's up, since our last demo, I removed all of the other services that were running except the visualization service so we have a clean cluster to work with. And then we are going to use this payroll image, and I'm showing you the page of tags for this image. And you can see we have versions 1, 2, 3, and then latest. Latest points at version 3. So we can change the version of this application. So let's go ahead and create this service. So I'll do a docker service and create here. I'll give this a name of pay for payroll. This is also a Node. js app so it's listening in on port 3000 in the container. So I'll map that to 3000 on the cluster. Remember, I got rid of my other services so it's fine to map that to 3000. Now I want to specify version 1. Remember, I said there are multiple tags here. I want to deploy version 1 of the payroll application. How do I do that? What's the last part of my service create statement that I need? Well, just like with docker run, I can specify the tag at the end here, and I can do payroll and then :1 so that I'm using the image with the tag of 1. Now I'm going to split the screen here and pull up the visualizer so you can see this, and you can watch the service as it's created. And we have one replica because we didn't specify anything, so the default of one is used. And you can see that the tag on this payroll container that we started up has a value of 1 and then the digest on the right-hand side. And that's in contrast, if we scroll down, actually if we scroll up here I believe, the manager node here, you can see that for our visualization service, we have a value of latest. I'm going to zoom out, and let's go ahead and scale this while we're at it. Set payroll to 5 and then fire that off. And you can see here some of these are red taking some time here to pull down that image. Docker service ps and payroll here, you'll see some of these are still preparing. These are still pulling down the image, and that's why I want to remove the image so you could see that it takes a bit of time for these to get up and running. And there we go. Our payroll service has now been scaled up. Five instances are running. Next, let's talk about updating the version of our application.

Adding Delay Between Task Updates
So right now in the task output for our service, you can see that we are using the payroll image with the tag of 1 and all five replicas. If I do a docker service and then I do an update here so that I can update the tag that's used, how do you think I go about doing that? Take a look at the help of docker service update briefly, and then see if you can figure out how we update the version of our application. In this case, we can use the --image flag to change the image, and we can specify swarmgs/payroll, and then I could specify a tag of version 2. I need to specify which service this applies to, the payroll service. And when I run this, if I run ps again, you can see we already have three of our tasks updated. And now all five are updated. And all but one are actually running as well. The process of switching over to this new version was really fast, and it's hard to see that happening. It's hard to know what's going on here because by this point in time, everything's up and running for the new version. You can see payroll:2 for each of the tasks that are actually running. All these Shutdown tasks have to do with payroll version 1. However, when we go to update a service, we can adjust the defaults so that the rollout is slower. So, for example, if I do an update here, and let's go to version 3 of our application, along with the --image flag, I have some choices for updating. I have a delay, which is the amount of time between updating each of the tasks in a service. So if we have pay. 1, pay. 2, pay. 3 up to pay. 5 here, when we update pay. 1, we wait a certain amount of time before we update pay. 2. That's the delay I'm talking about. And then of course that delay applies between pay. 2 and pay. 3, etc. And then one other I'd like to point out right now is update-parallelism, and this is the maximum number of tasks to be updated simultaneously. Now by default, this one has a value of 1. So if I go up to the update-delay, and I put some substantial time in between updates, how about 20 seconds, and you can just type out in terms of 1 minute or 20 seconds, or you could even do 1 minute and 20 seconds. Anyway, I'll put 20 seconds in here. That's enough time for us to see the change taking place. We will now only perform about one task update, so shutting down one of the tasks we have right now and running a new task with a new container. We'll only do about one of those every 20 seconds, and then we can see things happening here. So let me run this. And now let me do a ps. And then also I can take a look over in the visualizer and, remember, I bumped to 3 now. Notice that we have 3 right here, but the rest of these are still all 2. In about 20 seconds, we'll see the rest of these flip over. And I know this is a bit small, so I will point out where the 3s come up here. Maybe I'll make this bigger actually. You can see 3 just came up over here. We've got 3 here. We've got 2 and 2 and 2, though, over in these other three locations. I'm going to pause in between here so we can see this rolling out a little bit faster than 20 seconds. And there goes that one. You can see on worker2 we updated it. And just based on the strategy we have right now, it seems like we're getting rid of a task on a given node and then replacing it with a new task on that node. That doesn't have to be the way things work out, but oftentimes with this spread strategy, it will look like we're just taking the task off of a given node and replacing it with a new one. So now we have two over here still. One of them was just removed. But look here, this one was actually scheduled over on worker1 now, and there is it, version 3. So all of them except the last one here are migrated to the new version. We still have one task and container, though, at version 2 way over on the right-hand side. I'll speed things up here. And look, it's gone now, and it's added back as version 3. So let me go back to the command line too. By putting that delay in there, it takes some time for the change to roll out. You can even see in my very first call to docker service ps for the payroll service, you can see when I did the update that only one of the tasks, pay. 4, was updated the very first time I did this. And a new task was created for that 4th slot. So the slots stay around, and that's what we have the history for here, the 4th slot, a new task was created for that slot. And it was desired to have version 3 of our application. The rest of the slots, so 5, 3, 2, and 1, were not yet migrated over the new version. If I run this now, though, you'll see that everything has been migrated over so each slot has a new task that is using the new version 3 of our container image for our application. So update-delay puts a wait in between updating batches of task, and that parallelism parameter is what controls how big the batch size is, so how many tasks we do at once. Let's tweak that in the next video.

Updating Multiple Tasks Concurrently with --update-parallelism
So I've got our list of tasks up above. Everything's on version 3. I'm going to roll back to version 2 here. I'll set the update-delay maybe to 10 seconds this time. That gives me some time to talk. And then this time I'm also going to change the parallelism, and I'm going to set this to 2. So we'll do two tasks at a time, so up to two. So that means we'll update two and then another two and then the last one task since we have five right now. So I'm going to fire this bad boy off. But before I do that, I'm going to split the screen here so you can see our visualization. So just watch for these to drop off and switch back to version 2. And over on the left-hand side, I'm going to fire off the task with parallelism at 2. I'll also do the ps here. Now did you see that two of these are already switched over to 2 here. Watch the rest of these. We'll get two more here in a few seconds. There goes two more, and they're both then scheduled on worker2. And then we should have the last one here shortly. There we go. The last task was updated. And if I make the console big here again, I ran this docker service ps right away after firing off my service update within the 10-second delay that I'm using. And you can see as a result that only two of the slots, pay. 3 and pay. 5, were in the process of updating. And I know that just based on the number of items in the task history for each of these slots. There are four items for pay. 5 and pay. 3 versus three for the others. And since they're being updated, they're both using version 2. They're both desired to be running. And actually both of them were starting as of less than a second ago, so I caught them right in the middle of updating, shutting down the old task, creating a new task, and then actually creating the container. The container was right in the process of starting up when I did this service ps. If, however, I clear this out and run ps again, you'll notice they all have four tasks associated with each slot. So everything's been updated. So parallelism allows you to control the number of instances of your application that you want to update at a given point in time. And, again, this could allow you if you have a hundred containers for an application, you could update ten of them at a time and then wait a certain amount of time in between updating each ten.

Cleaning up Task History When Learning
I've got a question for you. I've got quite a bit of task history built up per given slot, and if I want to get rid of some of this, how could I do that? Do you have any ideas? How could I get back to just showing the most recent task for each of these slots? Well a couple of things. One thing I could do is scale the service. So I could scale the payroll service down to maybe 3, and then I could scale it back up to 5. And now when I get my task history, at least two of these are better, so the 3rd and the 4th slot look a little bit better here. So one nice way to do this is to scale all the way down to 0 then and scale back up to 5. And now my task history will look really nice. So use that if you build up too much history and it's confusing you. Go back to no task history by scaling to 0 and then back up to 5.

Quiz Recap on Update Delay and Parallelism
Let's do a quick recap and a quiz. So we have five tasks running for our payroll service, and they're all running version 1 of our payroll image. Now let's say that we're going to do an update to version 2, and we use an update-parallelism of 2 and an update-delay of 10 seconds. How are these tasks going to be migrated over to new containers? Well, we're doing update-parallelism of 2, so that means two at a time, a batch size of two tasks will be updated simultaneously. So two tasks will be cut over, and then there will be a 10-second delay, and then there will be another two tasks cut over, another 10-second delay, and then finally our last tasks will be migrated over.

Using watch to Visualize Updates in a Terminal
Just a little bit ago, I said that when a task is replaced or updated, the task is killed off, so, for example, this first task here on worker1 is killed off, and eventually its container follows suit. And then a new task is created, and then a container for that task is started up with the new version of our service definition. When I first covered this topic, I said that the new task is actually created at the same time as the old task is removed. It's just not yet started up. It's not placed in a desired state of running, though, until the old task and old container are actually in a shutdown state. I want to show you this in this clip because it's a great opportunity to show you how you can troubleshoot and learn a little bit more as you're getting started. I want to show you some tools that you can use to watch status from the command line to see exactly what's going on, the ordering of things when you shut down one task and start up a new task in the process of updating a service. So a command that I find really helpful is the watch command. If you're on Windows, you can run this command inside of your VM, so you can SSH into m1, for example, and you'll already have the batch command available. So you can run that inside of the VM if you're on Windows and you don't want to go to the trouble of maybe setting up Cygwin to be able to access this watch command. On a Mac, you can use this locally, though, if you use homebrew and you install the watch command. You'll have this available then. Anyway, watch allows us to specify a particular command that we'd like to run over and over and over again, for example, docker service and ls. And if I split the screen here, run my export to connect up to manager1, and then how about we scale our payroll service and scale it down to 0 replicas. Now watch up in the top here, watch what happens to the docker service ls command. This is being run every 2 seconds up above here, so just watch this value for replicas here. Do you see it go to 0/0 just like that? And then if I scale back up to maybe 3 instances, watch that replicas, 3/3, just like that. And if I do something like change the image that I'm using, so docker service and update, set the --image here to swarmgs/payroll, and we'll go with 1 (that'll be for the payroll service), watch the upper part there. Did you see that flip over to 1 right there? So this watch command allows us to see what's going on. And actually if I kill off the watch command, I can add some additional flags, one of which is -d, which shows differences. So it'll flash differences at you. Let's see this. Let me scale down to 0 again. See how it flashes the replicas 0/0. And if I scale back up to 3, that'll flash again 3/3 now. So it flashes the differences to you. So that's a helpful little flag to pass as well. And you can also control how frequently you run the commands, so you can use -n and then the number of seconds, and you can fractional seconds down to 0. 1 seconds. And this runs this much more frequently, which means I'll be able to see the changes as they happen. So if I go up to 0, I'll be much more likely to see changes as we scale down and scale up. Just watch those replicas up above. That was actually too fast. Let's go to 5 here. See how we're at 0/5, 2, 4, 5/5. There we go. So you can see if you run it more frequently, you will see more of the changes taking place, the intermediate changes. So this watch command is absolutely wonderful when you're learning about Docker Swarm. It gives you an alternative to visualize what's going on. Sometimes I also like to run a separate watch here, which I can do by splitting out another pane. So I'll do my export here, and I'll do my watch, and then in addition to ls, sometimes I'll do ps as well, and I'll put in the payroll service here. And now you can see I can watch the list of tasks and then as I scale down to 0, for example, watch that list of tasks in the middle. And, actually, let me change this. Let me do watch -d and -n of 0. 5 is probably plenty here. Now watch the tasks in the middle when I run this scale command on the bottom. Let me clear this out. Boom! Just like that. Now let me scale back up to 5, and you can kind of see the changes that are being made to bring up five of these tasks. That all happens pretty quickly because we already have those images pulled down. And at this point, everything's up and running. You'll notice that we're getting the difference on the number of seconds that these tasks have been in a current state of running. You'll also notice the watch command shows you some output up above, the last time that the command was run. It also shows the command right here that is being run, as well as the frequency of running that command. And, again, we're just overlaying the output of the command on top of itself and doing a diff here with the little flashing sections to show the changes. So now that we see this, let's use this to understand when we perform an update what exactly is going on on the level of a task. Let's see the ordering of a task by using this watch command.

Slowing Down the Update Process to Visualize Updates
I've cleared out my screen now because while I split some tabs there to show you how to run multiple instances of watch, I also have a script in here that uses tmux, a terminal multiplexer, to just automatically split out my screen and watch a given service. So I could pass in payroll here for example to this shell script, and you can look inside if you want. You can also just watch these demoes if you don't want to try and get this running on your machine. This relies on tmux and watch being available. Again, if you're on Windows, you probably want to run something like this monitor shell script inside of one of your VMs. Anyway, if you run this, though, you'll see all this does is create two watches for us, one on docker service ls every half a second and one on docker service ps every half a second. And then down below I have a little prompt where I can type in some commands here still. And all of these windows are automatically initialized with a connection to my manager1 node, and you'll see that all on the monitor shell script. So I could do docker service ls here as well. So I just have that monitor shell script to quickly spin up the three windows here and watch what's going on. So keep an eye on that middle window. It's our list of tasks. Let's first of scale down to just 1 task for our payroll service. One is a bit easier to reason about here. And then let's go ahead and update that one, and we'll use payroll version 3. Now keep an eye on that middle section when I run this. Watch for the changes. Specifically, focus on the Desired State and Current State columns. And just like that, we're up and running. Now if you broke down and paused the video, you could see some of the transitions there. But things happen so fast, it's really hard to tell what is exactly going on. And part of that is because our payroll application shuts down so quickly that we rapidly converge to having the new version up and running. Also we have these images locally, so we could get rid of them and then they'd have to be pulled down again. But most of this has to do with the fact that our app just shuts down too fast to see the ordering of tasks here. So what I've got is another image that we can use that has an app that takes 10 seconds to shut down. So let me detach here from this tmux session, go back to my terminal, and I'm going to change the service name here, which is what I'm passing to the shell script. I'm going to call this delaystop. We'll create this new service in a moment. So right now as you can see in the middle, there's no such service called delaystop because we don't see that in our list up above because we haven't created this yet. But once we create it, then we'll see the tasks show up here, and that's the beauty of this watch command. Even if the command you're running is failing, once it starts working, you'll have the output you want. So let's go ahead and create a service here, docker service create. We'll give it a name obviously of delaystop, and then the image that we want to use is swarmgs/ and then delaystop, and then this one's versioned as well. There's really no difference, but there are two separate tags here. There's a 1 and a 2, so if we want to roll over the number, we could use 1 here. Now I'll fire this off, and just watch the tasks in the middle and maybe also the list of services way at the top. The tasks are the most interesting part. So now we have one task added. It's Preparing, which means the image is being pulled down. So there's a little bit of time there for us to see some transitions. We went to Starting, and now we're up to Running. Our current state is now matching our desired state for the task. So now we're at a point where we can slow down the update process because we have an application that will take 10 seconds to actually stop. So let's do a docker service and an update here. We'll change the image to swarmgs and the delaystop and then :2. And then I need to specify the service here. Keep an eye on that middle section for the tasks. I really want you to watch what's going on here. And I'll give some commentary as this is happening. So we have a 10-second delay on our application shutting down. So I fired that off. Now take a look at that. We already have two tasks. This is the point I want to make. We have one that's ready, and we have one that's shut down. And this is the desired state. So we have one that's desired to be ready and one that's desired to be shutdown, though the shutdown one just now shut down, and now our new task is marked Running and in a current state of Running. So that's the point I wanted to make. The new tasks is actually created while shutting down the old one, but it's not moved into a desired state of Running until the old task is actually in a current state of Shutdown. Let's see that one more time just to see that now that you know what to look for here. So I'll pull my update here, and I'll go back to version 1. So, again, keep an eye on the Desired State and Current State columns. So we have a new task created already in a desired state of Ready, which is not Running. Our old task is desired to be Shutdown, but it's still Running. In about 10 seconds into that then, it shuts down, and then the new task is moved over to a desired state of Running, and then the current state is Running shortly thereafter. So that's what I wanted you to see. We have this process where we update and replace tasks. It's not entirely shutdown the task, wait for it to shutdown, then create the new task. We start the process of creating the new task because there's some work that can be done while the old task is being shut down. Now why do you think it is that we don't just go ahead and start up the new container while the old one is being shut down? Well in that case then, we could have multiple instances of our application running, and that could get really tricky if we have any failures and we need to try to create yet another task. We could have a lot more instances of our app running than we intended.

RollOut Mode and Other Update Settings Are WIP
Now that you've seen how the tasks are created by slowing things down, you can see that the new task is created pretty much right away, but it's not started up until the old task is actually shut down. With that understanding, know that this could change in the future. And there are a couple of issues out on the SwarmKit repo to address this possibility to be able to proceed with an update without waiting for the old tasks to actually shut down. So here's one issue filed. And then there's also a more specific issue with regards to adding a rollout mode so that we could have two separate configuration options when it comes to how we roll out. We could have the current behavior, which is to stop first so the old task has to shut down before you start up the new one. But then we could also have a new option here to go ahead and start first where you could start the new task and then wait for it to be running before shutting down the old task. And that might be desirable if you want to ensure you maintain 10 instances of your application all the time. You don't want to take one instance down or two instances down just because you're in an update. You always want 10 running, and you'd rather lean toward running a few extra containers as opposed to having a short period of time where you are short a few containers. So keep an eye out for these configuration elements. This rollout mode if it comes to be is going to be another element like update-delay. There'll probably be some sort of update-rollout mode just like there's an update-parallelism. So these are configuration aspects of how a service is updated, and they're stored in this update config. But what exactly is that? Well, let's take a look at that next.

Inspecting the UpdateConfig Policy per Service
This UpdateConfig represents what's known as an update policy. This policy holds all the configuration for how updates are performed. Now if you had to guess, where do you think we could go to take a look at this UpdateConfig policy? Well this update configuration is on the level of a service, so let's inspect our service. So let's inspect this new delaystop service that we created. And if you scroll through here, you will see an UpdateConfig object. But be careful, if I scroll up here, you'll see this is on the PreviousSpec, and there's also an UpdateConfig on the new specification. So this policy is a part of your service's definition. So you can change this, and it becomes a permanent part of your service. Let me zoom out a little bit here. I know that the font is going to be small. And let me go ahead and split the screen. And then on the right-hand side, I'll do a docker service and inspect on the payroll service. Now what do you think our payroll service is going to contain for this UpdateConfig? Want to take a guess? Well let's run it and find out. So now be careful here because we can have a PreviousSpec as well and a new Spec. And if we look in here at the UpdateConfig for our payroll service, you'll see a parallelism value of 2, whereas over on the left-hand side, our delaystop service has a parallelism of 1, and can you take a guess at why this is different between the two? Well, we've never changed the UpdateConfig parallelism when it comes to our delaystop service, but our payroll service, yeah, we changed the parallelism. Well we passed a flag anyway. And what I didn't tell you at the time is when you pass that flag, you're also updating the value thereafter. So we are also updating the parallelism policy to a value of 2 thereafter. There's also a Delay property here on the policy, and that's set to a rather large value. We used 10 seconds I believe the last time, so you can see that there are nine 0s after 10, so the delay is set in nanoseconds, whereas we don't have a delay right now on the configuration for our delaystop service. So there's an UpdateConfig policy, and we can manipulate this as well to impact how an update is rolled out to a given service. So different applications might have different needs. In one application, you might only want to update one task at a time, whereas in another application, maybe it's not a big deal to just update everything all at once, so you could control the parallelism then to pull that off.

Watching the UpdateConfig Policy as It Is Updated
Now one thing that I think is fun to do with the watch command--I'm going to come to the right-hand side here and clear this out, and on the left-hand side, I know this is small, but I'll talk through this, on the left-hand side, I'm going to take the inspect command, and I'm going to watch it. I'll put the -d on for flashing differences and then -n of. 05 seconds. Now I'll run this on the left-hand side here for the delaystop service, and you can see the UpdateConfig for the current specification. Now over on the right-hand side then, I make changes to this. You can see the changes in action on the left-hand side. I'll bump this up a little bit because we have a little more room here. So there's UpdateConfig parallelism on the left-hand side. Now just keep an eye on that UpdateConfig policy as I make changes to the service. So over on the right-hand side, docker service update. We want to update our delaystop service. And I'll set the update, and we'll set the delay to 5 seconds. So in this update, I'm not actually going to change anything but the UpdateConfig, so I won't actually affect any of the deployed tasks. If you want to verify that, we can take a look at the other tab, and you can see that we still have our task history of two stopped and one is running. So I'll perform this update here. And do you see over on the left-hand side how the delay is cranked up now to 5 seconds. We could also update the parallelism. So we could set the parallelism to 3. Watch that over on the lower left. And there you go. It flashed up to 3. And then we could change other aspects of the update policy. For example, we have the FailureAction, and then we could choose from continue or pause. So if something goes wrong with the update, do we keep updating subsequent tasks, or do we pause? And pause is the default as you can see over in the lower left, but I could set this to continue if I wanted to. When I run this then, look, FailureAction changed to continue. And, of course, I could change that back if I want to go back to the default policy. And now we're back to pause on the left-hand side. Now if you're scrolled down to look at a PreviousSpec, which by the way let me kill off the watch, and then I'll run the inspect once so I can scroll through the output here, and you can see in the PreviousSpec, the update policy here has a FailureAction of continue, whereas if I scroll up here and take a look at the new or current specification, the update policy is set to a FailureAction of pause. So as you make changes, the PreviousSpec holds the old values. So this is an aspect of your service definition just like the image that you're running, the command that you're running, environment variables that are made available, all of this is part of the desired state for the configuration of your service. Now fortunately modifying the update policy doesn't require changing any of the tasks. So as you can see over on the other tab here, we still just have our two Shutdown tasks. So we didn't actually affect the tasks by updating the update policy. Now maybe one last thing while we're inspecting and using the watch command. Watch this version index on our service definition. Let's say I bump up the update-parallelism to a value of 2. Watch that index over in the upper left. You can see it's bumped up a number. So our service definitions are versioned, and that index plays a pivotal part of making sure that the cluster state is properly replicated across the manager nodes. It's also just a great way for you to refer to the definition you're looking at here in case you've made some updates. You could check the version index that you have somewhere to make sure it's the latest version. And this is also applicable across all the services as well. So if I kill off the watch here for our delaystop, and I take a look at our payroll service instead, you'll see its version index is at 1692. So it was updated much earlier than our delaystop service has been updated. But if I update this one, let's just change the parallelism to a value of 3 here, and we'll do that on our payroll service this time, watch that version index. You can see it goes to 1737, which is a value after the index for our delaystop service. So this index is a global counter across all the services in our cluster that we're updating. So 1737, let's just do the docker service inspect on the delaystop. You can see it's 1736. So that means that delaystop was updated right before we updated our payroll service. So you can use these numbers to synchronize changes in your definitions.

Inspecting Task Errors
So while I was in the process of getting ready for the next demo, my computer was having some issues, and I had to restart it. And I wanted to take this opportunity to show you that in the task output for when you run service ps, and this is the payroll here, and this is delaystop service here, you can see that we have failures, and that's the current state of the shutdown task. And there's a new task up and running, but the old task is marked as failed. And specifically over on the right-hand side, you can get some information when this happens. Now in this case, this isn't too exciting, but sometimes there's some more helpful information in this Error column that can help you troubleshoot what went wrong. So use this service ps command to dig into what happens when a particular task fails. If you want some more information, you can do a docker inspect on these task Id's, for example, the one that was failed here. And then if you scroll up here, you'll get more details. There's the status information for even a failed task or a shutdown task I should say. So here's the desired state of shutdown. Here is the status though, and inside of here you can see more of that error information if any of that is truncated or if you want a little more detail about what exactly happened to a test that fails. You can also see things like what node it was running on, and this is the node's cryptographic identifier, the slot, the service identifier, even the specification for the task when it was created, when it was updated, the version, so that index that we've been talking about, the index applies also to the level of the task. And then other things like the network configuration, addresses, and what not. So don't forget when things go wrong, use the inspect command to dig into the old tasks, as well as the new tasks, which can help you troubleshoot the problems that you're having.

Rolling Back to the Previous Service Definition
Now while we're looking at the service definition, we have this PreviousSpec, and we have the current Spec. And as we make changes, the current Spec is moved over to the PreviousSpec, and then we have a new Spec up above with the changes that we've made. So you can always diff these two, but if we have a previous specification, can you take a guess at a new piece of functionality that we probably haven't taken a look at yet? If we're always tracking the current and the last spec, what do you think might also exist in terms of modifying a service? Well if we have the previous spec, that means we can roll back. So let's do this. Let me run my monitor script again this time on our payroll service. So we have one task right now. Let me scale that down to nothing. Let's get rid of that task history, and let's scale back up to one item. Now if I scale up to two replicas, we now have two tasks up and running. And so now let's take a look at what it's like to roll back to the previous spec where we only had one task. Now unfortunately at the time of the recording, the docs for the various new service and node and other commands with regards to managing the swarm mode features, the docs are not so helpful. They pretty much just list the flags, and then down below they don't have any really further examples or explanation. That's the official docs that are published. If, however, you come out to the Docker repository itself, there still are some docs inside of here that haven't been moved over yet. There is a reference section to the command line. And inside of here, you could look for service and then _update, for example, and these docs do have more than just the flags. They have many different scenarios down below, which are really helpful to understand some of these advanced features. For example, we can roll back to a previous version of a service. So, just keep in mind you might want to refer to the Docker Git repository until these are moved over and published to the official docs for some explanation of how to use things like rolling back to the previous version of a service. So as you can see here, we just need to pass the --rollback flag. So on the bottom, docker service update --rollback, and then I'll pass the payroll service here. And look at that. We went back to just one task. Now, if I run rollback again, what do you think will happen? Is that what you expected? So basically if we just keep rolling back, we're just toggling between the last two versions. When we roll back, then the previous spec becomes the current spec, and the current spec swaps with the previous spec. So when we roll back, we're just swapping the two specs. So we could continue to do that if we're having issues with an update. We can roll back to move backwards and then roll back to move forwards actually. So just keep in mind you can swap that previous and current spec with the --rollback flag. Also keep in mind, though, this is just one version back. You don't have like a whole history, and you're not just incrementally going back, back, back, back, back. It's just the last version and the current version that are being swapped.

Configuring Rollback Policies
Now in addition to rolling back, there are some configuration parameters that you can set up. So we have a delay value, a parallelism value, so you can control how the rollback takes place. And interestingly, I didn't see this in my completions for the --update-failure-action, but we saw that --update-failure-action earlier with a value of pause or continue. So if something goes wrong, we also can set to just automatically rollback. So you could have a service defined such that if an update fails, it automatically rolls back for you. There is also a ratio that you can specify before you trigger a rollback. So maybe you need 20% of your tasks to fail before you trigger a rollback. So this is something you want to look into. I'm not going to demo what each of these features are because you can refer to the docs for this.

Use --force to Test Changes to Update Policies
Another helpful option when it comes to performing an update, let's do a service update here, and let's just say we don't want to actually change any values, but we do want to roll out new tasks for our service. You can use a --force flag to do that. And so I can say, Hey, force an update on the payroll service. I don't care that nothing's changed. Kill off all the tasks that exist already and create new ones. So let's see this in action here. So watch that middle section, and there you go. We've created two new tasks. The old two are shut down here, and the new two are up and running. So you can use the --force flag while you're learning, if you don't want to change something and you just want to trigger new tasks to be created. So this could be interesting, for example, if we scale up to five tasks, and we want to take a look at what our update configuration policy looks like. And before I force an update, let's do docker and then service inspect and our payroll service, and I'll do --pretty formatted so it's more human readable. And then I can scroll up here and show you the UpdateConfig here with a parallelism of 3, a delay of 10 seconds, so we'll be doing three at a time with 10 seconds in between. And if anything goes wrong, we pause, but that doesn't matter because nothing will go wrong here. I'll clear this out, and I'll look for my --force again here. So let's force an update now with a parallelism of 3 and a 10-second delay. There you go. Three of them are in the process of updating, and then there should be a 10-second delay before we hit the other two, which should be slot 3 and slot 5. There we go, 10 seconds, and now the last two are updated. So --force is a great way to test out changes to your UpdateConfig policy.

Watching UpdateStatus During a Service Update
When you're performing updates, let me make a new tab for this, if we inspect the payroll service, there is an update status on the service. You can see this was completed. It gives a StartedAt and CompletedAt time and a Message of the current status, which is updated completed. This is another interesting thing to monitor. And then in the upper panel, I'm going to use watch to run docker service inspect pay, and then I have this jq command, which is like grep for JSON to grab just the UpdateStatus object. So I'm going to filter out all the rest of the service inspect information. When I run this, you can see we just have our object for our service update status. And right now, it's completed because nothing's going on. Let's go ahead and run some commands, though, in the bottom pane. Let's scale, for example, down to one. And you can see that the update is now complete. We've wiped out some of the information though. Let's go ahead and scale all the way down to 0 to clear out our tasks in the middle. And now if I scale up to five items, again, we're not affecting that object up on top at this point in time. If I run an update, and let's just force an update here in the payroll service, so we'll force the creation of new tasks, see in the upper part how the update is in progress right now. State is updating. And it looks like things are done as of this point in time. And eventually that update status as you can see there is flipped to completed. That can take some time though. Nonetheless, a good thing to keep an eye on, that update status, if you want a bit more information about what's going on behind the scenes.

Simulating and Monitoring an Update Failure
The UpdateStatus information can be really helpful if something goes wrong in the middle of an update, so let's keep an eye on that while we try here to trigger a failure in updating our service. So I'm going to give myself time to trigger a failure. I'm going to modify the service for a payroll service. I'm going to modify it so we set this update-monitor duration, so this is the amount of time after a task is updated to monitor for a failure, I'm going to set this to 1 minute just to give myself some time here to maliciously stop one of the containers associated with a new task during our update. I'm also going to modify the update-parallelism and set this back to 1. And I'm going to put in a big delay here maybe of 20 seconds just to give me some time here between each task getting updated. I'm doing all of this so that I can successfully kill off one of the new containers and show you what happens when an update fails to show you that the service is paused in this case because that's our policy on a failure. So boom! I run that. Now in this case, I didn't actually change any of the tasks. I'm just changing the update policy. That doesn't trigger any task changes because it's just configuration for when we're actually updating something that requires changing tasks. So let me clear this out now. And then next I'm going to scale down to 0 and scale back up to 5. It's just easier for me to not have the task history. Now let's do a --force update on the payroll service. And our monitor duration should kick in, also our other delay should kick in. Basically only one task should be updated right away. And I'm going to go and try and kill off the container that comes up for that new task to try to cause this update to stop in the middle. So I'm going to run this. Now you can see w3 has been updated here. So I could go into w3, go into and then I need to go into 213. I have a minute to get in there. And then docker ps, there is the container I want, docker stop 1313. Now I did that, and you can see that it's shut down, and you can see another task is marked Ready. But right now, the status of our update is paused. And you can see that the update is paused due to the failure or early termination of a task, and it gives a number 4o4 here, which is the one that I just killed off. Here's 4o4. You can see that in the name of the container 4o4 as well for the container Id that I then passed to docker stop. So I stopped the new container on the new task. And now I've effectively shut down the update process. We are now paused. No more updates are going to take place. I can sit here forever, and I'll do that. I'm going to wait a couple of minutes here and come back so you can see that no update is continuing here.

Resuming a Paused Update
It's been a bit here. I went to get a new cup of coffee. It's been about 4 minutes now, and the update has not continued as you can see here. We still just have the one slot that was updated. Now interestingly, we do have a new container running here, and that happened right about the time I was stopping that existing container that was starting up for the new task. This kind of takes me off guard. I would assume that if one of the tasks fails in monitoring that we wouldn't then continue the update, we'd pause at that point in time. But it looks like it went ahead and tried to do an update on that given slot again. But at the same time, it in parallel paused the update process. Maybe that's a bug, I don't know. Maybe that's desired behavior. The important thing is if your update fails because we have a policy that says to pause, which I can show you in the upper panel here if we look at instead of UpdateStatus, we can take a look at Spec. UpdateConfig, you can see that in this case our failure action is to pause. That's why we did that. So if we have that then and our MaxFailureRatio is set to 0, then we pause there, so we can't accept any tolerance in this case of a failure. Let me pull up the status again. That's why we have this paused update. Now you might be wondering, Well, how do I resume an update? When you're ready to resume, make sure you connect back up to the manager node, and then a simple call to docker service update and the name of your service is all it takes. So no flags here to resume the update. And you can see we kick off. We're updating pay. 4 now. And I'll go ahead and pause this until we're complete. And the last task just updated, pay. 1, so this service is now finished updating after we had paused for a failure. Now in real life, if you can't just resume the update, in other words if it's something about your service definition, you might need to make some changes when you call docker service update, add some additional flags to make a subsequent change to the definition to get your service to roll out successfully. The only time you wouldn't pass flags is if the failure has nothing to do with your service definition. So when a failure happens, remember, you could choose to just continue updating, you can have the pause like we've seen here, or you could even set up the auto-rollback. I will say setting the failure-action to rollback though, when I tested that out, that doesn't actually work yet, so I think that's a change that's being worked on. Those newer docs mention this, but it doesn't seem to actually be implemented yet. So just a heads-up, it might take some time before we see that feature for auto-rollback. I think we've covered enough with regards to updating your application. Now let's move on to talk about how containers can talk to each other, so container to container networking. And we'll see how we can scale different tiers of our application independently without having to do anything to get those tiers to talk to each other.

Container to Container Networking
Internal Container to Container Scenario
We've spent a lot of time in the course thus far talking about a single service that we're exposing externally to make available to external users. In real life, though, we have other networking concerns than just ingress traffic. We have concerns about how one internal application might talk to another internal application. For example, a web app might need a database, or maybe a web app needs to hit another internal API. And how can containers then running each of those different services, how can those containers find each other and communicate with each other internally? That's what we'll turn our attention to in this module. Just to make sure we're all on the same page, let's talk through a scenario that we'll be using in this module that we've been working on throughout this course where a user is going to access our balance API (we've already seen that way earlier in the course), and that balance API needs to access a customer API to retrieve customer data that is then used to build a balance report that's sent back to the end user that made the original request. So there are two tiers to our application here. And each of these tiers will run in its own container. And it just so happens that these are all Node. js apps so they all run on port 3000 inside of the container. Now, of course, we'll need to be able to scale each of these tiers independently, so we may need to scale up our balance API. We also may need to scale up our customer API, and we'll scale these independently. Now when we do this, then we have the challenge of load balancing our traffic as we've already discussed. And we've seen thus far how we can use load balancers at least externally to be able to expose a published port, for example, let's say 5000 to hit our balance API. So I like to draw a line in the sand here and say that this part on the left-hand side, we've already covered that, that's external traffic into our swarm. We talked about publishing ports. There's a separate set of networking concerns internally though. How do we internally load balance through a quest to go from the balance containers to the customer API containers? That's what we're going to talk about in this module, all the stuff on the right-hand side here. Now it's closely related to the networking that we've already discussed, but I like to draw a line in the sand and separate these two concerns. It helps me better understand each of these networking challenges, because these truly are independent networking challenges. It's one thing to get traffic into a swarm. It's another thing to talk about traffic within the swarm.

Ingress Network Is Special Purpose for Publishing Ports
Now it might be tempting to think that we could just simply take these containers, and they would already be hooked up to that ingress network that's created that we've been using thus far for routing incoming traffic. It might seem logical to assume that if the balance API needs to talk to the customer API that it can just do that right over the same ingress network. But that ingress network is not meant for this purpose. It is a special purpose network only meant for routing that incoming traffic on published ports. So, for example, if a user makes a request to our balance API, that ingress network is meant to get that request into the swarm and then across to one of our balance API containers. But that network is not meant for traffic between containers. Out on the Docker repo, you'll find an issue that's closed with regards to making this ingress network a special routing-only network. This network is meant to route external requests outside the cluster to services which are publishing ports. It's for one purpose only. So even though we will see it in network listings, for example, if I hop over to docker network ls, you'll see in this list here we have ingress. So it seems like a regular old network, but it's not. It's not meant to facilitate internal load balancing between these services. So to block that behavior, some changes were made, and these changes have already been applied to do things like disabling service discovery via an ingress network. So our balance API, for example, wouldn't be able to discover where the customer API is at even though they're both attached to that ingress network. And theoretically you could communicate over that ingress network. Instead, we want to create our own custom network. And you might be familiar with this when working with docker-compose in the past--docker-compose would create networks per compose file. Or you might have created these networks yourself, user-defined networks. And to do that, we can hop over to the command line, and we can do docker network create, and we have a whole series of options for the type of network we can create. And what we need to do is create our own overlay network that spans multiple nodes or multiple hosts so that we can allow container-to-container internal traffic. Let's take a look at this next.

Our New Network Topology
So what we're actually going to do is create our own new network. We'll name it backend. It'll also be an overlay network so it spans multiple hosts. And we'll allocate this subnet here, 10. 0. 9. 0/24. And then on that backend network, we'll connect up our customer API service, and we'll also connect up our balance API so that all of these different nodes could talk to each other. And then most importantly, then, our balance API can talk to the customer API. But one thing different, we are not going to publish a port for the customer API. We're not going to publish that to our swarm anymore. So our customer API service will no longer be hooked up to the ingress network. There's no reason for that as we're not going to be accepting any incoming traffic to it. It's just a backend service that's made available to the balance API, and this balance API that is publishing a port, port 5000, for people to access externally. So there's a nice separation here between concerns of our public-facing services and the internal supporting services. And this separation is wonderful because now we can protect those backend services that we might not want the outside world to be able to talk to.

Creating an Overlay Network
So let's go ahead and create that backend overlay network-- docker network create, and I'll get some help here. We're going to set the driver to overlay. We're going to set the subnet as well. And then we're going to provide a name for the network. So we need to do -d and overlay, do the --subnet=10. 0. 9. 0/24. Now you don't have to specify a subnet. One thing I will caution you to avoid is using a subnet that you have on an existing network within your organization. If you do that, you could have some issues with routing and communications. So that's why I'm explicitly specifying 10. 0. 9. 0. Now a common sticking point (if I split the window here, I'll show you this), if I go into my manager1 via SSH, and I take a look at the IP addresses, scroll up here, you'll see 10. 0. 2. 15 is in use. It's pretty common when you're working with VirtualBox to use this network. So if you were to use 10. 0. 2. 0, you're going to run into some trouble with routing. So avoid any of these IPs that you are using for your virtual machines as well. So, basically, you need to avoid collisions with that underlay network when you're creating these overlay networks. Enough of that, though, I am explicitly specifying that instead of letting Docker create it. If Docker creates it, by default it will allocate 10. 0. 1, 10. 0. 2, 10. 0. 3, etc., so it's likely to collide with that 10. 0. 2. 0 that I don't want it to, so that's why I'm specifying that. And then I just need to give a name here. We'll call this backend. When I run that then, I can do a docker network and ls, and you will see the new backend network that I created. Its driver is overlay, and it's scoped to the swarm so I can use this for communications between containers on multiple nodes. It looks a lot like the ingress network that's using the overlay driver and is also scoped to the swarm. However, because this is user defined and is not the special purpose ingress network, we have a few extra pieces of functionality now that we can use on this backend network that we just created. And we'll take a look at that throughout this module.

Inspecting Overlay Networks
When you create a custom network, you can use docker network and then you can inspect and then pass the name for backend here to get additional details. And you can see, for example, that the subnet is allocated 10. 0. 9. 0. Let me show you what happens if you don't specify a subnet. We can create multiple of these networks. So I will pull up my statement here, and I will drop off the subnet. I'll just use the overlay driver here, and we will create a backend2. When I do that now, if I inspect the backend2 network and scroll up just a little bit here, you can see that we have a configuration specified for our subnet on the backend network but not backend2. This will be automatically assigned the first time you use it. Let me pull up a diff of these two. So here's the diff, and as you can see, obviously, the name and identifiers are different. But the most important part here is we have a configuration on the left for our subnet.

Attaching a New Service to Our Overlay Network
So now let's get to work creating our services, our balance API first. We'll get that created, and we'll get it hooked up to our backend overlay network, and we'll also get the port published port 5000. Let's get that set up first. So this is somewhat a quiz for you. How can I create that balance API service? So docker service and create obviously. I'll give this a name of balance. Now what comes next here? And I'm going to break a line just so we can split this out and see each of the individual flags that we pass. Let's look at our diagram. We need to publish port 5000, so how about we do that next. So -p 5000 on the swarm maps to 3000 inside of our container. I'll break a line then. Next, we need to hook it up to that backend overlay network. And here's how you do that, --network, and then you just say equal to, and we just specify backend. Once we have that, then I'll break a line here, and then the last part we need to specify is the image that we're going to use. And I have a balance API image, so make sure you reference that. So it's swarmgs/balance, and this has just the balance API or website inside of it. Now if I run that, we create a new service. I can look at my services here. I can see I have my balance service up and running 1 of 1 replicas. And because I published port 5000, I should be able to go to a browser, go to my manager node's IP address and open up port 5000. And it looks like the site's working. And then if I go to /balance, I can make a request here. And we're getting a response back, though we do have an error, and that's because our customer API is not accessible. So we've got one piece of the puzzle in place. Next, let's take a look at getting our customer API up and running on that backend overlay network.

Adding a Second Service to Our Overlay Network
So, next, we want to hook up our customer API to that backend network, so let's create that service. So this is the first time that we have two services working in concert. So let's do this piecemeal--docker service create. We'll give it a name of customer. I'll break a line here. We're not publishing a port for this one. It's going to be only internally available. So I can skip the publish port. I need to specify the network again, and that's backend. Break a line again. And then I just need to specify the image of swarmgs/customer. Now when I run that and I do an ls here, you can see we have three services. Our balance and our customer services are the two we're working on. And you can see those are both up and running 1 of 1 replicas. Now because I didn't publish support for the customer API, I had no way of accessing that externally. But what I should be able to do is come over and make a request to the balance API, and I should be getting back a response, but I'm not. And there's a reason for that. Join me in the next video so we can talk about how we can get these two containers to talk to each other now, now that they're on the same network.

Viewing Service Logs
So right now, we have both our customer and balance APIs up and running. And when we make a request to our balance API, it's still not going to our customer API. Do you know why that is? Well even though we've hooked these up to the same network, we haven't told the balance API what hostname or IP address to use to communicate with that customer API. So how are we going to do that? So I'm going to step behind the scenes and show you the balance API. It uses an environment variable here, and if that is set, then that will be used to communicate with the customer API over the HTTP protocol. If it's not set, we'll use the default of localhost:3000, which obviously isn't going to have the customer API because that's not running locally. The balance API is only running locally. So we need to set this environment variable and point it at our customer API. But what exactly are we going to set this to? Do you want to take a guess? Well, first off, let's go take a look at what's going on here in the logs for our service because right now we're printing out the value we have. So let's take a look at the logs for our balance service to see what it printed out here as being used for the customer API, and let's just confirm that we are using localhost right now, which is my suspicion. So I'll come back to the command line, and this is a new command that we haven't had a chance to look at. Fortunately, we haven't needed it yet, and that is docker service and logs. This is recently added in 1. 13 I believe. And it gives you the ability to stream logs for your service regardless of which manager node you're connected to, which is really nice because back in 1. 12 or at least when swarm mode first came out, you'd have to connect up to an individual node that has a container running to look at the container logs. Now you can stream the logs regardless where the container's at, which is really, really awesome. So what we can do is docker service, we can do logs, and then we just need to specify which service, and in this case we want to look at our balance service. So if I run that, quite a bit of output here. We even have our 404 errors that we saw in the browser. But if I scroll up far enough here, there we go, we are using a CUSTOMER_API of localhost:3000. So, yes indeed, we have the wrong value. But what do we use instead? Well if you remember from previous discussions, we have service discovery available, and I haven't got into what that exactly means. But we have service discovery available based on the name of a service when our services are hooked up to one of these user-defined overlay networks. So now that we have our own backend overlay network, we have service discovery on that network. And we should be able to resolve any service via its service name. Long story short, that means all I have to do is change the service definition for our balance API to provide an environment variable that points at customer:3000 because customer's the name of the service that we have. So docker service ls. We're using customer right now. So this service will be accessible as customer, and port 3000 is where it's listening at. And we'll get to how that all works in a minute. But let's go ahead and modify our balance service and add this environment variable. So docker service and update, this is also a new flag that we'll be passing to service update here. I'm going to get help, though, and show you if you scroll up here, you can add and remove environment variables. Join me in the next video to do that.

Adding an Environment Variable to an Existing Service
So to make this modification to our balance service, I'm going to open up my monitor so we can watch the service tasks in the middle here. And then down below, docker service update --environment-add. I'm going to go to the code again. I'm going to pick up the environment variable that's being used here and paste that in. We're going to set that equal to customer:3000, which is the port that we're running on. And customer is the name of the service, so that's how we're going to be able to resolve one of the containers that actually has this service listening on port 3000. So just like that I can add the environment variable, and I need to then specify the balance service. That's what we're going to update. Now what do you think's going to happen when I run this command? Specifically to the tasks in the middle? Well let's find out. Is that what you expected? So changing the environment variables changes our definition of the container that we actually want to run because those environment variables are passed into the application inside of the container. So we have to kill off the existing task, as well as its container, and then start up a new task so we can start up a new container with this environment variable set inside of it. Now how can we confirm if we set this properly? Well, I'm going to disconnect here because I don't need my monitor anymore, and I can use docker service logs to check the logs for this service again. And I can scroll up here, and at some point I should see--there we go. We are now using a CUSTOMER_API = http://customer:3000. Now be careful. These logs are interspersed here. We have some of the old logs, as well as some of the new logs. You can see here we have balance. 1 and then some random characters. What do you think this represents? Well this is the task, and you can even see which node it's running on. So just make sure that you're looking at the proper logs here. And one way you might do that is to first do a service ps on our balance service. And then you can compare to the identifier here and make sure that that matches up with what you see in the logs. There are also some other options for filtering the log messages. I have noticed some of these are buggy with the 17. 03 release. So some of these will work for you that aren't working for me right now. Since, for example, you can specify something like 1m for 1 minute ago. You can also specify a timestamp. So you might try some of these out to narrow down what you're looking at. One other thing I like to do is just to scale down and scale back up. And then if I clear out and run the logs again, now you can see I have just the output from starting up a new task with a new container here. So that can be helpful when you're testing and learning, just scale 0, scale 1, and move on.

docker exec to Check Service Discovery on the Overlay Network
So our balance API is pointing at our customer API theoretically. We should be able to resolve customer to a given container running to port 3000 to respond with our API requests. So let's go to the browser, and let's just see what happens. And hey, hey, take a look at that. We've got a successful request now, which means we are sending traffic to our balance API, which is then sending traffic back to the customer API. But how is this possible? How is customer getting translated? To answer that, let's go into the container for our balance API. Now how would I do that? Well I can do docker service ps just to see where I have instances of the balance API. And that's running on worker1 right now. So I'll export and switch over to 211 for worker1. If I do a docker ps here, you can see we have our balance API container. I can grab that identifier, docker exec, interactively connect up to a bash shell, and I'm inside of the balance API container. Now how could I confirm service discovery to our customer API? Well I could use something like dig, pass in customer here, which is supposed to be what we can resolve as a DNS name, and take a look at that. We get back an IP address, and it's on our backend network, 10. 0. 9. Remember, that is the subnet that we used, and then. 4 is how we communicate with the customer service. Now interestingly, we just have one IP address right here. Right now, though, we only have one customer API container. Next, let's see what happens when we scale to a greater number of customer API containers, let's see what happens with this IP address.

Spelunking Service Discovery as We Scale Services
So right now, we only have one replica for our customer API. Let's crank that up to three. So I'm going to open up a new tab here. I'll run the monitor program on our customer API this time, and let's scale up here. Scale our customer API to three replicas. Actually, let's go with six. We've got six containers up and running. Well one of them is still starting up. There it goes. We've got six containers up and running our customer API. So how exactly does our balance API decide which one it's going to communicate with? Well let's go back, and we have our container for our balance API right here. We're exec-ed into that bad boy. You can even see a ps aux here. It shows our node server that's running. If I clear this out and I curl localhost port 3000 and go to balance/1, you can see that we have our balance API being fulfilled here. So I am indeed inside of the balance container still. And if I do dig here and specify customer, what do you think's going to come back this time? Remember, last time we just had that one IP address, 10. 0. 9. 4. What's going to come back now? Whoa, we have the exact same thing. What's going on here. We have six containers. Why are we only getting one IP address when we resolve our DNS name of customer to access our customer API service? Well this is a load balancer, and that's exactly what 10. 0. 9. 4 is. It is what's known as a virtual IP that's been assigned to this particular service. And all we have to do is hit that virtual IP address, and our requests will be load balanced over the actual backend containers, one of the six containers that we have available. Now if you want to see what's actually going on, there is a second DNS entry called tasks. customer. And if you do this, now you'll get the IP addresses for the actual containers, and you'll see that the IP addresses are 5 through 10 here, and it's not 9. 4. So 9. 4 is our load balancer for our customer API. In here are the individual IP address for the six containers. So this is the brilliant thing about this overlay network, and you can scale up, scale down. You can have as many containers as you want. It does not matter. You don't have to do a thing to be able to load balance requests to however many containers you have. Now if you'd like to know more about how this works, you can take a look at IPVS. This is something that's been around in the kernel for a long time now. It's a transport layer load balancer that's built into the Linux kernel, so it's highly efficient as you can imagine. It doesn't have to call into user space to do load balancing and, thus, it provides a layer for switch to be able to load balance traffic to all these different containers that we have. So you can take a look at this, as well as IP tables on the Linux side of things. Or on the Windows side of things, this is a great article that was just recently released explaining how overlay networking is available in Docker Swarm mode on Windows, at least on the latest build of Windows. And if you scroll down in here, there's a nice diagram that explains how this works. And behind the scenes on Windows, the Docker engine is talking via a Windows libnetwork plugin to the host network service (HNS). And that host network service is abstracting access to the virtual filtering platform, which is a part of a Hyper-V virtual switch behind the scenes to take care of these various different tasks that are necessary to load balance traffic and do other things with regards to an overlay network. So you can dive into this to learn more. So what you need to know for now is we have this load balancer for free internally now on a user-defined overlay network. And not only do we have it for our customer API, we also have it for our balance API if for some reason we had some other app or service that needs to hook up to this, or maybe even the customer API needs to talk to the balance API at times. If we need that, we have a load balancer as well in front of balance API because, well, it's just another service hooked up to our overlay network. All services automatically have load balancers put in front of them. And then what do you think I could type in here for the balance API? Let's just do dig and balance this time because that's the name of the balance API service. And just like that, I get 10. 0. 9. 2, a different load balancer or at least a different virtual IP address. Remember, the kernel behind the scenes is actually doing the load balancing, so these are just IP addresses that identify a service. And then if I want to see the IP address of the individual balance API containers, well in this case, I do tasks., and right now we just have one, so we have 10. 0. 9. 3. If, however, I switch tabs, I'll disconnect from monitoring the customer service and monitor the balance service instead here, and I'll come over and scale our balance service up to three replicas. Boom! Those are running, so I'll go back now. And now I'll do a DNS request. And look at that, we've got three IP addresses now-- 9. 3, 11, and 12. So that was really fast. We have a dynamic load balancer that is hooked in via DNS right into Docker Swarm mode so that as we scale up and down, the DNS is responding here. If we wanted to use individual IP addresses, the DNS has a real-time list of service name to IP address mapping. Let me hop over here and scale back down to just two this time. And by the time I hope over, I would be able to run DNS gain except it scaled down the container that I was connected to. So I'll need to find one of the balance API containers again. So I'll run service ps on balance API. And, okay, I can hop onto w2 or 3. So I'll hope over to w2 docker ps. Here is our balance API running in a container. 3ad, so docker exec -it, for interactive, 3ad bash. I'm in, dig and then tasks., and then balance. Now there're only two IP addresses. You can see 9. 3, the one I was on, was terminated. So I guess I'm pretty slow there to get into another container again, but I hope you understand that the scaling is pretty darn fast, and it replicates right into the DNS entries. If I go up to six, for example, there're four, five, six. There we go. So just that fast to bring up additional containers and hook them right into the DNS system. So we have service discovery immediately available as we scale up and down.

Using Curl to Validate Internal Load Balancing
Now to prove to you that we are load balancing requests, so if I do a dig here on customer, and we have this virtual IP address that's supposed to be load balancing, to prove that to you, let's run the curl command here. And part of the reason I have these images that I built for this course for you, the swarmgs/ and then whatever image, for example, the balance image here, is because I installed a bunch of tools for you to inspect things like dig and nslookup. I made sure all those things are available inside of these container images so that you have some tools to play around with, and you don't have to install these yourself. So I can curl here. And then the IP address is 10. 0. 9. 4. And we're on port 3000 for our customer API. And then if I run that, I'm not going to get a response back. I could go to customer/1 here for the customer API. But actually I have another endpoint available on the customer API that I haven't talked about yet called inspect. And this bad boy will print out some information about the connection between the client, wherever that is, right now we're inside the balance API container, so that's a client, and the server, which is the customer API. So let's run this, and you can see the request was made to 10. 0. 9. 4 on port 3000. The local IP address is 10. 0. 9. 10. So the local would be over on the server side of things. And the remote address, which would be us, is 10. 0. 9. 11. Now if I take a look at IP address and show here, you'll see 10. 0. 9. 11 here on the balance API. So this is the balance API right here, and this is the customer API right here, 10. 0. 9. 10. So if requests are load balanced, then this local address should be changing as we make multiple requests. Also notice I have a Bytes Read and Written if for some reason we were keeping a connection open. Let me clear all this out, though, and run the curl statement again. Now it's 9. 9 that time. If we run this again, it's 9. 8, 9. 5. So as you can see, this is changing here. Each time I run this, I get a different IP address back. So we are definitely load balancing over the pool of customer API containers that are available. In fact, I have watch available. So I could put watch on the front, -d -n, once every second, and I keep forgetting I don't need to put the s on there. There we go. See how we're fluctuating here? 10. 0. 9. 7,. 6,. 10,. 9,. 8. So as you can see here, the IP address is changing. It's being balanced across all the different containers that we have. Pretty darn cool. And if for any reason I wanted to do the same thing for the balance API, how could I test load balancing of the balance API? Well I need to get that virtual IP address, so dig and balance, even though I'm local, it doesn't really matter. I can get 10. 0. 9. 2 here. And if I clear out, and let's do a curl again, and this time it's 9. 2 instead of 9. 4. I also have a port 3000 on the balance API as well. It's the same port for all of my servers because they're all Node. js, and that's a typical port. And then I have the same inspect path here so that I can make a request to that endpoint. So this is the balance API this time. And notice that we are being load balanced here as well. So 9. 2 is our load balancer, our virtual IP with IPVS, and as you can see, we are being changed around to various different actual balance API containers behind the scenes. And you'll notice these IP addresses are different than the ones for the customer API. These are all connected to the same network, so obviously we'll have to allocate different IP addresses to each container that's connected up to the overlay network.

Validating External Load Balancing via the Ingress Network
Now we've seen the internal load balancing, but we have the external load balancing as well for the balance API since we published a port. You can hit that inspect endpoint in the browser for the balance API on port 5000, and you'll get back the same information. But this time you'll see that the local address is different. This is that ingress network. Let me zoom in a little bit for you here. So 10. 255 instead of 10. 9. We have 10. 255. 0 and the. 11 here. So we have the same load balancing going on when we publish a port. It's just made possible via that ingress network. One thing I will point out, though, browsers tend to keep connections open to web servers. So in this case, if you've refreshed, you're not going to get a different local address. So it may look like things aren't load balanced. They actually are. If you keep refreshing long enough here in the browser, likely things will change. There we go. We're on 0. 10 now. But just keep that in mind, browsers tend to keep connections alive, so that connection actually goes through the load balancer and has some affinity if you have a connection that remains open to whatever container you're load balanced to, which is important when it comes to load balancing concerns. If you want to see the load balancing in action on the ingress network, so when you're outside making requests, go ahead and copy this URL, hope over to the command line, and I'm going to open up a new tab here and use curl on my Mac, so I'm on my Mac. I'm not in those containers right now. And I'm just going to paste in that request here to the manager node port 5000 and inspect. And when I run that, you can see we have. 11,. 12,. 13,. 7, so our local address is changing here. It's load balanced over all of the balance API containers that I have running. So in this case, curl is not keeping a connection alive, and that's why you can see the load balancing in action.

docker service inspect for Finding the Virtual IPs for a Service
Now if you would like to find those virtual IP addresses, and you don't want to have to exec into a container somewhere (that can be a hassle), can you take a guess where we might also find that virtual IP address? Well keep in mind, this virtual IP is assigned to a service, so we should be able to inspect one of our services, like our balance API. And then in the output here, you'll see a section for VirtualIPs. And in this case, because we've published a port, we have 10. 0. 9. 2, and that's going to be the one internally on our backend network. And then there's a separate VIP for 10. 255. 0. 6. That one's on the ingress network and is just meant for the published port that we have for this particular service. Now another way you can run this with the --pretty flag, this will truncate the output a little bit, a little more readable. Now I could do that and also print this for the customer API, and we could compare these side by side. Unfortunately, though, this only shows you that the endpoint mode is VIP. It doesn't actually show you the IP address. So if you want more details like the IP for the customer API, you'll have to inspect that in the long format. And there you go, we've got our endpoint here. And in this case, we only have one virtual IP defined, and that's because we did not publish a port. So we're only connected up to our backend network. So we only have one virtual IP for that backend network. Each overlay network that a container is connected to, so basically each overlay network that a service is connected to I should say, will have its own virtual IP address. So if you connect up to four different networks, you're going to have four different VIPs.

Use DNS Round Robin Instead of a Virtual IP
The service discovery that we've seen is actually configurable. And what I mean by that is the means by which you can find a service on an overlay network and communicate with an instance of a container somewhere, that's configurable. I said that it uses IPVS behind the scenes to allocate a virtual IP, but you can change that. You don't have to use what's known as the VIP endpoint mode. So if I do a docker service inspect here on the customer API, for example, you can see inside of our specification, we actually have EndpointSpec set to mode of VIP. So that's inside of our previous spec, and actually our current spec has the same mode. We can change this mode, though, which will change how we then route traffic when we have multiple containers. So we could run docker service update here and set the endpoint mode. Instead of VIP, we could set it to dnsrr for DNS round robin. In the future, you're going to see new values here. And then I just need to specify here the customer service. When I do that then and I go ahead and take a look at the inspect for my customer service, you can see the old EndpointSpec is set to mode of VIP, and the new one is set to dnsrr. Now, this isn't going to look like much has changed. In fact, if I clear this out and go make a web request, you can see the balance API still works just fine. But that's the balance API. If I want to look at the customer API, I could do a docker service ps on the balance API, and I can see that I have one instance running on w1. Actually I pretty much have an instance running on every node here. So docker ps and here is a balance container. So 7c3, docker exec, I can exec into this container 7c3, open up a bash shell, now dig on customer, and now this is dig on customer, not on tasks. customer like we did before. And this time, you can see we have a list of individual IP addresses. So when I set the endpoint mode to dnsrr, we're using round robin via DNS, we have removed the load balancer internally, and now it's up to the client to pick one of these IP addresses when it communicates with the customer API. So this is great if you don't want a load balancer, maybe for performance reasons, or if you want to run your own load balancer and not use this built-in IPVS load balancer. Keep an eye out for additional endpoint modes in the future.

Networks Are Lazily Extended to Worker Nodes
Now one thing that might throw you off as you're playing around with your own customer overlay networks, if you do a docker network ls here, you can see the backend and backend2 overlay networks that we created. We haven't used backend2 yet, and we have used backend. So keep that in mind now. What I want to do is I want to use my run-on-all-nodes script here, and I want to do docker network and ls again. I want to run that on every single node that we have just to show you something that will trip you up if you're not aware of it. When we connected up to. 201 here, we have backend and backend2. On. 211, our worker1, we have backend, but we don't have backend2. In fact, if I look for backend, just that word, you can see that the only place that we have backend2 is on our manager node. And that's the way things are going to be if you're not using a network. So when you create a network for reasons of efficiency, Docker Swarm mode is not going to extend that network to any additional nodes that don't need it. It'll only have it on manager nodes until a worker node actually needs a network. And that means that the worker node is actually running a container that is connected to that network. So for reasons of efficiency, you can easily be thrown off if maybe you connect up to one of your worker nodes, and you're looking for a network like backend2, and you want to inspect it, and you can't find anything, well, that's probably because there's no container using that network on that node. So you'll need to switch nodes then. Part of the reason for this is that all the IP addresses for the various different containers that are necessary as far as service discovery is concerned, those are all gossiped over to the additional nodes in the process of extending the network to a node. All the information about the state of that network is then shared with that node, and that's one of the reasons why it's more efficient not to just extend a network to every node automatically. As you can imagine, if you have a network with thousands or even hundreds of IP addresses, that's information that has to be communicated that is unnecessary. And then as changes happen, as containers come up and down and IP addresses are acquired or released, all that has to be disseminated. And that happens over the gossip protocol. And it's just less information to communicate if you don't need it.

A Few Last Things
As we wrap up, I want to point out a few things. First off, this ability to have these overlay networks with service discovery, traditionally this meant some sort of external key value store. Now in swarm mode, that key value store is embedded inside of the Docker engine as a part of SwarmKit. The key value store is kept resident in memory so it's highly efficient. And most importantly, you don't need an external component like maybe HCD or console to perform your service discovery with Docker Swarm anymore like you used to with stand-alone. Now it's all embedded, one less thing to have to set up and worry about. Now if you want to use these overlay networks, you do need to make sure that some firewall ports are opened up for communication between your nodes. And those are specified in the docs. Always refer to the documentation for the latest update on the ports that you'll need to open up for your overlay networks. And one last key takeaway, something that can trip you up--you do not need to publish ports for services that are only used internally. So in this case, we published a port 5000 for a balance API because we need that externally, but we don't need our customer API externally, so we don't need to publish a port for it so that the balance API can talk to it. That's something that you can run into when you're first starting out, and some of this information is confusing. It might seem like you need to publish a port for every single service that you have. But that's not true. Internal services don't require published ports.

Deploying with Stacks
Enough with All the Flags Already
This is probably my favorite module of this course. So far we have been typing out these long docker service create statements that have a lot of flags, and we're only beginning, so we're only using a subset of the possible flags that we could be using. We're also running docker service update quite a bit to make changes to our services. In this module, we are going to see how we can put all those flags into a file, a config file, a. yml file just like docker-compose, in fact using the compose v3 format. And we'll see how we can use this to not only create services but update services all for the convenience of a single file that specifies the desired state or definition for that service so that we don't need to deal with things like passing flags to create versus passing flags to update if the service already exists. This is a huge timesaver, so let's get started.

Remove All Services
Let's clean the slate by getting rid of all the services we have. But before I do that, I want to inspect each of these services, and I want to dump out the result to a file here so that I have available if I need to the configuration for each of these services. So customer and then balance. And then how about we also network inspect our backend network, the one that we actually used, and we'll dump that out as well. So you can see here if I cat out the viz. inspect file, we have the last configuration for this service if we need to refer to it. So I'll clear all this out now, and I'll do a docker service rm. I'll specify viz balance and customer here, docker network and rm will get rid of both of our backends. Docker service and ls now, nothing. Docker ps, nothing on this particular node. It looks like we are still shutting down right now. There we go. Now we're cleaned up. How about we do a run-on-all-nodes docker ps here. It looks like everything's shutdown now. Docker network ls here. Those are the defaults. So we have a clean slate now.

The New Compose Version 3 Format
So now let's go about the process of adding these services back. And we'll start with the visualization service. And I just want to get right to work, so what I want to do here is make a directory, and I'll call it services. By the way, take is a command that's available on Z-shell, which is what I use for my shell. So I can take, which makes a directory and changes into it at once. And then inside of here, obviously no files. So I will go ahead and edit a new file, and I'm just going to call this viz. yml. Inside of here, I need to start out with version and then a value of 3. And this is. yml. If you've worked with docker-compose in the past, this is all going to look very familiar. This is the latest version of the compose format. If you want, you can hop out to the compose file reference out on the Docker docs and look at version 3. There is also a file versions and upgrading doc that shows you that version 3 is the most current. And then down here is a matrix. And you can see there's actually a version 3. 1 as well. Version 3 and 3. 1 require Docker engine 1. 13 or now because of the re-versioning, it is 17. 03 or greater is required for this 3. 0/3. 1 format. And if you click on versioning here, you can see the changes per version. And most important here is version 3. This is designed to be cross-compatible between both compose, so docker-compose from the command line, and also Docker engine's new swarm mode. So you have to use version 3 to be able to work with swarm mode. This version removes several common options, but it also adds some. So you need to pay attention to this. For example, here are some of the things that are removed. And most importantly, there is a new deploy section that we'll see that was added, and this has to do with constraints and other aspects of services that we deploy. There is an upgrading section down below. This link points to right here. You'll see version 2 to 3, what you need to do to migrate in the event that you already have some docker-compose files that you'd like to test out. For the most part, it's pretty straightforward, so let's just get right to work.

Creating a Compose File for Our Viz Service
So I'm back in my file here. I'm actually going to set this to 3. 1. So right now, I'm just editing a text file. You can use whatever text editor you'd like. And if you've never used docker-compose before, the way I like to think of these compose files, you're basically just typing up all the arguments that you would pass to docker service create or docker run, so all those various different flags that you would pass, you're just typing them up inside of a file instead. It's a much easier approach to passing arguments. So a file instead of CLI arguments. That's all we really have with docker-compose. But it makes a night-and-day difference. Now next up, I need to define services just like in the version 2 format for the docker-compose file. And I need to name my service. So I'll start out with viz here, which is the visualization service that we're going to work on. I need to specify an image. And this is exactly like we've done in the past. Now if I want for my reference, I could split the screen here and cat out that viz. inspect file that I created and maybe grep for image. And we have manomarks/visualizer, so I'll grab that paste that up here. I could optionally provide a tag if I want, but I'll just use latest. Now that I have that, I know that my visualization service had a volume that needed access to Docker to be able to inspect the current state of the cluster, so I need to cat out the contents of that visualization service again. And this time, I'm going to look for volume. That doesn't come back with anything. Do you remember what we used to map in the Docker socket? Well it's actually called a mount here, so that's the parameter that we're specifying. And there's Mounts. That's not enough for me to work with, so I will just cat out the file here and scroll up to Mounts somewhere in here. There we go. So we have a bind mount. The source is the Docker socket on the host and mapped into the Docker socket in the same location inside of our container. Now one of the things I like about using a compose file, volumes use a similar format as we've used in the past with docker-compose, so I just do a volumes section here, and then I paste in the absolute path on the host and map that into the container passing in the absolute path again here. Just for your reference, take a look at what we passed when we called service create. We had this big long --mount flag here, which is much more explicit but at the same time is much more verbose. I prefer this format in a docker-compose file. Next up, let's take a look at this constraint here to run our visualization service on a manager node only. In this case, we'll use the new deploy element, which has many subsections as you can see in the Table of Contents here. So deploy:, and then this gets a bit verbose, but we need to specify a placement constraint. So we'll use placement and then constraints as we could have multiple constraints. And then we just set the constraint, so - and then node. role, and that's equal to manager. And, again, we can refer to the previous command we ran and just lift the constraint and add it in, or we could inspect our visualization file and find out what that constraint was. And don't forget, you can always come up to the docs here that I've been showing you and scroll down here, and you'll see many good examples. For example, here are placement constraints, and here is node. role == manager. And one thing that can be confusing at times, you'll see little snippets like this, but this snippet is actually embedded inside of the deploy element, and it can be a little bit not so obvious because some of these snippets don't contain the entire hierarchy. So just be careful. That can be confusing at times. We still have to add our publish port, but I'm going to hold on that. I want to show you what it looks like to do an update of a service with a compose file. Next, let's take a look at how we can start up this service with this viz. yml file that we created.

Deploying a Stack with a Compose File
So we have our file here. I'll go ahead and save and close this file. And I'll get rid of the tab down below. So I can cat out viz. yml here, not much to it, just a. yml file with some text that contains the various different flags that we're passing to the command line, though in a nice hierarchical fashion, organized so that things make sense instead of just a bunch of flags at the command line. And, of course, we don't have any services right now. Now it might be tempting to think that we go into docker service and then we look for something here to use a compose file. But that's not the management command we'll use. Instead, there is a new management command called stack, which has many subcommands. We can get help for this, and you can see there are five subcommands right now. I suspect this list is going to grow over time. These subcommands should look familiar. For example, ls and ps for tasks. We have rm, and there is a services subcommand that is experimental. This list is going to look somewhat familiar because we're going to at the end of the day be managing services with a stack. So we'll need to drill into those services and then also drill into the tasks. So let's step back and take a look at the hierarchy here. So stacks, which we're creating now, have services, one or more services per stack. And, obviously, services have tasks then, potentially multiple of them. And then those tasks correspond to containers that contain the application that you want to run for a given service. And, of course, you could have multiple stacks. And another stack might only have one service, which might only have one task that only has one container, which is going to be the case here with our visualization service. The stack over on the left-hand side here is going to be more like our customer and balance APIs example. We'll get into that more, though, as we go through these examples. For now, let's go ahead and use docker stack, and then we need to use the deploy command. And this command takes a couple of different options. There is what's known as a bundle file, a DAB. I'm not going to use that, though, because I prefer the compose file approach. And, honestly, I don't think the bundle file approach is going to be the future of this command. I think using a docker-compose file that everybody's familiar with is the future. A bundle file is essentially a JSON equivalent of a compose file. So I use the -c option here. I point at my visualization compose file, and then I give a name to the stack. So I'll call this stack viz as well. Boom! Just like that, I get some output here that a network was created called viz_default. So just like with docker-compose, when you deploy a stack, each stack gets its own network automatically. So that backend network that we created in the last module, that we went to a lot of trouble to make, it's created for us automatically by virtue of deploying a stack instead of just a service. And our services in the stack are automatically hooked up to this private network that belongs just to this stack. We'll get to that in a minute though. We also have our visualization service that was created. Now just like with docker-compose, all the artifacts that are created are prefixed by the name of the stack here, so we have viz_default for the network, and we have viz_viz of the service because the service also has the name of viz in addition to the stack being called viz. Now just to see what happened here, I could do a docker service and ls, which is something we're familiar with, and look at that. We have a viz_viz service. It's using our visualizer image. There's one replica up and running. We can even do a docker service ps here. We can specify our viz_viz service, and you can see we have one container up and running. But when you start working with stacks, you probably won't go to the service command so much. You'll want to work with the stack itself, so you could do docker stack and then there's ls to list your stacks. And then, remember, I said you could have multiple stacks, and each stack can have services. So that's where services comes into play. And if you pass viz for our viz stack, you can see the services that it contains, which is just one. So this docker stack services is equivalent to docker service ls. If we want to see our tasks for one of our stacks, docker stack ps, and then we specify viz as the stack here, and there you go, we've got our one task for our viz service. So docker stack ps is the equivalent of docker service ps except we pass in the stack, and we see all of the tasks for all of the services that are a part of this stack.

Updating a Service with a Stack Is as Easy as Creating It
So at this point in time, all we've done is turn a service create statement into a compose file. And we started up a service. But I left off one of the pieces of our service definition, the port that we want to publish, which means that we can't check this website for our visualizer that we just started up until we get that published. So now let's see how we can update a service that we create via stack. So this is the thing I love about stacks. All I have to do is edit that viz compose file and come in wherever I would like, and I just need to add in my ports. And just like with docker compose in the past, I do ports, and then I can just specify whatever I want here, in this case 8090. And then we'll map that into 8080 in the container. We can double-check by taking a look at our create statement, yup, 8090 into 8080. And don't forget, if you're ever confused, just hop out to the compose file reference. This is a long list of everything you could possibly put in here with examples. So here you go, here's ports and all the various different combinations you might use. So here's the beautiful part. Once I then save and close that file, so you can see I've made that change now, we have 8090 to 8080, now I can go about deploying my service with this. So I can simply bring back my docker stack deploy command, exact same command I issued before, and just fire it off. I don't have to worry about what I changed, what I added, what I removed. I don't have to worry about calling service update versus service create. Even if this were the first time I was creating this stack, I could use the exact same command, and that's because all of my changes are put in that file, and Docker Swarm mode is smart enough to read that file and figure out what needs to change for me. The service is now updated. And if I want to know what's going on, docker stack and then I could do ps and pass in my viz_viz service, though it's just viz here for the viz stack. And you can see that we have the old task shutdown, and the new task is up and running. And most importantly, when I come over to a web browser and check my visualization service, it's up and running. And just to confirm that that's actually the same thing that we're working with over in the command line, I could go ahead and remove this stack. Join me in the next video for that.

Removing a Stack
So let's get rid of that stack that we created. So docker stack rm, and then we just specify viz here. And, boom!, it's gone. Over on the right-hand side if I refresh, you can see the site's not up and running anymore. I can hope over here, up arrow a few times, pull back my deploy statement, docker stack deploy with my compose file, back up and running, over in the browser, back up and running. So it's just that easy to work with stacks. I don't have to deal with all the flags anymore to docker service create. I don't have to deal with them to docker service update. In fact, I don't have to know the difference between creating and updating anymore.

Creating and Deploying a Multi Service Stack
So now that we've done the easy service, let's move on to using a stack to create our balance and customer APIs. And the reason that I will create these together in yet another stack is because these are applications that are related, and when you have applications that are related that you would like to deploy and manage together, that's where a stack comes into play. It's a means of managing multiple services. Now it's entirely up to you. You could have separate stacks, one for the balance API and another stack for the customer API, if they truly aren't related enough that they need to be as a part of one stack, if they aren't that related. But let's just say that they related. And let's go ahead and build out the stack file to deploy two services instead of one and connect that service together with its own network much like our backend overlay that we created before. So I hop over to the command line. I can edit a file here. And let's just call this apis. yml, set the version, so we'll go 3. 1 again, set up the services here. And then just for our reference, I built this series of network and service create statements so that we can refer to this. We'll start out with the customer service. So customer is the name. Image is swarmgs/customer. And that's all we have to do. As far as the network is concerned, because we're using a stack, this customer service will automatically be hooked up to a special network just for this stack. So we don't need to create the backend network at all. So let's do the balance service next. So balance, and then the image is swarmgs/balance. And then we need to publish port 5000, so ports, and then -, and then 5000, and map to 3000 inside the container. And then we need the environment variable. So environment here. I'll go ahead and paste in that variable. And we'll wrap our value with double quotes. And then we already have the network taken care of. So that's it. We've now defined our two services, the network is implicit. Let's go ahead and close this out and save the file. Cat out apis here just so you can see it. And now you tell me: How do I go about creating these services? Well in this case, docker stack deploy, point out the file, in this case it's apis. yml, and I just need to give this a name, how about apis. We create an apis_default network. We create two services, one for the customer and one for the balance API. How can I look at the services that are a part of the stack that I just created? Well in that case, docker and then stack services, and then apis is the name of the stack. And there are our two services. Now, if I wanted to use the other approach that we used throughout the whole entire course up to this point, how could I also list out these services? Well I could also do docker service and ls, and I'll get the exact same services in addition to our visualization service. So, again, these have equivalent information. Now if I want to drill in and take a look at the tasks that are affiliated with my services for this stack, what do I do? docker stack ps and apis. When I run that, now you can see both of the tasks associated with the two services that we have. Now what's the equivalent command I would have had to run to look at the tasks for both of these services? Remember, we have apis_customer and apis_balance. Well the equivalent would be docker service ps, and then we can do apis_customer, and that would give us our customer task. And then we could do apis_balance for our balance task. So we'd have to pull those separately. So, actually, when it comes to managing multiple related applications, it's nice to be able to query the tasks for all of them at the same time with a single call to docker stack and ps. But I think what's best of all, if I hop over to a browser here, and if I pop open a new tab, make a request to the balance API, so manager1, port 5000, which I published, and take a look at that. We've got our balance API up and running. And this means that the customer API has to be working as well. In fact, I could check that quick. I could be malicious here, docker, and I could go to service scale, and I could set the apis_customer to 0. And now I can't make my connection to the customer API. If I come back and scale it back up, there we go. We've got our customer API back, which means our balance API now works again. Isn't that awesome? Take a look at this. Which would you rather maintain, the stuff on the upper left or the stuff in the lower right?

Specifying Replicas in Compose File
So we have our services deployed now, and we only have one task for each. So how about we take a look at what it's like to scale these up. Now just a moment ago, I showed you how to scale down and back up just like a regular service, but we can also edit our. yml file here. And then under either of these in the deploy section, we can specify a number of replicas, and I could set this to maybe 5. And then down under the balance service, deploy, and then in this case, we could do replicas and maybe specify 2. Just like that, save and close the file. Do my deploy again with the same file, the apis. yml. Boom! I'm done now. And then I'll do a stack ps, and there we go. We've got all seven it looks like of our tasks up and running, five for the customer and two for the balance API. And then just like before, I could make a curl web request here to the manager on port 5000 for the balance API. And there you go, we're using local address of 0. 10, 0. 9, 0. 10, 0. 9. So we're load balancing between the two balance APIs that we have available, just like we did when we were working with services directly, the same load balancing applies to stacks. A stack is just an organizational concept to help you quickly deploy services.

Quiz and Key Course Takeaway
If you're to take one thing from this course, it's what I want to cover here in this slide. And I want to turn this around into a little quiz for you. So we've seen throughout this course that docker run is loosely equivalent to docker service create. By all means, they are not the same thing. But at the end of the say, they help us launch containers. Now if we were to extend this analogy with what we've seen in this module, I would say docker run is to docker service create what docker-compose is to (and then I want you to fill in the blank). What is docker-compose loosely equivalent to in this course? Well docker-compose is pretty much the same thing that we have here with docker stack deploy except that docker stack deploy helps us when we are working with a swarm, just like docker service create helps us start containers when we're working with a swarm. So docker run and docker-compose are kind of the old way of doing things. Now we have docker service create and docker stack deploy. And if you just walk away with this, I think most of the mystery around containers is going to start to melt away when it comes to managing a swarm. If you take all of your existing knowledge of docker run and all of the hundreds of flags I think there are, and you take all your knowledge of docker-compose, and you apply that knowledge to two of these new primary entry points in the swarm mode CLI, you will quickly find that it's just a matter of finding out what a new flag is with service create or service update, or it's finding out what that new element is in the. yml file. And in many cases, the flags are the same. Ports, for example, the flag -p for docker run, is the same for docker service create. And when it comes to the compose file, we have ports in both. Same thing with volumes. When it comes to the compose side of things, now that's different, we have mounts now with docker service create. So just take your existing knowledge and try and map it into this new domain, and you'll find it's just a matter of looking up in a reference somewhere, may that docker compos file reference, to find what it is you need.

The Trifecta Services - Networks and Volumes
As we wrap up this module, I want to point to a few of the elements of this compose file that I want you to start thinking about and perhaps playing with. So the deploy element, there are many sub-elements. We've seen things like the number of replicas, but you can also configure the update policy, the restart policy. You can constrain resources so there's a new element for controlling resources if you want to limit CPU, for example, or memory. There're labels that you can apply. So take a look at some of these new elements and watch for new elements to show up here under this deploy element in the compose v3. yml format. But don't just pay attention to that. Let's scroll down here and talk about what I like to think about as the trifecta when it comes to managing applications. So we have compute power, which is what a container is. We have a process we need to run for our application, and containers have revolutionized that. And compose files have services to manage containers. But they also have networking and volumes. And we've already seen how compose files when used with docker stack deploy give us an automatic network that's private to our stack of applications. And this stack of applications could span thousands of nodes, and it would all be taken care of for us. Now can you imagine setting up the networking for that all on your own? Especially if you wanted to use this overlay networking? Could you imagine bringing in service discovery and load balancing? It would be a nightmare to manage all of that. Networking is the second point of the trifecta that Docker manages for us, and specifically Docker Swarm mode makes it really easy to deal with when we start spanning multiple nodes. And then the last element of the trifecta is the volume configuration. And a compose file deals with volumes as well. Now at this point in time, volumes are somewhat limited when it comes to a swarm. You can use named volumes, and you can use bind mounts to access the host file system. But then beyond that, you have to install plugins if you want to maybe set up a volume that spans multiple nodes or connects out to some external storage system. At this point in time, we don't have some notion of a volume that automatically is replicated across multiple nodes. This is coming. I don't know what it looks like. I know it's coming, though, because with this piece of the puzzle, the Docker platform is just going to be something in and of itself that's absolutely amazing once volumes are solved as well on the level of a swarm. And then the neat thing is with compose files, whatever's coming, it's just going to be integrated with compose files. So I suspect in the future if we use a stack for a deployment of multiple services, somehow magically we're going to have volumes that just work so that if we have a database running somewhere and it crashes, we could just bring that database up somewhere else, and the volume will magically move with that container wherever it ends up.

Health Checking
A Running Process Is Not Necessarily Ready for Traffic
Obviously Docker takes care of restarting a failed container if our application exits for example, but just knowing that a process is running doesn't always tell us if that process is healthy and in the case of a website we may have a process up and running, but the web server might not yet be ready to accept requests and if we start routing traffic to that web server before it can handle the requests then we're going to start dropping that traffic. So there's a level of knowing that a service is healthy and specifically an instance of that service in the form of a task and actually a container. We can know that that's healthy by performing some more advanced checks than just whether or not the service is running, and that's what we refer to as health checking and that's what we'll see in this module and we'll see how this integrates into everything else we've seen thus far in the course so that we can provide robust health checks of our application and we can virtually eliminate the possibility that we send traffic somewhere before that destination is ready to handle that traffic.

Deploying a Cowsay Stack
For the upcoming demos on health checking, I'm going to get rid our API stack just to reduce the number of containers and tasks that we have running on our cluster. So we'll just have our visualization stack and that's it. And then for some of the initial examples, I have this cowsay service. I have a compose file for this service so that we can launch a stack. You can find this in the GitHub repository for this course and this stack contains a single service that uses a cowsay image that I have, publishes port 7000 to the swarm, and then I also have a constraint that will run on the manager node so we can easily get into the container for this service. If you come over to the browser and hit port 7000, you'll see that's not and running yet, so let's go ahead and deploy this. Now I've got a quick quiz for you. How do I go about deploying this cowsay stack? Well, I do a docker and a stack, deploy, specify -c for the compose file and point it at the cowsay. yml file. Be careful not to use the health. yml file; we'll be using that one a little later on. Use the cowysay. yml file and then we give this a name so we'll call this cow. Okay, our service is deployed and we have a network for that service. I'll go ahead and run a docker stack services, put in cow here, and you can see that 1/1 replica is up. If I come over to the right and refresh, now you can see the site is working and this is just a little site that generates a random quote and it looks like it's coming from a cow. You can just refresh this for a different quote to be generated. Now let's use this to explore health checking.

What Happens if We Break a Container?
Okay, so let's be a little malicious here. I'm sitting on the manager node and because of that, I have access to the container here for the cowsay service, the one container that we have running for the one task that's a part of our cowsay service. You can confirm by looking at the visualization and see that cowsay is running on the manager node and I did that so we could easily get to this container here to then go into it and manipulate the process and the files inside of that container to break our application. So let's do that. So I'll do a docker exec here. I'll make this interactive, and then I could type out this id here, but every time we kill off the container or do anything to the service for that matter, we're going to have a different container id. So instead I prefer to look for this id and just inject it in. So what I'll do is a docker and then a ps and then a -f and I'll set the name equal to cow. That narrows down to just this one container that I care about and I can use the same principle here now with docker exec; I can do docker ps -f name=cow -q for quiet, which will just return the identifier and I'll pass that to the docker exec command then. And then I can use -it for interactive and then on the interior I can use say a bash shell. This time if I do anything to the task and I end up getting a new container, I don't have to look at the container id again, that's why I'm using this command here. Alright, so I'm inside of the single container that we have for our cowsay service and maybe I'll split the screen. Okay, there we go. We've got visualizer in the lower left and we've got our site over here that still works in the lower right. Inside of this container though, if I were to be malicious, I might do something like move a file that's used to generate that quote and that random quote is actually generated by a command line utility called fortune. So if were to move that somewhere else, perhaps just rename it to fortune2, when I do that then, now the site doesn't work and we get an exception back and of course if I just put it back to where it was, fortune2 back to fortune, now the site's working again. What's interesting is that even though we broke our application and it no longer works here and I'll break it again, even though our app doesn't work, our app is still running so as far as Docker is concerned, there is nothing wrong with the application because it's alive. Now if the app were to crash such that it was no longer running, then we would get a new task with a new container and our app would be back up and running, but in this case here where the application itself is just misbehaving, Docker has no way to know that that's a problem and just to keep an eye on things, let's split the terminal up in the top here and I'll do a watch and we'll do that every half a second. We'll do that on docker, stack, ps and then our stack is cow. So we just have our one task right now. Nothing's happened to this task even though it's technically not operational. So while the process is alive, we have nothing to tell Docker that hey, something is wrong here, we need to recreate our container. In order for Docker to be smart about that, we have to tell it what it means for our application to be unhealthy and then this information can be used on top of whether or not the process is running as an additional condition to perhaps get rid of a task that is associated with a container that is now unhealthy. Let's take a look at how we can do this.

Automatic Service Recovery with Health Checks
So in this case where we've broken the container that we have for our application, it's reasonable to assume that we might want to then create a new task and shut down the existing one. So we could do something like docker service and then update and we could force a new task to be created for our cowsay service. And when I do that, the old task is shut down, a new one is up and running, and if I refresh the site, you can see the application is working again. Now wouldn't it be nice if we could plug in some of our own safety checks and if those safety checks fail, for example, our application returns an exception. Wouldn't it be nice if then Docker Swarm were to detect this on the level of our service and restart the unhealthy task in order to minimize downtime? Well that's exactly what we can do with health checks. Let me show you what this looks like. So I'm actually going to remove the stack that we just created so we'll get rid of the cowstack and then there's another cowsay and then health. yml file and inside of here the only difference is a different image that we're using. I'll explain what's going on behind the scenes with this image in a moment, but let's just get this service up and running and to do that we need to deploy the stack file. So docker stack deploy -c and then this time cowsayhealth. yml and then we'll use cowh for the name of this stack. Okay, our service is deployed. You can see it's up and running over on the manager or it almost is and it's up and running now. If I come over to the browser here, one other difference, this is running on port 7001. So you can see the same service is up and running, but this is a slightly different image and in order to understand what's going on here, I'm going to split the screen here and I'm going to pull back that watch on the ps, I'll put in cowh as the service this time, and there you go. You can see the one task that we have right now. Now in the upper panel I'm going to pull back my exec statement, docker exec it, and again it's going to fetch the latest container id based on the name of that container starting with the word cow so this should find our new cow container and it'll open up a shell into it and now if I do something like move and then usr and then games fortune, I move that to games fortune and 2. Let me make the middle panel a little bigger so you can see the tasks down below. When I run this I want you to keep an eye on the tasks in the middle here. So I'm going to run this, I'm going to refresh the app to show that the app is broken. I'm just going to sit here and refresh the app while you watch that middle section for tasks. It's going to take about 15 seconds so I'll cut out a little bit of the time in editing, and there you go. The site's down and the site is back up and running. Also notice in the top panel that was I was disconnected from the container because that container that I was connected to, that I was exec'd into, was shut down so it no longer exists for me to be connected to it. So it took a little bit of time to get things recycled, but you could see that the old task was shut down and a new task was created and it's now up and running. I didn't have to do anything and that's because there is a health check built into this image so this cowsayhealth image has a health check built into it that more or less reports back to docker when there's an issue and in this case then Docker Swarm that's monitoring the status of our service, realizes that we have a task that's unhealthy, it shuts it down and starts up a new task in the same way that we would start up a new task if a container stopped. So in the same way we would start up a new task if our application died, health checks will also label our application as unhealthy or failed and then a new task will be created, all automatically. Now let's go ahead and dive into this and see how this is all working.

Manually Forcing a Corrupted Service to Restart
Okay, so let's get rid of some things here. So we've got our stack for cowsayhealth. I'm going to get rid of that. On this particular node you can see we only have our visualizer running. We can confirm that over here in the visualizer; we only have the visualizer service up and running. And then let's come over to the command line because I have a different stack and service that I want to use to build out this health checking, something that might make a little bit more sense than our cowsay example and something that will give you another example to think of. So I've got this calc. yml file for a calc stack and this uses a different image called swarmgs/calc. This image does not have any health checking built in; it is just a website that runs and that we'll be publishing here on port 7000 on the cluster. It's a website that runs internally on 80 inside the container. I'll also be placing this onto the manager node so we have easy access to the container that's running. In this calc example, I'm doing that mostly so we can monitor the status of the container because there are some details of health checking that you can only check if you go down to the level of the container itself. Now if want to deploy this calc stack, how do I do that? We'll do a docker stack deploy -c to specify our compose file and then we'll call this calc. Fire that off. Our service will be deployed, docker stack ps, put in calc here, and you can see it's up and running. I already had the image locally so it didn't take me as long; it might take you a few moments to pull down that image. Let's take a look at the website that this deploys. So first off, on port 7000 there's an endpoint at /calc/iseverythingok. This is a little health check endpoint that I added to this application and you can see right now it says everything is OK. And then there's another endpoint /calc/divide. You could think of this as a division service to which we can provide a numerator and then a denominator and this endpoint will calculate the result. So I could change this to whatever I'd like and the cool thing is, if I pass in 0 here for the denominator, this will put the application into a so-called corrupted state, at which point in time it doesn't matter if I change this to a valid value for the denominator, the application will no longer respond to any requests and if I go back to that iseverythingok endpoint and refresh this, this will be corrupted as well. So I've built this application that you can use simply via browser and put it into a corrupted state. So you could think of this as an unhealthy application, even though it's still running and even though it can serve up responses here, the only response it serves up is that the app is corrupted. So in this case the app is unhealthy and we really would like to restart the application, which means we need to shut down the existing task and create a new one, but that's not happening automatically for us. No matter how long you wait here, nothing's going to happen and that's because we have no health checking set up. Now if I come back to the command line and take a look at the tasks, so you can see that there's still only one task here. Now if I wanted to fix the application, what could I do here? Well, about the only thing that we do right now is do a docker service update and force the update of our service, our calc stack _calc service and when we do that, and run our ps, you can see the old task will shut down so the pbe is now shut down. You can see that in the desired state and the new task, a1 is ready. And if I run this again, it looks like it's running now. Now if I go back to the application and refresh the 'is everything OK, ' endpoint, you can see everything's OK again because we restarted the app and as long as I perform division that's valid, for example, I could put an 8 in here, as long as the division is valid and not a divide by 0, I won't corrupt the application. So let's add health checking to this service so that if the app gets corrupted, the task for that app will be recreated automatically.

Options for Adding Health Checks
So I've got a challenge for you now. I'd like you to pause the video and go look for how we could update our application to automatically recover from these failures. If you really want a challenge you can try and change the stack and service to implement this, but at least go look up what we can use to make our service recover and then come back and we'll walk through this. So if you went out and looked, likely you found one of three different spots and maybe you found multiple of these. First off, we could update our stack file, the compose file version 3. We could update this and add in a section called health check. So that's one route that we can go. Another route, if we were to call docker service create directly or docker service update, in other words if we weren't using a compose file for our service, well if we scroll down there, there are some flags here for health checking, and if you were paying attention, you'll notice that these look similar to the options that we have over in the compose file and that's because the options in the compose file mirror what we have at the command line. Remember, a compose file is just the ability to pass arguments really from a file instead of from the command line. So those will match 1 to 1, though I have noticed from time to time, sometimes there's a flag that's available at the command line with docker service create or update that has not yet made its way over to the compose format. If that's the case, just watch for a future update to the Docker engine. Usually those flags that are available at the command line are added pretty quickly to the compose file format. Aside from that though, those modify our service that uses an existing image. The other route we could go so even though this is the third place you would've found, this is really a separate option from the other two. The other route we could've gone was to bake the health checks right in to an image. So if you'll notice there's a health check instruction in the Docker file reference and you're going to notice similar options here as well. We have interval, timeout, and retries. We also have a command and if you look at the service create statement you'll see command, interval, retries, and timeout. So we have the same options in all three places, it's just a matter of where we want to inject the health check. So if you're developing an application and you know what it means for that application to be healthy, why not build that right into the image itself so that people can just take advantage of that being baked into the image? If, however, an application doesn't come with bundled health checks or if you want to change the health checks that are provided, then you can use these later options, the compose file or the service create statement. Technically there's one more option and that is the Docker run reference because health checking commands are not just something unique to Docker Swarm. If you look through here you'll see health checks as well. So we have the same set of flags here that can be passed to docker run if we are not working with Swarm mode, but in either case we're really talking about either baking these into the image or injecting these at runtime with one of these different options. Now the easier approach here is not to need to create a new image so let's start there by injecting these options at runtime and then later on we'll see how we can bake these into a Docker file.

Adding a Health Check to a Stack Compose File
So right now we have a compose file so that's what I'm going to use to add in our runtime health checking. All I have to do is edit that cac. yml file and then anywhere in here wherever I'd like, I just come in and type in health check. That's a new section. It's a part of my service definition. And then I can set one of the various different options that I have available here. So test is the command or the probe if you will, that will check to see if the application is healthy and it could be any command you want. So I'll start out here with test: and then there are a couple of formats for typing this up. There is the exec format here. I prefer for these the shell format, it's a little bit easier to read, and you can see in this example here we're using curl to check a website. Now our website is a little bit different so let's go set this up. So let me hop over to my compose file and I can just type in a command here that I'd like to fire off. So curl -f and the -f means that we'll silently fail; we won't print the output of the website if things fail. We'll see that in a moment. I'm also going to add an -s for silent and a capital -S to show errors. These are just my preferences for the curl command, and then I need to point this at http: and then // and then this command is going to run inside of the container. So that means I have to have curl inside of my container as well. So whatever command you want to run to do your health checking, that has to be baked into the container image. I fortunately have already installed curl into this swarmgs/calc image so it's available to us. Now you tell me. Knowing that this curl command is going to run inside of the container where our application is executing at, what type of web request do I need to make here? What do I type in here? Well first off it's localhost and then don't forget, we have the /calc/iseverythingok. In some cases you can just check the root of the website, but in this case we have a dedicated endpoint to check the status of our application. If your application doesn't have a dedicated endpoint, well you could do something like perform some operation with some valid values and see if it comes back okay, but in this case I have a special dedicated endpoint. So let's quick open up a new tab and let's use curl here to simulate the request. So -f -s and -S. And then I need to go to my manager node on port 7000. Remember, right now I'm on my Mac so I'm making a request to the manager node on the published port just to test this out; it'll be different inside of the container when I run the health check. And then I add my endpoint here. So my calc/iseverythingok endpoint. When I run this, you can see everything is okay right now, just like if I go to the browser and check that page as well. However, if I go over and corrupt the application, now when I run this from the command line you can see the requested URL errored out with 500 Internal Server Error. So it looks like this health check will do the job. Another thing about this health check, if I look at the exit code you can see it's non-0, it's a value of 22. That's how Docker is going to know if your health check succeeded or not. It's not reading the output and knowing that everything is okay means everything's okay. It's going off of the exit code. Let me really quick split the screen here and let me fix the application. So do a service update force again to fix our application. Over in the browser I'll go back to the iseverythingok endpoint and everything is okay again now. I just wanted to fix that so I can show you now. If I come back here and run our health check again, everything is okay and most importantly when we echo out the error code you can see it's 0 now. Now if we hop over to the docs, and this tidbit is not in the compose file docs. That's why I wanted to show you these four spots because each of these spots has a little bit different information available. If you go over to the Docker file reference and you scroll down a bit here in the health check section, you will see this warning that the exit code indicates the status of the container, the health status of the container. And the possible values are listed, 0 for success, 1 for unhealthy, and 2 for reserved. So the docs here seem to indicate that we should only be returning one of these three values. I haven't had any problems returning a non-0 exit code other than the value of 1. It says don't use 2 here, but if you want to be safe, you can use this little snippet on the end of your command. If I come over to the terminal, paste that in, what this is going to do is when our health check runs here, if this is non-0 indicating a failure, then the rest of this expression will run and thus we will exit with a exit code of 1 instead. So you could think of this little part here as saying, hey, if the exit code is anything but 0 then make it 1, and let me show what this looks like by breaking the site again. So I'll corrupt the application again by sending denominator of 0. Now instead of exit 1 which will close my window here, I'll just do echo failed. And you can see we echo out the word failed here whereas if I go update the app, so it's working again, and let me show you that in the browser. The browser's working again. Now I'm going to run the same command with echo failed on the end there and you'll see in the output we don't have the word failed because that last part of the command did not evaluate. So now when I replace this with exit 1, that'll change the exit code when we run this health check. Now I've done this where I have a health check just like this without changing the exit code to 1 and it's worked, but per the docs let's just use the value that we're supposed to use. So what I can do is come in here and put exit 1 on the end. I'll save that then and there are a couple of other options and I'm going to change some of these from the defaults. Join me in the next video for that.

Configuring Interval and Timeout and Retries
So back over in my compose file reference, which by the way I always keep this up when I'm working with Docker compose or anything related to Docker, I keep these references up. I can come in here and see that there are some other options for interval, timeout and retries. And normally I wouldn't set these values, but I do want to change these because the defaults are a bit lengthy and if you want to know what the defaults are, you can find those over the Docker file reference. Scroll up a bit here, you'll see the interval by default is 30 seconds, timeout is 30 seconds, and retries is 3. Interval, that's the amount of time between health checks. Timeout is the timeout when we're running a health check. So if we're checking a website and it takes more than 30 seconds to respond, we assume that that means unhealthy. And then retries, that's the numbers of times before we mark the container as unhealthy. It's possible that an application would have a temporary blip and it would fail a health check one time. We might not want to mark that container as unhealthy on one failure and that's why the default for retries is set to 3. So after three subsequent health check failures, then we mark the containers unhealthy and the important thing is, when that happens, then Docker Swarm Mode is going to kick in and the manager node is going to notice that and it's going to shut down and create a new task. So that's why we don't want to necessarily mark a container as unhealthy the first time it fails. So what I would like to set is the interval and you can see in the compose reference that that's just interval and then I can just type in this nice, English-like expression. So I'll come over and set interval here and we'll set this to 5 seconds, so we're checking every 5 seconds for a failure and that way it'll take up to 15 seconds when we set up a failing application for the task to be shut down and a new one to be created. That'll keep these demos moving along. And then if you want, you could set the timeout in case you are trying things out. We might set that to 5 seconds as well instead of the default of 30 seconds and then retries, I'll just put it in here so you know that it's set at a value of 3 even though that's the default so I'll just comment that out. And we'll save this file then and close it and join me in the next video where we go ahead and deploy these changes.

Deploying Health Checks and Inspecting Container Health
Now before we roll out these changes to our service definition, I would like to do a docker ps here and show you that we have our calc container on our manager node and this is why I put it on the manager node with the placement constraint. If we do a docker inspect on the id for that calc container, if I scroll up here, there is a section called state and if I want to filter just for this, I like to use the jq command to do this. I can pass an expression here to grab just the state off of the items in the array and there's just one item coming back for one container right now. So I can grab that state object off of the overall inspect object and I get my container status here. Now right now there's not a lot on here. I want you to keep an eye on this though once we roll out our service definition because there's going to be a lot more information on here to give us a snapshot of the health status of a given container. Let's do a docker stack ps on our calc stack, if I could spell, and now you tell me. Quick quiz. How do I roll out these changes in the compose file? Docker stack deploy -c, pass in my compose file, point at my calc stack, boom! We're done. This is why I love using compose files. In fact, I hope at this point after using the service create service update throughout the earlier parts of this course, I hope you're as convinced as I am that you can forget about those and move on just to the compose file itself. One of my favorite things to do is actually check those compose files and diversion controls so I can see what has changed. Okay, so we've deployed our stack. Let's do a stack ps here. And now what do you think is going to come back in the output of docker stack ps? Is that what you expected? So we have a new task, right? Before I did the deploy we had wod. That's shut down now and we have gyn instead and that's because we changed our service definition. We now have a health check that we want to enforce and obviously that's on the level of the container so we'll need to replace the container. Now if I do a docker ps here and I'll filter on name=calc so it has to start with calc, there's the new container, there's the id for that, and now if I do my docker inspect again, paste in that new id this time, and run that, whoa! We get a lot more information back here. We have a whole health object here to tell us about the health of the container. All the way through here. We have the status as healthy right now. There's a failing streak which tells you the number of times that the container has had a failed health check in a row. It's 0 right now because it's healthy. And then there's a little log of the output and time codes of the last health checks. So you can see when the health check started, when it ended, what the exit code was, and then the output of everything is okay and that's the same for all the health checks and it'll keep a history here of about 5 items. So that's why it's important to monitor down to the level of an individual container when it comes to health checking. I haven't seen much of this information propagated up the stack in terms of tasks and services in Swarm mode. I'd hope at some point we could get some of this information up to the level of a task. If I come down and do my ps again, on the stack, docker inspect and if I put in the last task id. If you scroll through here you're not going to see anything with regards to the health of the container. I would like to see some of this come up to the status at some point. I don't know if that's in the works or not, but for now, go down to the level of a container if you need this information.

Monitoring When Testing Health Checks
Okay, so we have our service deployed now with our own health check added in when we defined our service and I've shown you a few ways that you can take a look at the status of an individual container using docker ps and docker inspect on that container id to be able to ascertain the health status of the container. Now as we work through some demos here, I find it helpful to have a monitoring script up and running that has several watches going on, on the information that we just saw in the last clip and so I've built that monitoring script here. If I you want to use it, you just pass a stack to it, in this case our calc stack, and then in the upper left panel we'll have a stack ps running all the time on our calc stack. So we'll see the tasks involved in this stack. And then in this middle panel here on the left-hand side, we have a docker ps on the manager node and then I'm using a special format here just to truncate down to 4 columns that I care about, the container id, the name, the created time, and the status. And by the way, if you're wondering about this 13 hours, I'm picking up a demo from yesterday. That's why the amount of time here has changed so much since the last clip. And then maybe the most important part here and our docker ps output and the reason why I'm looking at this as well, you can see the word healthy here. So this container is labeled as healthy. So our health check is passing. And of course we saw that when we took at look at the inspect output, which is what I have over on the right-hand side here. I have a docker inspect running on this calc container and it actually would be running on all calc containers if there were multiple of them, but there's only room for about one here on the screen, so just keep that in mind. You can see here is that state object off of the docker inspect on a given container and we have our health object most importantly with our log, our status, and our failing streak. So we'll watch these values as we play around with our application. We'll see how our application responds to a corrupted state. We'll see some of the health output here on the log and then we'll eventually see what happens with Docker Swarm Mode when it detects an unhealthy service. And then in the lower panel here I can issue commands.

Monitoring Service Auto Recovery from Health Check Failure
So keep in mind our health check is running every 5 seconds right now and that's why you can see the flash in the log over here on the right-hand side for the container as this is being updated about every 5 seconds. If I come over to the website, you can see that everything is okay and then if I come over to the tab here where I can issue a division request and I can issue an invalid division request, I want you to watch the right-hand side here. Also watch this word healthy here in the container output. I'm going to refresh this now to corrupt the application. This is a passed request here right now. Let me refresh this to issue the request and we've now corrupted the application. Now let's go watch the logs here. So watch over here on the right-hand side. Did you see our very first failure come through? The requested URL returned an error, 500 internal server error and we have a second one now and you can see our failing streak is 2. And boom! Oh, everything's gone. It's just that fast. So on the third failure, our container was labeled as unhealthy and that's all it took for Docker Swarm Mode to kick in and get rid of the task and create a new task. So you can see the task was completed 25 seconds ago and a new task has been up and running to replace it. So just like that, our application recovers and now you can see a nice, healthy log here with everything as okay, with this new container that was started up. And now this is why I like this monitoring script, because I would've had to change the docker ps filter here maybe; also I would have had to definitely change the docker inspect to point at the new container. This monitoring is set up to automatically point at the new container when it's created. Let's do that one more time. So I'll split the screen here. First off I'll show you that everything's okay on the app. I could even make a valid request here, for example 5 divided by 9, and then I'll corrupt it with 0. Boom! We've already got two failures here. And the container has been shut down. A new task is created and that new task has a container up and running already and watch the health here. There is nothing but starting right now. So when a container first starts up, the health status will be starting and then it will be healthy after it passes the first health check and now you can see we've had two successful health checks here. Three successful health checks now and that will just continue to go. If at any point in time you want to force an update just to learn about that health status here, come down to the lower left panel and do a docker service update and force an update at the calc service. When you do that, the existing task will be shut down, not from a health failure in this case. A new task will be created and you can see it's starting now as far as the container is concerned and it is now healthy. So now that we have this health check in place, if the application becomes corrupted, so I corrupted it again, after our retries times our interval, so 15 seconds in this case; after 15 seconds the application will automatically fix itself. That's pretty darn impressive. Take note here in the output of docker ps, you'll also see health: starting until the container is healthy.

Adjust the Health Check Interval When Learning
If things are happening too fast for you, you can go ahead and edit the service definition and I would just suggest coming in and changing the interval maybe to 15 seconds instead. This will slow things down quite a bit, and then do a deploy here. Once that's done then, you can see things are updating, our health check is starting here, starting. The health check is first run after that interval as well. So because we've set this to 15 seconds, our health check will wait 15 seconds before it first runs. And then of course, in the event of a failure, let's come over and trigger a failure here, and I'll speed up the video in between here, but it'll take about 15 seconds to even detect the first failure, which is quite a bit more than with 5 seconds and there you go, we have our very first failure. Now I'll speed this up. We have our second failure now after 30 seconds. We've got a failing streak of 2 now and then after about 45 seconds, our task was shut down and a new one was created. I am going to set the time back to 5 seconds. It's a bit more responsive here for demos, but just know that you can change that interval value.

Health Checks Prevent Traffic to a Container That Is Starting
Okay, I'm going to take a look at the impact on DNS of this health checking combined with Docker Swarm Mode. Remember we have service discovery based on DNS. So let's do this. Let's scale and first let's scale down to 0 and then let's scale up to 4. That way our task history matches here and then let me go ahead and start up my monitoring script again. Okay, we have four separate containers now. You can see they are all healthy in the middle here. And then over on the right-hand side we have too many containers to see the individual health status of all them so I'm actually going to stop that and what I'd like to do instead is go into one of these containers that we have and show you the DNS entries for our calc service. So I'm going to throw a watch on this, -d -n of 0. 5 seconds and then every half a second I'm going to run docker exec and I'm going to find a container id in a moment here and plug it into this spot. And I'm going to run the dig command. Now you tell me. If I want to query DNS for the calc service, what do I need to put in here to map back to an IP address and specifically what do I need to put in here to map back to all of the container IP addresses and not the virtual IP address because I want to see individual container IP addresses. Well, in that case I do tasks. and then the name of the service is calc here and now the only thing I need to do is go back and put in a container id right here. Now I could come copy one of these container id's from the left-hand side, but that could change. So instead I'll do a docker ps here -f name= and then calc, _calc. 4 so we're specific enough to only match one container and -q for quiet so I'll just get the identifier. So this will get me the identifier of the fourth slot, whatever container's running for that. And I'll use that to run the docker exec to execute the dig command. There you go. You can see we have our output of dig and right now we're getting four separate container IP addresses because we have four containers up and running. And if I come over here to the lower left window and if I were to scale again, maybe go up to six containers, those are not yet healthy so they don't yet show up in the output and now they're healthy so now they show up in the output here. So DNS is responsive based on the health status of our containers as well. So that's another added benefit. We won't route traffic to containers while they're starting up until the first health check is passed. Let's do that again so you can see that. Let's scale down to 0. Sorry, let's scale down to 4. That way we keep our fourth slot around hopefully. And then I'll scale back up to 6 this time. And just watch the DNS output over on the right-hand side. Also, watch the container health here in the middle. The container health will be starting initially. You can see starting right here and right here. So this IP addresses are not in the output of DNS over on the right-hand side and now they are. So this will take care of the problem of routing traffic to a website before that website is able to handle requests. Build your health check such that it indicates that your website is ready to handle traffic and you'll never have the problem of sending requests too early.

Health Checks Prevent Traffic to Unhealthy Containers
Now let's see what happens with health checking and DNS when we have an application that fails and needs to be restarted. So I'll come over and I'll corrupt one instance of our application and keep in mind that I'm pointing at the manager node right now with this web request so it could be any one of the six containers. Hopefully it doesn't kill off our fourth container that we're using for DNS, but we'll find out. If it does, we'll just try again. So I'll refresh here to corrupt one of the instances of our app and let's just watch here, watch the DNS on the right-hand side. As well as, okay, there we go. We lost one of our containers and the entry is gone from DNS. So just like that, a new container comes back up. It's starting right now and when it's done starting, then it will be added into DNS and there we go, we have our new entry. So as you can see, health checking is fully integrated with service discovery, not only when a container is starting up, but also when an application is corrupted and a task has shut down, the IP address for that task is cycled out of the list of DNS entries so that we stop sending traffic to that IP address. And of course then once a new container is started up, it'll be cycled into the list.

Health Checking with docker run
I think it's fair to say that we've covered quite a lot of ground in this module. I'm going to get rid of the stack for our calculation service so a docker stack and ls here. You can see we just have our visualization service. I want to do one last thing with regards to health checks. In terms of services as part of Swarm Mode, the second that Swarm Mode realizes that a given container is unhealthy, it shuts the task down and starts up a new one. So it's really hard to see the status information when we have a unhealthy container, but if we switch back away from Swarm Mode to the standalone version of Docker, which we can get to with a simple call to docker run, we can see a little bit more information about what happens when a container becomes unhealthy. So I'm going to run a container here and map similar ports. I'll do port 7000 again. I can do this because I removed that published port. Map that into 80 for the container and the container and the image is swarmgs/calc. I'll go ahead and fire that up. And then I'm going to start a new tab. I'll run docker ps in this new tab and you can see we have a traditional container running here based on our swarmgs/calc image. And I can grab the id for that, docker inspect on that. And if we scroll up here we have the same health checking information. At this point we have nothing because we have not defined any health checks. So I'll come back over to the original tab and kill off the container that I have running and I'll start up a new one and this time I'll add some of the health checking flags. So we have health- and then we have cmd for the command. I'll just go ahead and paste in the exact command that we are using inside of our compose file and I'll set the interval to 5 seconds and let me clear the screen out here and let's go ahead and run this then. And I'll come over to the new tab and we'll do our ps. You can see this container is starting right now, the health status is starting. It's wrapped around here and if I run ps again, you'll see it's now healthy because the first health check is passed. And if I do a watch every half a second, docker inspect, put in the new container id, I can watch for changes here and you can see the health information is right here. I think we are missing one log entry down at the bottom and that's okay. We'll watch this failing streak indicator here. So if I hop over to the browser and refresh here you can see that everything is okay and that's because I'm running this single container outside of Docker Swarm Mode right now and I'm publishing to port 7000 as well. So it looks like the same as what I had before, but don't conflate the two. I do have the same website up and running inside of a single standalone container. Now let me push this over to the right-hand side. Now on the left you can see the health status. On the right I have the website. Let me go ahead and just refresh here to corrupt this application that's running in a standalone container. Okay, we've already got one failing. The log message is showing down below, but you can see we have a failing streak of 1, now we have 2. Here's the status of healthy here. Now we're marked unhealthy. Now this is something you probably didn't see before. I think we saw it flash one time. It was really fast though and this is also something you didn't see and that's a failing streak of more than a value of 2 because once the failing streak hit a value of 3 and the container went unhealthy, Docker Swarm Mode kicked in and booted that container out the door, started up a new task with a new container. But now you can see here with the standalone container outside of a swarm that the health status just continues to accumulate. So I want to point this out because it's a great way to see what's going on behind the scenes; once the container becomes unhealthy you can now monitor the status of that unhealthy container and if I were to split the screen here and if I did a docker ps down here, you can see in this case it says unhealthy in the container status from docker ps. You might've seen that one time in the previous demos, it flashed up really quickly. So I guess the other added benefit of this demo and let me kill this off, is that we are now able to see that health checking works exactly the same when it comes to a standalone container running on a standalone Docker engine that's not part of the swarm.

Adding a Health Check to a Dockerfile
Way earlier in the course I had that cowsay image and application and I had one service that we deployed that didn't recover from failures and I had another that had health checking built in. The other one had health checking built in by virtue of having health checking baked into the image that it was using. So it was using a different image and here is the Docker file that was used to build that image. So I'm just using the health check instruction, specifying the interval of 5 seconds, and then specifying the command here. In this case I was executing the command that generates a fortune or a quote, and as long as that completes successfully, then the health check will succeed. Otherwise this second branch will execute to set the exit code to 1, indicating a failure. So this is just another approach to setting a health check where you can use any old program you'd like and in this case I'm using a program related to the actual application itself and that's something you want to think about is what indicates failure for my application and what's the best way to check that? You could use curl if it's a website, but you could also use a quote generator if that's what your application is doing. This is the actual quote generator used by that cowsay image when it generates quotes for the website. You could write your own executable or some script that could also determine the status of your application, so long as it's bundled up inside of your container it can be used for health checking. And the nice part of putting this inside of the container image, it's very descriptive. It's self-documenting for other people that come along and wonder how they can check the status of that application. Well, hey, it's written down right there inside of the Docker file.

Disable Health Checking
Now because we could have a health check baked into the Docker file itself, if you want to turn that health check off, when you do docker run there's a no-healthcheck. This is also available on Docker Service Create. And then inside of the compose file you can set disable to true to disable a health check. So three different ways that you can override when you're starting up a container to specify that you don't want to use the baked-in health checking. This brings us to a close on health checking. I hope you can see how integrating health checking helps Docker be able to better serve requests to your application and to reduce the likelihood of sending a request to an unhealthy container. Next up we'll take a look at how we can protect secrets.

Protecting Secrets
Environment Variables Can Leak Passwords
An important piece of deploying an application is making sure that sensitive information, for example, passwords or certificates or anything else that you wouldn't like to slip into the hands of somebody that shouldn't have it, making sure this information is protected. If I cat all the contents of a MySQL stack file that has a MySQL service using the official MySQL image. I might define the environment to set up a username of wordpress and a database of wordpress. So I might be setting up this MySQL instance to host a WordPress database and then I need to specify a password so I can connect into this container. So I can connect into this instance of MySQL and so I specify the root password here of root. And then I've also defined this stack so that it will be deployed to a manager node so I can easily get to the running container. So if I then deploy a stack based on that compose file, take a look at the containers running on my manager node here, you can see we have the MySQL container. So I could get inside of this docker exec, -it for inactive, paste in the id and do bash. That way I have access to the MySQL command line utility. I can specify the username of root and -p to specify the password here and type in root and I'm into the databases. Just like that I would get into that WordPress database then. Let me close out of all this though. So that works fine and dandy, but the problem is because we use an environment variable, we've potentially leaked some pretty sensitive information. So I've got a question for you. Knowing that we use an environment variable to specify our password, where could you go to see that password? Well, there would be several spots. The first one that pops into my mind is to run a docker inspect here, paste in that container id again. And if I look through the output here for root, look at that. We have the environment variable specified here and we can access this database now. We can sign in as the root user. This is obviously not a safe way to be setting the password for this MySQL database. And that's where secrets come into play. Secrets allow us to protect sensitive information and give us an alternative means as opposed to the traditional environment variable to be able to inject in some of these sensitive details for our applications. Let's take a look at that in this module.

Creating a Secret for a MySQL Password
So if we're not going to be storing our secrets and environment variables anymore, where could we put them? Well, it turns out that Docker has a management command called secret that has a series of sub-subcommands to help us manage secret information, to protect our secrets. For example, I could run ls here. You can see that we have no secrets at this time. I could clear this out and we could do docker secret and create to create a secret. This takes in either a file or a dash if you want to read from standard in. And then it takes the name of a secret. So we could do docker secret create and let's just say we're going to have a mysql_root_pass and then we need to get the value from somewhere. I'll use standard in here to echo out a value of supersecretpassword. I'll pipe that then to this command and then on the end I do the dash here to indicate that I'm reading from standard in. I get back an identifier here for the secret that was created. If I run docker secret ls here, you can see we now have a secret to find. Here's the id that we got back. Here's the name that we provided and when it was created and when it was last updated.

Granting a Service Access to a Secret
Now that we've defined a secret, let's see how we can use it and to do that, I'm going to copy the service definition that I had for the MySQL service that we just created. I'm going to copy that and call that with. secrets. I'll go ahead and edit that service definition inside of the stack compose file and then instead of an environment variable, I can add a secrets section and inside of here I can list out the secrets that I'd like to use as a part of this service and in this case it's mysql_root_pass. This is granting this service access to that secret and we'll see how the service accesses the secret in a moment. Once we've done that, let's save and close the file. So I can dump out the contents and you can see that we've added the secret in and commented out the root password. We can run our deploy again so I can come into my deploy command here and change over to the with. secrets. yml file and change the existing MySQL service that's a part of the existing MySQL stack. And we get back an error. Just like with everything else inside of a stack or compose file, the project or in this case the stack is prepended onto the beginning of whatever element it is. So we're actually looking for a key that doesn't exist and this is an important part of deploying a stack that has a secret dependency. The secret must already exist. Now in this case, because we're looking for something with mysql_mysql and we already have mysql_ on the front, I'll go ahead and just edit our file here and I'll change the secret over so that it's just root_pass. So if we wanted to add any other secrets to this particular service, we could prepend mysql_ as a part of the stack name onto the front of that secret and then we would just need the part after that when we list the secrets out within a given service. Save and close that now. And now if I do my deployment, you can see the service is updated. Next let's take a look at how we can access that secret now that we've granted access to it to our MySQL service.

Troubleshooting a Failing Service
So we've re-deployed our stack here and it looks like we're having some issues. We've got four tasks in a row here that have all shut down rather recently. Can you tell me what we should do here to figure out what's going wrong? Well, in this case I'd like to take a look at the logs so I'll use docker service logs mysql. In the future I wouldn't be surprised if we have some sort of docker stack logs, but we don't have that right now. So I'll go right to the service itself and it looks like we do need to specify something like MYSQL_ROOT_PASSWORD and we commented that out. One of our options is to allow an empty password so how about we do that for now until we understand how we can get that secret mapped in for our password. So I'll edit the stack file now and I'll allow an empty password. So I'll set that to yes and then I'll go ahead and save and close the file. I'll run the deploy again. Alright, the service has been updated. And a quick ps on the stack shows that we now have a task up and running. Alright, now that we've troubleshooted that issue with setting some sort of root password until we can map in our secret, now let's get into the container that's running here behind this particular task and see how we can access the secret.

Accessing Secrets in a Container via the Filesystem
Alright, so I'm sitting on my manager node right now and I finally have a task up and running for MySQL and this task is associated with a container here, a container that should have access to the secret that we defined earlier, but how exactly do we get access to it? Well, I want to get into this container so I'll grab the container id that corresponds to the task that's now up and running. So go int 415, which is the cid for the container and I'll open up a shell to that. Let me clear out the screen now. And then inside of the container I can change into the run folder. So / and then run and if I list out the contents you'll see that there's a secrets folder. I can change into that folder then and then inside of here, you'll find a file for our secret. So there's root_pass, which is a secret that we mapped in and if I cat out the contents of that, you can see our supersecretpassword. So the secrets are available via the file system instead of environment variables like we've been injecting them in the past. And this is a much more secure approach because this file system is unique to this given container so none of the other containers running on my system can access this file system unless of course they've also been granted access to this particular secret in which case they'll have a similar file in their file system. So our secrets are protected and only mapped in as we grant access to them via a service definition. So somehow we have to go from having the value in a file to passing that into MySQL or whatever entry point script we have for the MySQL image, needs access to this so that when it's setting up the MySQL database and server, it can set the root password appropriately. That's something that's going to be application specific.

Using a Secret to Provide a MySQL Root Password
So I'm going to exit out of the container here and get back to my master node and I'm going to edit that stack compose file. It just so happens that the mysql official image has already been updated to work with secrets. Unfortunately, if you look at the docs here, you're not going to see anything other than a sample password that has the word secret in it. In this case I had to do some digging, but I knew what to look for so I went into the latest image here, the image tagged as latest. Here is the Docker file for this. Most importantly, you can see that there is an entry point script. So this docker-entrypoint-sh. I'm over on a GitHub repository now for this. So what I typically do to find this then is go up a folder. There's the Docker file we were looking at. Here's that entry point script. If you're ever confused about where that is, let me go back. You can see the copy instruction to add this in. So if you're having trouble finding any of these entry point scripts, try and find the add or the copy instruction that brought that file into the file system and that'll help you find the location of it. Right here this is suggesting that this file is sitting right next to the Docker file, which is pretty typical. So I can go grab that file then and I can look inside of here, and if I look for the word secret, you can see that there are some notes here about using Docker's secrets feature. It's a bit terse, but you can see that there's this little _FILE on the end of a variable and what that happens to be is the environment variable that we've been using in the past. If we define one with _FILE instead, and point at a file, that will be used to get the value for a root password in this case, or for other configuration elements. So long story short, if I come over to the compose file and I add in a MYSQL_ROOT_PASSWORD_FILE environment variable, from here then I can point at /run/secrets and then root_pass in this case and then I can get rid of my allow empty password and the old root password. Now I have this environment variable pointing at the secret file, the secret file that will be there because we've defined this secret and granted access to it down below. Now if save and close all of this, and do my deploy again, let's run a ps on our stack. It looks like we're running, running for 4 seconds, 6 seconds, so I think things are okay now. If I want to double check, I could pull up the logs for this mysql service. And it looks like things are okay-ish. I'm not convinced from the logs alone though so let's do a docker ps here. The container has been up for about 32 seconds. Let me grab the container id. Docker exec interactive bash shell mysql user is root, password, and I'll type it in, supersecretpassword. And it looks like I either messed that up or forgot the value of it so I'll dump out the contents of the secret file just to check. Okay, supersecretpassword. Maybe I can copy that and just paste that in this time. And there we go. I'm in. So I must've fat-fingered that. Alright, and I can show my databases. And just like that, there's the WordPress database so off I'm going. Alright, I'll exit out of here and exit out of the container. Clear out the screen. So that's how we can wire a secret into our application, but just keep in mind this is going to be specific to each application. Let's talk about that next though.

Steps to Use Secrets
Let's recap the general steps involved in using a secret and then we can better understand how to take advantage of these. So first off we have to create the secret and we can do that with the Docker cli talking to a manager node. A secret could be anything. For example, it could be a password like we just used for our mysql database. It could also be an SSH key. Maybe it's a certificate for securing your website. It could be anything that you can store and up to 500 KB of data. Secrets are only applicable to Swarm services and that's because they're stored in the Raft log that's a part of all of the manager nodes. So if you look at the topology for a swarm, the manager nodes share this internal distributed state store and it's a state store where secrets are also stored, so alongside of the desired state for services and tasks and the state of the cluster, we're also storing our secrets here, which is a good thing because this storage is highly available and replicated among all manager nodes. And as of Docker 1. 13, the storage is encrypted as well. So your secrets are transmitted, encrypted, if you're using that TLS mutual authentication to your Docker engines. So when you communicate with the manager, if you're using TLS, then when you send the secret to the manager, it's encrypted. It's encrypted when it's stored and then it's also encrypted when it's transmitted to a given worker node when you have a container running for a service that has access to a secret. So your secrets are fully protected, both in transit and at rest. You can create a secret from either the standard input or from a file. And then another route you can go, you can define these inside of your compose stack file. So if you pull up the compose file version 3 reference and if you look over on the right-hand side here, there is a special secrets configuration reference. Much like we have a network section and a volume section, we now have a secrets section. So a Docker compose file for stacks can manage secrets as well. So your compose file could actually define where a secret is loaded from initially, from a file or it could specify that a secret is available externally just like we did in our example where we created the secret first. So you can define where the secret comes from in your stack file as well and that's in addition to granting access. So don't conflate that with granting access, which is what we did. Use the secrets section to grant access like we did here by listing out the secrets that a given service, Redis in this case, should be able to access, and then there's also this top level secrets management section. So however you want to define your secrets, that's up to you. They do need to be defined though before you can assign access to a secret to a given service. So that's the second step. Then you need to grant access to a service and you can do that when you're creating the service with docker service create, use the secret flag. You can also add or remove secrets when you're updating a service from the command line, but I think the best route to go is to use the stack compose file and use the secrets section that we just saw to grant access. Once that's done then and you deploy your service with the new configuration to grant access to a secret, then your application needs to read that secret from the /run/secrets and then / the name of your secret. So that's going to be app specific. The nice thing though about this standardized approach to secrets with Docker is that all applications are going to get used to being able to read secrets from this /run/secrets folder. So alright now, it might be a little bit extra work to get your app to work. There might be some official images for example that don't get support reading secrets from the file system. In time, you're going to see many apps converge on and standardize on reading secrets from this location. And that'll be an alternative to using environment variables so you'll probably still have both approaches, but in the long haul you'll definitely be able to rely on this standard location for reading secret information. And the really neat thing, this file system is an in-memory file system so it's not persisted anywhere. It's mounted into a specific container so no other containers have access to it and once that container is gone, the file system is gone, the memory location is wiped out so the secrets aren't just floating around on a given server anymore. Now one thing you might be wondering if you've ever built containers somewhat manually, the Docker commit command will take a snapshot of the file system of a running container. If you do that, the secrets will not be included in that snapshot, so that's a special feature of Docker commit to make sure that we're not taking a snapshot of the secrets and accidentally leaking those out in a container image.

_FILE Image Secrets Convention
So one last thing to be cognizant of, if you are creating your own images and you need to take in sensitive information, you should consider following this convention now. So if you have for example WORDPRESS_DB_PASSWORD environment variable, follow the convention of adding an _FILE environment variable to mirror this so that people can use some sort of file system-based secrets approach and specifically then they can tap into this new secrets system built into Docker. And then conversely as the user of an image, you should start to check the docs or dig into the details of the images that you're using and see if they support this approach and move to this as opposed to using environment variables. And just to be safe, let's walk through another example of this. Let's click on the WordPress image here that's linked. Let's pick php5. 6 here. We'll just go in the Apache one. There's the Dockerfile. Here's the docker entry point so I'll just go right to the entry point. That's likely where I need to look and viola! Look at that. We've got _FILE right here. So just look for _FILE inside of your entry point or otherwise to see if a given image follows this convention. You might see some information getting updated into the docs for images, but I haven't seen that move along as quickly as these scripts have been updated. So just because the docs for a given image don't mention the _FILE environment variable doesn't mean it's not available.

Removing Secrets
Alright, now that we have a secret and by the way, do you remember how I can see what secrets I have? So now that we have this secret, we might want to clean it up. So if we're done with the secret we can go ahead and remove it. Now I've got a quick quiz for you. What do you think is going to happen when I hit the return key here? Well, let's run it and find out. We get an error back because this secret is in use. So obviously we need to not be using a secret if we're going to get rid of it. So in this case I could go ahead and stack rm my mysql stack and now I can remove my secret. I get back the id to confirm that, run the ls again, and the secret is gone. If I try to deploy my stack again, what do you think will happen? In this case we get back an error that the secret is not found and so we'll need to go ahead and set up this secret. So we have a nice workflow here to remind us if we have not yet created a secret, and this actually blocks the stack from being created so if I do a docker stack ls you'll see that there is no stack set up for mysql. One more thing before we wrap up. Notice that when I removed the secret I didn't get any sort of prompt. So just be careful when you're removing these if you don't intend to get rid of them.

A Convention for Updating a Secret
Right now you'll notice there is no update command and I don't know if one is coming or not. Instead what people typically do is to create secrets and version them. So I could echo out and maybe passv1 here and that's the value I'm echoing here, and I'll pipe that into docker secret create and then I'll use the dash to read from standard in and then I need to give this a name and we'll call this mysql_root_pass and then in this case I'll put a v1 on the end. The naming scheme is up to you. Typically what people do then is they'll create a v2 if they want to change this password so let's just go ahead and create that now so we have it available, even though likely this is something that will come in the future, you wouldn't have both of these at the same time. Now if I do an ls on my secrets, you can see we have two of them now. Now you might be wondering, how am I going to map this name to the name that I'm expecting inside of my service? Well, obviously I could edit my service definition here and then I could come down here to the secret and add _v1 and then come up to the _FILE and add v1 on the end here, but this could become problematic. I might forget to do this so how about we leave the file exactly as it is? And then I'm going to get rid of this name right here. I was using what's known as a short form for specifying a secret and there is a long form instead. In this format I start out with the source and then I specify the name of the secret exactly like I named it when I created it. So mysql_root_pass_v1 here. And then I add a target to specify what I want it named on the file system and in this case I want root_pass. If I don't specify the target then the source will be used for the target and I'll have to update the file location then and I don't want to do that. If I want, I can also specify the user or the group that owns the file. I can also set the permissions on the file, but I won't do any of that, I'll just specify a source and a target. I'll save that file then. And so essentially all I've done is I've aliased the full name of my secret and specifically version 1. I've aliased that as root_pass then. I can save and close the file. I can deploy my stack again and recreate this service. And ah yes! I always forget about that prefix. Let me come back in and come down to my secret and just have root_pass_v1. Okay, save that, clear out and do a docker stack deploy again. There we go. We created our service. I'll do a stack ps and it looks like our mysql container is up and running. We can do a docker ps here, grab the id, docker exec, interactive, paste in the id, and then bash here. I'm inside of the container. Let me clear this out. First off, ls run and then secrets. We see our root pass file. If I cat out the contents of that, you'll see it's pass_v1. And so if I wanted to log into mysql, log in as root here, pass_v1, and I'm in. Exit out, exit out. Now if I wanted to bump up the password, first off I already have the secret created, but if I didn't I could create it. I could create the v2 secret. If I wanted to change this then I could just edit my service definition and I just have to update one location. Set that to v2. Save and close. Do my deploy again. Let's do a docker ps here. We've got a new container up and running, d. Let's docker exec into that one, interactive. Clear out the screen. Let's cat out the contents of /run/secrets/root_pass. There we go. It's pass_v2 now so mysql -uroot -p for password, pass v2 and I'm in. So that's how you can perform some sort of update process with your secrets. Obviously we're talking about changing the file system when we change a secret so that has implications for our running containers. For example, if mysql as a server is already bootstrapped with a given password, well if we wanted to change the secret for that password then, we'd have to reboot the bootstrap the server anyway. So likely if an application needs this to initialize itself you're going to have to recreate that instance of that application. So restarting isn't such as a hassle and it's nice if you follow this scheme here of aliasing your secrets to a given value and then behind the scenes using versions if you want to actually bump up the number when you're creating the secrets and have multiple secrets for what is ultimately a single secret that you're using. Now one thing that's interesting, if I do a docker secret ls here, we have a created and an updated column so I'm wondering if some update functionality is coming in the future. I even saw some notes out in the issues out on GitHub that mentioned update functionality. I just haven't seen this actually come through so keep an eye out for that.

The End
We are now at the end of the course. I hope you enjoyed this. There are many more topics though that I couldn't cover in a getting started course and there are a lot of things being developed for Docker Swarm Mode as a part of the SwarmKit project and if you'd like to stay up to date, what I recommend is to keep your eye on the GitHub repository and specifically take a look at the issues for the SwarmKit repo. You might also watch the issues for the Docker repo as well, but at least watch the SwarmKit repo and there's a milestones section that seems to be somewhat well organized, at least thus far where you can click on a given release here and you can see some of the issues that are related to this release; in this case these are bug fixes, and wrap your mind around what's being developed. For example, here's 1. 13. 0. So you could peruse through here to see some of the things that are being worked on. There are also issues in general and you can browse through these and look for specific labels perhaps to find what it is that you're looking for. For example, there are features that are being worked on. One that I'm really looking forward to which is the ability to attach to a task or exec into a task and by that I mean attach to a container or exec into a container on any node in the cluster. It looks like these are probably going to be worked on perhaps for version 1. 14 right now, which 1. 14 will be obviously greater than the newer versioning scheme, 17. 03 is where we're at right now. So later on in 2017 we'll probably see these features come through so keep an eye out for stuff like this. If you'd like to know more of what I do, I have a blog where I write about many things and I specifically have a newsletter that you might want to sign up for. You can also reach out and contact me if you have any questions and don't forget you can also use the discussion board for this course. I'd appreciate your feedback and I'd also appreciate if you would rate the course.
