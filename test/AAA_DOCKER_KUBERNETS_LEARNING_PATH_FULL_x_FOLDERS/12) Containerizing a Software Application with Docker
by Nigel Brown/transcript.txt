With the advent of the microservices architecture pattern and enabling technologies such as containers, the way applications are being architected and delivered is changing. Packaging software applications into read-only template images in order to derive immutable application containers, is a key ingredient in this approach, and Docker is at the forefront. In this course, Containerizing a Software Application with Docker, you'll learn how to package a software application into a Docker image. First, you'll explore the nature of Docker images, and their relationship with containers. Next, you'll master how to create Docker images, including authoring a Docker image for a real software application using a Dockerfile. Finally, you'll discover how to name and share Docker images with a wider audience. By the end of this course, you'll have gained the required knowledge of techniques and best practice to get started with containerizing your own software applications.

Course Overview
Course Overview
Hi everyone, my name is Nigel Brown, and welcome to my course, Containerizing a Software Application with Docker. I'm an independent consultant and trainer, with more than a few years of getting my hands dirty. My current obsession is the evolving world of containers and software application delivery. Containers and the Docker platform in particular are helping to revolutionize the way we deliver software applications. Organizations large and small across the globe are adopting a microservices-based container-driven approach to software delivery. In this course using best practice, you're going to learn how to package a software application into a container template, the Docker image. Some of the major topics that we'll cover include the anatomy of a Docker image, and how it relates to image size, techniques for building Docker images, including multistage builds, authoring an image for a software application using the declarative Docker file, and naming and sharing Docker images for wider consumption by users or automation tools. We can't turn you into a ninja Docker author over night, but when you're done with the course, you'll have sufficient knowledge and understanding to get started with containerizing your own software applications with Docker. Ideally before beginning the course, you should be familiar with the basic concepts of container virtualization. If you are just starting on the journey from traditional application packaging to a more modern distributed approach, then join me to discover how to containerize a software application with Docker.

The Purpose of Docker Images
The Purpose of Docker Images
Hello there, my name is Nigel Brown, and we're about to start the first module of a course entitled Containerizing a Software Application with Docker. The module is called The Purpose of Docker Images, and we're going to highlight some of the reasons why containers are so popular in the DevOps community, as well as take a bird's eye view of the techniques available for creating the images that are required by containers in order for them to function. This will give us the grounding we need that will enable us to containerize a software application. Without any further ado, let's make a start. The need and desire for organizations to quickly adapt and change seems to become more urgent as time goes by. The ability to innovate, fulfill customer expectations, or simply react to market activity has become a business imperative in the current age. If a business can't or won't adapt, it will surely die. This imperative has nurtured movements and cultures such as Agile and DevOps, which in turn have led to the development of software tools often in the open-source domain, whose aim is to aid the expeditious delivery of better-quality software applications. One such tool is Docker, which is a platform for packaging and delivering software applications in lightweight, portable, self-sufficient containers. Docker supports the trend toward architecting software applications and small, independent microservices. It does this by allowing a microservice to be encapsulated inside an immutable container, along with all of the dependencies it requires to function. The encapsulated microservice can then be distributed as a container unchanged to any number of different target platforms. Adopting the microservices pattern affords many benefits including service upgrades that have little or no impact on other services, the ability to significantly scale services independently from other peer services, minimizing the effect of a failure to the service in question, technological agnosticism when developing each service, a shallow learning curve for new team members, and there are more benefits. Docker not only facilitates these benefits, but augments them too. Docker aids continuous integration and delivery workflows by allowing a microservices application to run unchanged anywhere a container runtime environment is installed. This removes the proverbial bottleneck associated with missing platform dependencies, as software traverses the CI/CD pipeline. It also enables the use of consistent, reproducible shared development environments. And due to the light nature of containers and microservices, software developers can better represent a target production environment as they develop and test. Again, due to the light resource consumption attributable to containers, Docker helps software developers to be more productive as they iterate between coding and testing. Finally, Docker allows software developers to experiment with different configurations without polluting their development environments. These are just some of the many benefits that Docker brings to software application delivery, and in this course we're going to explore the steps involved in containerizing a software application. A key ingredient used in this process is the Docker image, and as we start to explore how to encapsulate a software application inside a container, we need to understand conceptually what an image is. A Docker image is the foundational unit of a container, and every container that gets created is derived from an image. An image can be likened to a read-only template and can be the source for multiple containers, which can be started by any number of different users on entirely separate Docker-host platforms, hence Docker images have a one-to-many relationship with containers. So what exactly does an image provide for the container? Well it provides a container with many things, but the key elements are the following; filesystem, including any relevant application code, packages, libraries, or other dependencies that the container needs to execute correctly, metadata such as environment variables or ports to expose, which can influence how the container is run, and of course a command, which determines what process gets executed when the container is invoked. Given that images dictate the nature and behavior of containers, creating them is a key aspect in the process of containerizing applications. So how are Docker images created? Let's have a look. The key point to remember is there are two different techniques for creating Docker images, each with its own merits and purpose. The first involves running a container, executing commands within the container, and then committing any changes made to the container's filesystem to a brand new Docker image. The second involves building an image based on a sequence of instructions produced in a special file called a Dockerfile. The Dockerfile approach requires a base image to work from and may contain other artifacts such as application-specific files. Deciding which method to use depends on a number of factors. Committing the changes to a container's filesystem can be considered an ad hoc technique useful when experimenting with containers. It uses the Docker commit CLI command, it's relatively straightforward to implement, but often results in suboptimal Docker images. In contrast, Dockerfile instructions are executed using the Docker build CLI command, and when defined appropriately, lead to images optimized for running specific application services. Writing Docker files is a considered approach to image creation and is the recommended method for authoring Docker images. During the course, we're going to take a look at both approaches to understand the techniques involved, the potential pitfalls, and to discern best practice. Now that we know about the promise of containers, and that the image is a key ingredient in containerization, in the next module, we'll see how to create a Docker image by committing filesystem changes made to a running container.

Committing Containers to Images
Module Overview
Hi, and welcome back to our course entitled Containerizing a Software Application with Docker. My name's Nigel Brown. In the last module, we had a brief introduction to Docker containers and the techniques for creating Docker images. For anyone seeking to take advantage of the benefits of containerization, the Docker image is an essential ingredient for container workflows and defines how our software applications run inside containers. In this next module called Committing Containers to Images, we're going to start to explore how to create those images. More specifically, we'll look at the process involved in creating images by committing a container's filesystem. We'll also see how it's possible to build up an image for a software application by iterating over a series of steps. The decisions we make when we use this iterative process have a bearing on the structure of the image we create, and so we'll explore the anatomy of a Docker image, and how it relates to a container's filesystem. We'll go on to see how the layers of content associated with Docker images affect the ultimate size of the image we create. Finally, we'll look at how we can mitigate large-image sizes using this iterative process by taking steps to flatten Docker images. At the end of this module, you'll have enough knowledge to create rudimentary Docker images, and an understanding of the anatomy of images, which you'll need as we progressed authoring images for software applications.

Committing a Container to an Image
Before we can run a container on a Docker host, we need an image to derive it from. Images are stored in registries, and the default registry for Docker is the publically-available Docker Hub registry. The Docker Hub, which has a web-based user interface for finding images is the source for a number of officially curated images for public consumption, which are entirely free for us. The image that you choose to use, often referred to as the base image, should be appropriate for your purpose. For example, if your need is for an object caching system, then you could start with the official memcached D image from the Docker Hub. The chosen image needs to be stored locally on the Docker host on which you're working, which means it needs to be retrieved from the Docker Hub or other appropriate registry. This can be achieved by explicitly pulling the image in question with the Docker image pull command. This command checks whether the most up-to-date version of the image is stored locally on the Docker host, and if not, retrieves the image from the registry, and stores it on the Docker host for subsequent use. Images can also be retrieved implicitly when issuing a command to run a container. The simple sequence for creating our new image involves our chosen base image, which is used to derive a container in which commands are executed, followed by the creation of a new image based on the container's filesystem. We can then use our newly-created image as a new base image from, which we can derive a new container, which in turn will enable us to create yet another new image. Using this simple sequence, we can iterate through any number of cycles, gradually enhancing the resultant image as we go. Let's consider for the moment the choice of base image to use. As a briefly mentioned earlier, the choice of image to use as the base image is important. As an image is enhanced in this iterative fashion, all of the enhancements sit on top of the original base image. If at a later point in time the base image is deemed inappropriate, the whole sequence needs to be performed all over again, which would be a pain. Some factors that might affect choice include fitness for purpose, there's no point in using a Python-based image as the basis for an image that defines a Java application. Clearly a Java-based image is going to be of more worth. Image provenance, can you trust the content of the base image that you intend to use, does it come from a reputable source, and are the contents what they purport to be? And base image size, is the base image optimally built with the minimum number of components necessary to satisfy your purposes? In order to create the container from the base image, we have to use the Docker container run command, specifying the image that we want to use, and the command to execute within the container. In this example shown, we're using the official Alpine Linux image, which is a tiny Linux distribution weighing in at just 4 MB. The command that is running your Alpine-flavored container simply invokes the Alpine Linux Package Manager to install Python. The command provides a lot of output as it executes, but when it finishes executing, the container exits. Whilst it is finished executing, Docker retains the spin container on disk until we instruct Docker to remove it. We need to retrieve the ID of the container, as we'll need this for the next step when we commit the container to a new image. The Docker container ls command enables us to do this. For the moment, don't worry about the -- format config option with its very long string, this just helps us to reduce the number of fields returned by the Docker container ls command. What is important is the -l config option, which tells Docker to only list the last container that was created. The output shows us the ID of the container, the image from which it was derived, and the command that was executed inside the container. We know the base image is read only, as with all Docker images, and we know the container has made some changes to the filesystem, which was originally based on the image, but what exactly are those changes? Making use of the Docker container diff command, we can determine the delta between the image and the container's filesystem. It provides us with a list of all the files that are being deleted, added, or changed in some way in relation to the original base image. Once we are satisfied with the filesystem changes, we can move on to create our new image. To create the image, we must use the Docker container commit command, specifying the container ID and the name of the image we want to create. In our example, we've also added a commit message. Again, ignoring the -- filtered config option for de-cluttering the output, we can use the Docker image ls command to show that the new image is located in the local cache on the Docker host. And we've seen how easy it is to use the simple sequence of commands to generate a new Docker image, and we'll take a closer look at this in a short while. Before we do this, however, we need to look at how images are constructed. This informs all of our decision making when authoring images and is essential for building highly-optimized Docker images.

Understanding the Anatomy of an Image
There is nothing very mystical about a Docker image, it's simply a JSON object that serves to define the image. There are two key components to the image definition, which are to be found within the JSON. The first is the configuration object, which governs how derived containers will operate. For example, it specifies the ports that will be exposed by a derived container, the command to execute as the container's process, the environment variables inside the container, and so on. The second component is the root filesystem object, which as its name suggests, defines the filesystem a derived container will have when it is created. This object containers an ordered list of references, each of which points to set of files and directories that collectively are referred to as a layer. After an image is created or pulled from the registry, the associated layers are stored locally on the Docker host as independent directories and are identified by a 256-bit hash of the layer content. The hash is referred to as a digest. This digest enables Docker to check the integrity of the layer content by comparing the computed digest of the content with the published digest for that layer. If these are not identical, the content will have been altered in some way. In turn, the overall image is imbued with an ID, which is a 256-bit hash of the whole JSON object. As the object incorporates the layer digests, the image, by virtue of its ID, references specific content. In Docker, this is called content addressability. As we've already learned, the layers defined in an image have an order, and because they are associated with an image, they are read only. This enables the same content to be shared by different images, and any number of containers that are derived from those images. This makes Docker containers very efficient in terms of resource consumption and speed of operation. So, how is a container filesystem constructed from these layers, and more importantly, how can a container alter its filesystem is the content is read only? Some applications may not need to alter their environment, but some certainly will. When a container is created, the layers are assembled as a union with any conflicts and content resolved in the favor of the more upmost layer. An additional unique container-specific layer is added on top of the union, and the whole union is mounted as the container's filesystem. Importantly, the final layer is writable by the container. Because the image layers are read only, if the container needs to amend a file that comes from one of the imaged layers, it needs to make a private copy of the file by copying it up to its writable layer before it makes the amendment. This mechanism is called copy on write and is a key attribute associated with Docker containers. Similarly, if a container needs to remove a file that comes from an image layer, the copy on write mechanism obscures the file in the imaged layer as if it weren't there. Now that we know about the anatomy of Docker images, and how they are used to construct container filesystems, we can return to the process of creating new images by committing container filesystems. When we invoke a container from the base image we've selected, the command that gets executed makes changes to the filesystem, which occur in the writable layer of the container's filesystem. In executing the Docker container commit command against the container, a new layer is created based on the container's writable layer, and a new image is created with a new JSON object. The root filesystem object for the new image references all of the layers that are referenced by the base image, but also references the new layer created from the container.

Relationship Between Layers and Image Size
For a variety of reasons, the size of the images we create should be kept to a minimum. There are no Docker image policemen to watch over the creations we make, but the size of a Docker image will often be a factor in determining its ultimate utility. Creating minimal Docker images is considered good design practice. When we create things, we tend to do so with discipline as one of our guiding principles, so try to ensure that you don't include unnecessary or leave temporary artifacts in an image. Containers are designed to be light in nature, and as we now know, a Docker image provides its filesystem. A small efficient image means the container consumes less resources, which in turn makes for faster startup times. Images stored in registries and the distribution of images to and from those registries occupies a finite amount of time. Continuous integration and delivery workflows make use from registries, and therefore the size of an image has a direct bearing on the time required to execute such workflows. Let's look at an example to illustrate how layers can affect the size of an image. By iterating over a base image, we'll gradually build up the image layers to provide an image for the lighttpd web server. We'll build lighttpd from source. Our starting point is the latest official Docker image for the Debian Linux distribution, which has a single layer of size, 123 MB. We're going to need to install some dependencies in the image, but before we can do this, we need to update the package index files, so that the dependencies can be found. Running the command to update the index in a container derived from the base image, followed by committing the container to a new image, yields a new layer of 9. 89 MB. Now we can install the wget utility for retrieving the lighttpd source and the tools necessary for building it. Running the container and committing it creates a layer of 218 MB. The next two iterations retrieve the source and extract the archive, while the last three steps configure the build, perform the build, and install the resultant artifacts in the appropriate locations in the filesystem. We now have an image consisting of eight layers, from which we can derive the container to serve lighttpd. The image has trebled in size from the 123 MB base image to the completed 317 MB lighttpd image. It's clear there are a number of things that have been used to build the image, which are not required in the final image. For example, the files associated with the package index on no longer required, as are the wget utility, the build tools, the lighttpd source archive, and this extracted contents. Let's remove the wget utility and the build tools, which created a layer of 218 MB to see the affect on the size of the overall image.

Demonstrating the Effects of Copy-on-write
The lighttpd image we've created over a number of iterations is called funnily enough lighttpd and has a tag of 1. 0. We can see the image using the Docker image ls command, which shows the image as a size of 370 MB. Let's make one more iteration in order to remove the wget and build-essential packages, and all of their dependencies. This will take a few moments for the command to complete, but when it is complete, we can commit the container to an image, and we'll tag it as lighttpd 1. 1 in order to differentiate it from the previous version. First we can use the Docker container ls command to return the ID of the container. And then we can use this to commit the container to a new image. Note that we can use just a few of the characters associated with the container ID to uniquely identify it. Now we can list our new image using Docker image ls again, but of course we need to specify the new tag so that we list the new image. Well this is unusual. Having run a command to remove some packages from the filesystem, we might've expected our image size to decrease by the 218 MB we saw when those packages were added to the image originally. Instead, our image has increased in size by a small amount. So what's happened? If you think back to how Docker's copy on write mechanism works, all we have done is masked out the contents of the packages that exist in a layer lower down in the image. The contents still exist within the image, and therefore it still contributes to the overall image size. The small increase in size can be attributed to changes made to the files associated with the package index. If image size is important, then we have a problem if we want to control the size of our images. We need a way to optimize our images for size.

Flattening Images
The Docker CLI provides some commands that enable us to squash or flatten the container's filesystem into a single-layer image, Instead of the multilayer alternative we've already seen. The Docker container export command takes the container ID as input, and writes its filesystem content to an archive. You would normally write this to the standard out stream, but as this example shows, a file can be specified using the -o config option. The important point is that the archive only has the contents of the container's filesystem, and not the image content that is redundant. If we exported the filesystem of the container we used to build lighttpd 1. 1, where we tried to remove the wget and build-essential packages, then those packages, while still part of the image, are not part of the exported archive. A complimentary CLI command Docker image import allows us to take the archive of the filesystem and create an image from it. Docker creates a single-layer image from the archive. With regards to our lighttpd example, as the filesystem archive didn't contain the wget and build-essential packages, then they are not present in the image, and its size is 152 MB as we'd expect. Let's see this process in action.

Exporting and Importing Container Filesystems
The last container we created was derived from the lighttpd 1. 0 image, and the command executed in the container removed the wget and build-essential packages. It's easy to reference the ID of this container using the Docker container ls command, specifying -l to filter the output to just the last container created, and -q, which just returns the ID of the container. We can assign this to a shell variable for use with the Docker container export command. We can check that the archive has been created. Let's also check the archive to make sure there is no trace of the wget utility. So far, so good. Wget doesn't appear to be in the archive. Now we can go ahead and import the archive as the lighttpd 1. 1 image. We can use Docker image ls to ensure that the image has been created, and also that it is the size that we expect it to be. The image with the tag 1. 1 is 152 MB in size, exactly 218 MB smaller than the image with tag 1. 0. Whilst we achieved our goal of reducing the image size, we've paid a penalty for doing so, which may not yet be immediately apparent. Docker images have a history associated with them in the form of the various commands that have been used to create them. This history is important for enabling us to reliably recreate our images, and for the consumers of our images to understand and trust the content of the image. An image's history can be viewed with the Docker image history command, we can apply for both variants of our lighttpd image. Docker doesn't have a format option for the output of this command, so I've used a custom utility to truncate the output for readability purposes. First let's look at the image with the tag 1. 0. This gives us a great insight into the construction of the image. Now let's contrast it with the image that has the 1. 1 tag. The contrast is striking and emphasizes the difficulties associated with committing container filesystems to Docker images. On the one hand, we want small optimized images, but on the other, we want to ensure the comprehension, integrity, and maintainability of those images. Of course, we could devise all manner of schemes using external tools in order to circumvent these issues, but this adds a layer of complexity we could do without. Fortunately, Docker has this covered and provides an in-built mechanism for authoring and building images that promotes efficiency, transparency, and maintainability, which we'll cover in the next module of this course.

Building Docker Images
Module Overview
Hello, my name is Nigel Brown. Welcome back to the next module of our course, Containerizing a Software Application with Docker. In the previous module, we learned how to create Docker images by making changes to a container's filesystem before committing the container to a new Docker image. We also saw that this technique has some shortcomings, and that it's difficult to control the size of those images whilst retaining the image history for ease of maintenance and development. In this next module, we're moving on to discuss the authorship of images using Dockerfiles, and the Docker image build command. The module is called Building Docker Images. Let's take a look at what we're going to cover. Our first topic will be the image build process itself. We'll define the ingredients required for building images, and analyze what happens when a build is instigated. One of the inputs to a build is the build context, which can be provided from a variety of different sources. We'll cover those build context sources. The main ingredient of an image build is the Dockerfile, and we'll provide a gentle introduction to the Dockerfile before we cover this in more detail in future modules. To wrap up the module, we'll discuss and demonstrate the use of the build cache, which is an important tool in the process of building Docker images. When we get to the end of the module, we'll have a good understanding of the overall process involved in building images, which will provide a foundation for the upcoming modules.

Building Docker Images
In order to affect a Docker image build, a number of ingredients come into play. Firstly, a base image needs to be specified on which an authored image will be built. Usually this is an existing Docker image carefully selected for its suitability and fitness for purpose. It is possible to specify the base image to be scratch, which as the name suggests, informs the build process to commence the build without a base image. The base image is specified using an instruction within a file called a Dockerfile, along with a variable number of additional instructions, which are used to define the content and characteristics of the image. Along with the base image, the Dockerfile is a required ingredient of the build process. Optionally, additional artifacts can be provided for the image in the form of files. These artifacts can be anything that is required in a derived container in order for it to function correctly. Examples of artifacts common in use include application binaries, source code, configuration files, static data, and so on. Each of these ingredients are used in the build process to create the new image. The build is often invoked via the Docker commandline interface, where we can specify the name we wish to tag the image with and the location of the Dockerfile. Typically, an image build is invoked with the Docker image build CLI command. A number of config options are available for the Docker image build command, including a tag option for naming the image, but it is a requirement to specify a path to something called the build context. In the example shown here, the build context is located in the current working directory as specified by the dot. The build context is the location where the build will find the Docker file, which should be at the root of the build context unless otherwise specified, along with any of the artifacts required for the image. The contents of the build context are sent to the Docker daemon, so that it has the necessary ingredients required to perform the build. Once the Docker daemon has the build context, it then processes a number of steps, one for each instruction in the Dockerfile, the first being to reference the base image. After the first step, each subsequent step involves the creation of a temporary intermediate container based on the image from the previous step. For example, step 2 uses the image from step 1, which is the base image. The instruction in the Dockerfile, which corresponds to the step in question, is executed within the intermediate container. The instruction may simply add metadata, or it may involve executing a command or commands. When the container exits, it's then committed to a new image before the container is removed. For those of you who can remember the previous module, Committing Containers to Images, this will all sound very familiar. That's because the build process uses the same sequence associated with committing containers to images, but automates the otherwise manual multistep process with the use of the Dockerfile in conjunction with the Docker image build command. Often build steps will fail due to incorrect syntax or command failures associated with user error. Once, however, these have been ironed out, and the build successfully completes, the final image that is produced is tagged with our image name if we provided one. This is our end product from which we would want to create containers running our containerized application. Intermediate images are retained for the purposes of aiding subsequent builds, which you'll find out more about in a short while.

Providing a Build Context
We've already discussed the need to specify the location of the build context, which at a minimum contains the Dockerfile for the image build. Consider for a moment, however, that it is a Docker client that will initiate the build, but it will be a Docker daemon that will perform the build. In development environments, the clients and the daemon will probably be co-located on the same host, but it is entirely possible that they may well be running on different hosts separated by a network. The build context may be stored locally on the client, a developer's laptop for example. However, the build context may be located remotely, and Docker provides two possibilities for storing remote build context. The first possibility is to remotely store a Dockerfile or a complete build context in the form of a tar archive addressable via a URL, which the Docker client can retrieve using a HTTP Get call. The second possibility is to store the build context in a Git repository addressable with the URL. The URL can reference a Get branch of rag and optionally specifies subdirectory. In order to retrieve the build context, the client will clone the repository into a temporary directory on the client. Wherever the build context is initially located, once retrieved, the Docker client will create an archive of the content and send it to the Docker daemon in order for the build to be conducted. In a local build context scenario, if you are managing a number of image builds for a project or different projects, ideally these should be organized into projects or subprojects with a relevant Dockerfile located in the root of each project. Other image artifacts should then be organized within the project directories. It's possible that a side effect of development may result in unwanted artifacts existing within the build context. It's undesirable, especially if the client and daemon are separated by a network to send those artifacts to the daemon as part of the build context. Docker provides a special file called. dockerignore that specify files to exclude from the build. The file should contain text content for specifying what should be excluded from the archive sent to the Docker daemon. The text should conform to new line-separated patterns as defined by the golang programming languages file path. match function with some minor enhancements.

Introducing the Dockerfile
A Docker image build cannot take place without a Dockerfile. A Dockerfile is a text file and can have any name the author desires, but typically the name of the file that is used is simply Dockerfile. In fact, this is the default name for the file, which Docker expects to find at the root of the build context. An alternative name and path can be specified using the -- file config option, but the file must exist within the build context. The purpose of the Dockerfile is to provide a series of instructions for defining the image that we're building. The Docker image build command acts on each instruction in turn, which must be one of the finite set of Dockerfile instructions, each of which has its own set of expected parameters and syntax. Each Dockerfile instruction is passed and then executed inside a container derived from a specific Docker image. So what exactly does a Dockerfile instruction create? It can create filesystem content either by executing utilities and commands, or by adding artifacts that we provided in the build context. But it can also add metadata to the image, which can be either active or passive in nature. This is an example Dockerfile, which packages the Amazon AWS CLI. At this point, we're not going to dwell on its content, as this is the topic associated with the next module. The contents of the Dockerfile, however, shows us that there are a number of different instruction types, each with their own parameters and syntax.

Build Cache
Docker provides a build cache to optimize image builds. The purpose of the cache is to speed up image builds by making use of existing images. Let's see how this works. We've already seen that each Dockerfile instruction is executed in a container before the container is committed to a new image. Each build step then produces a new image. During a build, each of the newly-created images records a reference to its parent image. That is, the image that derived the container from which the current image was created from, this results in an implicitly defined chain of images, which Docker uses to implement its build cache. Let's demonstrate this pictorially. Assume for a moment that we initiate an image build based on a Dockerfile. The Dockerfile has an instruction which defines the base image, which we'll represent with an orange square. The next build step uses the base image to create a container, processes the next instruction within the container before creating a new intermediate image. The new image has its parent image ID, that of the base image defined in its JSON object. The next build step is executed, and the new image produced also records its parent, and so on until we reach the end of the build. Each image has recorded its parent and the leaf image is tagged with the image name. Now let's assume we make a change that affects step four of our image. Perhaps we have altered the fourth instruction in the Dockerfile, or we have altered the content of an artifact that the fourth instruction adds to the image. When a subsequent build is invoked to account for the change, the build process passes the Dockerfile, and for each instruction, assesses the build cache to determine whether a previous sequence has already produced a corresponding image. If so, it reuses the associated image. If not, it executes the instruction in a container and creates a new image. Hence when performing this second build, the build will use the cache for the first three steps, but because the cache is broken at step 4, because of the change we made, it executes the instructions for step 4 and 5, and creates new images. Both versions of our image share the first 3 intermediate images, because the Dockerfile instructions used to create them are identical. As part of our development process, we now make a change to the third Dockerfile instruction. During the subsequent built, it's determined that the cache is broken at step 3, and therefore the images for steps 1 and 2 are reused, whilst new images are built from step 3 onwards. The process we undertake to develop an application and its associated Docker image will involve repeating build steps due to change in content. Inevitably, this will affect the duration of builds, but through the judicious sequencing of our Dockerfile instructions, we can minimize the effect this has on our builds courtesy of the build cache. We're going to use the Dockerfile for the Amazon AWS CLI we saw earlier to demonstrate the effect the build cache on image builds. In particular, we'll see how Dockerfile instruction sequence has a bearing on the effect of the cache.

Making Use of the Build Cache
Let's start by ensuring we have our Dockerfile in the current working directory, which will be our build context. The Dockerfile is where it should be, so let's go ahead and review the Dockerfile instructions we're using. On line 1, we have used the from instruction, which is how we specify the base image we want to build upon. We're using the latest version of the official Alpine Linux image. Next on line 3, we've specified the ARG instruction, with an argument of user. The ARG instruction allows us to inform the build process to expect a variable called user to be passed as an argument to the build command. This will allow us to specify at build time the user we want the container's process to run as. Then on lines 5 through 12, we are using the RUN instruction with a number of concatenated commands, which adds some dependencies for the image, and store the AWS CLI using Python's Package Manager before tidying up. Following this, we're using the run workdir and user instructions one after the other in order to create our specified user, change the working context of the user's home directory, and specify that the container's user be set to the value of the user variable. Finally, using the CMD and entry point instructions together, we specify what gets executed when a container is created from the image. With a broad understanding of the Dockerfile instructions we've employed, we'll commence a build, passing the user as a build argument with the value aws. We'll tag the image the name aws:1. 0, and we mustn't forget of course to specify the current working directory as our build context. As we've already learned, this is the first time that we've executed the build and Docker processes each of the instructions in turn, creating intermediate containers, alerting the image's content according to the instruction before committing the container to a new image. Once we get to the end of the build, the final image is tagged with a name that we supplied, awscli:1. 0. Let's list our new image to make sure it's being created. With the first version of the image built, we're going to return to the Dockerfile to make an amendment in order to cater for the situation where the user forgets to specify the user variable as a build argument. With the current version of our Docker file, the build will fail when the add user command is executed if the user variable doesn't have a valid value. To fix this, we can amend the ARG instruction to specify a default value in the absence of one being specified as a build argument. Having made the change to the Dockerfile, we can run another build without the build argument this time, and we'll tag our image with the name awscli:1. 1. Because we have made a change to one of the first instructions in our Dockerfile, we haven't made particularly good use of the build cache. The cache is broken early on, and as a result, the build recreates new images for each of the Dockerfile instructions and takes just as long as the first build. We could've had a much better result if we had thought more carefully about the positioning of our instructions. Let's remove the images and start over. For the purposes of this demonstration, we'll return the Dockerfile to its original content. We need to think carefully about the position of the ARG instruction in the Dockerfile. Instead of defining the ARG instruction as the second instruction, we'll more it to precisely where it is needed, which is just before the RUN instruction, which adds the user. Let's create our initial build again using the build argument as we did previously. We removed the images that we previously created, so there are no images available in the build cache for our Dockerfile, and the build creates an image for each of the instructions in the Dockerfile as we'd expect. At the end of the build, we will have a new image tagged with the name awscli:1. 0. Now we'll repeat the change we made by adding a default value for the user variable. Finally, we can build the revised image and tag it as awscli:1. 1. We'll also save the output so we can better examine it. This time notice how much quicker the build is. If we examine the build output, we can see how the build reuses the image associated with the RUN instruction that executes the concatenated commands, thereby saving substantial time for the build. This shows just how important it is to take great care when determining the sequence of Dockerfile instructions in order to maximize the value of the build cache.

Module Summary
In this module, we've seen how to conduct Docker image builds using a Dockerfile and additional ingredients. We've also been introduced to the build cache and seen how the proper sequencing of Dockerfile instructions can significantly aid build times. The module demonstrated the use of some of the Dockerfile instructions, but we haven't really got to grips with the meaning, purpose, and syntax of those instructions. That's where we're headed in the next module.

Authoring Docker Images with Dockerfiles
Module Overview
Hi. You're watching a course entitled Containerizing a Software Application with Docker, and my name is Nigel Brown. In the last module, we covered the process involved in building Docker images using a Dockerfile and the Docker CLI command, Docker image build. We caught a brief glimpse of some of the Dockerfile instructions, we go to define the nature of the Docker image that we're building. In this next module, which is called Authoring Docker Images with Dockerfiles, we're going to take a closer look at Dockerfile instructions, and what they can do to characterize our images, so that they are fit for our purpose. Let's have a look at what we're going to cover. Every image build needs a starting point, and we'll see how to specify the image we want to build our image upon. The use of environment variables can considerably enhance in flexibility for Docker image builds and container execution. We can make use of two Dockerfile instructions to give us this flexibility. Running commands and encapsulating the results on filesystem content is how we define filesystems for containers. We'll see how to make use of the RUN instruction for these purposes. In addition to creating content by running commands, we can also employ a technique for adding pre-prepared artifacts to our image. We'll here about the copy instruction, which helps us to achieve this. A container generally runs a single process, but how do we define what command to execute to invoke that process? We'll explore the two Dockerfile instructions that help with this, and how they can work in unison to provide greater flexibility for you, the image author. Sometimes things go wrong, and we need a means of monitoring the health of the process running inside our container. An image can be defined with a health check mechanism for derived containers, and we'll cover the Dockerfile instruction that provides this capability. It's sometimes advantageous to create images that cater for a general situation, which can then be used in more specific circumstances. Defining an image, which is based on Dockerfile instructions, whose execution can be deferred until they are required in specific circumstances is achieved using the ONBUILD instruction. We'll explore how and when to make use of this instruction. Finally, we'll mention some of the Dockerfile instructions that add metadata to our Docker image. These can be used to influence the operation of a derived container, or to simply add passive metadata. As we explore these Dockerfile instructions, as well as detailing the purpose and mechanism for each instruction, we'll also start to discuss some of the assumed best practice for their use.

Specifying a Base Image
The first instruction we'll need to consider is the FROM instruction, so let's have a look at some of its characteristics. The purpose of the FROM instruction is to specify the base image upon which the image being authored will be built. The filesystem content and metadata associated with the base image we choose to use will also appear in the authored image, unless of course it's altered as a result of executing subsequent Dockerfile instructions. Other than lines with whitespace and comments, which are deemed marked with a hash character at the beginning of a line, the very first instruction in a Dockerfile must always be a FROM instruction. Often an authored image will be built upon a base image that is closely related to the purpose of the authored image, for example, an image where a Java application might well be built on a base image, which provides the Java runtime environment; however, it is possible to use the keyword scratch to commence the build without a base image. Building applications with static libraries and copying the application binary into an image built from scratch can result in a tiny authored image, but with all it needs to serve the application. When specifying an image, the FROM instruction argument must contain the name of a valid Docker image repository, and they also include a repository tag appended after a colon. If no repository tag is provided, Docker assumes that the tag is latest. In the place of a tag, it's also possible to specify a digest. The digest we're referring to here is not the digest which is the image ID that we discussed earlier in the course, instead it's the digest of the image's manifest, which is stored in the registry where the image resides. It's the digest that the Docker CLI prints to screen at the end of a Docker image pull operation. It can also be retrieved in the repo digest JSON key when inspecting an image using Docker image inspect. Locally-built images don't have an image manifest and therefore cannot be referenced by digest. So you may be wondering what the purpose of referencing an image by its manifest digest is. Well an image with a tag may get superseded by another image which acquires the tag when it's build. Perhaps you only trust the original version of the image. If you reference the image by tag in the FROM instruction, you'll end up basing your build on the newer image instead of the one you trust. Using a digest to reference the image in the FROM instruction ensures you're using the very image that you desire as your base image.

Defining Environment Variables
When defining and consuming applications, we often make use of environment variables. In fact, in the more modern world of application development, following the principles of the 12-factor app methodology encourages us to store configuration in the environment. Docker provides the ENV instruction for such purposes. The ENV instruction allows us to define environment variables for consumption during the build process, as well as for the operating environment of a derived container. The variable must also be accompanied with a value. Multiple variables can be defined in a Dockerfile, and the scope of each individual variable is limited from the point of its declaration onward. Hence a Dockerfile instruction that is employed after an ENV instruction can make use of the variable defined by it. The variable and its value persist into a derive container. The purpose of the variable, therefore, can be related to the build itself, for the operation of the container or to both. Two different syntax forms exist, one for declaring a single variable, and for declaring multiple variables. Let's have a look at the differences between the two. If we have multiple variables to define in a Dockerfile, it's perfectly possible to use multiple ENV instructions to define them. Whilst it is a matter of personal taste, if multiple variables can be defined sequentially, then it seems to make more sense to define those variables with a single ENV instruction. On the left, we see a snippet from a Dockerfile with a definition of three variables using three different ENV instructions. On the right, we see the definition of the same three variables in a single ENV instruction. Notice the use of the equal sign, which is syntactically different from these shown on the left. The variables could've been written on the same line, but we've used the backslash character to break up the instruction definition over multiple lines in order to enhance readability. Using a single ENV instruction is more efficient, as it helps to minimize the number of layers to maintain in an image. If a variable defined in a Dockerfile changes often from build to build, it's not convenient to keep having to edit the Dockerfile each time a build is required. Similar to the ENV instruction, Docker provides the ARG instruction to cater for such scenarios. The ARG instruction defines a variable which the build process takes as a cue to expect a build argument to be defined as part of the Docker image build command. Build arguments are passed to the build with a --build ARG config option. We saw this in the previous module. Proper build arguments that are not defined with an ARG instruction are ignored with a warning message, whilst it's permissible to omit the definition of a variable via the build argument even though an ARG instruction for the variable is defined in the Dockerfile. To cater for scenarios where a missing build argument will cause a build failure, the ARG instruction can also define a default value. The value component is entirely optional, which is to converse to the ENV instruction. As with the ENV instruction, the variable can only be consumed from the point of its definition in the Dockerfile. Unlike the ENV instruction, however, a variable defined with an ARG instruction does not persist into a derived container. This is because the ARG instruction is principally designed to provide variables for the build itself and not for the operation of a derived container. Finally, from one build to the next, should the value of a variable defined by an ARG instruction change due to the build argument provided, the build cache will be broken at the point where the variable is first consumed, not where it is defined. A moment ago we said that variables defined with the ARG instruction don't persist to derived containers. In some circumstances, a variable required for both the build and for the operation of a derived container may need to be defined at the point of build. We can make use of the ARG and ENV instruction in concert to make a variable available for the build, as well as for the derived container to consume. In order to understand how this works, we need to acknowledge that the value of a variable defined with the ENV instruction will take precedence over a similarly named variable defined with the ARG instruction. In our example here, we've defined a variable called VERSION with an ARG instruction, and we expect a build argument to define the value of that variable at build time. Subsequently in the Dockerfile, the ENV instruction uses shell syntax to set the value of the variable called VERSION, setting it to 3. 5. 1 if the variable is not already set by the combination of the ARG instruction and the build argument. Either way, courtesy of the ENV instruction, the variable and its value persist into any derived containers.

Running Commands to Create Content
The RUN instruction is one of the Dockerfile instructions that enables us to add content to the image we're authoring. The RUN instruction takes shell commands or executables and their parameters as arguments, which must of course exist within the image that the build uses to derive a container for executing the command. The commands will more often than not alter the filesystem content, which will be encapsulated within the resultant intermediate image. A RUN instruction may install a utility, build an application, create a user, clone a Git repository, amongst a whole raft of other possibilities. There are two different forms of the RUN instruction, the shell form and the exec form. The shell form is the most common form used and simply involves specifying a command and its parameters as arguments to the RUN instruction. Each command is executed in a shell, which enables the RUN instruction to make use of shell constructs and capabilities such as environment variables and globbing. The exec form is generally used when the filesystem defined by the image doesn't contain a shell, rendering the use of the shell forming possible. The exec form is defined using JSON syntax, and accordingly, each component of the instruction argument must be enclosed in double quotes. Running commands which rely on accessing external content, such as updating and operating system package index or retrieving a remote file can have different outcomes from one execution of a build to the next. This is because the remote content may change between builds; however, the RUN instruction cannot account for this, and therefore the build cache will remain intact between the execution of different builds in such circumstances. The cache only breaks when the argument to the RUN instruction changes. Of course, it's possible to instruct the build to bypass the build cache via the commandline. We've already discussed how important it is to judiciously place instructions within Dockerfiles. Additionally, when a number of commands need to be executed to derive the necessary content for the image, care should be taken not to create an excessive number of layers, which will have an adverse effect on the image size and maintainability. Let's see how to mitigate excessive layers. Here we have three separate RUN instructions, each altering the filesystem content of the image. The first instruction is required in order to execute the second instruction, whilst the third RUN instruction attempts to undo the effects of the first RUN instruction. However, whilst a derived container's filesystem will reflect the effects of these commands, the size of the image will not. We saw earlier in the course how the individual layers contribute to the overall size of the image, and the three RUN instructions each create a layer in the image. As an alternative, we can achieve the same content outcome with a single RUN instruction. All three commands are executed inside a single intermediate contain, which produces our desired content in a single layer, thereby minimizing the size of the image we're authoring. Effectively, files are added, used, and then removed in one step. The concatenation of the commands is considered good practice and is achieved with a logical && shell operator. The backslashes are used to make the instruction more readable. Again, where you place the && operator and backslashes is a matter of personal preference, but the goal should always be to achieve maximum efficiency whilst maintaining readability and maintainability.

Adding Artifacts to Images
We've seen how the RUN instruction enables us to create or retrieve remote content for the Docker image we're authoring, but if the content is held locally within a build context, we need a means of copying that content into the image. That's where the COPY instruction comes into play. Before we look at the COPY instruction, let's just mention that an instruction called ADD also exists, which has very similar capabilities to the COPY instruction. In order to head off any confusion, the ADD instruction is older in terms of Docker's history, has been deemed to cause unnecessary confusion, and it's highly recommended to forgo its use. The COPY instruction should be used for adding local build content, and the RUN instruction for remote content. The COPY instruction simply copies artifacts from a source location within the build context to a destination within the filesystem provided by a Docker image. The source path must be within the build context, otherwise the build instruction will fail. If there are multiple artifacts that need copying to the same location, we can specify multiple source arguments within the same COPY instruction as required. They will, however, be copied to the same destination directory. So if multiple artifacts need to be copied to different locations, multiple copy instructions are required. The definition of a source can contain globbing characters, which need to conform to the rules of the Golang programming language's filepath. match function. The destination can be a relative or absolute path in the image, and the content is copied into the image with filing group ownership set to UID and GID 0. Let's take a look at some examples of the copy instruction in order to familiarize ourselves with the syntax. In the first example, we're copying a source file or directory called foo to an absolute destination in the image, which is the root directory, and renaming it bar. The use of a leading forward slash indicates the use of an absolute path, just like you'd find with a path name in any Unix-like operating system. In the second example, the destination has a trailing forward slash, which signifies that it's a directory. If the source is a file, a file with the same name will be located in a directory called bar within the image. If the source is a directory, it's copied to the image and renamed bar. The third example is similar to the first, except that the source has a path. The path is not recreated in the destination, and the final directory is copied to the image as bar in the root directory. In the fourth example, the source argument uses the wild card character, which means that any entities located in the path whose name starts with tmp are copied into the directory /bar in the image. With the absence of a leading forward slash, a final example uses a relative rather than absolute destination, which means the source is copied to a location relative to the working directory. Normally this will simply be relative to the root directory; however, it's possible to alter the working directory in a Dockerfile as many times as is desirable with the use of the WORKDIR instruction. After a WORKDIR instruction has been processed, relative destination arguments of subsequent copy instructions will be relative to the new working directory.

Forming the Command to Execute
Once we have all our content defined for a Docker image, we need to turn our attention to what a derived container will execute. There are two Dockerfile instructions for defining what a container executes as its process when it's started. Each instruction behaves slightly differently, but they can also work together in order to cater for an enhanced user experience. We'll take a look at both instructions, and demonstrate how they work independently, as well as together. The first of the two instructions is the CMD instruction. The CMD instruction is generally used to either define a default command for the container to execute, or to provide default parameters for the other execution instruction, the ENTRYPOINT instruction. The key thing to remember is that the CMD instruction provides default scenarios. First let's consider the default command situation. Typically you would use the CMD instruction if you want to specify a default command for your container to execute on container invocation, but also allow for other commands to be executed by the user if she wishes. If the purpose of the image is general in nature, and you want to give the user the ability to easily execute a different command to that specified in the CMD instruction, then the CMD instruction is the one to use. In the second use case, we don't specify a command at all, we simply define some arguments, which will be used as parameters by the ENTRYPOINT instruction if required. As with the RUN and COPY instructions, the CMD instruction can be defined using the shell form or the exec form, and the exec form is the one that's preferred. In fact, when the CMD instruction is used to provide default parameters to the ENTRYPOINT instruction, it must use the exec form in order to avoid unexpected behavior. Remember, the shell form will attempt to execute the argument in a shell, which makes no sense at all when it's intended for them to be parameters and not commands. Finally, if arguments are provided to the Docker container run command during container invocation, then these arguments override what is specified by the CMD instruction. This capability allows the default command or default parameters to be overridden on the commandline. We've already mentioned the other pertinent instruction, the ENTRYPOINT instruction. The ENTRYPOINT instruction is more opinionated than the CMD instruction, and it's used to define an executable for the container. The main difference between the ENTRYPOINT instruction and the CMD instruction is that it's designed to constrain the user to executing something specific. The user gets to provide parameters to what's defined in the ENTRYPOINT instruction by specifying commandline arguments to the Docker container RUN command, which are then appended to the ENTRYPOINT instruction. Again there are two syntax forms, the shell form and the exec form, and the exec form is the preferred syntax. The reason why the exec form is preferred is that as we've already discussed, the shell form attempts to execute what's provided in a shell. This would mean that the shell would be the main process inside the container with a process ID of 1, and our executable would be a char process of the shell. If we wanted to send signals to our process to instruct it to stop, or get it to reload a config file, for example, that signal would be sent to the shell and not our intended process. Use of the exec form bypasses the shell, and the content of the ENTRYPOINT instruction is executed as PID 1 in the container, which will be the recipient of any signal sent to the container. Let's demonstrate how these two instructions work. We'll use an image to serve a trivial example.

Making Use of the CMD and ENTRYPOINT Instructions
First of all, let's review a simple Dockerfile that installs two trivial programs, cowsay and screenfetch. Cowsay simply generates an ascii image of a cow with a speech bubble containing the content of the argument we provide on the commandline. Screenfetch displays an ascii logo of the Linux distribution being used, along with some other incidental information. Initially, we're using the CMD instruction in our Dockerfile to run cowsay with a quote. Let's build the image and tag it with the name demo. As it's a first build, no build steps are cached, and the build will take a few moments to complete as it executes each of the Dockerfile instructions in turn. We've successfully built the image, so now let's use the Docker container RUN command to execute a container based on our image called demo. Arguments for the container's process are defined after the image argument, demo in this case. On this occasion, however, we won't pass any. This means that the command that is executed within the container is what is specified by the CMD instruction, which is cowsay with the quote as its argument. We get a cow with our quote as the speech bubble. Because we have used the CMD instruction on its own, it provides a default command, which we can override using arguments provided to the Docker container run command. Let's do that and specify screenfetch as the argument. Notice that instead of cowsay, we have executed screenfetch, just as we'd expect knowing how the CMD instruction works. Let's return to our Dockerfile, comment out the CMD instruction, and add an ENTRYPOINT instruction defining cowsay as the command to execute. We'll rebuild the image, which will take no time at all as it will make use of the build cache created during the first build. Now let's execute a container based on the new image, again with no arguments. We get what we expect, which is an empty speech bubble, because we didn't specify anything for it. Let's run another container and provide some arguments. -f tux tells cowsay to print a penguin rather than a cow, and we've also provided a quote for the speech bubble. The arguments are appended to the command defined by the ENTRYPOINT instruction, which we specified as cowsay. Let's try and override the command like we did previously by specifying screenfetch as an argument. Because the image uses an ENTRYPOINT instruction rather than a CMD instruction, it can't be overridden, and the argument is simply appended to the command specified in the ENTRYPOINT. It's interpreted by cowsay as the content for the speech bubble. We'll make one final change to the Dockerfile to show how the CMD and ENTRYPOINT instructions work in tandem. We'll specify some default arguments for cowsay using the CMD instruction, and then rebuild the image. First of all, we can run a container with an argument that provides a quote for the speech bubble. We get the cow with our quote in the speech bubble. Now let's run a container with no arguments. This time we get the penguin with our original quote. Because there were no arguments specified, the default argument specified in the CMD instruction are appended to the ENTRYPOINT instruction. Whilst this demonstration has employed a trivial use case, it's easy to see how the CMD and ENTRYPOINT instructions can provide great flexibility when defining how applications are consumed by users of our Docker images.

Monitoring the Health of Containers
After a container has been started from an image and been running for a period of time, it may not always be obvious that the container's process remains functioning correctly. All may appear fine outwardly, but a container's process may have degraded or stopped working all together. As its name suggests, the HEALTHCHECK Dockerfile instruction provides a means of defining an appropriate check for the health of a container's process. The instruction defines a command that will be run internally within the container to determine the state of health of its main process. The exit status of the command translates into the process' state of health, 0 for healthy, 1 for unhealthy. Alternatively, if the base image used in a Dockerfile contains a health check, it can be neutralized by using the HEALTHCHECK instruction in combination with a NONE argument. The HEALTHCHECK isn't a one-time operation, the command runs periodically within the container in order to provide ongoing checks for process health. This period is specified with the instructions --interval option, which is set to 30 seconds by default. If the health check takes longer than 30 seconds to execute, then it will be deemed to a timeout, and the container is considered unhealthy. The default timeout duration can be altered with the --timeout option. Further, unless customized with a --retries option, the HEALTHCHECK command will execute and fail the health check three times in succession before the container is considered unhealthy. When a container is derived from an image with a HEALTHCHECK instruction, it acquires a health status, which can be one of starting, healthy, or unhealthy. This status can be viewed in the output of the Docker container ls command, as well as the Docker container inspect command for a specific container. So, if Docker determines our container is unhealthy, what happens next? That all depends where the container is running. If the container is running as a task on a Docker swarm cluster, for example, the scheduler will attempt to replace the task. If, however, the container is simply running atop a Docker engine, then Docker will raise a health status event in its event subsystem, which will need to be trapped for alerting purposes. We're going to emulate the failure of a container's process in order to demonstrate the use of the HEALTHCHECK instruction.

Defining and Using a HEALTHCHECK Instruction
Let's start with a Docker image that serves the Nginx web server on the container's main network interface, as well as on its loopback interface. We can see the definition of the HEALTHCHECK instruction in our Dockerfile using the cat command, and filter in the output based on the pattern HEALTHCHECK. We've left the timeout and retries parameters as their default values, 30 seconds and 3 respectively, but we've specified the interval between health checks as 3 seconds, rather than the default 30 seconds. The command that performs the health check is curl. It queries the localhost interface on port 80, and will fail silently if a successful response is not received from Nginx within 2 seconds. On success, the shell exits with status 0; otherwise, it exits with status 1, which notifies Docker of an error condition. We can run a container based on the Nginx image, and we'll map port 80 onto all of its interfaces, including the loopback interface. Additionally, we'll enhance the privileges associated with the container by adding the cap NET_ADM Linux kernel capability. This Linux kernel capability is not available in containers by default, in order to minimize the attack surface for would be miscreants. We need it however, to help us to crudely simulate a failure scenario for Nginx by taking down the container's loopback interface. Taking the loopback interface down will cause the container's health check to fail. Let's check the state of health of the container using the Docker container ls command. Notice the status of the container has its health appended to the normal status output we would expect. It's healthy, which means the HEALTHCHECK command, which is executed every 3 seconds, received a 200 status code from the container's process, along with the served content. So far so good, we have a healthy running container being monitored with a health check. Let's track the health of the container as we introduce and remedy a problem. First we'll use the Docker container exec command to run a command inside the running container. The shell that gets created will run sleep for 10 seconds to give us enough time to set up a command to watch the status of the container. After the 10 seconds, the loopback interface will be taken down, and another sleep command is executed. At the end of 15 seconds, the loopback interface will be brought back up. We'll execute this command in the background, so that we can monitor the container's health as we take the interface down, and then bring it up. In order to watch the status in real time, we'll run the Docker container ls command as an argument to the watch utility, which will update every second. The container starts off in a healthy state. Approximately 10 seconds later, the container will enter an unhealthy state. After the loopback interface is brought back up, it returns to health. When the loopback interface was taken down, the HEALTHCHECK command was executed 3 times with 3-second intervals, and failed each time after a timeout of 2 seconds. Docker will have recorded the state change approximately 10 seconds after the loopback interface was taken down. We can also use the Docker CLI to show us the health-related events generated as a result of the success and failure of the health checks. Initially, a health status event recording the container as healthy is generated shortly after the container is started. Another health status event is generated when the container becomes unhealthy, before a third event is recorded when the container finally returns to health.

Deferring Instruction Execution
Quite often, organizations or teams within organizations find a way of working that is efficient, dependable, and repeatable. In effect, method becomes policy, and team members are required to adopt the defined way of working for the benefit of the wider team. Docker images often encapsulate a method of working, for example, a Docker image might provide the tools and techniques for a reliably building software packages for a particular programming language or framework. Perhaps an image is authored in order to provide a container by building's a team numerous Java applications. How do team members make use of a such a generic image in their individual circumstances? One way is for each team member to copy the Dockerfile instructions from the generic image into their own specific image in order to build their application or service. That would work, but it also introduces a problem. What if the generic image is updated with an amended instruction? How do we know which team members have updated their specific image to account for the change, and which team members haven't? And whilst we're at it, which team members have altered the instructions of the generic image to suit their own purposes? Simply copying Dockerfile instructions from one image to another is not an elegant or robust solution. Another approach would be to require each team member to use the generic image as a base image for their application build. After all, this is how we build images, basing an image we're authoring upon another useful image. But this also leads to a problem. We want to specify the relevant instructions in the generic image, but some of those instructions cant' be executed, and so they are used in a specific context. For example, we can't define a copy instruction in the generic image, because the content is not available until the image is used as the base for a team member's application build. Consequently, the build of the generic image would fail. We need a way of defining our Dockerfile instructions in the generic image, but deferring the their execution until an image is built on top of the generic image when it's used as a base image. Fortunately, Docker has this covered with the ONBUILD instruction. The ONBUILD instruction is provided to impose method on the consumer of the image by allowing definition of Dockerfile instructions, but deferring their execution. An ONBUILD instruction takes a Dockerfile instruction as an argument and adds a trigger for the instruction to the image's metadata. The image is then used as a base image for any number of images that can make use of the stored triggers. A Docker build of an image using such a base image will process the deferred instructions immediately after the FROM instruction specifying the base image, and before any subsequent instructions native to the authored image's Dockerfile. Deferred execution can apply to any of the Dockerfile instructions, except the FROM instruction and the ONBUILD instruction itself.

Adding Metadata to Images
A number of other Dockerfile instructions exist whose purpose is to add metadata to the image we're authoring. We won't discuss these in detail here, but it's important to mention them in passing, as some of them are important in terms of influencing how a derived container operates. We'll see some of them in action later. The EXPOSE instruction specifies which of the container's ports are exposed for potential external communication. Generally this is how you make a container service consumable. The LABEL instruction enables a image author to add passive metadata to an image in the form of key value pairs. Labels are useful for annotating images, but can be used for a wide variety of purposes. By default, Docker sends the SIGTERM signal to a container's process when a user issues the Docker container STOP command. Some applications are configured to respond to a different signal, and the STOPSIGNAL instruction allows us to specify an alternative signal to send to the container's process instead of a SIGTERM. A container's process runs as UID 0, the root user, but the USER instruction allows us to specify an alternative user for the container's process. If our application or service or doesn't require root privileges, then some consideration should be given over to creating a non-privileged user within the image's filesystem, and then making use of the user instruction for the container's process. The VOLUME instruction defines a mount point within the container's filesystem, where an external persistent data source will be mounted. Docker volumes bypass the container's copy on write filesystem, and thereby allow data to persist after the life of an individual container. Finally, we've already seen the WORKDIR instruction, which allows us to change the current working context during the image build and for subsequent container operation. We've covered the majority of Dockerfile instructions, their purpose, syntax, and some associated best practice. Now that we're armed with the necessary knowledge to define Docker images for a purpose and the techniques for building those images, in the next module, we're going to go through the process of defining and building an image for a specific software application.

Authoring a Nginx Docker Image
Module Overview
Hello, I'm Nigel Brown, and you're watching a course entitled Containerizing a Software Application with Docker. Throughout all of the proceeding modules, we've learned a lot about the nature of Docker images and the mechanics associated with building them. In this next module, which is called Authoring an Nginx Docker Image, we're going to put that knowledge to use. We're going to build a software application from its source before we use the created binary as the image's executable process. Of course the planning and authoring of the image could apply equally well for any software application you might have in mind to containerize. Let's see what we're going to cover in this module. Making sure we get an image that's fit for purpose requires some forward thinking, so we'll need to plan the content of our image before we embark on authoring an appropriate Dockerfile. We'll spend some time piecing together the Dockerfile for our image, instruction by instruction, implementing best practice as we go. We'll also build and test the image once we finish defining the Dockerfile. After we've successfully built and tested the image, we'll assess whether we've achieved the best result we can. We'll move on to look at the concept of and benefits associated with multistage builds. Finally, we'll apply the multistage build approach to our scenario, show how we can improve on our first image build. Let's get started.

Planning the Image Content
The application that we will containerize is Nginx. Nginx is an open-source HTTP server and reverse proxy, it's written in the C programming language, and continues to grow in popularity. There are already numerous Docker images available that use Nginx for specific functions, so we could just go ahead and benefit from other people's hard work. Well our goal is to learn how to containerize a piece of software, and whilst this could be some proprietary code, Nginx will serve us well as an example use case. Nginx comes packaged in many Linux distributions, and we could simply build an image that installed the relevant package. Again, this is a sure cut and doesn't fit our purpose, so we'll build the application from its source code before configuring the image to run Nginx. When you come to containerize your own applications, you'll need to follow similar steps. So how do we go about planning the content of a Dockerfile for our Nginx image? Like any other development exercise, we need to plan ahead rather than just opening up an editor and hacking together an ad hoc image. Often an organization or team might define a process and/or a set of principles to follow when authoring Docker images. We don't have those restrictions here, but we'll use a common-sense approach instead. We'll try and break our endeavor into high-level tasks before we add some detail to each of the tasks that we've identified. We know that we need to get the Nginx source code and compile it amongst other things, which means we need to ensure that we have the relevant tools available for these purposes. In essence, we need to do some preparation work before we do anything else. Following this, we need to acquire the Nginx source code, which will be followed by building that source into an artifact for subsequent use. In previous modules of this course, we've discussed some of the Dockerfile instructions which add metadata to Docker images, some of which are akin to configuration and influence how derived containers function. We'll need to consider the configuration aspects of our image. Finally, we'll need to address how our image serves the Nginx binary that we have built. Let's try and break these tasks down a little to define more detailed steps. For the prepare task, we might want to consider introducing some flexibility into our image, for example, we may want the image to have the capability of building and serving different versions of Nginx. We also need to ensure a number of tools are available, which we will depend on for acquiring and building the Nginx artifacts. For the acquire task, one of the obvious requirements is that we download the Nginx source code from a URL. The download page at nginx. org has the relevant information, including the links to different Nginx versions and the Pretty Good Privacy, or PGP, signature used to sign the source code archive. It shouldn't be enough to just download the source and blindly make use of it, we should attempt to verify the contents of the archive we've downloaded in order to make sure that we've retrieved what we expected to retrieve. This will involve us downloading the signature as a prelude to making the verification. The final component of this task will be to unpack the source code ready for us to consume in the build of the Nginx binary. Moving on to the build task, the very first thing we need to do is configure the build. Nginx is very configurable, and this is just a small selection of the configuration options that are available. The configuration options are provided to the configure command, which creates a make file. The make file is then used to make the binary, and then install it at an appropriate location within the image's filesystem. With the build complete, we'll then clean up the debris associated with acquiring and building Nginx. We need to attend to a few configuration items before we're done. The first of these is to ensure that logging works in the most appropriate manner for a containerized application. We also want to provide some content to serve via Nginx. We're using a very simple configuration of Nginx, which serves a single-static HTML page, but nevertheless, we'll provide our own content instead of the default page provided as part of the Nginx installation. Our last step in the configure task will be to customize the image by way of settings some metadata for the image, which will be necessary for running derived containers. The final task, the serve task, is where we define in the image what gets executed when a container is derived from the image. At this point we've got a pretty good idea of what we need to do, so let's move on to creating our Dockerfile as a prelude to building an image from it.

Authoring a Dockerfile for the Image
The very first instruction needs to define the base image we want to build from. Our choice of base image can depend on a number of different factors, but in this scenario, our criteria will be size alone. Whilst we could literally start from scratch, it's wiser to make use of an existing, officially-curated image, and the official Alpine Linux image is ideal for our purposes, it's a mere 4 MB in size. As we learned in the previous module, we could pin our base image choice to a very specific image build, but we'll use the latest tag to specify the most up-to-date version of the image. This ensures we get the image with the most recent updates available. If we cast our minds back to our planning exercise, we decided that we wanted to provide some flexibility within our image in relation to code versions. We saw that Nginx has stable and mainline versions, and indeed, a number of legacy versions also. In order to provide us with this flexibility when it comes to building our image, we can make use of a build argument called version. This enables us to produce an image containing the Nginx version of our choice, and we'll use the variable in the URL we specified to retrieve the correct version of the source code. It's sensible to supply a default Nginx version for the build, just in case a build argument is not provided on the commandline. The build will fail if the variable is not set. The next part of our Dockerfile is where most of the work gets done in defining our image. It involves the use of the RUN instruction, and because we are aiming for a small image size and as few layers as possible, there are a number of discrete parts to the instruction, which span most of the tasks we identified in our planning. All of the commands are concatenated together into the single RUN instruction. As we're using the shell form of syntax for the RUN instruction, the first command is the shell set built in, which sets options for the shell, and we're using the -x config option. The -x config option instructs the shell to print each command and its arguments as they are executed. This is particularly useful when we're trying to debug a failed RUN instruction that contains multiple commands, as it helps to pinpoint where the error occurred. Remember, the logical && shell operator allows us to concatenate commands, whilst the backslash character allows us to specify line continuation, which helps us to improve readability of our Dockerfile. This syntax has been padded with whitespace in order to make the && operators and line continuation characters line up on each line. This is just my personal preference for formatting Dockerfiles. Continuing with the preparation task, we need to install the dependencies that would enable us to build the Nginx binary. To achieve this, it's necessary to use the Alpine Linux Package Manager to install some essential packages. The Package Manager is called APK, and not surprisingly add is the subcommand required to add packages. We've used the --no-cache config option, which forces APK to read the package index from the network, rather than retrieving and storing the index locally. This ensures the exclusion of unnecessary file content for optimization purposes. We've also specified that APK make a virtual package called. build-deps that comprises of the packages we subsequently defined. This enables us to perform operations on the virtual package as a whole, instead of on each individual package. The APK command aligns with the previously-defined set command. So, let's explain what's being installed. The build-base package provides the build tools such as compilers and the make utility, which are necessary for building on Nginx binary. The gnupg package provides gnupg privacy guard utilities, which is an implementation of PGP, whilst wget is the tool we will use to retrieve the Nginx source code. Two libraries, the Perl Compatible Regular Expression library, and the zlib compression library are required for the default configuration of Nginx that we will make use of. Whilst the packages defined are just arguments to the APK command, notice how they are on separate lines and are indented in relation to the APK command. They are also ordered alphabetically. These conventions help to make our Dockerfile readable, understandable, and maintainable. That marks the conclusion of the prepared task we outlined earlier. Let's move on to the acquire task, which involves retrieving, verifying, and then unpacking the Nginx source code from nginx. org. First up, we create and change into a temporary directory, in which we'll perform all of our built-related operations. This will help us to contain any filesystem pollution during the build and installation. In order to verify the digital-assigned Nginx source archive that we intend to download, we will make use of the detached signature that we will also download from Nginx. org, but we also need to check the integrity of the signature itself, and we need the signer's public key to do this. The public key can be retrieved from a key server, imported using the key's fingerprint, which is published on the Nginx. org website. The GPG command helps us to achieve this. Next we use two wget commands to retrieve an archive of the relevant version of the Nginx source code as defined by the version environment variable, along with a digital signature used to sign the archive. We now have all the elements necessary to perform verification. And once again, we make use of the GPG command to perform this operation. The last step in this block of commands uses the tar utility to expand the source archive into the current working directory. At this point, we have all that we need to build the Nginx binary from the source. In order to do this, we first need to change into the correct directory in the expanded archive, and configure the build by executing the configure command. We know that Nginx is extremely configurable, and we could make use of a plethora of configuration options, but as we are building a vanilla version of Nginx, we're only using two arguments. The first --with-ld-opt allows us to pass through an argument to the linker to specify that we want to produce a statically-linked binary. The other configuration option, --with-http_sub_module, allows us to modify the HTTP responses from Nginx by replacing one string with another. This allows us to define strings in the index. html file, which will be replaced by strings derived from some of the default variables built into Nginx. The configure command creates a make file, which is used by the make command with the install target to first build the Nginx binary, and then to deploy the built artifacts to default filesystem locations. The built binary is located in the directory /usr/local/nginx/sbin. The last step in the build process is to strip the Nginx binary of symbols that are not required for it to function, and which will reduce the size of our binary. The build is now complete, and we can clean up and remove any unnecessary files and directories produced for and during the build. Changing out of the temporary directory that we created for the build, we can go ahead and remove this directory, which removes all of the source files and the object files created during the build. We're also done with the packages we installed as dependencies for the build, so we can use the apk del command to remove the virtual package we created. This removes all of the individual packages that are encapsulated in the virtual package. That's the build task completed, and we can now move on to the configure task. The first subtask associated with the configure task is also the final component of our RUN instruction, and it's to change the location where Nginx logs information. Rather than access error logs being written to files located in the container's filesystem, we send the logs to the standard out and standard error streams instead. Docker's default logging mechanism captures data written to these streams and makes it available to view using the Docker container log CLI command. Without the symbolic links defined with the ln command, the logs would only be available from within the container, which is of limited use. That concludes our RUN instruction, which has enabled us to build an Nginx binary from source and install it within the image we're building. We're not quite finished with defining the image, however. We decided whilst we were planning that we want to be able to provide a custom static HTML page to serve. If we place this content in a file called index. html, ensure the file is located within the build context, we can use the COPY instruction to copy the file into the image. Similarly, in order for the string substitution we discussed earlier to work, we need to provide a custom configuration file with the appropriate Nginx directives defined in a file called Nginx. conf. Again, we can make use of the COPY instruction to copy the configuration file into the image. Let's continue with the configure task. By default, when a container is stopped via the CLI using Docker container stop, a SIGTERM signal is sent to the container's process. In order to stop Nginx gracefully however, it needs to receive a SIGQUIT signal instead. The stop signal instruction allows us to change the signal sent to Nginx when we issue the Docker container stop command. We wouldn't suffer much damage with our simple configuration if we emitted this Dockerfile instruction, but in more complex configurations, this could be vital to the integrity of data. Most Docker images are designed to provide a service, which is normally consumed through a TCP port. Nginx is no different, and it's designed to serve its content on port 80. Containers derived from our image will need to map port 80 onto an appropriate port on the Docker host. The EXPOSE instruction facilitates this. That concludes the configure task. Finally, we need to define what gets executed when a container is invoked from our image. We could configure this in a number of different ways using either the ENTRYPOINT instruction and/or the CMD instruction. In our image, we'll define an ENTRYPOINT instruction which references the Nginx binary, and we'll provide some default parameters in the CMD instruction. This enables us to specify alternative configuration options. For example, if we've forgotten what version of Nginx is embodied in our image, we could quickly run a container specifying -v as the argument, which will return the Nginx version. The main purpose of our image though is to serve content using our Nginx binary, and by default, Nginx will start as a long-running daemon and operate in the background, but this will cause our container to exit immediately. We need to specify the -g daemon off config options to instruct Nginx to remain in the foreground. That's it, we're set to go. With the Dockerfile complete, the next step is to build the image, and we'll take it with the repo name nginx. The build's going to take a short while to complete, so let's fast forward in time to the point that it's finished. The image build has completed, and we didn't get any errors. In the real world, of course, the initial work on the development of an image will inevitably take a few iterations before we get a successful build. Our image was designed and authored to be optimal in terms of size, so how big is the image? The Docker image ls command will help us with this. It's just shy of 6 MB, which is not bad considering we started with a 4 MB base image. Let's see if it works as we intended it to. First of all, we have to start a container derived from the image. We'll run it in detach mode, that is in the background, and we'll publish port 80 at the container to the same on the host. By default, Docker publishes ports to all host interfaces, so our Nginx servers should be consumable on the localhost interface. So let's try it. Success, our customized static HTML page is displayed in the browser and shows us the host name of the container, its IP address, along with a version of Nginx that we built.

Making Use of Multi-stage Image Builds
Having successfully authored and built our Nginx image, we might be tempted to think that we have achieved all that we can in terms of producing an optimal image fit for our purpose. But let's take a moment to consider whether this the case or not. Yes, we've produced an image of approximately 6 MB, which will certainly help us during image distribution and starting and stopped derived containers, but this carries an associated cost. Despite our best efforts to lay out our Dockerfile instructions in a nice, neat fashion, and to heavily comment the Dockerfile, we have ultimately lost some readability and maintainability in our quest to be minimalists. It's far more intuitive to have multiple RUN instructions for each planned task or subtask. In fact we may even lose some productivity as we iterate over our image, this is because we have had to concatenate a whole bunch of commands into a single RUN instruction. If just one of those commands fails, the next build iteration will result in the execution of all of the commands again, irrespective of whether they completed correctly on the previous iteration or not. This can lengthen the process of authoring images. What we need is a means for producing optimal images from a size perspective whilst maintaining our goal of producing readable and maintainable images. How can we achieve this? One solution is to split our image into two. The first image would handle the build element, creating the artifacts that are required for the service we want to provide. The second image would be the serving element, serving the service, making use of the artifacts produced by the build image. Within reason, we could be as extravagant as we like with the build image, as it's the other image, the service image that gets distributed and consumed. However, in resolving our dichotomy in this way, we're introducing a new problem. We now have two symbiotic images to maintain, which might require the use of external tooling if we want our process to be autonomous. Fortunately since version 17/05, Docker has this covered with multistage image builds. A Dockerfile which caters for multistages has multiple FROM instructions. Conceptually, we might consider that multiple different images get built, one for each FROM instruction, which will include all of the instructions up to the next FROM instruction. The image associated with the last FROM instruction is the one that gets tagged with the image name. The power of multistage builds, however, comes from the ability to copy artifacts from one stage into a subsequent stage. To do this, a copy instruction is employed, which needs to reference a source in a previous stage using the --from argument. A stage can be referenced with an index starting from the index 0 associated with the first stage, or by a name provided as part of the FROM instruction. In the example here, we've used the name build for the first stage, and referenced the stage with a --from argument of the COPY instruction. In effect, we're copying the directory /usr/local/nginx in the build image to the same location in the next stage. Let's see how we can apply this to the Nginx image we've been authoring in order to optimize for size whilst making the image more readable and maintainable.

Applying a Multi-stage Build for the Image
This is a revised Dockerfile, which takes account of a multistage build for our Nginx image. The Dockerfile is largely split in two, containing a build stage and a serve stage. Let's have a look at the build stage. The first change that we've made is to name our first stage build. We don't have to do this, we could simply refer to the stage as index 0, but it's a useful descriptive name for us to use. The next obvious change from our previous Dockerfile is that the large compound RUN instruction has been broken down into separate discrete RUN instructions, one for each of our subtasks. As we've already discussed, this helps us to make our Dockerfile more readable and maintainable. A side effect of this change, however, is that we can't easily make use of the temp environment variable we originally used to point to our build directory. This is somewhat irrelevant now anyhow, as we'll see in a moment. A temp variable defined in a RUN instruction is scoped to the shell associated with that RUN instruction, and will not be available for use in a subsequent RUN instruction. It worked well when we had the large compound RUN instruction, but won't work for our revised content. Instead, we'll perform the build in the /tmp directory. We've introduced a WORKDIR instruction, which is set to the root of the expanded archive. We won't be performing the cleanup subtasks that we performed originally, and as all of our build commands are executed in the same directory, we can use the WORKDIR instruction to change the context for all subsequent Dockerfile instructions. The remainder of the build stage contains a RUN instruction for each of the subtasks associated with building the Nginx binary and for configuring container logging. What's missing is the cleanup subtask. There is little point in performing the two commands that made up the cleanup subtasks. Both the packages we installed as dependencies and the debris from the build are already in tuned in previous layers courtesy of their associated RUN instructions. If we clean up these items at this point in the Dockerfile, it would have no effect on the size of the build image. Omitting these cleanup commands is of no consequence at all, as it's the serve image, that is stage 2, that we want to be of optimal size. That's the build stage catered for, so now let's move on to the serve stage. In our previous Nginx image, we used an Alpine Linux base image, which had a tiny footprint. For the serve image, we could be even more minimalist and use scratch as our starting point. We've statically linked our Nginx binary, so that doesn't appear to be a requirement for any other filesystem components. Whether this is correct or no remains to be seen, but we'll press ahead with the use of the scratch base image. The obvious addition to our Dockerfile using a multistage build is the COPY instruction, which copies the entire /usr/local/nginx directory from the build stage into the serve stage. Without this, of course, we wouldn't have the NGINX binary to serve. The remainder of the serve image is identical to that of our original image, so no further changes. The time has come to test our revised Dockerfile, so let's go ahead and build the image. Again, this will take awhile to complete, so we'll fade to the end of the build. The build has completed successfully, so first of all, let's see how big the image is. Because we have a multistage build, and because we based our second stage on scratch, our image weights in at just 1. 32 MB. Now let's see if it works. Whoops, something has gone ary. Let's check the status of the container, it looks like it exited as soon as it started. We can try and find out what went wrong by having a look at the logs associated with the container. Nginx handles client connections using worker processes, which by default run as the unprivileged user nobody. On startup, Nginx tries to read the record for the user nobody from the password database held in the /etc/password file, but of course that file doesn't exist in the container's filesystem, because we elected to base our image on scratch. Nginx will also try to retrieve the record for the nobody group from the group database held in the /etc group file. This highlights one of the potential pitfalls of using scratch as the base image. Software may have implicit hidden dependencies that may cause problems if an image is built from scratch, so care should be taken to ensure that those dependencies are understood and catered for. We can fix this problem relatively easily. We can copy the two missing files into our image from the build stage. We can rebuild our image, which will take no time at all, because most of the build steps are already cached from the previous build attempt. Now let's try and test our image again. First of all, we need to remove the previous unsuccessful container. Once we've issued the command to run the container, we can make sure it's running and hasn't exited as it did previously. This time the container has started successfully, and if we use a web browser to navigate to localhost on port 80, we get the content that we expect. So in conclusion, by making use of multistage Docker image builds, we've benefited from our image being more readable and maintainable, whilst at the same time delivering an image optimized for efficiency in terms of size. Multistage builds are a very powerful feature of the Docker platform.

Module Summary
In this module, we've applied much of the theory we encountered in previous modules. We've containerized a simple and straightforward configuration of the Nginx HTTP server by authoring a Docker image using a Dockerfile. Authoring of our image was proceeded by some perfunctory planning in order to ensure it was properly designed. We've applied some objective and subjective best practices in order to create a Dockerfile that is both readable and maintainable. We've also constructed the image so that it is as small as it's reasonably practicable. Our quest to balance the need to make our Dockerfile readable and maintainable, and at the same time to produce an optimal image in terms of size, led us to explore multistage Docker image builds. We learned how it's possible to compartmentalize our image into different stages, one to build the artifact, and one to serve the artifact. Whilst our software application Nginx was created with a trivial configuration, it's not difficult to see how this could easily be extended to cater for a more real-world scenario. Indeed, the steps we've taken could be similarly applied to any software application written in just about any programming language. Now that we know how to containerize a software application, in our next module, which is the last module of this course, we'll look at how we make our authored Docker images available to other potential users.

Tagging and Pushing Docker Images
Module Overview
Hi, I'm Nigel Brown, and welcome back to our Pluralsight course entitled Containerizing a Software Application with Docker. Up to this point in the course, we've been learning about Docker images, and how to encapsulate a software application within such an image. If we assume for a moment we've successfully built an image for building or serving our application, what do we do with it next? The answer to this question depends on our use case. We may want to publish our image for wider user consumption, or we may want it to be an artifact produced as part of a continuous integration workflow. Either way, we need a means of distributing our image for use beyond the realm of our local development environment. This next module, the last in our course, is called Tagging and Pushing Docker Images. Let's have a look at the agenda for the module, and we'll see what we'll cover. Sharing our images generally requires us to store them in a location where they can be accessed easily. Usually this means in a repository within a registry, and so we'll briefly discuss these concepts as a precursor to what follows. When it comes to distributing Docker images, the name of the image is all important. To the uninitiated, the names associated with Docker images can be confusing, so we'll take some time to understand the rules and conventions associated with Docker image names. Naming images is not necessarily a one-time task. In fact, a Docker image can exist without a name or even have multiple names. We'll see how to tag images with names. With an appropriate name applied, we can distribute Docker images to remote registry locations where they can subsequently be consumed by other people or automation tools. We'll see how to push an image from our local Docker host to the cloud-based Docker Hub registry. Let's start by taking a look at registries and repositories.

Introducing Docker Registries and Repositories
Within certain syntactic bounds, the name of a Docker image can be anything an author desires. It's common sense, however, to tag the image with a meaningful name that conveys some meaning to the consumer. Additionally, once successfully built, an image is likely to reside in a registry rather than on or in addition to a local development environment. The registry of choice will also have a significant bearing on the image name. One such registry, which is arguably the most popular public registry, is the Docker Hub registry. Other public registries also exist, and it's possible to self-host registries using either commercially-available software or the open-source Docker Registry software. The main component associated with registries is the image repository. An image repository is a bit like a folder or directory in a filesystem. Just as a filesystem directory contains a collection of hopefully related files, a registry repository contains a collection of related Docker images. In the example here, we have a Ubuntu repository, which is part of the Docker Hub registry and will be populated with Ubuntu Docker images. One such image is identified by an ID, which as you remember, is derived from its content. Additionally, however, the image has been assigned with some tags, trusty and 14. 04. This image provides a specific optimized version of the Ubuntu operating system. Trusty is the code name of the operating system release, whilst 14. 04 is the version number. Semantically they mean the same thing, and therefore it's helpful to be able to refer to the image with either of the tags. This is not the only image in the repository, however. Several other images exist with unique content, which represent other versions of the Ubuntu operating system. All have similar tags, but three images have an additional tag, which details the nature of the release. The Ubuntu repository is not the only repository located in the Docker Hub registry. For example, there is also a Cassandra repository for the open-source NoSQL database, it contains four different images with tags related to specific versions of the software. The Jenkins repository contains just two Docker images for the popular continuous integration platform. One appears to be based upon a more conventional Linux distribution, whilst the other is based on the minimal Alpine Linux distribution. Of course these repositories are a small example of the many, many thousands of repositories hosted by the Docker Hub registry. As an author of Docker images, when it comes to tagging your images with names, it's important to bear in mind which repository and registry your image will be stored in.

Understanding a Fully Qualified Docker Image Name
Throughout this course so far, whenever we've needed to tag an image with a name or address a remote image to pull, for example, we've used a relatively sure image name. A fully-qualified Docker image name, however, can be quite long and comprised of numerous components. Sometimes we need to name or address an image with more of these components. So let's take a look at the fully-qualified image name. A fully-qualified image name has four components, one specifying a registry host, one a namespace, another a repository, and specifying a tag or digest. In some circumstances, these components are optional, except in the case of the repository, which is mandatory to specify. Every image name must have the repository name explicitly defined. The square brackets here encase the optional components. Let's look at each of these components in more detail. The first component is the hostname of the registry where the image is to be pushed to or pulled from. Optionally, depending on how the registry host is configured, a port number can be provided if it's not listening on the default HTTP port. The registry host component can only be omitted if it's intended that the image's registry host is the Docker Hub. In other words, the Docker Hub is the default registry for Docker images. Some registry hostname examples are shown here, Docker. io is the hostname for the Docker Hub registry, and quay. io is the hostname for an alternative public registry. If a registry was self hosted, registry. mycorp. com might be a valid hostname, whilst localhost:5000 specifies a registry service running on the local machine with port 5000. The next component to discuss is the namespace component. The namespace allows for a subdivision of repositories based on a name. In a self-hosted registry, this name could be anything, a user, a team, department, organization, company, and so on. In the case of the Docker Hub, which also has an application which controls the creation of namespaces, the namespace is used for unique users and organizations. Some examples are shown here. When a registry hostname and namespace are both omitted from an image name, the namespace defaults to a special user in the Docker Hub. The user namespace is library, and it's the namespace used for all of the Docker Hub's official images. Hence, whenever an image is pulled by a repository name alone, an image is pulled from the set of official Docker images. The third component in a fully-qualified image name is the repository. We've already stated that it is a mandatory component, and it's designed to be applied to a similar set of images. This is where we should use a word to convey purpose or meaning. Typically a repository name might be the name of a software application or utility. The last optional component is the tag or digest. We've already covered the use of the digest in the previous module, so we'll focus on the tag component. The first thing to note is that the other image name components are separated using a forward slash, whereas the tag component is delimited with a colon. If a tag is omitted from an image name, Docker appends the tag latest to the image name before performing any operations on the image such as a push or a pull. Tags are intended to be descriptive in order to provide a consumer with relevant information, as well as to help distinguish one image from another in the same repository. As we've already seen, images can have multiple tags. Let's have a look at some image name examples. The first example image name is simply Ubuntu. Because a registry hostname is absent, the distribution operations, that is push and pull operations, performed on the image are done so in the context of the Docker Hub at docker. io. Similarly, a namespace has been omitted, which defaults to library, which in turn means we're referring to an official Docker image. The word Ubuntu refers to the repository name, and because we've omitted a tag, we're referring to the image tagged latest. The next example refers to exactly the same image as the previous example, but we have used the fully-qualified image name instead of the shorthand version. The third example refers to an alternative registry hosted by a company called CoreOS located at quay. io. It has a namespace called ukhomeofficedigital, the repository is called kafka, and it has a tag of version 0. 0. 2. The final example refers to an image, which will be pushed to or pulled from a registry running on the local Docker host at port 5000. It has no namespace defined, the repository is called redis, and its tag is latest. We know this because the tag has been omitted from the image name. Image names then are important, and need to accurately reflect a source or destination repository and registry.

Tagging Docker Images with Names
Earlier in the course when we were building images, we opted to tag our image with a name as part of the Docker image build command. But what happens when we need to provide an alternative image name, so that we can, for example, successfully push an image from a local Docker host to a remote registry? Using the knowledge we now have about image names, we can easily achieve this with the Docker image tag command. The command takes two arguments, a source image and a target image. The two variants of the command shown here both specify a source image and the target image. The source image must be a preexisting image located in the local cache of images on the Docker host. The target image is purely an alternative name that you'd like to give that preexisting image. After the tagging operation, there is still one image and one copy of its content, but there will be a new associated name that can be used to refer to that image. The source image can be referenced either by its image ID or by its name if it has one.

Pushing Docker Images to Registries
Assuming we have correctly named an image, we can now move on to the task of pushing that image to its intended registry. In order to achieve this, we need to make use of the Docker image push command. Using the name of the image we supply on the commandline, and using the various rules inherent in the image name, the Docker daemon determines where the image needs pushing to. In our example here, it's determined that the registry is the Docker Hub at Docker. io, the namespace is nbrown, the repository is nginx, and the tag is latest. Each of the image's root filesystem layers is pushed to the registry, and at the end of a successful push, the digest of the image's manifest truncated here is displayed in the terminal. Different registries employ different authorization schemes. For example, the Docker Hub enables repositories to be public or private, and provides a degree of fine-grained access permissions to private repositories. If a user is not authorized to push to a repository, the push operation will fail. The Docker Hub employs a token-based authentication scheme, and a user is required to have a Docker Hub account with the required access levels for the target repository, and must be logged in before a successful image push can be achieved. Let's see the image tagging and pushing operations in practice.

Tagging a Docker Image and Pushing to the Docker Hub Registry
In the previous module, we built an image to serve the Nginx HTTP server, which we have stored in our local image cache. We're going to push this image to the Docker Hub registry. At the moment, the image is tagged with the name Nginx:latest, so it just has an explicit repository and tag component. The registry hostname and namespace components are missing, so these will default to the library namespace on the Docker Hub. What will happen if we try to push this image to the Docker Hub? The Docker daemon prepared to push the image, having extrapolated the image name, but authentication fails, and the operation is terminated. The library namespace is a reserved namespace for official Docker images, and we don't have access to it. Instead, let's tag our image with an appropriate name. We'll tag the image with the namespace nbrown, which is my personal namespace on the Docker Hub. We can list the image using the new name we have provided. Notice that listing the image with the new name returns the same ID as when we listed the image with the name Nginx. Let's try and push the image again, hopefully this will be more successful. Whoops, the push failed again with exactly the same error message as before. This is because we are not logged in to the Docker Hub registry on our localhost, so let's go ahead and do that using the Docker login command with a -u config option, specifying nbrown as the argument. The login succeeded, so let's try and push the image again. This time each of the root filesystem layers is pushed to the Docker Hub, and we are provided with the image manifest digest. The push was successful. We can also navigate to my Docker Hub namespace using the web-based Docker Hub application, and there we'll find the newly-pushed image. This short demonstration shows us the importance of correctly naming our Docker images for distribution purposes.

Module Summary
In summary then, in this module, we've covered the semantics and rules associated with Docker image naming. We've learned how repositories and registries influence how we must name images, and what each of the components of a fully-qualified image name actually mean. Correct image naming is precursor to image distribution operations, and we've seen how to tag images with names using the Docker CLI, as well as how to push images to Docker registries. This marks the end of this module, and indeed the entire course. I hope you've enjoyed watching the course as much as I've enjoyed preparing and presenting it. The course has given you a good starting point for taking the first steps towards containerizing your own software applications, and there's plenty of other material here on Pluralsight to help you get an even better grounding in container-based technologies. I'd be delighted to receive any feedback or any questions you may have on the course, so please don't hesitate to use the discussion board for the course for this purpose. Until next time.
