Containers are one of the hottest topics in IT, and it’s hard to know where to start. In this course, Docker Deep Dive, you'll go from zero-to-Docker by learning everything you need to know to take your career to the next level and give you the confidence to start working with containers. First, you’ll explore the basics of what containers are and the foundational technologies that make them possible on Linux and Windows. Then, you’ll discover the core concepts of images and containers – how to build, manage, and work with them. Finally, you’ll dive into orchestration and some of the enterprise features that are now available. When you’re finished with this course, you'll be prepared for most of the topics presented on the Docker Certified Associate exam, and be ready to work with containers.

Course Overview
Course Overview
Containers are white-hot at the moment, especially Docker and Kubernetes. And in this course, Docker Deep Dive, is everything you need to get on the path to container mastery. On the Docker front, you will go from zero to Docker in a single course. I mean, by the end, you'll be brimming with confidence and raring to go. Now on the Kubernetes front, well, Docker and Kubernetes go together all the time, so you'll be primed and ready to crack on with Kubernetes if that's your plan. Anyway, I'm Nigel. I'm a Docker captain and a container addict. And I am really excited about the time we're about to spend together. You see, I'm confident you're about to give your career one of the biggest boosts it could ask for. Like I said, zero to Docker in a single course. So that'll be all the theory and the practical. On the theory front, we'll get down with the fundamentals: name spaces, control groups, union file systems, all the jazz that underpins a modern container. And we'll cover Linux and Windows. We'll also get deep with images, containers, swarms and services, secret stacks, the whole shebang. No stone left unturned. But theory's theory, so we'll be backing everything up with loads of hands-on labs and exercises. And by the end, you'll be skilled up and ready to take your career on to the next level. As well, while it's not an exam cram course, it will cover most of what you need to pass the Docker Certified Associate Exam. Man, I'm excited. So yeah, let's buckle up and let's do some serious Docker.

Course Intro
Course Introduction
Docker Deep Dive. I tell you what, if there was ever a course you should take or ever a course you should be excited about, this is it. I mean, the potential for this to change your life and maybe put you on a new trajectory, I am telling you, this is an exciting course. I mean, Docker is here right, and it's disrupting, but there's Kubernetes as well, and there is no better way to prepare for Kubernetes than by getting skilled up with Docker. So this is a win‑win. Anyway, I'm excited for you, so I have put, like, a crap ton of effort into making this great. It's going to be time well spent. Speaking of time actually, we're about to spend a bunch of time together, so I think some intros are called for. I'll go first. I'm Nigel. I'm a containerholic, and I'm a Docker Captain. So, you know what? From now on, if you'd call me captain, I'd appreciate that. It's just out of respect, yeah? I'm totally kidding. Don't you dare. Anyway, look, I'm a Brit, I love Docker and Kubernetes, and, oh yeah, I support a shocking football team, and I like loud American muscle cars. That's me. Now it's your turn. Reach out to me on Twitter, and tell me who you are. Okay, so a few things about the course. First up, as the name suggests, we are deep diving. Now, I'm totally cool for you to be here, even if you're a newb. But if that is you, just beware, right? I've got a couple of starter courses that you might prefer to take before you enjoy this one. Your call though. Just don't get sad if this is a bit too deep. Also, right, throughout the course, we're going to cover a bunch of stuff from the Docker Certified Associate exam. Now, is this a Docker Certified Associate study course? No, it's not. But are we going to cover a bunch of stuff that's included in the exam? Yeah, we are. So, I think long story short, the course is going to help you prepare for the exam, but in no way am I putting the course out there as an out‑and‑out exam prep course. I mean, there's probably going to be stuff on the exam that we don't even cover, and the aim of this course really is to be real world. But you know what? For most modules, I'm going to call out a list of the exam stuff that we cover. So, speaking of covering stuff, what are we going to cover? First up, we'll see some installs, but I promise you this will be quick, just enough to get you started, right? I have absolutely no interest of spending, like, I don't know, 35, 40 minutes walking through a bunch of installs. Check out my Getting Started with Docker course for that, or maybe check the docs. All right, once we're done with the installs though, we'll do an architecture masterclass. Proper solid theory, right? The kind of stuff that's going to make you feel like you're a maintainer. Then we'll get into images, how we build them and all of that stuff. Then we'll get down with containers. Now, at this point, if you're following along in order, by the time we've covered all of this off, you have got the fundamentals, so we'll start to switch gears and make things happen. We'll look at orchestration with Swarm, including using Swarm as a secure cluster substrate for deploying apps, even Kubernetes apps. Then we'll look at networking, volumes, and secrets, then containerized apps. After that, we'll finish up with some enterprise‑grade tools and a discussion on where to go next. Now then, that where to go next is going to be important because it is a big old world out there, and you're going to want some pointers. But that's the course. Now, if you looking at other courses as well, this is part of the Container Management learning path, so take the rest of the courses on there. But on its own right. I reckon the course represents the ultimate in standalone courses. But the thing is, right, there's always more, so check out the library, right? There's stuff that'll help prepare you for this course, and we've also got stuff that'll take you on to the next level. Now, I've also got a companion book. The content is similar. It's all Docker, right? But the different delivery mechanisms, I think they complement each other, so I recommend both. Your call though. But that's it. So, make yourself comfortable, and maybe grab a drink, and definitely get your notepad out. Oh, and you know what? If you're new to this, right, maybe give it a try at 1.4 or 1.5 speed. I hear really good things about that. Alright then, let's do this.

Installing Docker
Module Intro
Alright then, we're deep-diving here, so we're not about to spend half an hour finding out how to do 50 types of install, but, I do want the course to be as useful as possible to as many of you as possible, so to help make it a bit of a one-stop shop for those of you who want to go from zero to Docker in a single course, we will take a look at a few installs, but the emphasis is going to be on doing this quickly right? If you need more, there's always the Docs and of course Google's your friend. Anyway, this is what we'll do: Play with Docker, without doubt, my favorite way to get my hands on Docker, we'll do Docker for Mac and Docker for Windows, then we'll do a Windows Server install, and then a Linux install, then once we're done with that, we are cracking on with the proper learning, so let's do it.

Play with Docker (PWD)
Okay, Play with Docker. I cannot tell you how much I love this, in fact, I don't know what I'd do without it these days. So, you point your browser at play-with-docker. com, hit Login here, and then Start, and we're in. We then just start adding instances here, and you know what, you can add a bunch, yeah, okay, it looks a bit better with a decent resolution, but never mind right, this is it, it's a playground. And each one of these nodes is a fully fledged Docker node, so we've got three here, and they can talk to each other, they can be clustered, whatever. For real right, fire up your browser and just start adding instances, we're playing with Docker, I just don't know what there is about this that is not to like. Okay, a couple of points to note are the timer up here, right now you only get four hours of playtime, definitely keep that in mind, this is not for production right? And then there's the version here as well, you can't really control that, but you know what? If you need a quick playground to mess around with something, this is as good as it gets. So that's Play with Docker, I love it.

Docker for Mac and Windows
Okay, Docker for Mac and Docker for Windows, these are a couple of really slick developer-focused tools from Docker, Inc. The big idea being to get you a slick development environment on your Mac or your Windows 10 laptop. No joke right, installation is a breeze, and I'm not about to show you some kind of Next, Next, Next install, just go to docker. com, hit Get Docker up here, and then click either of these, then it's just a case of following the breadcrumbs till you get a download link, after that it really is Next, Next, Next, and you know what? Before you know it, you've got Docker on your laptop. So, I've got it here on my Windows machine that I'm doing this session on, and we can see here right, look, it's fully integrated into the Windows experience, same goes for Mac right, open a terminal and just whack in Docker commands. As well as that, you get this kind of little GUI experience, I mean look, we can see my repos here, any swarms that I've got in Docker Cloud, you name it right, the idea is bringing that Docker experience to your laptop so you can develop locally, but then integrate seamlessly to the wider ecosystem. In fact, the version on my Mac's got Kubernetes integrations as well, these will come to Windows soon. But a couple of things I do want you to note right? First up, they're both about a local development environment, this is not production-grade stuff right, all you get is a single instance of the Docker engine, no HA, nothing like that. Also, on Docker for Mac, you're getting Linux containers, 'cause everything's implemented in the background via a lightweight Linux VM, so that means all the integrations like the command line and stuff, are all talking to Docker, running inside of a Linux VM somewhere in the background. So don't be fooled right, just because it's Docker for Mac running on your Mac, you're still getting Linux containers. Windows is a bit different right, that can do native Win32 containers or it can do Linux containers, and it lets you flip between the two, though of course, your apps can't right, those are going to be either Linux or Windows, check the Docs though or maybe my Getting Started with Docker course for more detail. For us though, that's Docker for Mac and Docker for Windows, a smooth, local development experience.

Docker on Windows Server 2016
Alright then, Docker on Windows. What we're going to do here is show you a quick install on a Windows Server 2016 machine, and unlike what we just saw with Docker for Windows, this definitely can be production-grade. Anyway, I've got a brand new Windows Server 2016 machine here, and it's got the Windows Containers feature pre-installed, and that's important right, if you've not got Windows Containers installed, install it now, if you need the details, that's over in my Getting Started with Docker course. Anyway, guess what, this has already got Docker installed, yeah, you know what, I'm not a fan of that, I mean, I don't know, how old is that, it's like a dinosaur. So let's update it, and what we'll do here right, will work if you're doing a clean install or updating an existing version. And yeah, of course, there's other ways to do this, but what we're showing here will get you started. Anyway, as long as you've got the Windows Containers feature installed, these two commands here are all you're going to need. So let's have that first one, just give it a second, and then we'll have this one, and give that a second as well. And look, it's done right, so if we run this again, okay, client and server both updated, and the ee bit here, this little bit here, that means Enterprise Edition, so you're getting the full-on Enterprise Edition with Windows. But that's it right, that's Docker installed on Windows Server, easy.

Docker on Linux
Now then, Docker started on Linux, so it's only right that the Linux install should be the easiest. But, before I go any further, there are of course a million ways to install Docker on like a million different flavors of Linux, and I'm just showing you one, a scripted install on a clean installation of Ubuntu Linux. So, here's my Linux box right, Ubuntu 16. 04, I take this command here, and I run it, that's it. Basically, the command's running a script from docker. com and piping it through a shell. Let me speed it up a bit, okay, that is it, Docker installed, then I just grab this little command here, put my user onto the end, and we're ready to go, that's just added my user account to the local Unix Docker group, so I can run Docker commands without using sudo. But look right, we have got Docker, hey. Now, remember, this was just one way to install Docker on Ubuntu Linux, there's a zillion other ways on a million Linux's right, and for those, the Docs are your friend, really, installing is easy these days. And that's us done with installs, short and sweet, time to crack on with the good stuff, so coming up next, oh yeah, a master class on Architecture and Theory, get your pencils out.

Architecture and Theory
Module Intro
Mastering something like Docker demands a lot more than just knowing the commands. Real expertise demands that you know how things fit together under the hood. Well, with that in mind, the plan for this module is pretty much for me to waffle. It kind of is. So let me lay this down upfront. This is probably going to be pretty long module, and yeah, it's a bunch of theory, and yeah, I am probably going to waffle a bit more than normal, so, if that doesn't sound like your cup of tea, maybe you're happy not being an expert and are fine just getting by day to day, that's OK; just skip on over to the next module. But if you do want to know some of the internals, or if you're like me, and you need to know this stuff, then stick around. I mean, after all, this is a deep dive course, and I don't want to short-change you, and I reckon you'll love it. So here's the plan. Yeah, OK, just kidding, well, probably not, actually. But this actually is the plan. We'll be looking at two main things. First off, we're going to take a look at the stuff in the kernel that we used to build containers, and we're going to be doing Linux and Windows, by the way. Then, the other thing is the Docker engine, or Docker itself, the stuff that makes driving these kernel things nice and easy. Now, to do this, I'm thinking we'll start out with a bit of a big picture. So we'll talk about what a container actually is, which I think you'll find interesting, then we'll do a high-level view of the kernel internals, then a short description of the Docker engine itself. After that, we'll start drilling in. We'll do the kernel stuff first, because that's where the container building blocks live, then we'll do the Docker engine. OK, now, depending on how we get on the ones above, we might, or we might not, do this one. You see, the master plan is to cover Windows, as well as Linux, but if I get to this point, and I'm thinking, 'Eh, maybe we've been a bit heavy on Linux, and I don't know, a bit unfair to Windows, ' then we'll take a step to the side and we'll dedicate a few minutes entirely to Windows. But let's see how things go. We may or may not do this one. Either way, though, we'll finish things up with a recap, which I'm thinking is going to be important, right, because I feel like we're going to cover a lot. Now then, as part of this module, we're going to come over a couple of Docker Certified Associate Exam Objectives, and both in Domain 3, Installation and Configuration. We'll show how the engine can be operated, without effecting running containers, and we'll explain name spaces and C-groups, and this is going to be huge, okay? Okay, now a final word, before we crack on. If you're thinking that maybe you might skip this module, just don't. I mean, look, if I had things my way, right, I'd make it mandatory, and I'd ban you from Docker until you passed a test on it. I just feel that this kind of stuff is that important, because I'm not interested, even in the slightest, in you guys walking away from this course, which you've paid good money for, remember, but I don't want you walking away from it with a few commands under your belt, and thinking you can take on the world, only to come undone at the first hurdle. That's not what we're after. I want to prepare you like you've never been prepared before, so yeah, this module is about helping you really get it. And yeah, I'm probably going to waffle a bit. But we're getting under the hood, right, and we're revealing the secrets, so grab your usual coffee or whatever, and let's crack on.

Architecture Big Picture
So, the Big Picture. This module's really about two things. The low-level kernel stuff that we used to build containers, and the Docker Engine stuff that makes it all easy. So, starting out with the kernel stuff, all a container really is, is this ring fenced area of an operating system, with some limits on how much system resources it can use, and that's it. And that is a container definition, right there. Okay, obviously, we can have more than one, and I know it's a really simplified definition. In fact, it's my own personal definition, I suppose, because the thing is, there's actually no official definition for a container, and you know what, you can even build all different types of them, but the popular containers that we always talk about today, the ones made popular by Docker and, I suppose, LXC before that, well, these pretty much fit our definition here. Now, to build them, we'll leverage a bunch of low-level kernel stuff, in particular we use namespaces and control groups, and these fit into our diagrams just like this. However, right, I know this picture makes it look really simple, but these are low-level kernel construct, and out of the box, they are a pig to work with. In fact, so much so, that they've actually been around for ages, but they've remained obscure and esoteric, all because of how hard they are, and that is where Docker comes in to play. It's actually the Docker engine, that makes all of this really easy, and that's our two main topics. The kernel building blocks, and the Docker engine. Now, sticking with the big picture for another second, okay, the Docker engine is just like any other engine, and I don't just mean IT stuff, so even like a car or a motorbike engine, it is modular. So, although we might interface with it, through the CLI, directly to the API up here, a single end-point here, under the covers, it's actually a bunch of smaller, moving parts. Now, don't worry too much about the names, right, we'll get into that later. But the workflow is pretty much like this. We used the command line to create a new container, the client takes the command and makes the appropriate API request to the container's create endpoint here in the engine, right? Create new container, yeah? And the engine here pulls together all of the required kernel stuff and out pops a container! It is a beautiful thing. But this is all big-picture stuff. Time to turn things up a notch, and take a look at the kernel stuff that underpins it all.

Kernel Internals
Now, I know the container's started on Linux, and I know that Linux and Windows are very different beasts, but Docker's multi-platform these days, so I'm going to try and be as fair to Windows as I am to Linux. Well, here goes. Linux containers have been around for ages, or at least, the kernel stuff that we've build them with has, and I guess there's been a few people out there using them for ages, but for most of us, they were just too hard. You pretty much had to be a kernel hacker to get them working, and I suppose that's why it was really only the likes of Google and maybe a handful of other companies that were using them, the kind of companies, you know, that are only the bleeding edge with kernel engineers on staff, fair play, okay? They'd all seen the light while the rest of us were fumbling around in the dark and getting high on virtual machines. The point I want to make, though, is that the stuff to build containers is not new. It has been around in the Linux kernel for what seems like forever, definitely way before Docker was even a twinkle in Solomon's eye. Now, the Windows world was a bit different. Yeah, sure, Microsoft had a few internal projects, like Drawbridge and server silos, but those were dead in the water, and that's no disrespect to Microsoft, it's just a fact that the genesis of modern containers was all on Linux. Anyway, we used two main building blocks when we're building containers. Namespaces, and Control Groups. Both of them are Linux kernel primitives, and now, we've got equivalents on Windows. Hooray. Namespaces are about isolation, and Control Groups are about grouping objects and setting limits, so Namespaces first. These letters take an operating system, and carve it into multiple, isolated, virtual operating systems. It's a bit like hypervisors in virtual machines. So, in the hypervisor world, we take a single, physical machine, with all of its resources like CPU and RAM, and we carve out one or more virtual machines, and each one gets its own slice of virtual CPU, virtual memory, virtual networking, virtual storage, the whole shebang. Well, in the container world, we use Namespaces to take a single operating system with all of its resources, which tend to be high-level constructs like fire systems and process trees and users, and we carve all of that up into multiple virtual operating systems, called containers. Well, each container gets its own virtual or containerized root file system, it's own processed tree, it's own zero interface, it's own root user, the Full Monty. And just the way a virtual server in the hypervisor world looks, tastes, and smells like a regular, physical server in the container world, each container looks, smells, and feels exactly like a regular OS, only it's not. All three of these here are sharing a single kernel on the host, but everything's isolated, right? So that there's stuff inside of one container that doesn't even know about these others over here. It's ignorant! It's like, 'What? Other containers? What are you on about? There's only me here. ' Well, in the Linux world, we know we've got Namespaces, and in the Windows world, we've also got Namespaces. Now, I've no doubt that the implementation specifics are different, but I suppose, to keep the jargon to a minimum, Windows folks are kindly referring to their OS isolation stuff as Namespaces as well, thank you for that. Now, in the Linux world, we've got these Namespaces, and Docker container is basically an organized collection of them. So, this container here, is it's own isolated grouping of these Namespaces. It's got its own process ID table with PID one and everything, its own network namespace with an id zero interface, IP address, its own root file system, blah blah blah. Oh, and it's secure, so it's a secure boundary, and yeah, obviously we can create more, each one isolated and looking and feeling like a standalone OS. Well, you know what, real quick: the PID Namespace like what I think we just hinted at, right, gives each container its own isolated process tree, complete with its very own PID one. This means, right, that a process in one container, say this one here, is blissfully unaware of any over here. Can't see them, doesn't even know they exist. Alright, well, then that Namespace gives each container its own isolated network stack, so its our next IP's rooting tables, the lot. Mount gives a container its own isolated root file system, that'd be C: on Windows, obviously / on Linux. IPC lets processes in a single container access the same shared memory, but it stops everything from outside of the container, isolation remember? UTS gives everything its own host name, and the username space which is actually relatively new to Docker, but that lets you map accounts inside the container to different users on the host. The typical example is mapping the container's root user to a non-privileged user on the host. Okay, well, the concepts the same for Linux and Windows, right? Slice and dice the OS, and provide isolation. So that's Namespaces, and it's great and all, but like, any multitanen system, there's always the fear of noisy neighbors. I mean, the last thing that this container here, oh, I've taken the Namespaces off, just to make it a bit easier on the eyes, right, the last thing Container A wants here is D over here throwing all night parties and chewing through all the CPU and RAM. So, right, to realistically have containers, you know, something that you'd run in production, we need something to polish the consumption of system resources. In the Linux world, this is control groups, and if you call, you called it C-groups. In Windows, it's Job Objects. But, credit to the Microsoft folks. They seem to be playing nice and generally calling them Control Groups as well. After all, right, they do a similar thing, and who wants multiple names for everything? But call them what you want, right? The idea is to group processes, and then impose limits. Now, I'm sure you've already got it, but Control Groups that are, say, okay, Container A over here is only going to get this amount of CPU, this amount of memory, and this amount of disk IO, and then, Container B, this amount, and these as well, and with these two technologies, Namespace and Control Groups, we have got a realistic shot at workable containers in a union file system, or some way of combining a bunch of read-only file systems or blocked devices, lashing away to the layer on top, and presenting to the system as a unified view. Take these three, and we have got modern containers, and that's exactly what Docker did. It came along and it made all of this easy, and the rest is history. Now, there is more, okay. There's always more. I mean, modern Docker containers leveraging things like capabilities and setcomp and a bunch of other stuff to add security and the likes, but honestly, these three are at the very center, and everything else is like icing on the cake, and that's enough, I think, of the kernel stuff. Time to switch tack and take a look at the Docker Engine.

The Docker Engine
Okay, the Docker Engine. If we remember back to the Big Picture, the engine's right at the core of what Docker does, and it's what makes containers easy. But the top here, it exposes an API for us, down here, it interfaces with all the kernel magic, and out pop containers, brilliant. Now, before we dive into the detail, I want to be clear about something. These days, Docker is a full-on platform, so, at the core of everything, yeah, we've got the engine, and that's what we're focusing on, but plugging into the engine is a ton of stuff. I mean, from Docker Inc., we've got the native orchestration stuff in swarm, there's the on-prem secure registry, there's universal control plane with its ops-ui, R-back policies, loads of goodies, and there's a bunch more, right? Plus, there's the eco-system, so Docker is definitely a complete platform. But all we're focusing on is the engine. Now then, I want you to understand the story of the engine, so step into my DeLorean, and buckle up, because we're going back in time. Alright, way back when, when TVs were black and white, well, Docker came out of a company called dotCloud, and in the beginning, it was a pipe-and-till called dc, d for dot, and c for cloud. It was basically a wrapper for LXC and AUFS. AUFS is a union file system, and LXC, a bunch of tools for interfacing with the container primitives in the kernel. Looking familiar, yeah? And it was all Linux back then. Now, I know, Docker gets all the props for making containers popular, and rightly so, but it's only fair to say that it really started with LXC. Oh, I like that. So, props to LXC. Anyway, this relationship, with LXC, didn't last long. As soon as Docker got popular, things got complicated. One of the issues was just the sheer pace of things that were developing at. I mean, Docker was developing as a technology, and as an echo system, and at the same time, and not by coincidence, by the way, but LXC started cranking up development as well. So, of course, the inevitable happened: stuff started breaking, specifically changes in LXC would break Docker, which is no surprise, right? I mean, Docker, being reliant on an external tool like LXC, which is so insanely integral to the project and at the same time, so out of Docker's control, I mean, look, it was never going to be plain sailing. What Docker really needed was something that did the job of LXC, but was under their control. Enter libcontainer. So, libcontainer is pretty much a light for light replacement for LXC that interface to the container stuff to the kernel, but it's under Docker's control, and this was a key move. Anyway, right, by this time, the DC tool had become Docker, and things were developing like crazy, and before we knew it, Docker became a monolith. Now, we call this the daemon, by the way, but rather than being a lightweight and fast like it was supposed to be, it got bloated and slow. I mean, look at it. It's doing everything. It's implementing the HTTP server, and REST API, images, builds, registry stuff, networking storage, authentication, you name it, it was bloated, and it lost its mojo, and it's ironic, right? Because on the one hand, Docker's leading the charge towards micro-services, but on the other hand, Docker itself is a monolith. Like, what? And nobody was happy. For the Docker folks, it ran on a monolith just in there. For the ecosystem, well, they wanted to work with Docker, but really, just the run-time stuff. They didn't care about all this other stuff. Well, actually, sometimes they did. I'll give you an example. So, Kubernets was out there, and positioning itself as a container orchestrator, and it was using Docker as its run time. But obviously, pulling in Docker meant it was also getting all of this stuff, including Docker's own, built-in orchestrator. Uh-uh. So, Kubernetes, as an orchestrator, was shipping with Docker, which had a competing orchestrator already built in. Talk about mental! Safe to say, the ecosystem wasn't loving the bloat. It was killing usability, and composability, simplicity, security, you name it, but, but, then users weren't ecstatic, either. But everything's fixable, right? So Docker set about this massive project about picking it all, and refactoring the core plumbing stuff into separate tills. And, at about the same time as this, the Open Containers Initiative starts making its way up the stage, and we start getting some standards, specifically an image spec and a container runtime spec. So a bit of a perfect storm going on here, yeah? Docker re-factoring, and at the same time, we start to get standards. Fabulous. Well, fast forward to today, and we're looking like this. We have got the Docker client, where we slap in commands like Docker Container Run, we've got the Daemon implementing the rest API, Container D here is the container supervised that handles execution and lifecycle operations, things like start, stop, pause and unpause, and we've got the OCI layer, that does the interfacing with the kernel. Alright, on Linux, it works like this. The client asks the daemon for a new container. The daemon gets Container D to start and manage the containers, and runs C at the OCI layer, actually builds them. Run C, by the way, is the reference implementation of the OCI runtime spec, and it's the default runtime for a vanilla installation of Docker. Now, Docker on Linux has looked like this since 1. 11, way back in April 2016, but that's just Linux. Things are a bit different on Windows. We still got the client and the demon here at the top, but instead of Container D in the OCI layer, we've got something called the Compute Services layer. Now, OK, I suppose that does make it architecturally a bit different, but the user experience is the same. This kind of plumbing stuff here, at these kind of layers, it's interesting, but it's all super low-level. Day to day, we're not effected. I mean, the API up here in Windows is exactly the same, so no stress. Now, I want to throw Microsoft a bone here. They're not doing anything nefarious, or trying to be different. In fact, they're doing a cracking job in the community. It was just a timing thing. So at the same time Docker was doing all this refactoring into Container D and Run C, Microsoft was also getting ready to ship Server 2016. Now, in order for them to hit their ship dates, they just couldn't wait for Container D and the OCI stuff to bake, so we ended up with some low-level differences. Now, I'll give you my opinion here, which I probably shouldn't, but I don't think it'll be long before we're seeing Container D and an OCI runtime here on Windows, but that's just my opinion, right, I don't speak for Microsoft, or anybody else, for that matter. Anyway, this is the engine today. Now, let me run through the creation of the new container on Linux. Generally speaking, right, we use the Docker client to create containers. The command's docker container run, I'm giving it a bunch of parameters. Okay, well the client takes that command, and it posts it as an API request to the container's create endpoint in the daemon. But guess what? All this engine refactoring has left the daemon without any code to execute run containers. Seriously, Docker no longer even knows how to create containers, all that logic's ripped out and implemented into Container D in the OCI. So to create the container, the daemon calls out to Container D over a GRPC API on a local Unix socket, and even though Container D has got container in its name, even it can't actually create a container. What? Yeah, that's right. All the logic to interface with the Namespaces and stuff in the kernel is implemented by the OCI. So, for Docker on Linux, that defaults to Run C, though you can switch that out for pretty much any OCI compliant runtime. Alright, well, things work like this, OK? You're in that process starts the daemon. The daemon starts Container D, which is a daemon process, so a long-running process, and this is kind of the glue between the daemon here and Run C below, anyway, Container D starts a shim process for every container, and Run C creates the container, and then exits. So Run C gets called for every new container. But it doesn't stick around. The shim does, though, and it's the same for more. So C here how Container D effectively manages multiple Run Cs, or shims? And that's pretty much the process. And you know what? Architecturally, it's great. I mean, it's modular, and composable, and reusable, all the buzzwords that we love, as a matter of fact. Actually, in case that's all jargon to you, all we're saying with composable and reusable is that Container D and Run C here are potentially swapable, definitely Run C, you can swap that out for pretty much any OCI compliant runtime, and they're both reusable as well, so both easily reused by players in the eco-system. And like I was saying, it's all good. But you know what? All of this de-coupling of the container here, from the daemon, and even the Container D, it lets us do really cool stuff, like re-start the daemon in Container D, and not effect running containers. And if you do Docker in production, oh my goodness, you will know what a huge deal this is. I mean, Docker is cranking out new versions of the engine like nobody's business. And upgrading them in the past, like when an upgrade would kill all of you running containers, let's just say, it was a challenge. But now, we can re-start these here, leave all of our containers running, and when they come back up, they just re-discover running containers, and reconnect to the shim. It's a beautiful thing. Okay, so, a few last things. The daemon here still implements a bunch of stuff, but these days, it's mostly higher-level, value-end stuff, that core plumbing stuff is mostly broken out into Container D and the OCI layer. In fact, more's being broken out to Container D, even as I record. It's things like swarm orchestration, overlay network, builds and stacks, cool stuff like that that's getting implemented in the daemon. Oh, yeah. Container D is a Cloud Native Computing Foundation project. In fact, so is GRPC, and they're a great fix together, actually. In fact, keep your eye on GRPC, it is the future. What else did I want to say? Oh yeah. So, it's a one-to-one relationship here, between containers and Run C, or the shim. For every container that you create, a new Run C process forks it, and then exits, leaving every container with its own shim process, connecting it back to Container D. Okay, so it's clearly a one to many relationship between Container D and shems. So, you only ever have one Container D process running on a system, and it's a daemon process, right, long-lived? But Run C isn't, that just starts a container and then bows out. Now, there's also a lightweight seal on Container D, but it's not for general consumption. But the whole point of Container D and Run C are that they're driven by higher level tools. And you know what, I could probably talk all day about this stuff, and you're probably nodding off, if you haven't already, so with that in mind, right, know that this has been long, but it's kind of been a Linux fest, so I'm going to do a quick five minutes on Windows, and then we'll do the recap.

Windows Containers
Alright, Windows came to the Container game late. Nevermind though, Microsoft has done a cracking job, getting them into Windows 10, and Server 2016. And I'm talking about native Windows containers here, the ones that run true blue Win32 apps. Now then, as part of the work to get them onto Windows, Microsoft had to put a bunch of new stuff into the kernel. Now, at our kind of high-level, we're talking about the two main building blocks that we've got in Linux. Namespaces, and Control Groups. The Namespaces stuff lets Windows create additional isolated userspaces, Containers, if you will. And the Control Groups, or Job Objects, yeah? These letters group processes and slap limits on them. Looks familiar, right? Well, as well as that, they did a bunch of work on NTSFS and the registry, so that we can get image layering and all of that goodness, like we do with AUFS and overlay AUFS on Linux. Remember, a union file system or a union mount system with some copy on right is an integral part of a Docker container. Anyway, they also ported a Docker client and daemon into Windows. I mean, literally ported it, right? So we got a docker. exe process as a client, and dockerd. exe is a Windows service, and that's the daemon. What all of this means is that we get the same API and the same user experience. We didn't get integration with swarm and other stuff, magic. Below this, though, we start to diverge from Linux. Remember, we said that instead of Container D, and then OCI runtime, we get this Compute Service layer. But it does the same stuff, so as far as we're concerned, like as far as user experience goes, it's all the same. Same commands, same containers. Now, if you poke around inside a Windows container, you're going to see a few differences there as well. We're talking about naked Windows containers, remember, one's running Windows apps. Well anyway, inside, you're going to see a long list of system processes, and that's different, right? In Linux, we're used to seeing a single process per container. Well, you're not going to see that in Windows, and it's not a fact of how Windows works. Basically, over time, Windows has developed a bunch of interdependencies, so apps need certain systems services, DLLs, to be available, and in turn, some of those rely on others, and if they're not there, things break, and it's not different for containers. Every container needs these processes. When we start a Windows container, it gets this process called SMSS. It's a bit like in it, if you know your Linux. It's got the job of making sure that all of these are the required system service and the likes are rolled up. Once they are, then your app can start. Now again, I know it's a difference, and I know it's easy to point fingers at and make fun of, but does it really impact the user experience? I don't think it does. Now then, Hyper-V containers. OK, yes, it's true, we've actually got two different types of Windows containers. Native Windows containers, and Hyper-V containers. But both have a native Win32 apps, but you can't do Linux inside of a Hyper-V container. But why two options? Well, native containers use Namespaces for isolation, and down at the bottom, they all share the host's kernel, but sometimes, that's not what you need. I mean, from a security perspective, maybe you don't think Namespace isolation's all it's cracked up to be. Or, maybe you've got an app that needs a slightly different kernel, or a different patch set, or whatever, right? Enter Hyper-V containers. When you set up a Hyper-V container, Windows actually spins up a lightweight Hyper-V VM in the background. Now it is lightweight, right? So it's got less of a performance overhead than a full-blown VM, but inside, you still get a full-blown OS, so it's not using the host's kernel, a separate isolated kernel, and then you run your container on that. So, look at the diagram, right? On the left side, we've got the host OS at the bottom. Well, look, it's at the bottom for the lot really, but on the left side, the native containers are actually using it. They spin up, directly on the host, leverage its kernel, and isolationist done with Namespaces. On the right, we've got a VM. It's lightweight and all that jazz, remember, but inside, it's still running a totally isolated kernel, and it can be Windows or Linux inside, right? Both work with Hyper-V isolation. Anyway, the container runs on that, and obviously, we can have a load of these on each host, but it's always one container per VM. That's kind of the point. Cool and all? But doesn't that make life a bit complicated? Well, not really. You just develop your containers in Windows up, the same way that you always have. There's only one type of build, and then, whether or not you go with native containers or Hyper-V, it's just a deployment decision. If you want Hyper-V, you give the Docker run command, the minus minus isolation equals Hyper-V switch, and to run it natively, you leave that switch off. And that's it, and I reckon that's the gist of Windows containers. A few low-level implementation differences, but user-wise, it's all the same. So if you're familiar with Docker on Linux, Docker on Windows is going to be a breeze. Now then. I know that this has been a ton of theory, and you might be like, 'Oh my goodness, how am I supposed to remember all of that? ' But lucky for you, we've got a short, sharp recap coming up.

Module Recap
Drinking from the fire hose, yeah? Love it. Well, let's quickly recap. We said that the stuff to build containers has been in the Linux kernel for ages. It was just hard to use, like too hard, okay? Windows was a bit different, they had a couple of internal proof of concepts, but nothing got off the ground. So, Windows is playing catch-up. But the gap is closing, and parity is not a million miles away. Anyway, the major kernel technologies that we are interest in are Namespaces and Control Groups. They're pretty similar across Linux and Windows, and I reckon they're the two fundamental building blocks of containers, plus some sort of a union file system. Well, Namespaces let us ring fence, and isolate areas of an operating system, and this is basically the container. Then, we sprinkle some control groups on top for capping resource usage. Add in a dose of union file system or blocked device snapshoting, and we are staring at the model for a docker container. This was all hard, remember, though, and along came Docker, and everything got easy. Now, Docker iterated like crazy, and mistakes were made. Nothing worthy of death, right? Just a bit of a beating. Though, to be fair, we're only talking about the kind of mistakes that every start-up makes. I even feel harsh, calling them mistakes, it's just part of making something great. Either way, though, one of the issues was bloating at the daemon, it became way more than a simple reusable building block, and nobody was happy. Users weren't happy, Docker wasn't happy, and the eco-system certainly wasn't. So. Docker fixed it, and the community helped, of course. But these days, the engine is way more of modular beast. Not only is it in line with the tried and tested Unix philosophy that a few small, simple components are better than one big one, but it means that a bunch of it is swappable, and reusable. OK, so, the client talks to the daemon, over a REST API. A daemon uses container D to create and manage Container life cycles, and Container D uses Run C to actually interface with the kernel, and build containers. Now, there is some stuff left in the daemon that I'm not showing you, but the point I'm making is that the core bits are broken out into separate projects. Anyway, this effectively decouples running containers from the daemon, meaning that we can stop and restart and even upgrade the daemon, without killing our containers, got to love that. And that's the Docker engine. Ah, yeah, there is some low-level implementation differences between Windows and Linux, but the API and the user stuff is pretty much the same. And who knows? Future versions of Windows might even ship with Container D, and an OCI runtime, but maybe not. I honestly don't know. Either way, though, I just think it's all of it. Eh, no big deal. We're talking low-level stuff that doesn't impact us. Sticking with the Windows side of things, though, there's native containers and Hyper V. Natives run directly on the Windows host, so leverage its kernel and use Namespace technologies for isolation. Hyper-V, though, spins up a lightweight VM for every new container, the idea being that VM isolation might be better or more secure than Namespaces. Also, though, you get to run a different OS in the VM, if you want, so different versions of Windows, and even Linux. And yeah, I think that's what's done. Now, thanks for sticking with me. I admit that I kind of love this stuff, but I'm hoping that you do, too. In fact, reach out to me on Twitter with your thoughts, is this kind of stuff good, or bad? I'm really keen to know, anyway, we're done, right? We're about to switch gears now, and get hands on with Docker images. Bring it on.

Working with Images
Module Introduction
A proper understanding of how images work is massive if you want to master Docker. In fact, it's so important the next two entire modules are dedicated to them. In this one, we'll crack open the implementation stuff like how the heck is an image even constructed and what's inside it and the likes. Then, in the next one, we'll look a bit more at how we build our own images. So, for this module, we're looking like this. We'll do a quick big picture to cover off the main concepts and it will be quick, if you're super new to this stuff, then I highly recommend taking my Getting Started course first. Now, I can't force you, but beware, right. This is going to be a bit of a deep dive. Anyway, after that, we'll dig into how they're built. Like what's all this layering stuff and what are these crypto IDs all about? Then, we'll do registries, we'll share a few best practices, and then we'll bring it all together with a solid recap. Now, honestly, I'm going to be as hands on as I possibly can. Plenty of examples and poking around on the CLI. You're obviously welcome to follow along in your own lab or you can just sit and watch. Either way, I think it's going to be fun. Now, as hands-on as we are, we're still going to be explaining a bunch of theory along the way. But I have got the PowerPoint shakes from the last module. Nearly 40 hours just on the slides for that one. I'm worried (chuckles) I've done some irreversible damage. So slides will be kept to a minimum. Anyway, I think that's the plan, oh, and although most of what I'm going to show is going to be from a Linux machine, we'll cover the Windows stuff too, just don't sweat it if I don't log onto a Windows machine. Remember, most of the stuff's the same between the two so it shouldn't really matter which platform I'm on. Also, right, we're going to cover a bunch of exam stuff and all of it from Domain two, so we'll use the command line to play with images. We'll inspect images and look at specific attributes. We'll push an image to a registry and pull an image from a registry, describe how layers work and then look at layers, we'll delete an image, tag one, and store one in a registry. Well, let's crack on.

Images: The Big Picture
So, a quick big picture. Hopefully you've completed Getting Started with Docker as a pre-req. That's the idea but it's your call, but cracking on. An image is a ready-only template for creating application containers. Inside of it is all the code and supporting files to run an application. Now, images are build-time constructs and containers are their run-time siblings. So think of a container as maybe a running image or, I suppose, vice versa as an image as stop container. Either way, right, all an image really is, and we're high level at the moment remember, is a bunch of files and a manifest. The files obviously include your app files, but they're also going to be all the OS and the library files and stuff that your app needs to run and ideally, just the ones it needs to run. Then the manifest is a JSON file explaining how it all fits together. Now then, drilling a bit deeper. An image is a bunch of layers that are stacked on top of each other. That's why I'm always drawing them like this. But it looks and feels like a unified file system. So, the stacking's important and we'll get to all the luscious details soon but for now, there's magic going on to hide all this layering and make it feel like a single flat image. Anyway, we take this and we start containers from it. So umpteen containers all sharing a single image. An it's insane for efficiency and container start time. Now, we store images in a registry, which can be cloud or on prem. We pull them down to our hosts using the docker image pull command. Once on our hosts, we can start containers from them. Now, we're only showing a couple containers for simplicity but remember, they are both sharing a single image and the image is read-only, so, ah, how do the containers write stuff? I mean, read-only and writing, I don't know about you but from my experience, they're usually not a great combo. Well, for each container, we create a thin writable layer and effectively lash it on top of the ready-only image. Then, any writes and updates the container needs to make get preformed here. Now, because the writable layer pretty much is the container then it is a one to many relationship with the image here. One writable layer per container. Details to follow, yeah? But I think that's the big picture. The templates for starting containers, the read-only, and the shareable. Under the covers, it's a bunch of layers and a JSON manifest. Love it, let's crack on with the detail.

Images in Detail
Buckle up, because it's hands on time. We said an image is a bunch of layers. We read the manifest file, we stack them up, then we throw in some magic to make it all look unified. Well, you know what, to get an image in the first place, we have got to pull it. I think, probably the easiest way to do that is with docker image pull. And we'll have redis. Now, if you're on a Windows machine, I recommend maybe Microsoft slash dot net. Anyway, each of these pull completes here is a layer and it's the same on Windows. Now then, there's actually no such thing as an image, at least not as a single blob or object. You see, an image is actually a bunch of independent layers that are very loosely connected by a manifest file, which I know is at odds actually with the way always talk about them as a single blob. But they're not. Under the covers a bit, the manifest file, which we sometimes call a config file, right, but as you'd expect, it describes the image, like its ID and tags and when it was created, but importantly it also includes a list of the layers that get stacked and how to stack them. Now these layers are blissfully unaware of the bigger image. Just a bunch a files that are what they are, an independent layer. They have no clue that they're going to be stacked up and made into something bigger. So what I mean by that, right, is that one layer in image has no references to other layers. All of that's done in the manifest. And I think we get an idea of this actually with the pull we just did. I mean, look, we're not pulling the image, so to speak, as a single object with a single progress bar. It's obvious that we're pulling individual layers and it looks like six of them? Anyway, behind the scenes, the pull command's generating an API request to the docker registry API on a registry somewhere. Now, Docker's a bit opinionated about registries, so, unless you tell it something else, it's going to assume Docker Hub, okay? Well, pulling an image is actually a two step process. Get the manifest, pull the layers. Now the manifest's a bit interesting, right? We've got this push towards multi architecture support and content addressable storage. One's about supporting a single image across multiple architectures, maybe X86 and ARM and PowerPC or whatever, and the other's about using cryptography to guarantee we get the images we asked for. Anyway, step one's getting the manifest. Now, the client first looks for something called a fat manifest, and that's like a manifest of manifests, right? Basically it's a list of architectures supported and a manifest for each of those. So, if your architecture's on the list, it points you to the image manifest that you need. Well, for us, we're on Linux X86 or AMD64 or whatever. Okay, yep. So for us, we get the fat manifest, have a look to see if there's an entry for Linux on X86 and if there is, we go get the image manifest for that. So, I suppose, getting the manifest is a bit of a two step process these days. Get the fat manifest first, then get the image manifest for your architecture. Once we've got that, we pass it for the list of layers and we download them from the registry's blob store which is what we saw here with the pull. Now then, we mentioned content addressable storage. So back in the day, Docker didn't hash its images and there really wasn't an easy way to know if the image we got was the image we asked for. Well, that's obviously not good, especially when most of us are pulling our images across the internet. So, problem. Well, solution, and thanks a ton to the community and OCI, but in Docker 1. 10 we got content addressable storage, which at the high level says, "okay, every layer has got "a bunch of files and stuff inside. "So now I tell you what, "let's grab a hash of all that content "and use it for the image ID. " That way we can say, "Okay, registry, "give me the image with this particular hash, please. " We download it, run the hash again, and see if the two match. And in one fell swoop we have got a way more secure storage model. So, you're seeing the image manifest here that we're referencing stuff via a hash. These are the layer IDs. The syntax is just algorithm sha256 in this case colon and then the hash and hex. Anyway, we get the manifests and we pull the layers and, hey presto, we've got the image on our host and actually we can see the hash if we want. Hmm, okay, right. Anyway, where are we? Okay, we've got out layers, they're linked and stacked thanks to a manifest, and behind the scenes we've got some crypto goodness giving us immutability. Now then, behind the scenes, there's a storage driver pulling together all this layering. For us it is the aufs driver, oh, and most of our images and container stuff is going to exist somewhere here in valid docker. On Windows, this is going to be c program dated docker and your storage drive is always going to be Windows filter, though, those could be famous last words. Here on Linux, right, there are a ton of storage options. Aufs is the oldest driver and you still see it everywhere. Personally, I think overlay two is the future. Now then, layering pretty much works like this; There's a base layer at the very bottom. I tend to think of this as like the OS layer. It's the one that's got all the files and OS that build a basic OS, like ubuntu or something. In fact, let's assume it is ubuntu, right? But don't forget, when you run this image as a container, it's going to be using whatever kernel you've got down here on your host and that could be Suzer or Centos or whatever. So all it's actually making this base layer here ubuntu, it's things like, I guess, the usual ubuntu file system layout and maybe a few common ubuntu tools. So it is totally possible to be running a Centos docker host with ubuntu based containers on it. Not Windows, mind you, so it's got to be Linux on Linux or Windows on Windows, from a kernel perspective. Now I'm talking about hyper V containers here. Anyway, look, that container is going to be using whatever kernel your Centos OS here is running. But the file system and the likes in inside of it are going to feel like ubuntu (sighs). Mental maybe but it works. Then, on top of the base layer, we stack more layers. Things like your app code and the like, right. Now, I know this is a bit simplified 'cause I hate PowerPoint but it gives the picture. OS at the base layer with files and libraries and stuff. Copy in some app files as another layer, I don't know, maybe add another layer of updates later, and the storage driver waves its magic wand and aba-cadabra and the layers disappear and we're staring at what looks like a single unified file system, fabulous. Now then, each of these layers is represented in a directory in the file system under ver lib docker and then your storage driver name, for us it's aufs and then diff. Windows, that's going to default to c program data docker Windows filter. Okay, six directories, and funnily enough, six layers. Simple right? Mmm, maybe not so much. You see, since we got this fancy new content addressable storage model, there's no easy way to match the layer IDs with their respective directories on the host. And it's definitely annoying. Well, I reckon, this one here with the most objects is probably the base layer. Right, so that right there is the content of the base layer of the redis image we just pulled. And if you know your Linux, you're going to know that we're looking at a root file system and that's what we said, right? The base layer of an image has all of the OS files and directories. Then these other layers are going to have like the app code and the config files and stuff. In fact, what's this one up here with only four objects in it? Oh, it's got a couple of hidden objects as well, right. That's where the four comes from. But let's see, what's been added in here? Okay, looks like someone's maybe been updating the users and groups for the image. So, if we go docker history redis, now don't be put off by this, right. I'll explain it in a second but I'm looking for something here that, okay, I bet it's this. See how this command, and this whole list here, right, is commands and stuff that build the image, but command here is adding redis users and groups. So, that's the one that's going to have created this layer up here, cool. Now, a few things about this docker history output. First off, just ignore this column here, right. Nothing wrong, it's just that the command's older than the new storage model so it gets a bit confused. Anyway, the operations here are a history of the image and how its layers were built. This here is at the bottom right is adding all of the file system objects. Then any other line that's got a non zero value at the end here will have created a new layer. Anything that is zero, well, that probably added something to the image's config JSON. So, I reckon, we should have six non zeros here. One, two... Okay, well we've definitely got six layers. Okay (chuckles), ah, right, this one here. It's creating a directory and setting some ownership. So, I'm guessing, I don't know, maybe it's produced such a small layer that it's got rounded down to zero, probably 10k or something, I don't know. The point is right, when building images, some of the things that we do create new layers and some of them add stuff to the image config file. This here's a list of everything that built the redis image. We can see these obviously created new layers. Where as things like setting environment variables and exposing things like network ports just add config to the image's JSON. Now, I want to show you one last thing before we move on. Running docker images inspect on the image gives us its config and its layers. So, at the bottom here we see the layers, yeah? But these are their content hashes. Now, sadly, they don't match up with the directory names or the image IDs shown in the pull. But we see six layers and these are their hashes. If we look further up, we see other stuff that makes up the image. Working directories, environment variables, network ports, you name it, right? But that, I think, I hope, is probably enough for now. So, a bunch of layers with files and stuff in them. Stacked on top of each other and unified by a storage driver according to instructions in a manifest. Then, we start containers from them. Multiple per image if we want. We pull them with the docker image pull. Inspect them with the docker image inspect and delete them, oh yeah, with docker image rm like this and it's gone. Including all of the files in var lib docker aufs diff. (clears throat) Pretend we didn't see that. This endput of the directory, right, is different if you're using another storage driver obviously and on Windows, remember, it is all under c program data docker Windows filter. Anyway, last but not least, an image is basically a manifest and a bunch of loosely coupled layers. Alright, let's switch track and talk about registries.

Registries
Right then, images live in registries. When we pull one to our docker host, we are pulling it from a registry. Docker defaults to using Docker Hub but other registries do exist. Google's got one, Amazon does, there's loads. But those, right, are all on the other side of the internet along with the public cloud, right? But if that happens to be not your thing, well, you're in luck, you can totally get on premises registries as well. In fact, Docker's got one of its own called Docker Trusted Registry that you get as part of Docker Enterprise edition. Anyway, even when you pull an image to your local host, I suppose we could say it gets pulled to a local registry. On Linux, that's under var lib docker and then the name of your storage driver. On Windows, it's under c program data docker Windows filter. So, images live in registries. Now then, image naming's important. The easiest images to address are the official ones. So, basically, Docker Hub's got this notion of official images and unofficial images. The official ones live at the top level of the Hub namespace. So you address them as docker. io/redis or nginx or whatever. And because Docker defaults to using Hub, if you're pulling from Hub, you can even leave this bit off and just call it redis or nginx. Couldn't be easier. But it's all a bit misleading in my opinion. You see, to me, that makes it look like the image is called redis or nginx, and it's not. It works like this, right, you've got your registry. It can be Hub, Google Container Registry, whatever, right? But that's the first part of the name. Leave if off if it's Hub, though, right? Anyway, within a registry, we have got repos and within repos, we've got images or sometimes we call these tags. Anyway, in the redis example, redis is the repo name. These are the actual images. Actually, a thing, okay, if we look, yeah, even though we only said pull redis, Docker's kindly added latest onto the end. It does that. If you're not explicit about the registry, it assumes Docker Hub and if you're not explicit about the image, it assumes latest. But the point I'm making is that technically speaking the image is actually called latest. It lives in the redis repo in the Docker Hub registry. Now, sometimes we call this bit the tag, in fact, most of the time we do. But it's the image bit, right? The other bit's the repo. So, we got away with quite a bit with that simple command. And we could've said, okay, docker image pull and be explicit about Docker Hub and from the redis repo we'll have, I'll tell you what, let's say we'll have the 4. 0. 1 image. And, yeah, cool. It's actually the same image. See how the two hashes are the same? So, that must mean that latest and 4. 0. 1 are actually the same image. So, if we go here. Right, in fact, actually (chuckles), all four of these refer to the same image. So, behind the scenes back here, Docker's not pulled the image again. It knows it's already got it. So this output here shows a single actual image with two tags, two names. And lashing the minus minus digest flag on, okay, it's a bit messy to read but extra confirmation it's the same image. Same digest, same ID, same size, same image. So, registry, repo, image. Now, on the topic of the latest tag. Although, Docker automatically adds it if you don't specify one, there's nothing automatic about the latest tag at the repo level. So for example, right, pushing a newer image to a repo does not automatically tag it as latest. It's a manual thing. That means just because an image is called or tagged as latest, does not mean it actually is the latest. In fact, okay, maybe it is in this repo but, you know what, I know if we come to the alpine repo here, right, edge is actually the most recent here. The one's that tagged as latest just means it's the latest stable release. But even worse, right, if we step into the shadowlands of non-official repos, like where my stuff is, and we look at the techon plug demo here, hmmm, okay, I guess you can't tell from here. But I know if we pull them locally, adding the a here, right, says to pull all images in the repo that match my platform and architecture. Well, we'll give that a minute. Okay, three images pulled. And see all these already exists lines? That's Docker saying, "Hey, I know that layer "and I've already got it, so, I won't pull it again. " Anyway, if we list them all again, and look closely at the tu demo lines here, we can see that v1 and latest are the same, v2 is different and that's dangerous, right. Latest is actually oldest. So, yeah, it's just another arbitrary tag for v1. So, moral of the story; just because something says it's latest doesn't guarantee it actually is, especially in the wild west of unofficial images. Speaking of which, right, you might've noticed that I've been adding my name to the beginning of these images. That's because they're not official. Official ones live at the top level of the name space. Unofficials live beneath a username or organization. For the most part at least. An exception is definitely the Microsoft images which live under the Microsoft organization, those are still official. Now then, I need to circle back a bit and talk about hashes and digests again 'cause there's a bit of a quirk when it comes to registries. So, walking through the process of pushing an image to a registry. Well, first off, right, there's no such thing as an image, remember? It's a bunch of independent layers with a config file that loosely couples them. Anyway, we've got layers containing data and for each one we compute its content hash that we use as its ID. And I want to be clear, we call this a content hash. Okay, now then, when we push an image to a registry, we do a couple of things, we create a manifest that defines the image, you know, what layers are in it, yeah, and we push the manifest and layers independently but when we push the layers, we compress them. Okay, that's normal when we're hauling stuff over the internet. Remember, the layer IDs are content hashes and compressing a layer, changes its content. So when they arrive at the registry and a hash is computed to verify integrity, we'll get a fail, they won't match. So, to get around this, when we build the manifest that will push to the registry, we populate it with new hashes of the compressed layers. (laughs) Hope you're following. So, on the host, we've got the uncompressed layers and their content hashes. Over the wire and in the registry's blob store, we've got compressed layers and what we're calling distribution hashes. That way, the compressed layers and manifest get pushed to the registry and the registry uses the distribution hashes to verify that everything arrived as it should. Epic, sounds a bit low level though, yeah? So, why am I boring you with this? Well, when you don't know something like this and you're pushing and pulling images and poking around and trying to figure stuff out, and you throw into the mix the fact that each layer also has a random ID for storing it in the file system and you end up with a single image and each layer has got at least three different IDs in umpteen different places, apparently, it can get pretty freaking soul destroying. So, to save you pulling your hair out and wondering the chuff is going on, now you know the score. Uncompressed layers on the host have a hash that we call a content hash, pushing and pulling though using a hash of the compressed data called a distribution hash. And then just to keep you on your toes the UIDs used for storing layers in the file system are random, (groans). Annoying, okay, but there's method behind some of the madness. And, my goodness, I hope that's enough on registries. If you're feeling a bit overwhelmed, don't worry too much, right, we are deep diving. There's a much simpler overview in my other courses. Anyway, quickly, right, registries are where we store Docker images. The main one's Docker Hub and within a registry we store images in repositories. And these are broadly divided as official and unofficial. Now, we're not done, actually. I have got to tell you this. You should in theory, and I give no warranties of course, right, you should in theory be able to rely on official images from repos. There's a good article on Docker. com and a bit of stuff on github here but the gist is this; anything in an official repo has been vetted. So in theory, it should be up to date, ideally free from known vulnerabilities, though I think that's a bit of a big ask. It should be simple to use though and well documented. And the Docker files, man, these should be living lessons in how to write a good Docker file. Living lessons, eh? All in all, great starting point and probably the first place you're going to see cool stuff like multi architecture support. Now in contrast, unofficial repos are anybody's game. I mean, even I've got stuff on there and my stuff is not to be trusted. I'm deadly serious, I mean, one of my repos has over half a million pulls on it and it's got more bugs than a public toilet in an airport. I mean, honestly, now, look. I'm careful not to paint all unofficial repos as cesspits, they're not, in fact, there's some proper good stuff out there. You just need to be more careful. I mean, heck, look, it's a public service across the internet so I'm hoping you're going to be careful anyway. But for unofficial repos, there's no team curating them. There's absolutely zero guarantees and documentation (gasps) what's that? Anyway, that is registries. The way we store images. There's loads out there but Docker Hub's the biggest. Official images are curated and they set a high bar. Unofficial images, well, they're anybody's game. When we push and pull stuff from them, we're pushing and pulling individual layers that are grouped by a manifest file. Stuff being pushed and pulled gets compressed so we need a second set of cryptographic IDs called distribution digests. Now, digest and hash is just the same, right. Anyway, they get uncompressed on the host and then we use their content hashes. Eh, yeah, different hashes can be a bit annoying at times but at least you know why we have them. And that is us definitely done with registries. Coming up next we're going to run through a few simple best practices that will, honestly, make your container life a whole lot easier.

Best Practices
Okay, a few best practices to make your lives a bit easier. First and foremost, where possible, use official images. It's not rocket science and we said it in the last lesson. They're vetted, documented, and generally speaking, moderately free from vulnerabilities. Alpine here on Docker Hub's a shining example. Look at that, I mean, it's practically a see of green, right, pretty safe. And look here, right. Compressed size two meg, I mean two meg, are they for real? Well, yeah, and that's our next best practice. Keep your images small. Security principle numero uno, smaller is better. It means a smaller attack surface with fewer vectives. In fact, I think a big reason the alpine repo is so staggeringly free of vulnerabilities is that there's not a million and one packages in the image. Less packages, less potential for bugs. It's security 101. So, use official images and keep things small. And that's okay but what if they don't have what you need in the official repos? Okay, fair play, that happens all the time. All you do is you take something like Alpine latest and you build on it. That way you know you're starting from a decent base and you add your app on top of it. It's obviously way better than starting from huge old image with a bunch of known issues already in there, I mean, yeah. The next thing though is to be explicit referencing images. And we'll get more into this later in the course but there's a danger in always addressing images with the latest tag. 'Cause for instance, right, Alpine latest tomorrow might be a totally different image than Alpine latest today. And the differences between the two can break your app. So be more specific. Using latest in production is just laziness in my opinion. Be more like Alpine three dot whatever, yeah? But that's probably it. Just a few really simple best practices. Now, let's go and recap everything.

Uber Recap
Right, an image is a template for starting containers. It's read-only and we build it by stacking layers and having the storage driver make it look like a normal file system. But remember, we said it's not a monolithic blob. It's actually a config file plus a bunch of independent layers. And it's inside these layers where all the application binaries, and files, and libraries, and stuff, where they all live. And then config file has the instructions on how to run it as a container. So, how to set the environment, which ports to expose, and how to start the packaged app. Okay, well we can see it's possible, in fact, it's normal to start multiple containers per image. Each container gets its own thin writable layer where it stores changes and each one of those can be linked back to a single image here. But at no point does any of the data in the layers get changed. They are read-only. If a container wants to change a file in one of them, there's a copy on write operation, where it finds the image from whatever layer it's in, copies it up to its container layer, and makes the change there. Remember, images live in registries. These can be in the cloud or on prem and even when we pull an image to our hosts, it still lives in the local registry of sorts. Now we recently got a content addressable storage model. This let's us run a cryptographic algorithm over the contents of a layer and use the resulting hash as the layer's ID. Which is great, right, 'cause it makes the image and its layers immutable. Okay, so, the layers get a hash of their content. The image ID, though, is a hash of the image config file. The result's the same though, right. Change anything in the image config or anything in any of the layers, and the hashes change. Believe me, this has been massive thing in securing what is a vital part of the container pipeline. Now when we push images to registries, we compress them and compressing means changing content. So, we needed a second hash for use with the registry. It's a low level implementation detail but it can make matching things up on the host hard. I mean matching it up for you and me poking around not for the Docker engine. Docker obviously keeps a mapping so it's a piece of cake for the engine but it can be complicated for us. Anyway, on Docker Hub repos are divided into official and unofficial and the two are not equal. Official repo's the safest and follow best practices. Unofficial, mmm, not so much. In fact, unofficial can be dangerous but they can also be great, I mean, there's some absolutely cracking projects out there in unofficial image land, you just got to know where to look. Yes, there's other registries as well and there's even on premises ones. Finally, I think we used a few Docker image commands. Push and pull letters, upload and download from a remote registry. Inspect gives us the image config including layer data listed by content hash, I love that command by the way, and rm gets rid of old images. And that's it, I promise. We're ready to move on and see how to build our own.

Containerizing an App
Module Intro
So, we're switching tack a bit in this module. I mean, we're all good with what an image is and the likes. So, now it's time to take an application and containerize it, or put it in an image. So, first up, we'll cover off what the process looks like from a high level. I mean, containerize an app? What does it even mean? So, I figure we better cover that off. Once we're cool with that idea, then we're going to take a web app, like some code, and we're going to make it into a docker image, and we'll run a container from it. Now, to do this, we're going to work with something called a Dockerfile. And we'll see in a second, right? It's just a plain text file with a bunch of pretty simple instructions. But the power of this file is insane! I mean, not only to build an image. I'm talking about the potential to bridge the gap, nay, the chasm between dev and ops. Seriously. This file is a way for developers to describe their apps and how they work, and for ops to read the file and understand. Whew! Anyway, then we'll dig a bit. So we'll get under the covers of what we just did, and I think we'll probably share a few best practices at the same time. Then we'll look at multi-stage builds, which is really taking everything that we're going to learn and making it really production-worthy. Then we'll wrap it all up with a solid recap. Now, again, as we go we'll polish off a bunch of stuff from the exam. And again, all from Domain 2. So, we'll show the main parts of a Dockerfile and we'll describe a bunch of the options. We'll show how to create a production-worthy, efficient image, and use a file to create an image, a Dockerfile, yeah? So let's crack on.

The Big Picture
It is all about the applications. Everything. It's all about the apps. Just making sure you know, which I'm sure you did. Anyway. It all starts with app code, and it all ends with a running app. It's like, once upon a time there was some code, blah, blah, blah, container, container, container, and the app ran happily ever after. The end. Anyway, we've got our code. And we're a Docker course, right? So running it means running it in a container. But how do we do that? Well, that is the million Bitcoin question. Okay, well, the process looks pretty much like this. Here's our app code. To get it into an image, we create a Dockerfile, details to follow, yeah? But this is basically a list of instructions on how to build an image with our code inside. Well, once we got the Dockerfile, we use the docker image build command to actually build the image. Then it's obviously easy to start a container from that. But that is the flow. Take your app, write the Dockerfile with some instructions in on how to build an image, and point the docker image build command at that. And out pops an image! Job done. So, let's go do it.

Containerizing an App
Okay. So. All right. I'm on a Linux machine. But it could be Windows, Raspberry Pi, whatever. The principle's going to be the same. The point is, right, here is some code. And again, look, what the code actually is isn't massively important. The point is, it's code, our application, yeah? Now then, to get this code into a container, we need something called a Dockerfile. Now, right from the start, it's normal and probably a good practice to put your Dockerfile in the root folder of your app. Also, it's a bit opinionated about its name, so it is Dockerfile, all one word, and always with a capital D. Okay. Now, all a Dockerfile is, is a list of instructions on how to build an image with an app inside. But remember, this is going to document the app. So, describe it to the rest of the team, and to new starts, and also to the ops guys. It is a beautiful thing. Okay, so we've got our code, right? And it needs something to run on. The example I'm using here is a Linux app, right? So we need some form of a Linux base image. I'm a fan of Alpine these days, so I'm going to start it from that. Now, it's convention that instructions in a Dockerfile are capitalized. In fact, all this file really is is a list of key value pairs. Almost. So, it goes instruction, usually in caps, yeah? And then value. So, we're going from the instruction and then alpine, the value. Now, you always start a Dockerfile with a FROM instruction. This is going to be the base image that we add our app on top of. Now, if you remember the best practices from the previous module, you'll remember we said, go official and go small. Well, Alpine's an official repo, and it is freaking tiny. I mean, check it out, right? It's like two meg compressed, and probably about four meg uncompressed. So, we're official and small. Next up, I'm going to list myself as the maintainer. Now, don't get carried away. I actually have zero intention of maintaining this. It's just a dead simple web app that I only ever use for demos and courses. All I'm doing here, right, is I'm showing you the right way to do things. And also, I guess, how to use labels. But it is normal to list a maintainer, right. So, go ahead and list yourself, or maybe your team inbox or whatever, right? It's good for documentation. Okay. Well, the app that we're containerizing happens to be a node app, so we need to install node and npm, the node package manager. Cool. Well, that's what this run instruction here does. It lets us run commands and perform build activities and the likes inside the image. Kind of, sort of. Again though, we are instruction and then value. Now, let me be clear, okay. The from instruction up here pulls the alpine image and uses that as the base layer for the image that we're building. The label that I'm using here, all that does is adds a bit of metadata to the image config. But then RUN here creates a new layer. And we'll see it in a second, right? But we'll be taking the alpine base here and we're adding a new layer with the software that we're installing. Apk is just a package manager for alpine, right? Like apt for Ubuntu and yum for Centos, and, I don't know, chocolatey maybe for Windows. The point is, we're adding some software to the image, so we get a new layer. Anyway. Next, we want to copy in our source code, so everything from the same directory as our Dockerfile here, and we'll copy that into /src image. Well, guess what? We're adding more content, so we get another layer. All right? Well, we're set the working directory to /src. We won't need a layer for that, it's just metadata. Next up, we want to install our dependencies. So, npm install here is going to run against whatever is listed in packages. json. Look, the detail's not important. What we care about is it's the RUN instruction again, and like before up here, it is running commands to install and build stuff. So, this one we're installing node itself and npm. Well, now down here we're using it to install dependencies. And again, it's adding content. So, you know it by now, we get another layer. This particular app listens on port 8080, so we'll expose that. Just metadata again. And then last but not least, right, we need to run an app. So, node, and we'll tell it where the app is. And this is relative to WORKDIR up here, okay? Anyway, these here don't really create layers. Just metadata in the config. So, working directory, exposing that work port and specifying app to run when we start the container from it. So. We've got our code. For this example, it's a nodejs app. For you, right, it can be totally different. Go. net, whatever it is you do, you've got your code. You've also got your Docker host, as well. Now, that's got to be running the right platform and architecture. So, 64-bit Linux or Windows or Raspberry Pi, yeah? That's got to match what your code is written for. Then we write the Dockerfile that says how we take the app and build an image from it. For those, we're starting out with a slimbase image from the official Alpine repo. A bit of metadata setting me as the maintainer, add a new layer with node and the likes getting installed, add our source code in as our third layer, I think? Yep. A fourth layer building in dependencies, and then some final bits of metadata saying, okay, set working directory to /src, the app runs on port 8080, and you know what? When you start the container from the image, make sure that this is the app that runs. So, yeah. Four layers. We should remember that, right? Four layers and some metadata. Well, time to build it, I think. So, from within the directory of our source code and our Dockerfile, right? We'll go, docker image build. Docker build on its own works as well, right? Well, we'll tag this one as psweb, and then the dot here says use the current directory as the context where the code is, yeah? And away that goes. Now then, to keep the momentum going, I'm going to bend a bit of space-time. Right. That's finished. Okay? And we'll get under the covers a bit on the next lesson. But the image is built, and it's tagged, so we're done. That means that if we do docker image ls, there it is. Psweb, and only just built. Tell you what, let's run a container from it. Well, that'll be docker container run. We'll run this one detached, right, so it doesn't steal our terminal. And you know what, it's just a web app, right? So it can tick away in the background. We'll call it, whatever, web1. It runs on port 8080, so I'm going to map that to 8080 on the host, as well. It's host container, like this. And we're going to run it from that psweb image that we just built. So, I guess moment of truth. Okay, we're good. That's a container ID, which means, if we open a web browser here. This is the server the container's running on, yeah? And it was 8080. And there we are! That's our app. So, from source code to running web server in, whatever that was. Probably less than a minute, if you take out my waffling. But I think pretty flipping cool, yeah? Now, if you're on Windows, or Raspberry Pi, or whatever's your thing, right, the commands in the process are the same. I mean, sure, you're not going to run a Win32 app on Raspberry Pi. But assuming you've got your app on the right architecture, the process is the same. Code, Dockerfile, docker image build. Simples, right? Have some of that. Well, that's cool and all, but let's take a look at what went on under the hood.

Digging Deeper
So, we just containerized our web server. Pretty sweet, yeah? We had our code, threw in a Dockerfile, mixed in a bit of docker image build magic, and out popped an image. Well, let's do away with some of the mystery. First up, there's absolutely nothing magical about a Dockerfile. It's just a text file with a bunch of instructions. All right, it's got a few quirks like you need to name it with a capital D, and it's Dockerfile, all one word. But inside, it's just a bunch of really simple text instructions. Then, docker image build comes along, reads it, in order, one line at a time, starting from the top. Proper simple. But as well as that, it's going to be read by the ops team, by your fellow developers, by new hires. Pretty much everyone is going to look to this to understand your app. Anyway, each line in the file is an instruction. And you don't have to, but it is convention to write them as uppercase. Just the instruction name, though. The contents of the instruction you can totally write lowercase. Anyway, every Dockerfile starts with a FROM instruction. This is what starts the ball rolling. It pulls down the base image that everything else will build on top of. And following our best practices, if we can use an official image and a small one, you know what, we probably should. Well, we've gone Alpine, and because we've not put a tag on, we'll get the latest. Now, if you're doing this in production, right, you really want to get specific with tags. But you know what? The FROM instruction will go away and pull whatever image you specify from Docker Hub. Cool. Well, that is layer one. Next, we're adding a label to set the maintainer. And like I said before, honestly, I am not going to be maintaining this. I mean, it'll always be around for you if you're following around and the likes, but I have zero intentions of patching it and keeping it up to date. I'm really just showing you this so you can see how to use labels which are great for adding custom metadata. RUN here, and here, are actually how we execute commands and stuff inside the image. We're running some package installs here, and installing dependencies here. So, another couple of layers there, right? Well, COPY here is how we pull files and the likes into the image, and you probably guessed it. More content means more layers. And then we've got WORKDIR expose an entry point for adding metadata. Beautiful. But there's still a few gaps, I think. First up, I think I mentioned the build context at least a couple of times. Well, all it is is the location of your source code. If all is right, it's been the working directory. So, I've got my shell sitting in a folder called psweb here, and that has got all of my source code in it. That way, when I run the docker image build, I can just say dot at the end to use the current directory. If my shell was somewhere else in the file system, that's still cool, right? I just have to spell out exactly where the build context was. So, yeah. Build context, that's where your code is. And you can nest stuff, as well, right? Because it gets read recursively. So subfolders are fair game. Which you need to be careful about, actually. Because when you do the build, everything in your build context get sent to the Daemon. And if you've got a ton of stuff in subfiles that you don't need, you're going to waste resources, especially if your Daemon's across the network somewhere. So, yeah. Only have the code that you need in your build context. And feel free to throw your Dockerfile in there, as well. We did. But when you type docker image build, whatever's in there gets sent to the Daemon and processes part of the build. Now, for us, the client and Daemon are on the same host, but it is totally possible to have clients talking to remote Daemons over the network. Which brings me to probably our last thing, actually. Your build context can absolutely be a remote Git repo. In fact, let's do that. So, we've got nothing on this host right now. No images, and then, obviously, no containers. And then over here, this is the URL of the server that I am on. And we can see, yeah. There's definitely nothing listening on 8080 here. So, we've got a clean slate. Well, if we run another docker image build, and again, you can just go docker build if you want. Old habits die hard for some, I know that. We'll tag it the same again, but this time, instead of using dot to use the current working directory as the build context, I'm going to point it to a Git repo. This one here. And away that goes. Exactly the same as before, right? Only this time, the Daemon is pulling the build context across the wire from GitHub. But the process is just the same. And you know what? Sorry, I lied. That's not the last thing. Let's take a look at this output. So, sending context to Docker daemon, well, we just talked about that. Then the first instruction, one of eight, actually, FROM Alpine, pulls the latest Alpine image. And that there is its ID. In fact, yeah. Computer? (computer beeps) Stack each image layer in temporary containers as I'm going through the explanation, please? (Computer Voice) Acknowledge Ok, so we got the base image at the bottom. Next up, we have the maintainer label. To do this, Docker spins up a new container, and it churns out a layer. Once the layer's created, it gets rid of that intermediate container. Poof, gone. Next up, we're installing node and npm. I think we see this as doing a bunch of stuff, fetching and installing and all that goodness. But again, it is doing it inside a temporary container up here. Anyway. It spits out a layer here, and it loses the container. After that, we're copying in the app. Hmm. Huh. Yeah. Looks like maybe we don't need a container for that. Huh. Okay, actually, I'm not sure about that. But look, we get the layer. Then we're setting the working directory. Again, spits out a layer, shoots the container. Then we run npm install. That runs inside a container. Does all of this. And, as usual, spits out a layer and shoots the container. And the same for exposing ENTRYPOINT. Spin up that intermediate container, produce the layers, and shoot the containers. That's the process. Now, then. Only the layers that contain actual data are kept. So, even though it looks like these entry points and expose instructions create layers, they really don't. At least, not ones that we keep. So if we do this... Okay. First up, we can see that only four of these layers actually contain any data. Magic. But see these IDs over here? See how they match what we built in the picture here, commands and all? More magic. But really, we only need these ones. The others are just metadata. So if we inspect the image, like this, okay. Only four real layers. And no, the IDs don't match, annoyingly. Go back to the previous module if you want to know why. But, yeah, you know what? That is the greasy, oily detail of building an image. Next up, the exciting world of multi-stage builds. This stuff is the future.

Multi-stage Builds
I am hoping by now that it is abundantly clear that size matters when it comes to images. And smaller is definitely better. So, with smaller images, we're talking about things like faster builds, faster deployments, less money wasted on storage, and the killer one for me, less attack surface for the bad guys to aim at. So, running our apps with a minimal OS and minimal supporting packages, that is the gold standard. I mean, I've said it before. If we could run our apps commando, so without any OS at all? Heck yeah, we definitely would. I mean, go check out Unikernels. Anyway. Small, small images is what we're aiming for. The problem is, more often than not, our build pipelines look like this. So, we start out with a pretty big image. You know, one that's got a shedload of packages and build tools that, who knows? Maybe we'll need them. But maybe we won't. Then we copy in a bunch more tools that we know we need. We throw in our app code, and then we do the build. And the image is huge! And this is where it falls apart, right? Then we ship it to production with all the build stuff left in! I mean, it's crazy! It's like building a car in some ginormous factory with robots and lifters and paints stations and all that jazz, and then shipping the car with the factory still attached! I mean, who'd do that? Nobody, right? Only, in the software business, we do it all the time. It's just crazy. Now, good software developers are those with the time? Well, they came up with ways around this, but it was always at the expense of added complexity. So, we'd usually have a pattern that involved multiple Dockerfiles, all glued together with scripts and automation voodoo. And it worked all right, don't get me wrong. It was just clunky and extra work. Well, multi-stage builds to the rescue. This here, what we're talking about now, is official support to technology from the core Docker project. And it's simple. We're talking a single Dockerfile and zero scripting wizardry. Here's the high level, right? We've got a Dockerfile here, and by now, it should look safe and familiar. Only, you know what? It is a bit different. The main thing to note is that we have got multiple FROM instructions, three in this example. Okay, well, that's new-ish. Well, each one marks a distinct build stage. So, internally, right, they're numbered from zero at the top. But even better, we can give them friendly names. So, stage zero here is storefront, and stage one here is appserver. And then the last stage, production. Well, a regular docker image build steps through this from top to bottom. So the storefront stage here's standard stuff. Pull a base image, a big one, right? Set the working directory, copy in some app code, and do some build work. Par for the course so far. The base image it pulled is already big, and then we fattened it up even more by adding stuff in a doing builds. The appservice stage, same again, really. Pull another big image, set a working directory, copy some stuff in, run a build, rinse and repeat. End result, we spit out another even bigger image. All standard stuff so far, though. But let's step back and take an inventory, right? At this point in the build, we've got four images. The two that each build stage pulled down, plus the two that each build stage churned out. The storefront stage here churned out an image based on node:latest, so an image with a bunch of OS and build tools, plus a small, tiny app-related piece of code that we actually want at runtime. Same again for the appservice stage. We churned out an image based on mavenlatest with a bunch of OS and packages, and again, a pretty small amount of code we're actually interested in. So, four images with a ton of build machinery inside, and precious little production code. Okay. Onto the production stage where it actually gets interesting. Importantly, we're starting from the Java Alpine image, which right from the start, okay, is significantly smaller than the node and maven images we used before. So that's our best practice, right? Starting from a decently small image. Then a bit of standard stuff here, and we get this COPY from instruction. And this is where the magic of multi-stage builds really comes into play. The COPY from here grabs that image created by storefront build stage, and it picks out just the application code that we want, and it leaves the rest of the image behind! Brilliant. Then another COPY from, this one taking the image built by the appservice stage and again, pulling out just the application code we want. And it sticks it in this image, then we tell the app how to run. So, recapping, right. Because I want to be clear about this. Stage zero of the build here takes a giant image, great for building, okay? Not so great for production. Well, it builds some app code and it spits out even bigger image. Stage one is the same. Pulls a big image, adds some stuff in, throws out an even bigger one. At this point in the build, we've got four images, and inside of them, we have got a boatload of build time machinery, but just a small amount of production code. Maybe think of these two stages as building a car. Stage zero at the top builds like, I don't know, the chassis and the body work, right? Then, stage one here builds the engine and power train. But they've both still got the factory attached! Now, the production stage here grabs the body from up here and the engine from down here, but it leaves the factories behind, and it puts them all together and ships just the car. In the real world of software, the real world of software? Yeah. Well, it's pulling out just the bits of application that we need at runtime, leaving all the build stuff behind, and spitting out an image with our application plus a tiny amount of OS in dependencies. Love it. So, let's do it. Now, I've got a copy of this repo here, which is actually a fork of this repo in the app directory where the Dockerfile is. So, although it's a newish format to the Dockerfile, it's just a regular docker image build command. We'll tag the image like this, and we'll build it from here. And that's away. Now, the build's going to take a minute or two while it pulls the images and the likes, right? So, I'm going to fold space-time and bring us out somewhere in the future when this is done. All right, we're back. And... Right. These images here are the ones that were used and produced by the first two build stages, and they're pretty big, right? Well, this one here? This is the one we're going to use in production, and it's tiny by comparison! So, that's it, right? Not hard to see. The production image is about, what, a quarter of the size of the two build stage images? That is a pretty solid saving in my book. And the best thing about it? It was easy. A single Dockerfile, a normal docker image build command, and absolutely no scripting. How cool is that? Well, I think time for a recap.

Recap
All right, that was some proper action, yeah? Real apps made into real images. We're recapping, right? We said right at the top that it is all about the apps. Containers, yeah, they're crazy cool and all, but on their own, they're not a lot of use to our businesses. So, we put apps in them. And to do that, we start out with our app code, like our source code and dependencies and stuff. We create a Dockerfile, which really is just a bunch of instructions that tell how to take our code and make an image from it. So we say things like, what base image do we want to use, if you're app's Windows, then obviously choose an appropriate Windows base. If it's Linux, choose Linux, yeah. And a good place to start is the official repos, yeah? Anyway, once we've got the base image, that's when we start throwing our app on top, copying the files, bring in any dependencies, build stuff that we need, and then set things like network ports and the application that we want to run when the image's started as a container. Well, we feed all of this into a docker image build command, and out pops an image ready and raring to start containers. Love it. Well, once we're cool with that, then we can really start turning up the heat with multi-stage builds. These give us leaner and better images ideally suited for production environments. Brilliant. Now, let me beat my drum one final time about those other mystical powers of the Dockerfile. So, outside of letting Docker know how to build an image, it is documentation and it's a potential bridge between dev and ops, a way for developers to describe their app to ops. And also, yeah, a way to quickly onboard new hires and the likes. So, you get a new hire and she's like, how does such-and-such an app work? Easy. Just get her to check the Dockerfile. I do think it's a beautiful thing. Coming up next, we're going to start focusing in a bit on containers.

Working with Containers
Module Intro
Containers. We're already hours into the course and it's only now that we're finally getting into containers? Wow! Well here we are and here's the plan. We'll have a quick Big Picture. Then we'll dive in, we'll touch on logging, and then we'll wrap it up. Now of course, as we go about things, we're going to touch on some of the exam stuff. From Domain 1 we'll inspect images, add networks, and publish ports. From Domain 2, we'll look at some of the Docker inspect outputs. And from Domain 4 we'll publish a port, then see how we can find out which port a container is accessible on. That's it, let's do it.

Containers: The Big Picture
Okay, first up, in the Docker world, the most atomic unit of scheduling is the container. It's the smallest unit of work we can create. So, in the virtualization world, that's the VM. In the Kubernetes world, it's the pod. Well with Docker, it is the container. This means, if we want to roll out an out on Docker, we do it as one or more containers. Sure, there's high level objects like services and stack and pods if you're rocking with Kubernetes on Docker. But the smallest unit of work, in the Docker world, is the container. Now then, containers are running instances of images. We've build-time here, run-time here and we know that images are a bunch of layer. Read-only yeah. Well, digging a bit deeper, all a container really is, is a thin, writable layer latched on top. So build-time down here, run-time up here. This means any rights and changes that we make while the container's running, all happen up here in the writeable container layer. Now all of this is done through the magic of union file systems or union mounts, where all of this layering and stacking here gets hidden and everything is made to look like a single unified layer. Anyway, we can have a one to many relationship between images and containers. So each container that we're showing up here, well, it's really just a unique and separate writeable layer, remember. And each on in this example is linked back to a single read-only image down here. So if we're running a couple of containers off of this image and this one here wants to edit a file in the image, well it can't 'cause the image is read-only. I mean can you image the carnage if individual containers could modify shared images without the other containers knowing? Holy cow, all hell'd break loose. So instead of modifying the file inside the image, the container locates it in the layers down here, makes a copy of it in it's own writeable layer and makes the change there. That way, right, the container gets the full read, write experience, but without having to have write access to the image. And we call this copy on write. Anytime the container needs write a change to an existing file, it makes its copy and writes the changes there. Cool. Well switching tack a bit. From on OS perspective, we know that every container is its own isolated pod at the OS, Linux, Windows, whatever, yeah. Well that's not massively different from the VM on a hypervisor. Only instead of virtualizing hardware resources, containers are virtualizing operating system resources, file systems, process trees, networks stacks, yeah. Now from a a life cycle perspective, we can start containers, stop them, restart them, pause them, and delete them. And until we actually delete them, they stick around. The techie term is persist. So stopping or pausing the container does not get rid of it, nor does it get rid of any of the data inside of it. Everything sticks around 'til we put a gun to its head and tell it to delete. You know what, just like at the end yeah, start, stop, restart, then once we're done, boom delete. Now back to the OS for a second. Linux containers need a Linux kernel. Windows containers, they need a Windows kernel. So as portable as containers are, if you build your apps for Linux, they are only going to run as Linux containers on a Linux Docker host. And the same goes for Windows. If you're writing Windows apps, they're only going to be Windows containers on Windows Docker hosts. 'Cause remember, all that's really in a container, so all that's in that isolated set of name spaces, is a bunch of files. Now obviously these include your application and dependencies right? But the point is, there is no kernel inside. Every container on a host, shares the host's kernel. So trying to run a Linux container, so one that needs a Linux's kernel, over here on a Windows host, yeah, good luck with that. Now I know, Windows has got a couple of ways that it can run Linux containers. But, both of them make it so that Linux kernel primitives are available to the container here. So I guess yeah, you can run Linux containers on a Windows machine, but there's magic going on to make it happen. Now amid all of this right, I want to make sure we don't lose our focus. Containers are all about apps. And the so called gold standard when it comes to containerized apps is microservices. This is where each container generally runs a single process and has a single job. So instead of a monolith like this, we break each function out into its own container and then we glue everything together with APIs, hopefully simple, documented, and versioned APIs. The idea though at the container level is that you keep your container as small and simple as possible. In the Linux world, it's usually going to be a single process per container and that process does a single job. I don't know, run a web service or something yeah? This way though we get separation of concerns and all the other goodness that comes with microservices. But, containers are not just for microservices and I've been beating my drum about this for ages. You can totally run some of your more traditional apps in containers, and all without the scary thought of having to refactor into this microservices scariness. And Docker Inc or big D, Docker yeah, they're well on board with this now with their Modernizing Traditional Apps program, where the idea is something like this. Take a bit of a safer phased approach. So lift and shift one or two of your traditional apps into a container as is, no code changes, nothing like that, but still get the benefits of containers. And then once you're done with a bit of that and you're a bit more comfortable, I don't know, maybe then you starting looking at rewriting some of this stuff, microserviceizing it. I just made that up. Anyway, look, that's the big picture. A container's basically a think, writeable layer on top of a read-only image and it's a run-time construct. So an execution environment for an app. Every container gets its own file system, process tree, network stack, all of that jazz. Oh yeah, they've got a life cycle not unlike a VM, start, stop, restart, and delete. And we can lift and shift some of our traditional apps into them and that's fine. In fact, Docker Inc. will probably even help you do it. Probably for a price, they are a business after all. But the ideal for a container is that they should be treated as ephemeral and immutable. We don't half love our funky words in the tech industry. Okay, by ephemeral we mean that they're short lived. You're not usually aiming for your containers to have year after year of interrupted uptime. Then immutable, this means we really don't want to be logging into them and poking around. The idea with containers, is that we build the image and deploy the container, then when we need to fix and change stuff, we build a new image with a fix and then we switch out the old containers for new ones. None of this logging on and making fix after fix after fix, which in my experience, rarely gets reflected back to the gold build. Anyway, enough theory, let's hit the command line.

Diving Deeper
So I'm on a Linux host with Docker installed. We'll be on a Windows host later. But a quick look here, yeah, shows us we've got no containers running. Okay, well spinning up a new one is as simple as docker container run. This used to be just docker run, which still works by the way. Anyway, I want to log into this container. Wait, what? Didn't I just say that we normally don't log into containers. Well yeah I did and that is right. But we're in a lab here dissecting stuff so different rules apply. You'll see why in a second. Anyway, to make it so we can log into, we go -it. Look the flags up in the help if you want, but I always think of them as interactive terminal, i-t. Anyway, then we tell it which image to use. And finally we tell what process or application to run. I just need a shell. Okay, so the first thing note, see how my shell prompt's changed. That's 'cause the container is already running and we've switched our shell into it. We've basically attached the standard in and out streams from my local shell here to the container. So anything I type in here is going to run in the container. Coolio. Now hmm? Okay, actually, before we look at the output of that command did you notice how fast the whole container start thing was? Like maybe a second tops and that included downloading the image from Docker hub. Now fair enough, the image we're using in this example is disgustingly small. Most others are probably going to require a few seconds to download the first time you run it. But you get the picture. This stuff is quick. Now then, this ps here, remember it's running inside then container, so we've only got these two processes running. Only it's only one really, 'cause this one here is the ps command which is already exited. So one process, which if we remember plays nicely into what we talked about a minute ago when we said the gold standard for containers is the microservice model with a single process and concern per container. Fabulous. Now things are a bit different over here in the Windows world. This is a similar ps command running inside a typical Windows container and clearly there's a little bit more going on here. And that's 'cause Windows works different to Linux right? We won't get into it, but the net result is, every Windows container runs this handful of processes. Smss here manages all the system processes and POWERSHELL is the main app that I asked the container to run. Okay, well, back here right, if we start a bunch more of the same container, I use Ctrl+P+Q to get out of the container by the way. See how my terminal's back to normal? We'll talk about Ctrl+P+Q in a minute, right, but keeping on track for now, we can see the container is still running. If we start another one of those containers, only this time in the background please, d for detached or I think actually maybe daemon mode. Either way, it won't grab the terminal this time. And we'll tell this one to sleep. Go to sleep. Ha, I tell ya, what I'd give to sleep for a day. Okay, first up, that was closer to a sub one second start time and that's mainly because we've already got the image on the host from the last container we ran. So it was literally setting up the name spaces and stuff. Really quick. And we should see two of them now. Right, so two containers sharing a single image. The containers, though, totally isolated from each other. We remember this right? To start one of them, we go docker container stop, then we give it the name or the ID. If we go with the ID, right, we only need to give it a couple of characters. It's just after uniqueness. Ah, right, this waiting around here is 'cause the container has been given 10 seconds to sort it's stuff out before being forced to stop. We could go down a Unix rabbit hole here about init processes and signals and the likes, but I think suffice to say, right, stopping a container sends a signal to the main process in the container, PID1. If that process knows what to do with the signal, then it tidies things up and says hey Docker, I'm ready to be stopped. But if it doesn't know what to do with the signal, like ours clearly didn't, then Docker gives it a 10 second grace period, after which it forces it to stop. All right, well if we list them now, we only see one. But if we slap -a on the end here to list them all, okay we see two again and this one that's in the exited state is the one that we just stopped. So if we start it up again, you've probably guessed it, we change stop for start. Run this. And it's back, started a minute or so ago, but it's only been up and running for a couple of seconds. And the thing is right, any data that we might have saved into it before we stopped it, it's all still going to be there. In fact, let's see that. Okay, yeah, let's do it. Now look, this isn't recommended. Yeah sure, you can save data into container, but it kind of breaks all the containers are ephemeral and immutable rules. So in the real world, you don't want to be doin' this. Use a volume instead. But we're in a lab, just slicing stuff up here. So we can get into a running container with docker container exec. Make it interactive again, give it part of the container ID, and run another shell session. And we're straight in right? Now first up, look here. See how it's got a couple of processes going on now? That's 'cause execing into a container starts a new additional process. So the original sleep we started this container with, that's still snoozing away there, but this one here is the shell that we just started with exec. Anyway, we're in the root folder of the container here so, you know what, I'm going to be a bit naughty. I'm going to pipe some text here into a new file, right here in the root directory. Ah, it's a lab people, we can do what we want. Don't do this in the real world. Anyway, there it is right, that's our file. So if we exit out of this, this is going to kill the shell process, but it's going to leave the main sleep process alone. So we have to manually stop it here. And I'm doing it like this just to make sure we get the right container, yeah? Okay, stopped. Start it up again. Exec back into it. Actually, know what, let's do this instead, so instead of getting a full and interactive shell session again, let's just run some commands in it. That's our file and if we cat it, that's what we wrote. So there we go. Stopping and then restarting a container does not destroy any of its data. Brilliant. Well what was all this Ctrl+P+Q business about? Well we start the container, in fact, let me clean up a bit first. Okay, when we start a new container we tell it an application to run. That's this bit here. For this container it's just a shell, sh for shell. In the real world though right, this is going to be your application. Now then, if we kill that app once we've started the container, we also kill the container. And it makes perfect sense right? Think about it. A container's just an execution environment right? A set of name spaces for an app to run in. So if that app exits, well what's the point of the container anymore? There isn't one right? So exiting the app, exits the container. Simple, makes perfect sense. That means if we go into this container here, okay, and if I type exit, I'm telling the shell process to exit. And seeing as the shell is actually the app that the container is running, the container's going to exit as well. Quick check. No containers. If we run another one, so we're inside again right? But if this time we go Ctrl+P+Q see how we're back out, but this time the container's still running, 'cause we didn't kill the shell. That Ctrl+P+Q is a graceful way of getting out of a container without terminating its main process. Now speaking of main processes, we can sometimes start the container without telling it a process to run here on the end. And look, we've still got a shell prompt? Why? How come? Well every image has a default process that it'll run if we don't tell it something different at run-time. So if we inspect this image here. This is it here right? Cmd/bin/sh, that's what a container based on this image will run if we don't tell it something different. But like we said, we can override this when we start the container. A bit like we did actually with the sleep container earlier. And that's how this cmd instruction works. Anything that you tell it to run at run-time is going to overwrite what cmd says. Now if the container was built with the entrypoint instruction instead of cmd, then anything passed in at run-time gets appended to entrypoint as an argument. I hope that makes sense. Cmd instructions get overridden on the command line, whereas entrypoint instructions interpret command line stuff as additional run-time arguments. Okay, we've seen how to start, stop, and restart the container and how to exec into them. Let's just finish things up with a quick look at ports. And you know what, we'll show some love to Windows for this one. So I'm running all of this now on Docker for Windows on my Windows 10 lappy. If we go the usual docker container run, we'll put this one in the background. Call it web1 and we'll map port 80 here on the Docker host to 80 in the container. And we'll use iis and we don't need specify command 'cause iis runs automatically. So quickly, new container based on iis, which has a default web server listening on port 80. Great, well we'll map that through to 80 on the host as well. All right, straightaway we see a bit of wait while the image gets pulled. This one's multiple gigs in size right, but it's all compressed over the wire. If you remember back to when we talked about content hashes and distribution hashes. Anyway, let me supercharge this. Cool. Now a docker port and then the name of the container, that's going to give us a list of the port mappings. This is saying 80 in the container mapped to 80 on all interfaces on the host. Magic. We'll there's actually a bug at the moment that doesn't let you use the local host identifier with iis. But when they fix that, you'll be able to point your browser to local host and see the webpage. For now though, I need to get the IP of the container. Lovely and we'll hit that with a web browser. Beautiful, iis. So there we go, that's ports. And that's it right? So a quick cleanup before we look at logging. So this here's saying, docker container rm, remove or delete, and run it against the output of this block here, which is basically just a list of the IDs of all the containers on the system. And then we're forcing the operation so we don't have to do a separate stop first. Okay, cool. Let's go chat about logging.

Logging
Logging is important okay, so don't let it be an afterthought and definitely don't forget it all together. Anyway, we're interested in two types of logs, daemon logs and container logs. Sometimes we call container logs app logs, right? Well the daemon logs are simple. Theses are the logs from the Docker daemon, or the Docker engine, depending on how you refer to it. Now most modern Linux systems use systemd. In those instances, daemon logs get sent to journald and we can read them with a journalctl command. If you're not on Linux with systemd, you might want to check var/log/messages. On Windows, they go to AppData/Local/Docker and you can also check the Windows event viewer. But the daemon logs are the easy part. Container logs or app logs, this is where the action is. Starting with the basics. Docker's hoping that apps log to standard out and standard error. Basically, the PID1 process in every container is getting captured and it's getting forwarded. So design your containers so that your applications are running as PID1, and ideally, logging to standard out and standard error. If you logging to a file though, don't worry, it is possible to do sim links and shunt rights to those files to standard out and error. Or you could do something totally different and maybe mount a volume to those locations so that you can access them outside of the container and make sure they persist when the container's gone. Anyway, since about Docker 17. 05 or 17. 06 if you're using enterprise edition, Docker has supported the idea of logging drivers. These are plugins right, that integrate container logging with existing logging solutions like Syslog and Gelf and Splunk. The basic premise is to take the normal container logs and forward them to whatever logging solution you're already using and most of the major logging tools have a Docker driver these days. Anyway, you set your default logging driver for the system via daemon. json config file. Then any new containers will start using that driver. If you've got the occasional container with specific logging requirements, no sweat. Just give it the --log-driver and --log-opts flags when you come to start it. That'll override whatever's configured as default for the host. Okay, by default, most Docker hosts default to JSON file logging. This writes out logs to a simple JSON file format. And it's cool right, 'cause you can use the Docker logs command to view them. It's just docker logs and then the container name. And if you want, you can dash aft to follow them and all that goodness. As well as that though, it's really easy to configure journald if you're on a Linux host using systemd. Aside from those two though, JSON file and journald, which by the way, both work with the docker logs command. But aside from that, we start getting into third party specifics like configuring a Splunk server or whatever and we're not getting into stuff like that here. Suffice to say, application logging takes the standard out and standard error streams and forwards them to somewhere else, Syslog, Splunk, Gelf, whatever and it all gets done via logging drivers and we configure these in daemon. json. Brilliant. I'm thinking that's enough to get you going. Let's do a quick module recap.

Recap
I'm going to make this quick, because I've already talked longer than I planned. Containers are run-time cousins of images. Under the hood, they're an isolated application execution environment. We know it by now, it's a bunch of grouped name spaces that look and feel like a standalone OS. But containers are OS specific. There's no running Linux containers on Windows and vice versa, at least not without some magic going on behind the scenes. 'Cause the thing is, containers don't contain a kernel. They have to talk to the kernel of the host and Linux containers need a Linux kernel and Windows need a Windows kernel. We also talked about containers being a thin writeable layer that's gets latched on top of a read-only image using a combo of union mounts and bit of copy on write. Brilliant. It's also cool if they can be ephemeral and immutable. Ephemeral being short lived and immutable meaning, we don't really want to logging onto them and messing about. The idea, right, is once they're deployed we should leave 'em alone. If we need to do a fix, you know what, build a new image and deploy a new version. Ohh, keeping it short, what else? Yeah, we said the main process inside of a container is what it's all about. That process is usually going to be your application right? Well kill it and you kill the container. And logging as well right? That's also about that main PID1 process. And that's it. Well it's not. I mean we're only just getting to the good stuff. Coming up next, we're going to see how ridiculously easy it is to build a secure swarm and start doing some orchestrating. Bring it on.

Building a Secure Swarm
Module Intro
Managing a single docker node and maybe a couple of containers on your laptop, well, that's one thing, right? But managing a bunch of nodes and tens or hundreds of containerized apps, wow, that is a whole other thing, believe me. Well, enter Docker Swarm, and Kubernetes. Here's the plan. We'll paint a big picture, so what's all this orchestration about, and what's a Swarm, and what's Kubernetes. Then, we'll do a bit of a deeper dive into a Swarm cluster. Then we'll build one and proper secure, right, with encryption, Mutual TLS and all of that jazz. Then we'll look at Orchestration, but just a bit, right, because we've got a whole module on that later. But it'll be a good primer. Then we'll finish up by doing our usual of reminding ourselves the stuff we've forgotten. Now then, if you're studying for the Docker Certified Associate Exam, you're going to pick up the following, right? In Domain 1 under Orchestration, complete the setup of a Swarm cluster, with managers and worker nodes. Demonstrate steps to lock a Swarm cluster, and paraphrase the importance of quorum within one. Then under Domain 5 Security, describe Mutual TLS. Alright, buckle up.

The Big Picture
I want to keep this short, okay? It's a deep dive course, and I'm hoping you've already got the basics. Anyway, Swarm is the future of Docker, seriously. Now, I know that might sound weird, especially when Kubernetes is looking for all the world like the industry standard Orchestrator for containerized apps, and don't get mad at that if you love another orchestrator. It's just an observation. I mean, feel free to disagree, and I'm happy to have it out on Twitter. Anyway, despite the rise of Kubernetes, including its native support in certain versions of Docker, Swarm is absolutely still at the heart of everything Docker is doing. Stick with me for a second, alright? The reason is Swarm has two parts. The Secure cluster part, and the Orchestrator part. And it's this secure cluster part, here, that is absolutely central to the future of Docker. Especially Docker Enterprise Edition and the higher-level stuff that's going on there. Okay, I get it, Swarm's obviously not even a factor in the lower-level stuff like containered and the OCI, right? But the higher-level value-add stuff, including native support for Kubernetes, Swarm is front and center. So that's the clustering bit of Swarm. The Orchestrator bit, eh, okay, that's maybe not quite as strategic. I mean, it's not about to disappear, so don't panic if you're running it in production, it's just, going forward, it's my guess, right, that the Swarm orchestration bits are going to give way to Kubernetes. Anyway, the Secure clustering bit's absolutely key to the future of Docker. But that begs the question, what is a Secure Swarm cluster? Well at the highest level, it's a cluster of Docker nodes. We've got managers and workers, and everything secure. So we've got Mutual TLS where workers and managers mutually authenticate each other, and all of the network chat is encrypted. Plus, the cluster stores encrypted, and it gets automatically distributed to all managers. And we can use labels to tag nodes and customize the cluster how we want it. Anyway, once we've got the cluster, then we can start scheduling containers to it. So instead of running individual Docker container run commands against specific nodes, and every time having to think about which node we should be running them on, well instead of that, we just throw commands at the cluster, and we let Swarm decide. So Swarm does all of the workload balancing and the likes. Great well, we can run two types of work on the cluster. Native Swarm work, and Kubernetes. Though, at the time of recording, right, you can only run Kubernetes work on Docker Enterprise edition. That might change, I know that. I'm just saying for now, if you want Kubernetes on Docker, it's going to be Docker Enterprise edition. But that's the big picture. A Swarm is a secure cluster of Docker nodes. I don't know, right? Kind of like a giant Docker node, in a way of sorts. Anyway, once we've got it, we run work against it. Native Swarm or Kubernetes. Pretty sweet. Okay, now that we know the basics, let's peel back the covers on the clustering side of things.

Swarm Clustering Deep Dive
Alright, first up, Docker is a set of nicely packaged tools. If you pop the hood, you're going to see a bunch of smaller tools. Docker just bundles them, and wraps them in a slick API. So stuff like the Mobi engine and containered, runC and SwarmKit, they're all separate tools, right? But bundled and slightly integrated for that slick Docker experience. Magic. Well, one of the smaller tools is SwarmKit. It lives here on GitHub, and importantly, it's what powers Swarm mode. Hmm, Swarm mode, what's that, then? Okay, back in the day, Docker had this separate orchestration piece called Swarm. You'd installed Docker, then you'd lay a Swarm on top. It was okay, as long as you didn't mind frying your brain configuring it. The thing is, though, it kind of led to a separate toolkit called SwarmKit. The idea being to build an open source all of the small component tooling so that people can pick and choose what they want. Okay, well, in Docker 1. 12, SwarmKit got integrated into the overall Docker package. I mean it's still available as a separate toolkit, it's just as of Docker 1. 12, it's fully integrated into the Docker engine. The point being, right, ever since 1. 12, Docker's had this notion of Single-engine mode, and Swarm mode. Single-engine's where you install individual Docker instances, and you work with them all separately. Swarm mode, though, that's where you bring them all together as a cluster. And like we've seen, we can start throwing work at the cluster, instead of having to hand pick a node for each and every container; brilliant. Well, for some people it was. Others, let's just say not so much. Anyway, look, any node running as part of a Swarm cluster is in what we call Swarm mode. Not in a cluster: Single-engine mode. So let's go through building a cluster. We've got a node here, Docker installed. Throw a simple command at it, and presto, we've got a Swarm. And that node now, is flipped into Swarm mode, yeah? Now then, behind the scenes, oh my goodness, there's a ridiculous amount of magic just happened. You see, this node's now the first manager of the Swarm. And the first manager in any Swarm is automatically elected as its leader. And that makes it a bit special. For starters, it's the root CA of the Swarm. I mean, you can configure external CAs if you need, just parse in the --external CA flag. But if you don't, this one gets the job. It's also issued itself a client certificate, built a secure cluster store, which by the way is ETD, and that's automatically distributed to every other manager in the Swarm and it's encrypted. And all of this for free, right? As in, we didn't have to do anything. It's done for us, including a default certificate rotation policy. Epic. Oh good grief, there's more. It's also created a set of cryptographic join tokens. One for joining new managers, and another for joining new workers. So let's join a new manager. We take another node running Docker in Single-engine mode, and we run a Docker Swarm join command on it. We give it the cryptojoin token for managers, and that's it, it's part of the Swarm. And obviously operating in Swarm mode. And see how the cluster store's been extended to it. Oh, and it's been issued its own client certificate, right? That identifies who it is, the Swarm it's a member of, and its role as a manager. Sweet. Same again for a third. Now then, hmm, the picture's getting a bit busy. There's a lot going on, though, so stay with me. Swarm managers are automatically configured for high availability, so any of these can fail, right? And the cluster keeps going. Behind the scenes, though, only one of them is truly active, and that's the leader, which is important, right? Every Swarm has a single leader manager. The others, we call them follower managers. It's rough terminology if you know your Raft. Anyway, when you issue commands at the cluster, go ahead, you can issue them at any of the managers. It's just if you hit a follower manager, it's going to proxy commands to the leader. Now, if and when a leader fails, we have an election. And as with all elections, the one with the biggest budget wins. (laughs) No, I'm just kidding. One of the followers gets elected as a new leader. But all of this is handled by Raft, so within a Swarm, okay, all that distributed consensus stuff is done by Raft. The same as Kubernetes, actually. Raft's the go-to solution for distributed consensus these days. Now then, the ideal number of managers is three, five, or seven. Any more than seven, and you can end up spending more time thinking about decisions than actually acting on them. It's like deciding what to eat. If there's a handful of you it takes two minutes. If there's 20 of you, man, you can kiss goodbye to half of your night arguing. And it's the same with Swarm. So three, five, or seven for managers. Just make sure it's an odd number. That increases the chances of achieving quorum, and therefore avoiding split brain. You know, where you've got a network partition or something, and you end up with an equal number of nodes on each side in a stalemate or a split brain. Neither side has got the majority or quorum, right? So updates stop happening. Badness, we don't want to go there. So an odd number. And, actually one is better than two. Two just increases the chances of that one of them will fail. And when it does, it offers absolutely nothing to help. So yeah, one, three, five, or seven, but preferably not one. (laughs) Now, as cool as Raft is, right, it's not a fan of slow or unreliable networks. I mean, who is, right? But this is important. Connect your managers over decent, reliable networks. So for instance, okay, and this is just an example. If you're in Amazon Web Services, don't go putting them in different regions. Across availability zones within a region? That's probably alright. But going cross region is just asking for pain. Okay, well look, adding workers is the same. Docker Swarm join again, only this time give it the Worker join token. And we can have a mix of Linux and Windows. Great if you're running hybrid apps. Now when you join a worker, it does not get access to the cluster store. That's just for managers. But what each worker does get is the full list of IPs for all the managers. So if one of them dies, the workers can just talk to the others. Oh, and they all get their own certificates, right? Gosh, I hate this diagram. I hope it's valuable, because it's ugly as heck, and it took hours to create. Anyway, like with the managers, the certificate identifies who the worker is, the Swarm that it's a member of, and what its role is: worker, yeah? Now then, the workers do all the application work. And on a Swarm that's either a native Swarm work or it's Kubernetes, and we'll look at these in more detail later. But I think for now, that's a Swarm. A cluster of managers and workers, a full-on PKI where the lead manager is the root CA, and it issues every node with a client certificate that gets used to a mutual authentication, role authorization, and transport encryption. I love it. And on top of that, we've got a distributed encrypted cluster store, cryptographic join tokens, and loads more. And the best bit, it's all just built in and works out of the box. Take a minute to let that sink in. I kid you not, it's something special. Secure out of the box, and easily configurable. Thank you, Docker. And I don't care if that makes me sound like a fan boy, that is genuinely some seriously good stuff. Enough theory, though, let's go and build one.

Building a Secure Swarm
Now then, after all that about what's going on behind the scenes, there's a pretty good chance you're going to be underwhelmed by this. Or, actually, I don't know, maybe you won't. Maybe you'll see it for what it is and be impressed by the sheer simplicity. We'll see. So I've spoken at a few events where I'm at the front and I ask for a couple of volunteers. One who likes cookies, and one who's never deployed a Swarm before. Then I put up this slide, and I tell everyone that the Swarm node's going to build all of this before the other guy can eat five cookies. And the Swarm guy wins every time. Well, except for this one time when the Swarm guy couldn't use the touch pad on my laptop, but never mind. I'm on a Linux node, and it's currently in Single-engine mode. We can see somewhere here, yeah, Swarm inactive. That means Single-engine mode. So to initialize and use Swarm, make this the leader in CA, issue a client certificate, create distribute and encrypt a cluster store, and create the join tokens and certificate rotation policy, we go docker swarm init. That's it, done, serious. Check this again. Okay, we are Swarm active. Then we've got a crypto NodeID, it's a manager, bunch of other stuff, Raft, and then the CA config. A quick Docker node LS tells us it's the only node in a Swarm. It's ready, active, and it's the leader. Alright, but a single-node Swarm's a bit boring. Let's add some more managers. Well, we need the manager join token for that, so Docker Swarm join-token manager, we can put worker here for the worker token. But that there's the full join command token and all, so we'll have that, thanks. Switch to Node2, and that's it. Yeah, a two-node Swarm. You can run Swarm commands like this on any manager node. And Node2 here is showing as Reachable in the manager column. That means it's a follower manager, right? Leader shows Leader, followers as Reachable. Now, of course it's got its own client certificate, and a copy of the cluster store. But two's the worst number for managers, right? Well, maybe a hundred's worse, but two's bad for split brains. So let's have a third. (laughs) Easy as pie, right? And, two becomes three. And three's a decent number, so a three-node Swarm with three managers configured for HA; love it. Well, adding a worker's the same. We just need the worker join token this time, have that, and there we go, and it's a worker, right? Now, workers cannot query the cluster store. There we go, but if we run that on a manager, here, right, four nodes. And the empty manager status field, here, means that this node is a worker. Now, we can take that same join command and token, and run it on a Windows node. (laughing) And that's it. A hybrid Linux-Windows Swarm. Brilliant. Now, we've got the join command and the worker token on the clipboard. So let's have a go at rotating that worker token. I don't know, assume it's been compromised. Well, rotating it is just a Docker Swarm join-token, tell it to rotate, and worker. Got to run it on a manager, though, okay? But that there is the new token. So if we try and join another worker here with the old on that we've got on the clipboard, (buzzer sound) no-no to the join-o. But look at the Swarm here. Existing node membership is unaffected. So the old token can't be used any more, it's rotated out, but existing workers, ones that were joined with the old token, they're cool, they don't get bumped, they get to stay. Now, if we want to look at the client certificates, we just run this OpenSSL command. And, in the subject field here, the organization is the Swarm ID, the organizational unit is the node's role, Swarm manager, and the canonical name is the cryptographic node ID. Brilliant, well let's have a look at the join tokens now. First up: Swarm token here identifies what it is. And you can pattern match on that to stop them being accidentally posted on your website, or whatever. Then this long string is a hash of the cluster certificate. Mutual authentication, right? And then the bit after the dash is the bit that determines if it's going to be admitted as a worker or a manager. So we see that the two are identical, right, except for this last field. The bit before the dash, that obviously matches, because they're from the same Swarm. It's the bit at the end that rotates when you rotate tokens. Now then, we've got all this security going on, Mutual TLS and encrypted this and that. And it's all good, yeah, but restarting a manager, or restoring an old backup both present a couple of concerns. So Docker gives you the option to lock a Swarm. It's called Autolock. At a high level, it stops restarted managers from automatically rejoining the Swarm. And then subsequently loading the encryption keys into memory and decrypting the Raft logs and the likes. It also stops you from automatically restoring an old copy of the cluster config. And when I say stop, I mean it makes you enter the unlock key first. Let's have a look. First up, right, Autolock is not enabled by default. So you either give the Docker Swarm init command, the --autolock flag, or you do what we're going to do, and you Autolock an existing Swarm. So we're in a manager, here, and we go Docker Swarm update, then --autolock equals true. And that's the unlock key. Keep it safe, yeah? Anyway, this Swarm's locked, so if I go, service docker restart, it's only for managers, right? Because they get access to all the privileged stuff. Anyway, let's inspect the cluster. Ah-ha so, we're not back in, yet. Everything's still protected. So like the Raft logs on the node, and the likes, they're still encrypted, so safe. Well to rejoin the Swarm we go docker swarm unlock, and then we give it that key. Should be good now if we try that again. And yeah, we are good. So that's Swarm Autolock. And it only affects restarting managers. Workers are not impacted. Oh, and the process of joining a new manager? That doesn't change, just join them as normal. And, I think, yeah, I think that's all I wanted to say. Two simple commands to create a solid, secure Swarm. Docker Swarm init to initialize a new cluster and set up all the required constructs and security stuff, then docker Swarm join to add new nodes. Oh, right yeah, to update the certificate expiry time, we just go docker Swarm update again, - -cert expiry, and then whatever you want. Two days yeah, whatever. Quick docker system info, and there we go: two days. Great stuff. Alright, time for a quick look at Orchestration. But it will be quick, right, because we've got a whole module on it later in the course.

Orchestration
A quick high level on Orchestration, really quick. Doing anything at scale demands automation. There's no way you can individually manage tens or hundreds of nodes and hundreds or thousands of containers. Billions, if you google. You need help. Something that's going to self-heal and do smooth updates and rollbacks and stuff. Well, Swarm and Kubernetes do all that. So in the Docker world, we start out with a Swarm cluster. We know all about those. A bunch of Docker nodes clustered together as a kind of pool. Once we've got this, we can deploy applications onto it. Here Swarm, go run my app for me. And we can be declarative. So, here Swarm, run my app for me, oh, and while you're at it, make sure there's always four containers supporting the web front end. Cheers! And it deploys the app and records the desired state of four web front-end containers. And that's the app running. But things break, right? And Swarm and Kubernetes know this. So they're constantly observing the cluster and any time the actual state diverges from the desired state, maybe a node's failed and we've dropped from four web front ends to three, well Swarm and Kubernetes step in and self-heal. Now, there's other stuff like balancing the work across the cluster and rolling updates and stuff. But we've got a whole module on this coming up, so that's enough for now. Let's have a quick module recap.

Recap
Okay, a quick refresher. Swarm is integral to the future of Docker. But there's two main aspects to it. There's the Secure clustering, and then there's the Orchestration. And while both are important, I think the secure clustering is what is integral to the future to Docker. Because on the Orchestration side, it looks for everything like Kubernetes is dominating. So, on the clustering side, we add nodes as managers and workers. Managers are in charge of the cluster, and they make all of the Control Plane decisions. Workers, they do all of the application leg work. Great, well we build a cluster with a simple docker swarm init, and thrown in for free we get a boatload of security stuff. Stuff that used to be so hard, we rarely bothered actually doing it. But in Swarm, it's the opposite. All the security stuff is just configured out of the box. A root CA, Mutual TLS, an encrypted store certificate rotation, secure join tokens, all the good stuff. And it's all built with that single command. Well, adding more nodes is another simple command. So we can literally go from zero to secure cluster in way less than 60 seconds. Then once we've got the cluster, we're ready to rock and roll with app deployments. In that space, we can go with native Swarm, or Kubernetes and we'll talk about them in more detail in a later module. Coming up next, though, we're going to look at Container Networking.

Container Networking
Module Intro
Networking. Now, networking's vast, and it's complex, and to be honest, probably deserves its own course. Well about that, it's got one. How about that? So in this module of this course, I'm giving you the CliffsNotes version. If you want deeper knowledge, and who doesn't right? Well, hit this other course here, it's got all the details. And if you're following along in the container management learning path that we've got, kudos, right? It's already a part of that. Anyway, here's the plan for now. Wait? No big picture? That's right, no big picture. I like to keep you on your toes. We're straight in with networking types, then it's network services, and then it's a recap. But remember, we're scratching the surface here, right? Enough to get you going. Get over to the full Docker networking course for the gory technical details. Now then, if you're studying for the Docker Certified Associate exam, we're going to nail these objectives. And they're all from Domain Four: Networking. So, create a Docker bridge network for a developer to use for their containers, publish a port so that an application is accessible externally, identify which IP and port a container is externally accessible on, describe the different types and use cases for the built-in network drivers, and deploy a service on a Docker overlay network. Now, we're going to cover more than that. These are just the bits from the exam. Let's go.

Network Types
Containers need to talk to each other. And guess what? Sometimes they even need to talk to VMs and physicals and even the internet, who'd of thought, right? I know, I know. And on the flip side, sometimes those VMs and physicals and the internet? They need to talk back to containers. This is new stuff, right? Well, to help with that, we've got a bunch of container networking options. First up, there's the bridge network, sometimes we call it single-house networking. Anyway, it's the oldest and it's the most common, and honestly, it's the crappiest, but it's turned on by default, so we'll cover it. So you've got a host and it's running Docker, yeah? Windows or Linux, we're all good. And it's got a built-in network called Bridge, or Nat on Windows. Now, if you hear people talking about Docker zero, they're talking about this, the default bridge network. Anyway, you can create more of your own if you want, knock yourself out. But each one's an island, so we plumb containers into it and each one gets its own IP on that bridge network. Sweet, they can all talk to each other. But containers on separate bridges? That's a bit of a hassle, because each one of these bridges is isolated, so these two that we can see are isolated layer-two networks, and it'd be the same even if they were on the same host. The only way to get in or out is to map ports to the host. Straight IP-to-IP at layer two or three, that's a negative, Ghostrider. And it's the same for connecting to the outside world. It all requires port mappings, so it's okay, but really, you know what? It's a bit pants, not very good. And to overlay networks, oh yes. Now, you'll hear them called multi-host networks and although they're becoming commodity these days, they are powerful as heck. So instead of isolated bridges scoped to a single host, an overlay is a single layer-two network spanning multiple hosts. And the cool thing? It doesn't matter if these are all on different networks down here. It's a beautiful thing, and it is so simple to build. No wonder the hood, it's a networking tech fest. If you want all the gory VXLAN stuff, remember, hit the full networking course. The thing is though, it's a single command to create one of these and then you just attach containers, so in the example here, all three of these containers can talk to each other as if they were side by side. And it doesn't stop there. Encryption is a walk in the park. The control plane, that is encrypted out of the box and encrypting the data plane, it's a single command line flag. It is the future. In fact, it's the now. But the built-in overlay is container only. I mean, what if you need your containers to talk to VMs or physicals out on your existing VLANs? Well, enter MACVLAN or transparent on Windows. This gives every container its own IP address and MAC address on the existing network meaning containers can be visible as first-class citizens on your existing VLANs, no bridges and no port mapping, directly on the wire if you will but it requires promiscuous mode on the host nick. Boo, if you're living in the public cloud because cloud providers generally don't allow promiscuous mode. So, maybe take a look at IPVLAN for that, it's similar but it doesn't require promiscuous mode though it's not fully baked, it's been in experimental since I don't know, like about 1. 12 and I'm not holding my breath for it to make it into the stable channel any time soon. Anyway, sounds simple enough but talk is cheap, let's go and create some. So, I'm on a Docker host and it's a manager of a three-node swarm. So, if we take a look at the networks that we've already got, this one here's the default bridge. That means any new containers that we create will go onto that network unless we tell Docker differently and if we have a closer look at it, bridge, all right, name and ID, scoped locally using the built-in bridge driver, the original driver I suppose and then containers are going to get IPs from this range and speaking of containers, none. Now, remember that it's called Nat on Windows and it's going to look like this. Pretty much the same other than the name and the driver. Well, back here let's run a container. Just something in the background. Now then, note that we're not telling it which network to use, it's going to use the default. So, if we inspect that network again, now we've got a container here and look, it's got an IP but it's pretty much stranded on this host or this bridge actually. To talk with the outside we need to map a port on the host to a port in the container and I think the web service is the easiest example, so if we do this, Docker container run, we'll run it detached, we'll call it web and then we'll map port 8080 on the host to 80 in the container and we'll base it off of nginx. So, nginx runs a web server on port 80, that's inside the container so that's a container port here and when we're mapping that onto port 8080 on the host. Give it a second. Okay, now the Docker port command shows container port mappings and you just give it the name of the container. Okay, 80 in the container to 8080 on all interfaces on the host meaning if we point a web browser to our host, this is the IP of my host, and the host port was 8080, we get nginx. That's nginx running inside the container. Cool, to create a new bridge network let's just Docker network create, tell it to use the bridge driver although this is the default, so you can leave it out and then we give it a name. It's a bridge, golden-gate, never mind. Docker network ls and there it is. So, to run containers on it, we just, let's find one of these old ones, okay, we just add the network flag. Boom, that is on the Golden Gate Bridge. Yeah, never mind. Okay, let's switch tact to overlays. For these to work we really need swarm mode. The reason is they leverage a bunch of the swarm security stuff but that's all behind the scenes, we just give it the normal Docker network create, tell it to use the overlay driver and we give it a name. List them again. Right, there it is. It's an overlay and it's scoped to the swarm. Now, this scope to the swarm bit means it's available on every node in the swarm, multi-host, remember. So, we can create new containers and services on any node in the swarm and attach them to this network. So, let's do it. All right, we've got a command tick creating a new native swarm service. We're calling it pinger, creating two tasks or replicas, putting it on the new overlay and telling it what to run. Off it goes. So, we'll check that. All right, we've got the service with two replicas and because they're on the same overlay, they can ping each other, so if we just check which nodes they're on, okay, two and three, so we'll switch over to node two and let's inspect the network. All right, on the containers here there's the pinger task and this is its IP and we'll park that up here in the corner because we're going to ping that in a second but over here on node three, let's find the details of that other replica, okay, this is the pinger task, so we'll have an exec session into that, please, and time for the moment of truth. Paste that in and there you go, we're pinging a different container on a different host using the overlaying network. How easy was that? Brilliant, but you know what? Let's switch gear for a bit, though, and look at some fundamental network services.

Network Services
A couple of built-in network services are service discovery and load balancing. Service discovery is all about being able to locate services in a swarm and load balancing, that let's us access a service from any node in the swarm, even nodes that aren't hosting the service and it's well right, it balances load across them. Let's look at service discovery first. The premise is pretty simple, every new service gets a name. That name gets registered with DNS on the swarm and every service task so every container in a service gets a DNS resolver that forwards lookups to that swarm-based DNS service. Long story short or net net, all swarm services are pingable by name, with on caveat actually, so long as you're trying to ping it from on the same network. Shall we see it? Well, we've already got this overnet overlay network. But let's clean all the services up, we want to start from new. Okay, now let's create a new one. Put it in the background and we'll call this one ping. Put it on our overlay network, we'll have three replicas, thanks, and we'll make it a simple alpine sleeper, no fancy apps to cloud the detail. Great, let's have another one only we'll call this one pong, yeah, you get what's I'm doing but this is all super simple. Two services, ping and pong and critically they're both on the same overlay network, so if we look at the containers running on this node, this one's here from the ping service, so if we exec onto it, we should be able to ping pong. It's stupid I know but look, it works. We're on the ping service and we can locate the pong service by name. Excellent. So, that's service discovery. Auto magic, right? Create a service and give it a name. That registers it with the swarm DNS and every other service in the swarm on the same network can find it using that name. But let me be clear, it is network scoped meaning services on different overlays, they can't find each other. More detail in the Docker networking course. Load balancing though, this is a couple of things, first, ingress load balancing. That's where external clients can access a swarm service via any of the nodes in the swarm and like I think I said before, those nodes don't even need to be running the service, so say we've got like, I don't know, 10 nodes in a swarm, and a service with just two replicas, you can still hit any node in the swarm from the outside, even a node that's not running the replica from the service and still reach the service. Coolio. The other aspect is load balancing work across all replicas in a service. Let's have a look. So, we'll create a new service here, call it web, stick it on the overlay and just the one replica, thanks. Now, that's the default if you leave this out, I just want to be explicit for the course and then this. So, -p maps a port and because this is a service, it maps this port swarm wide, so on every node in the swarm and that's important because we're going with just one replica, so we've got three nodes in the swarm and we want to be able to hit this one replica from any of those nodes. So, that means 8080 on every node in the swarm gets mapped back to 80 on this service replica and we'll just run a default nginx again. Okay, let's have a look. All right, what we're interested in is this bit here. It's published swarm wide on 8080 heading to 80 on the service containers and it's published on the ingress network, so that's the swarm wide bit. Every node in the swarm gets the ingress network. So, I think we're ready. Services running, this here's the swarm, three nodes, so I can pick any of these, yeah, we'll have this, and hit on 8080 and there it is, nginx but you know what? I could have got lucky and hit the node with the replica, so I'll tell you what, let's just grab a different one here, and we'll try that. Nginx, so that's ingress load balancing. Now, there's internal load balancing as well, so if say we've got 10 replicas running, Swarm's going to do some simple DNS-based load balancing so that all requests get moderately nicely balanced across all 10. But this stuff's cool, right? Because it lets you point an external load balancer at any node in the swarm without having to tell it which nodes are hosting the service and it just works. Brilliant. Let's do a quick recap.

Recap
All right, networking's complex, we know that. Well, Docker makes container networking simple. Out of the box we know it provides a bunch of drivers for different use cases, the bridge driver's find for development and really simple use cases, it's all single host and external access requires port mappings. I'm not a fan of it. Overlay's way better. This is proper multi-host networking. So, it creates a multi-host, layer two network so that containers on different hosts can easily join the same secure network. And it's the bizo but it's container only. If we want to plumb our containers into existing VLANs, we want to take a look at MACVLAN, transparent on Windows. This gives every container its own MAC and its own IP address on your existing VLAN but it requires promiscuous mode on the host adaptor and that's just not happening on most cloud platforms. Well, there's also built-in network services. Service discovery makes every service on the swarm discoverable via a built-in swarm DNS and load balancing? This makes it so that every node in the swarm knows about every service. Cool thing, right? It lets us point external load balancers at any or in fact, every node in the swarm if we want and no matter which node we hit, we reach the intended service and the stack's pluggable, so you can plug in third-party drivers for things like IP address management and maybe different network topologies and last but not least, this was the CliffsNotes version. We cover all of this and more in all of its splendid detail in the accompanying Docker networking course. Sweet, well, coming up next we're going to look at volumes and persistent data.

Working with Volumes and Persistent Data
Module Intro
Okay, containers are the bomb when it comes to non-persistent. They're here today, gone tomorrow and they do immutable as well, where they roll one out and then we never touch it. If we do need to touch it, we don't, we create a whole new one instead and push that out. And that's all great, only it's not great if you've got data that you need to change and you need to persist and let's face it, who doesn't have at least some of that? So this is where volumes come into play. Volumes are a great way to store persistent data and they're nicely decoupled from containers. Anyway, here's the plan. We'll do a big picture, what's all this persistent and non-persistent stuff anyway yeah? Then we'll look closely at volumes, create them, list them, delete them. Then we'll see how to hook them into containers and we'll finish up with a recap. Now as we crack on, we'll cover some stuff from the DCA exam, in particular from domain one orchestration, mount volumes. And domain six, storage and volumes, describe how volumes are used with Docker for persistent storage. Okay, let's do this.

The Big Picture
From a big picture perspective, containers are ideal for non-persistent, ephemeral stateless stuff, talk about buzzword bingo. But you spin up a container, it does a job, maybe it performs an authentication, checks an item stock, I don't know, tracks a shipment yeah? Anyway, then it goes away. It doesn't need to stick around and it doesn't generate any lasting data. Got it. But they're also great for immutable design patterns, that's where we deploy something and then it's hands off, never to be touched again. If you do need to change it, you don't log on and live edit, you just build a new one and deploy that. As well, right, you're unlikely to have this precious long-lived container that you need to migrate to a new data center, that's just not containers. They're usually short-lived, immutable and don't generate data you want to keep. Okay, bringing it back onto topic though. When talking about data, there's broadly two types, persistent and non-persistent. Persistent is the stuff we want to keep, customer records, orders, audit stuff, you know the type. Non-persistent though, that's the stuff we don't care about and I think it's pretty obvious containers are a cracking fit for the non-persistent stuff which is fine, but then what do we do about all that persistent stuff? We've all got it. Well that's where volumes come in. Backing up a bit though right, in the Docker world, every container gets its own non-persistent storage. It's free and it comes with every container. Usually it's on local block storage managed by the storage driver or the graph driver, yeah? Well focus is usually on performance and this is what usually does the container's union file system and all that magic. Persistent storage is different though. We call this volume storage and we have to specifically create it and as such, right, it leaves outside of the world of the graph driver so away from all that union mount stuff. Now, generally speaking a volume's a directory on the Docker housed that's mounted straight on the container at a specific mount point. But behind the scenes, it can be sitting on your fancy Dan high-speed uber resilient SAN or NAS with all the bells and whistles, yeah? Just so long as your storage system has a Docker volume driver, it's going to work. But you know what, right, it's two things really, yes it can be plugged into some high-performance, highly-available backend right? But it's also an object in Docker that is managed and handled on its own entirely separate from containers. So when you spin up a new container, Docker container run or Docker service create, yeah, the usual commands. Well each one auto-magically gets its own non-persistent graph driver storage, this is the copy-on-write union mount stuff. On Linux, it's carved out to var lib docker and on Windows it's C, program data docker windows filter. It's local block storage, right, and it lives and dies with the container, ephemeral but it is tied to the life cycle of the container. Volume data though, that's different. This exists outside of the container space and has its own Docker volume sub command but it seamlessly plugs into containers and services whilst still being fully independent. What it means, right, is we can create and delete containers without touching volume data, which is cool right? I mean it gives us a clear line of demarcation, we can start, stop, update and even delete containers and any volume data that they've been using is un-impacted. It also means we can attach a volume to more than one container, though you do need to be careful to avoid corruption. But it's doable and it can be all backed onto your D Dew Pin compressing, self-replicating net app or EMC or whatever's your thing yeah? But I feel like I'm waffling, it's time to see this.

Managing Volumes
So to cement this idea that volumes are fully independent of containers, there's a full on Docker volume sub command and as we can see, it follows the usual syntax. So an LS shows us that we've currently got none. Well to create one, it's just Docker volume create, we'll keep it simple and just give it a name, looks good. Yeah, there it is, cool. Now we can inspect it as well. Alright, a few things. We created it with the default local driver, it got created on the system under here and it's scoped locally. I tell you what let's create another one. List them both now. And if we look under here. There they both are, so those are our volumes. Now to delete them, it's just Docker volume rm and then the names. And those should be gone, we'll just check. Yeah, totally gone. Now, we never actually did anything with them so it's a bit like, meh, what was the point Nigel? Well the point was this, number one to show you the Docker volume command but also to back up what we said about volumes being totally independent of containers. I mean we created, listed, inspected and then we deleted two full on volumes without even touching Docker run or Docker service create. So yeah, Docker volume, your one-stop shop for creating and managing volumes. Now coming up next, this is when we'll see how to use them.

Attaching Volumes to Containers
Okay let's run a new container with a volume. Right now we've got no volumes, right. So if we go Docker container run, make it detached and interactive, I think we'll call this one voltest and then we give it the mount flag, this is how we attach a volume. So we'll say source is ubervol and we'll mount it to slash vol and the container and we'll use alpine latest. Okay, so a few things to note, I think first up right, we've obviously telling it to use a volume that doesn't exist, I mean we just showed that we haven't got any volumes right? So if you specify a volume here that doesn't already exist, Docker's going to create it for you, which is alright but if you think it does exist, like maybe you're trying to specify an existing volume and you make a typo or something, well you're not going to know straight away. because instead of saying, wait hang on, that volume doesn't exist, Docker's just going to create it so be aware of that. Anyway that should exist now, right? Yeah. Now the second thing to note is that the syntax is like this, source tells us the name of the actual volume and then target tells us where in the container to mount it. Anyway it's a new volume so it's squeaky clean, but because they live in the Docker-host file system we can inspect them directly here. Now in fact any data that we put in them goes in this data directory. Okay, empty right now. Well I tell you what, let's jump onto that container and take a look. Normally we don't do stuff like this, yeah? Immutable and all but you know what, we're cool because it helps with the demos. Okay and we mounted it to slash vol and we're squeaky clean here as well which is expected, right? Now then, if we write some data to it put it into a new file let's just make sure that took. Right, now we've got data. And if we look on the host again, we should see it here as well. Alright, there it is. So see how this is just a host directory mounted into a container, yeah? Well you know what, because it's managed separately from the container, if we stop and remove that container but the volume should still exist, which is does so fully independent, not tied in any way to the life cycle of the container. Meaning if we start a totally new container, I'll find the command for the old one, this time we'll base it off of nginx, you know what let's mount it to app this time as well and we'll call the container volmore. Okay so you know what, let's take stock for a second. We created a new volume called ubervol and we attached it to an alpine container, great. We wrote some data to it, then we deleted the container. But volumes are independent of containers right? So the ubervol stuck around. Well, now we just started a totally new container and attached the same ubervol into it meaning, right, any data that we created before should still exist in the exact same condition. Let's test it right, so if we jump into this new container okay, and we mounted it to app this time. Okay and it was called newfile. And there it is, exactly how we left it. In fact, let's add some more data. Okay, now if we back up to the host here and if we find that cat command, there it is, that's our data. And you know what because it's managed separately to containers, we should be able to delete it, or should we? What do you reckon? Eh-eh. It's in use. So as long as a volume's in use by a container, you can't delete it, it's a safety latch. But if we delete that container and then try that volume delete again, okay this time it goes and hopefully it's removed from the file system as well? Yeah, so that's the crux of volumes, persistent storage for containers but managed separately with the Docker volume command. And they work with services as well, just slap the dash dash mount flag on the Docker service create command, exactly how we did with Docker run. And you can use them in Docker files with the volume instruction. But that's volumes and the model's pluggable so you can integrate with external, third-party storage systems using plugins and drivers. I think time for a quick recap.

Recap
Okay, recap. We said all containers get this local graph driver storage, it's what stacks the image layers and adds the writeable container layer but it's bound to the container so you delete the container and the graph driver storage goes with it. Which is fine for some workloads, not so much for others. So for containers that create persistent data, we need volumes. These operate outside the graph driver and have a life cycle totally independent of the container, so you can start, stop and delete containers, attach volumes to them and when it comes time to delete the container, the volume stick around and obviously any data written to it. There's also this whole Docker volume sub command for managing volumes, and thanks to the plugin architecture, volumes can exist not only on the local block storage of your Docker host, but also on high-end external systems like SAN and NAS. So if you've got something enterprise class with dedupe and compression and pin provisioning magic and all the nine's availability, as long as it's got a Docker storage driver, you can plug it in, sweet. Alright well that's us done with storage and volumes. Coming up next, we've got secrets.

Working with Secrets
Module Intro
You know what? I'd love to talk to you about this, what's on the slide there, but I can't. It's a secret. Sorry, that was bad. I know, really bad. Anyway, secrets, here's the plan. We'll start out with a big picture. I mean, what even is a secret and how do they work? All the theory stuff, yeah? Then we'll do a demo on the command line to reinforce that theory. After that, we'll deploy a secret into a WordPress app, but we'll do it with Universal Control Plane, the Docker web UI. Now, that's part of Docker Enterprise Edition. The commercial, "stick your hand in your wallet" edition, yeah? And I fully appreciate that not all of you are going to have access to that. But the focus is not on the UI, it's going to be on the app and how the secret fits in. I think you'll like it. Then, we'll do a recap 'cause repetition is the mother of learning. Now then, as usual, we'll touch on some exam stuff. From Domain 1, we'll add networks and publish ports. And from Domain 4, we'll deploy a service on an overlay network and make it available on an external port. Let's crack on.

The Big Picture
First up then, what is a secret? To be honest, it's anything that you want to tell your app that's sensitive. Now, that's normally going to be things like passwords and certificates. But to be honest, it could be things like names of servers and services, basically write anything that you think would be a security threat if it got exposed. Now, in the Docker world, we can be a bit more concrete with our definition. At the time of recording, a Docker secret is a text blob, and it's up to 500k, half a meg. So you're covered for your typical passwords and SSH keys. Anyway, and we're going to see this in a bit, right? But a typical use case is telling a web front-end what the password is to a back-end persistent store. And it's nothing new, right? We've been doing this for years. But in today's world of dynamic, short-lived apps that we deploy here, there, and everywhere, the bolt-on solutions that we've been hacking together are just not up to task. We have been crying out for something safe and secure, and you know what? Something that's infrastructure independent. And that's what we get with Docker secrets. So long as you're running in Swarm-mode, secrets are baked right in. That means, if you're on-prem, in the cloud, on your laptop and you're using Docker, you've got secrets. Now, it needs Swarm-mode because it leverages the security stuff. So let's walk through it. We've got a swarm, right? So you can create a secret. This gets sent to the manager over a secure connection, and the manager puts it in the store where it's encrypted at rest. Brilliant. Then you create your service or maybe update one. The point is, you explicitly grant a service access to the secret. After that, a manager then sends that secret over a secured connection to just the nodes in the swarm that are running a replica for the service we just authorized. It's a least-privileged model, right? Only the workers that are running a replica for a service explicitly granted access to the secret get it. Workers not running replicas for an authorized service, they don't get it. In fact, they don't even know it exists. Now then. Once it's delivered to the node, it gets mounted inside the service task in its unencrypted form. On Linux, that's a file in /run/secrets, and importantly, that's a temp FS volume, so an in-memory file system, meaning at no point is the secret ever persisted to disk on the node. It's only ever in memory. Now, this part is a bit different on Windows because Windows doesn't do in-memory file systems. So on Windows, yeah, it does get persisted to disk on the node. So, I don't know, you might want to mount the Docker root directory using BitLocker or something. But that's it, yeah? Pretty impressive. Oh, and when the service is terminated or the secret's revoked, the worker node is instructed to flush it from memory. Brilliant. Now a couple of things to know about, right? Well, actually, we've said that it's only in Swarm-mode. But that means services, not standalone containers. So even if you've got standalone containers running on a node in Swarm-mode, it won't work, it's just for services. Okay, now you want to be running Docker 1. 13 or higher. Before that, the raft logs weren't encrypted. And yeah, for Windows support, that's going to need to be 17. 06 and later. And that's it. Let's can the theory and see it in practice.

Secrets on the CLI
Alright, I'm on a Windows node and it's the leader manager of a single node swarm. It's only single node for the demo, right? Obviously, that's not a requirement. Anyway, I've got a file here called classified, and inside the file is the contents of my secret, but I'm not telling you what that is yet. To create a secret from it though, I use the docker secret sub command. We give the secret a name, and having something within the name to indicate a version is a good idea, right? It makes rotating and the likes easier. Anyway, then I tell it to use the classified file. So, create a new secret, call it this, and use the contents of this file as the actual secret. Now, that can be an SSH key or whatever, right? You'll see what it is in this particular example later. Anyway, that's done. My Docker client has sent that secret to the swarm manager demon over a secure channel, and it is now securely stored in the swarms raft. Okay we can see a list of secrets with the usual ls command. There's just the one, that's obviously ours. And you probably get it now, but we can inspect it with the usual inspect. Alright. Now, especially looking at the inspect here, the take-home point so far, I hope, is that nowhere here can we see the unencrypted contents of the secret. Now that it's created and securely stored in the raft, the only way to see it is to grant the service access to it. So let's do it. So it's a normal docker service create. We'll call it secret-service, as in, like, the Secret Service. Come on, it had to be done, right? Anyway, we tell it about the secret. This is what actually grants the service access. And then we'll just make it a simple service, right? We'll see a more realistic example in the next section. Ah, now, Windows images are mahussive, I mean, mahussive. So, I'm going to step out of our three-dimensional space for a second, go forward in time, and it will be quite far right, 'cause these Windows images, like I said, they are big. Anyway, I'm heading for some time in the future when this background download is done. See you there. Okay, I'm back. Whoa, it's dark outside? That was longer than I thought it would be. Anyway, hope it didn't miss anything. But look, we've got a running service. And if we inspect it, and somewhere up here we're going to see the secret config. There it is. So we know that this service is using the secret. Well, we're a single node swarm, so the service replica is definitely going to be running on this node. That's it. So if we exec onto that, and it's in Windows, right? So the secret is going to be mounted as a file under C:\ProgramData\Docker\secrets\. And there it is. This is /run/Secrets on a Linux container. Alright? But what's in it? I know you're dying to know. Ah, what? All of that at-rest and in-flight encryption stuff, and the least-privileged model, and that's what the secret is? Disappointing, Nigel, disappointing. Anyway, look, you can't delete a secret that's in use. So if we try this here. Denied. The service is using it, right? But if we delete the service and try that secret delete again. Okay, this time it's worked, we'll just make sure though. Yeah, gone. Well, do you know what? That's the simple version. Coming up, we're going to do it all again, but this time we're going to use a real app.

Secrets in Apps
The GUI, yeah? For those of you who might not know, this here is Docker Universal Control Plane. It's Big D Docker's all-singing, all-dancing web UI for ops teams, and it's a "stick your hand in your pocket "and give us some money please" product. But I kind of like it, though, actually, I'm not loving the latest UI redesign. But you know what? It's decently powerful as a tool. Anyway, we've got a job at hand. Secrets. Specifically, right? We are going to deploy a WordPress app. So a WordPress front-end and a MySQL back-end. And we're going to use a secret for the database password. So if we come here to create a secret and then all the way over here to create a new one, and just like the command line, we give it a name and some content. I think wp-sec. And we'll give it a bit of Pluralsight love, right? We all love Pluralsight, yeah? And we're good. Seriously, that's our secret. And again, look. Like the command line, we can poke around in here all day and we're never going to see its unencrypted content. That is all locked away now on the underlying swarm. Okay, well, the app that we're going to deploy needs a network, so we'll have a new one. Call it wp-net. And we're keeping it simple, so that will do. There it is. Now to deploy the back-end service. We'll call this WordPress db, and like we said, we'll go with MySQL. Stick it on that overlay we just created. And now for the secret, which is done under here under environment. So we say, Use Secret, and we pick it from the list. Now, we can, if we want, give it a different name and location to what is default. But we're cool with this, right. This is what will be called on the host, yeah? Anyway, looking good. Now at this point, we've done everything that we need to to grant the service access to the secret. But we need to configure the app to use it, right? So we're using a MySQL back-end and that needs a password, and we're going to tell it to use our secret for that. So this variable here needs setting to the location of the secret in the container. Now it's a Linux container. So the secret we created is going to get mounted into the container's file system at /run/Secrets and then the name of the secret. So that matches this here, yeah? And I reckon we're good, so let's deploy that. Okay, it's running. Time to deploy the front-end. So same again. We'll call this one WordPress front-end, and we'll use the WordPress latest image. We'll put it on a network, the same one as the back-end, right? So they can discover each other by name. But with this being the front-end, right? We want to publish it on a port. So this web service is configured to listen on port 80, and we'll go with ingress so it's available on all nodes in the swarm. But we want to publish it externally on 8001. And that's good. So we're on the right network and we're published on the ingress network. Time to tell it about the secret. So again, use the secret. We've only got the one and we're cool with the defaults. Okay. Now for a couple of environment variables to configure the app. This is app-related stuff right now. We've already done everything that we needed to do to give the front-end access to our secret. Well, first up, we want to tell it to use our secret to access the database. run/Secrets/wp-sec again. Make sure that matches, yeah. And then another variable to tell it where to find the database server. Now, for this, we just give it the name of the back-end service and then we tell it which port to talk on. Cool. Now remember, because both of these services that we created are on the same overlay network, they can discover each other by name. So that wp-db here, that is the name of the database service. And you know what? We could totally have used another secret for this here. Maybe I should have. Nevermind though, we'll deploy it. Okay, that's running. Now, because we published this on the ingress network, we can open a browser tab here, paste in the DNS name of the lab server, and I think, did we say it was 8001? And there it is. WordPress, in all its glory, secret and all. So yeah, Docker secrets, the way to securely deliver passwords and certificates and the likes to swarm services. And as we've seen, it works on the CLI and the enterprise class web UI. Sweet. Time for a quick recap.

Recap
So, Docker secrets, a safe and secure way to publish secrets to applications, and it is platform-independent. So we start out by creating the secret. In the Docker world, that's a string that can be up to half a meg in size. When we create it, it gets sent to the swarm over a secured network connection, and then it's placed in the raft where it's encrypted at rest. Then we create or update a service. As part of this operation, we authorize the service to access the secret, which causes the control plane to send it to nodes in the swarm that are running replicas for that service. Again, it's encrypted in flight. When it hits the node, it is never persisted to disk, at least not in the Linux world. Instead, it gets mounted into the service replica, which is a container, yeah? As a file in an in-memory file system. And it's unencrypted at this point. Then once the service is done, the control plane tells the client on the worker nodes to totally forget about it, and it is flushed from the nodes. Cool. Like a thousand times better than storing them inside your image in plain text or inserting them as plain text environment variables. This really is enterprise-grade in my opinion. Fabulous and all, but time waits for no one. Coming up in the next module, we're going to look at stacks and services.

Deploying in Production with Stacks and Services
Module Intro
We've said a few times already, containers are all about the apps. Magic. But we need a way to deploy and manage them in production. Enter stacks. This is like the icing on the top of the cake, it's almost like everything so far has been building to this point. Anyway, here's the plan. We'll set the scene with a quick big picture, then, we'll inspect the stack file, and that'll be a full on microservices app fully-defined in a single declarative file. Then we'll take that file and we'll deploy the app and manage it. Then we'll recap. Now then, we're going to cover some Docker certified associate exam stuff while we're at it. Specifically, we'll cover the following from domain one orchestration. Extend the instructions to run individual containers into running services under swarm. Convert an application deployment into a stack file using a YAML compose file with docker stack deploy. Manipulate a running stack of services, increase the number of replicas, and mount volumes. Let's go.

The Big Picture
I think it's fair to say that most apps are a bunch of smaller services that work together. And in the docker corner of the world, that's Services with a capital S. So the Service object in the Docker engine API. And although we've touched on services a little bit already, we've not actually seen a good way to manage a bunch of them that are working together as a single app. Enter stacks. Now before I go any further though, stacks work on the command line, the universal control plain GUI, and even Docker Cloud. Anyway, look, we build an app. And that's usually a bunch of different pieces of code that talk to each other and work together to form a meaningful app. Beautiful. And they can all be different languages and all that. But, we make each one a container. Then, for things like scalability and self-healing, we deploy them as Services. That's capital S again remember, we're talking about deploying them as Docker Services. But each service is still deployed and managed separately, and that's not ideal really, so, we group them as a stack, and voila, meet the highest layer of the Docker application hierarchy. Now then, let me turn things upside down and flip this on its head a bit, cos I think that was the dev view starting from the code. The ops view though, that's the inverse. Ops are going to start with the stack, details to follow, yeah? But that defines a bunch of services and networks and volumes as well, so it's a great piece of application documentation, yeah? But, those services define containers, and containers run code. Anyway, we deploy and manage stacks with the docker stack subcommand, or through Docker Cloud on the web UI. It's the business. Now then, the stack file is basically a compose file, so if you know your docker compose, this'll be a breeze. But, this all brings a host of things that are simply game-changing. For starters, defining the desired state of an entire application in a spec file, this is at the heart of things like self-healing and the define once deploy many philosophy. So we define the app and how many replicas and all that kind of stuff, right? We feed it to swarm, and swarm deploys it. But it doesn't just deploy it, it manages it as well. So it's not like your typical developer that just fires and forgets, just kidding, relax. So, it records the spec of each service in the cluster, and then it implements a bunch of background reconciliation loops that watch it. And if things fail, it fixes them. It really is game-changing. I mean, I assume you're not a fan of phone calls at 4:20 AM? If you're not, then this is for you. Like, if a node fails, and maybe it takes down a bunch of replicas with it, swarm doesn't page you to fix it, page by the way is what we had before mobile phones. But no, swarm doesn't page you. It just fixes it itself by spinning up new replicas on a surviving node. Oh my goodness! Now, of course it needs you to have written your app so that you don't lose a bunch of state and alikes when the replicas fail. But if you're doing that, and you've got this, heck yeah, you can stay tucked up and warm in bed while swarm does all of the out of hours break fix work. Love it! As well as all of that though right, a stack file is a great way to document your app, so a developer defines an outright in a stack file and it hands the stack file to operations. And right there, ops has a great description of the application. So it knows what services make it up, what images are in use, networks, volumes, number of replicas, all of that goodness right, and its idea of the change control! What is not to like? Oh, as well as that, you can fork different versions of your stack file for things like dev test and prod, so it's the same app. But I dunno, maybe ease up on the number of replicas in dev and test, or a different secret or something? You get the picture. So yeah, if you deploy containerised apps that fit the microservice model, and you want a way to deploy and manage them as a single app? Stacks are a great way to go, and you get all the self-documentation, version control friendliness and all the declarative self-healing magic thrown in for free. Let's see it.

Stack Files
So here's the app we're going to deploy. It's pretty long. Well, I mean it's not, but it is for a video course. Anyway look, I've included a copy of the file in the course notes, and I've got a copy here as a fork from here. So, that means you can use either of these commands to get your hands on the file, and the commands are in the course notes as well. Anyway, it's a lot to look at, so what we're going to do here is a bit of a compose primer and a breakdown of the app. Right, stack files, as I'm calling them, they're basically compose files. But, they need to call at least the compose V3 file format, cos all the stack related stuff in them is only supported in that version and later. But I'm getting ahead of myself here and I don't want to scare you off. We have got an app here. Defined, in a compose V3 file, or a stack file. And at the highest level, it's defining six services, couple of networks, and a volume. So actually, we've got four top level keys. Version, services, networks, and volumes. Okay, well we always stick version at the top. I'm saying 3. 4, then we define our services and we list any networks and volumes that they'll use. For services right, okay, we've got six. And each one of these is its own JSON dictionary with its own set of keys. But I think the main point for us now, is that each of these represents one of our services, so we've got a redis service, db, vote, result, worker, and visualizer. And then these are going to use a couple of networks called frontend and backend, and a volume called db-data. Magic! We're already starting to get a bit of a feel for our app. So, if we expand the services, okay, right at the top we've got the redis service. Now we can see what image it's using, the port it listens on, and for this one we're just listing the container port, right? Letting swarm map it through to a random high numbered port on the swarm. Then it's going to attach to the frontend network, and then we get to the good stuff. This deploy key right, this is pretty much what was new with the V3 format. And it's where we define all the swarm and the stack stuff. So we can see right now, we're deploying one replica of this service, so one container. Then, when we do rolling updates of it, we'll update two replicas at a time. Which, I guess doesn't really apply here because we're only deploying one replica. Well you know what? If we ever scale it to 10 or something, well then when we come to update it, like say to a new version of the image, swarm will iterate through those 10 replicas two at a time, and with a delay of 10 seconds in between each two. Then, we've got a restart policy here which basically says restart replicas for this service, if the failure code on any of them says it failed on error. And that's the redis service. Look, it's pretty much saying for the others like db here we're using the postgres image, this one's using a volume, it's on the backend network, and it's got a placement policy, so we're telling it just to run on manager nodes. And that's probably because it's using a volume and maybe the volume's only available on managers or something like that. Which makes me think actually. Swarm is decently intelligent when it comes to scheduling. I mean, this here is telling it only to run replicas on managers. We can do the same for workers. But this is what we would think of as topology-aware scheduling. But it does health-aware scheduling as well, so it'll only schedule work to nodes that it knows are healthy. And, it does H/A scheduling, so this makes sure that replicas of a particular service are not all running on the same node, in case the node goes down. Anyway, it's pretty much the same for the rest of the services. Each time, we define an image, set some ports, join a network, and a bunch of swarm stuff. And then finally the networks and the volumes, which swarm will create for you automatically. But this right here, in this file, this is our app. And it's described in a way that ops can read it and understand it, the cluster can store it as part of its overall desired state, and we can slap it in version control. I'm not kidding. I mean, there is very little that's not to like about this. But you know what? We've been staring at a file for five minutes. Let's go and deploy it.

Deploy and Manage Stacks
Okay, I'm on a Docker host here, and it's in swarm mode. And, I've got a stack file here. Now, it's the exact one that we've just looked at, so six services, couple of networks, and a volume. Basically, it's all the stuff that we'd normally define using things like docker container run or docker service create on the command line, but all nicely packaged in a declarative file. So, to deploy it, we just go, docker stack deploy -C for compose file, and then we tell it our stack file. Now you can call this file whatever you like. Most people still have it as docker compose. yaml or some named version for dev or test or whatever. But you don't have to, you can call it whatever you want. And as we can see, we're using the native docker stack subcommand, so no need for that extra docker compose Python binary that we used to need with legacy compose. This is all native go stuff that's implemented directly in the engine. Anyway, we give it a name, and off it goes. And done, deployed. Though, to be fair, it's probably pulling images and the like in the background. Hmm. Okay, busy output, long image names I guess, but, yeah, some of these are still preparing. But we can see the desired state here is running for all of them. It's just current state for some, well some are running but some are preparing. And you know, we get a decent view with the docker stack services command here as well. Cool. Well that's it, the app's deployed. We took our spec file, and we deployed it using docker stack deploy. And behind the scenes, like we see in this command here, it's deployed our six services. And in fact multiple replicas of this vote one here. Which in fact, actually that one's the front end, and we can see it's published on port 5000 externally, so if we come over here, and we can hit any node in the swarm, on port 5000, and it's the voting app. Dogs, magic. Well you know what, there's a bunch of other services. 5001 and 8080, so back here again. We change this to 5001. Right, this is the result service, so it's showing our one vote. And the other one I think was 8080. Okay, that's the visualiser tool, so it's telling us what's running on the swarm. We've got three nodes and then these here are the service replicas. Cool well, with a single vote being cast, clearly we need to scale things up. So the voter_vote service, it's the name of the stack underscore name of service, so the stack is called voter, and then in the stack file, this service is called vote. Anyway, look, two replicas, not enough for frontend demand. We need 20. Now, you know what? We can do this the imperative way, in fact let's do that. So we go docker service scale voter_vote and 20. Cool. Neh, cool-ish. You see, doing things imperatively like that, fine, it's updated the service, and yeah, we can see the service spec in the swarm here should be updated as well. Replicas, right, so we're at 20 in the service spec. So desired state 20, current observed state 20? Well you know what, back in our spec file which hopefully we've got in source control somewhere, uh-uh, that is still going to be saying two. Meaning, if we update that file sometime in the future and redeploy from it which is the way we'll normally work, we're going to fall back to just two replicas. So doing it imperatively works, but it's not the right way, things get out of sync. The right way is to update this config file here. We find the vote service. And you know what, let's say 10. Save that, and then we'll reapply it, which to do that, all we do is we find that original docker stack deploy command, and we run that again. It's going to run against the new version of the stack file. So there you go, that is basically updating every service in the stack. So if we look at it again... We're down to 10 replicas for that vote service now. And if we check the service spec on the cluster again... We're showing 10 there as well. So everything's in sync. The desired state recorded on the cluster, the actual state currently being observed on the cluster, and our master config file which again hopefully we're storing in version control somewhere. Brilliant, declarative for the win, really. Now one last thing. Because we've told the cluster its desired state of 10 replicas for this vote service, we, yeah we've got one running on every node in the cluster. So you know what? I'm going to come over to the cluster backend here. I'm on Amazon Webservices, you can be on whatever, it doesn't really matter. I'm just going to pull the plug on node three. Nothing graceful, just get thyself out of here. Off you go. We check our stack again... Right, we've got some shutdown action going on here. So basically, the swarm has noticed that something is wrong. One of the nodes is gone, meaning some of our replicas are missing. So, in the background, it's spinning up more replicas on surviving nodes. And this is that 3AM phone call being dealt with by swarm, thank you very much. And that stacks, right? Multiple services, declaratively defined in a stack file. We'll post that to the swarm, it gets recorded as the app's desired state, and then it gets deployed. Then when it comes to update time, the best way to do that is the declarative way. So that's checking the stack file out of version control, making our changes there, and reapplying from that. Pretty sweet. And of course, we can do the same in the GUI and in Docker Cloud. Same stack files, just with a pretty point and click interface. Tremendous. Let's have a recap.

Recap
Okay, we started out by saying it is all about the apps. But, the apps are generally a collection of smaller services, and they all play together nicely and do something useful, brilliant. Well in the Docker world, these apps are made from containers that we run as services. Then, we group these services into stacks, bingo, we've got an app. So, a stack really is just a bunch of services that make up an app. And we define it in a YAML file, pretty much a compose file with a few extensions to deal with swarm stuff. And it really is just a compose file. Though, you do need to be using version three or later of the compose file spec. But, even though it's using a compose file, we don't need to install compose as a separate tool. This stack stuff, it's all baked directly into the engine. But it needs to be in swarm mode, got to be clear about that. Stacks are about swarm. Anyway look, we've got this stack file defining our app, and we deploy it to the swarm cluster with a docker stack deploy command. And in the background, this does a few things. First up, it records the desired state of the app on a cluster. And of course there's raft in the background making sure that every manager's got the latest copy of it. But second up, it deploys the app, which includes all the services, networks, and volumes and all of that kind of stuff. But then, thanks to background reconciliation loops, swarm manages the app. Cos it's got this notion of desired state, which is really just what we put in the stack file here saying how the app should look on the cluster. Number of replicas, which images, all of that kind of stuff. But if anything ever changes, and we use the example of a node failing and taking a bunch of replicas with it, well swarm sees that change and that the observed state of the cluster no longer matches the desired state, so it goes to work fixing it. Brilliant. We end up with a self-documented, reproducible app that fits nicely into version control. Let me tell ya, it's the future. So yeah, that's stacks. Coming up next, we're going to look at some of the native Docker enterprise tooling.

Enterprise Tooling
Module Intro
Alright then, we're near the end, I can feel it. And if you followed along from the start, then you're going to be pretty skilled on this whole Docker thing by now. Get in, I mean, that's what we came for, right? Anyway, now that we know how all the underlying stuff like imagines and containers work, we're going to switch tack one last time, and we're going to have a bit of a look at some of the stuff that Docker, Inc. offers in the enterprise space. And let me be clear for a second here, yes, I did say Docker, Inc. So, Big D Docker, the commercial entity, yeah? Now, other companies provide similar product, and in no way am I endorsing a particular product here, I'm not! The thing is, I'm trying to cross two streams here, and if you know your Ghostbusters, crossing streams is bad. Like, total protonic reversal kind of bad. Anyway, look, buckle up 'cause we're about to cross streams. So, Stream 1, I want to give you a feel for the types of products and the kind of value-add stuff that's out there. In Stream 2, I want to cover off a few of Docker Certified Associate objectives. So, two streams, and the best way I could think of covering both was to show you the Docker, Inc. enterprise products. But really, it's not a sales pitch and I'm not endorsing. And you know what? There is some seriously cool stuff on the card here. So, here's the plan, big picture, yeah? The usual, what and why, and short and sweet. Then we'll look at the universal control plane operations UI, I'll show you how to install it, and we'll take a whistle-stop tour of what it does. Then, it'll be Docker Trusted Registry, others do exist, right? But what we're giving you here is the idea of a private registry inside your own firewalls. Then we'll look at some of the features, all of their springs, right? We'll start with role-based access control, then imagine scanning for known vulnerabilities, and a bit of Layer 7 load balancing, then a recap. Now then, this is the reason for crossing streams. If you're thinking of taking the Docker Certified Associate Exam, we'll be looking at these topics. Now remember, okay, this is not a DCA study course. We're a bit more real world than that, so we're not covering everything in the exam, and we're not going into massive depth on exam objectives. But we will be looking at these things. Domain 2: Image Creation, Management, and Registry, we'll do deploy registry, configure a registry, login to a registry, and push an image to a registry. For Domain 3: Installation and configuration, we'll do consistently repeat steps to deploy Docker engine, UCP and DTR on AWS and on premises in an HA config. Domain 4: Networking, use Docker to load balance HTTP and HTTPs traffic to an application, so configure Layer 7 load balancing with Docker EE. Then finally, Domain 5: Security, demonstrate the image passes a security scan. Configure role-based access control in UCP, and describe the difference between UCP workers and managers. Let's go.

The Big Picture
Okay, these days, Docker are bundling all of their enterprise stuff under the umbrella of Docker Enterprise Edition, EE for short. Now then, it's basically a bit of a hardened engine, and Ops UI, and a secure on-premises registry. I'd say those are the building blocks. Then as part of that, they're bringing things like RBAC, image signing and image scanning, image promotions, and a bunch more, all value-add, yeah? Well, without EE, you get CE, community edition, and the boundaries between the two are a bit fluid, I think, right? So, some of the stuff in EE might slip its way into CE in the future, I don't know, I'm just guessing. But CE is basically the community edition of the engine. So, things like the OCI layer and the container D stuff. But all bundled, right, with the API and daemon to give that core Docker experience, the opinionated runtime, if you will. And it's on its own pretty aggressive release cycle. Then, stuff that's gone in CE finds its way into EE, which has its own separate release cycle, a bunch of additional products, a support package, and a bunch of certified this and that. Now, at the lowest levels of the engine, right, CE and EE are pretty much the same, just EE has fewer release cycles, and stricter configs and stuff. Then, the other EE stuff wraps around that and brings all the value-add, and you pay for it. Again, other vendors offer similar. But the idea is that everything up here is nicely packaged and plays well together, they should seamlessly fit. Anyway, back to the big picture. For the rest of this module, we're going to drill into the UCP web interface, the trusted registry, and then some other stuff that's built around it.

Docker Universal Control Plane (UCP)
Right then, Universal Control Plane from the folks at Docker, it's a GUI. Yep, there's no getting away from it, it's a GUI, and its raison d'etre is to manage Swarm and Kubernetes apps, cool. Anyway, you install it as a containerized microservices app on top of the Docker EE engine. We'll run through an install in a second, but yeah, you install Docker EE on your node, and then you pull and start the containers required for UCP. Eventually, building a cluster of manager and worker nodes. And no surprises for guessing, but it is a Swarm cluster under the hood, obviously, right? We've already know that Swarm's got a ton of security and other stuff for the infrastructure level. But that means that managers in the UCP cluster are the control plane, and the workers, that's where you want to run your apps. Anyway, we end up with this enterprise-grade cluster to run Swarm and Kubernetes apps on. So, to install it, and I know that not everyone can do this, I guess, 'cause it's a paid-for product. But, you first of all need to install Docker EE, the Enterprise Edition of the engine. So, I'm logged in as me here at store. docker. com, and if I look at my content, there's Docker EE. Now, I'm going to need this here, right? This is my unique key of sorts, my own personal repository. But I'm installing it on Ubuntu, so I just follow these instructions. There's obviously options for the platforms here as well. Anyway, I've got four Ubuntu nodes and I'm building this cluster. One manager in three workers, with UCP and DTR. Alright, well, this here is Node1, and I just couldn't paste those commands from the store page, yeah? So, first of all here, we're making sure we've got a few packages installed and up to date, particularly, this HTTPs transport. We're going to install over HTTPs. Okay, add the key from my custom repo. Okay, this is my custom repo, yeah? Add the right repo, again, this needs my unique key here, obscured the course, not that I don't trust you, more so that Docker don't shoot me. But now to install the EE engine. Okay, while this is going, there's instructions on your Docker Store page and on the Docker website for the versions of EE, so like, for Red Hat and Windows, or whatever. We've just shown you the way for Ubuntu. Anyway, once it's done we can see it with the usual Docker commands. Magic, so that's EE. Now, I'm going to go an do the same on the three other nodes in the cluster. Okay, with all that done, it's time to install UCP. So, UCP's a containerized app. I mean, you'd hope so, right? Eat your own dog food and all. But we start the installation with this kind of long Docker run command. Now, versions and IPs are going to be different in the future, that's my AWS IP, yeah? Yours will definitely be different. But straight away, it wants some admin creds. This is enterprise stuff we're talking about here, right? So, follow your corporate standards for this, yeah? I'm a fan of admin and password, 123, it's been uncrackable so far. But look, it's pulling the images for the different UCP bits and pieces. So, agent, auth, metrics, all of that jazz, right? A proper little microservices app. Anyway, any more names or IPs that we want it to go by, again, this is going to be specific to your environment, I'm an AWS, so I'm slapping in all the AWS names and IPs here. Now then, I'm going with a simple install, so the built-in CA, right? You can totally customize this for your environment, see the docs for that. But look, we're done, and we've even got the URL. So, I'll tell you what, let's see it. Oh, yeah, obviously, I'm totally fine with the self-signed cert. You won't get this in the real world if you're using a trusted CA, I just went with the easy self-signing option. Ah, and look, this is my favorite bit, this monkey logo here. Please don't change that, Docker, it's brilliant. But we give it admin and password, 123, oh, good grief, and we give it a license. 'Kay, that should be good. Right, we're in, and a pretty looking UI. So, this main page is the dashboard and it gives us a quick overview. We've got no errors, we're running a bunch of containers, and a service, systems stuff, yeah? In fact, probably UCP itself. But one node and a bunch of other stuff. But look, no workers yet. So, to add a worker, we come down here, and we'll stick with Linux, but clearly we can go with Windows. Yeah, we want it to be a worker, and I'm happy with the default networking. So, custom listen and advertise addresses for if you've got multi-homed instances, or if you're working behind load balances. But in this lab, we're not. So, I'll have this command here, and see how it's all Swarm, yeah? Well, we'll paste that into Node2 here, and that's all joined. So, if we come back here, it should be showing up. Maybe we'd give it a refresh. Right, one worker, and we can see the worker over here as well. Brilliant, now then, I'm going to add two more workers, but using the exact same process that we just used there. But yeah, that's UCP. This fancy web UI for stacks and services, and images and networks, and volumes, and secrets, and you know what? Soon to be Kubernetes. And we're going to see a bit more in a minute. But yeah, that's UCP installed.

Docker Trusted Registry (DTR)
Okay, we've got our form of UCP cluster, and I want to stress two things about it. One, it's Swarm under the hood. So, most of what we've learned about Swarm applies here to UCP, secure networking, client cert, secure store, all of that goodness. But point two, you can deploy this anywhere you want, on prem, in the cloud, even on your IBM mainframe. Anyway, now that we've got it, it's time to build a private registry. Now, as a primer, registries are where we store images. The most famous one's Docker Hub, but that's out there on t'internet, much a scary place for enterprises. What we need is a Docker Hub that you can deploy behind your own firewall. Enter Docker Trusted Registry, ta-da! So first up, architecturally speaking, DTR sits on top of what we've already deployed. So far, that's been the Docker EE engine and then the UCP cluster. Fabulous, but going lower, still. These engines here can be on anything we want. For this demo, I'm running on AWS. But you can in Azure, or whoever your favorite cloud provider is. You can even be on prem, and on VMs, or bare metal. The point is, you deploy where suits your organization. Alright, now then, it needs UCP, we've got that. But no surprises again here, it is a containerized app. And to install it, we're here on the admin settings, and then it's Docker Trusted Registry. Now look, there's all kinds you can do here. I mean, Swarm... Look, we can see our join tokens and rotate them. Cluster Config, we can config a port, and scheduling. All this kind of stuff, really cool, but we want DTR. So, the first thing to do here is tell UCP about your load balancer. Now, configuring that is beyond the scope of this course, but it's just standard stuff. And you can even leave it out if you want. But we select a node, Node2, I'm just going to let the cluster figure out the Replica ID, this is just a demo lab, so I'm not worried about man-in-the-middle attacks, you should be in the real world though. Now, I don't know if you noticed, but checking these options up here builds this install command, alright? Well, I'm going to stick that on the clipboard, and then come over here to Node2 and I'm going to run it. Alright, and we can see that it's pulling the required images, a containerized microservices app, remember. Alright, we need to authenticate with UCP, don't want scary people installing apps now, do we? And that's away installing. Now, it's going to take a minute or two, so I'll pause everything else in the world until that's done. Okay, we're done, if you're feeling a bit sick, it's nothing to do with listening to me for ages. It's 'cause I just paused your entire world, and then restarted it. Anyway, if we come back here in UCP, we'll close out this window, ah, look. See how we've got two apps now? UCP, and now DTR, and they're stacks. We know all about stacks, right? But these are systems stacks, so not for us to mess about with. Anyway, if we come back to admin settings, and DTR again, okay, it's showing our DTR now. So, if we grab that, and we'll put it into a new tab here, okay, self sign certificates again, we'll sort this out. Oh, alright, well these will be the UCP creds, normally, this goes straight through, nevermind. And we're in, this is DTR. Our very own private registry, running inside around firewall. And if we quickly look at users here, see that, right? I created already in UCP, and they're showing up here in DTR. And it's the same the other way, right? If we create a new user here, that gets seen in UCP as well, so a shared user database. But then, down here it's got its own settings, so, updates, proxies, SSO, all of that stuff, right? Back up here, storage, ah, now, you should definitely configure a proper shared backend. This lab's in AWS, so a good chance it's going to be an S3 bucket. Now, I just happen to have one in the same region here as UCP and DTR, and it's got an IAM Policy, which is just Amazon jargon for some permissions that let a user read, write, and delete objects in the bucket, but you know what? This is all Amazon stuff, right? So, I'm not focusing on it, we're just telling it about an AWS bucket, and we're giving it some creds, and cool, a shared backend. Anyway, security. Ah, now, I'm going to say yes, we'll scan images, and I'm going to do yes for online syncs, right? Now we'll see this is in a minute, but DTR can go binary-level scans of images, and look for known vulnerabilities. It's really cool, right? And this online here means that we'll sync the vulnerability database over the internet. Brilliant, though not brilliant for everybody, right? If you happen to be air gapping your DTR from the internet, you want to go with Offline here. This lets us download a TAR file separately, and then manually load it into DTR. Anyway, last but not least garbage collection, but you know what? I'm not bothered about it for now. So, under repositories, we can create and manage image repos. So, where's my name? And I don't know, web-fe, right? To store frontend images. We can make it private or public, and do we want images scanned on push? Well, yes, actually, we do. I'm a real fan of that. It's what we just said before. This will do binary-level image scans and hunt for known vulnerabilities. And setting this here just means it'll do that every time we push an image. Okay, cool, then under Advanced Settings here, we can mark the repo as immutable or not, fabulous. And you know what? I think by now, we've probably got everything we need to look at some of the more advanced features. Speaking of which, first of is role-based access control.

Role-based Access Control (RBAC)
Role-based access control, which users have what level of access to which objects? It's absolutely vital, and enterprises, they just cannot work without it. Luckily, the Docker EE Stacks supports it. And it does so through something a grant, which if we pick it apart, is basically a user or a team, we call this the subject, a bunch of permissions, we call this the role, and then a set of resources, we call this the collection. Now, these are just the UCP terms, right? But a user or a team gets a certain set of access or permissions against a certain set of resources, standard stuff. Well, in UCP here, if we come under User Management, we've basically got organizations and teams and users, that's the subject part of the grant, then, roles and grants. So, if we come under orgs and teams here, let's create some orgs. And this is it, right? Just a name. Anyway, we'll create a few of them. Okay, a few there, and then within each org, we normally create teams. And look, again, just a name and a description, right? So, we'll create a few of those. Finally, we need some users. And guess what, this is not rocket science either. Username, password, and a full name, mega simple stuff. Well, I tell you what, I'll create a few of those in the background as well. Now then, following standard approaches, we add users into teams. So for now, that's come up here to orgs and teams, drill into an org, and then a team. Then we come up here to add users. So, we select a bunch of them, and Add. Now, you get the picture, I'm sure, so I'll sort things out so that we've got users and teams in the likes. Magic, so we're all set now with the subject bit. Now for the role. Well, these ones here are the out-of-the-box roles. Eh, you know what? Maybe they're okay for some of your requirements, but definitely not all of them. So, this is where things start getting cool. In fact, this little Create Role button up here, this opens up the rabbit hole. Well, the simple stuff first, we'll give it a name, network-ops, and then hit Operations here. And this is where the magic happens. This is basically an expandable list of granular API operations. And it's what we assign to the role. So, an obvious as an example would be container operations here. And look at them all, it's all of the container-related API operations, and we can allow or deny every one, so it's tick to allow, and untick to deny. But you know what? We call this the network ops role, so we're interested in network operations, and I'm just going to say give them all. And that's it, I don't want this role to have access to anything else. And there it is, that is our custom role. But take a second to consider how powerful this is 'cause granular is the name of the game when it comes to RBAC, the more granular, the better. I mean, you need to know your stuff, right? But granularity is power when it comes to access control. Alright, well, for the collection though, this is a set of resources. We'll create a new one, and we'll call it zones. Imagine our environment split into three zones, dev, test, and prod. Well, these are hierarchal, right? I mean collections are hierarchical. So, if we hit View Children, and there aren't any right now, but I can create some, so Dev, and I think we'll have Test and Prod as well. Alright, we've now got the three building blocks for a grant, some users and teams for the subject, a custom role, and some collections. So, we can create a new one. The list a bit, well, it's alphabetical, but it's a bit backwards for the way that I work. Anyway, for the subject, I'm going to pick organizations and equus, and then for the team, I'm going to say equus-ops. Under roles, I'm going to pick that network-ops one that we just created. And then for the collection, well, I'm going to drill in, and I'll say Zones, and I think Prod. And with all three of those, we can hit Create. And that's our grant, everyone in the equus-ops team gets the network-ops role, that custom one that we just created, to anything that we stick in the zone's Prod collection, pretty sweet. Now, there's nothing in the zone's Prod collection yet, we just created it, but... Yeah, if we come down here to nodes, this is just an example, right? I'm not saying it's the best one, but we've at least got some nodes at the moment. And if we look at one of them, right, see how under collection here it's showing as Shared? Well, that's the default, right? But we can configure it here under Collection, and then we can say, Zones, and Prod. Now, grant's a dynamic, right? So, we can definitely move objects in and out of collections like we're doing here, and the grant keeps up. Now to see if this works. Okay, cool, it did, I wasn't sure 'cause with nodes, there are some restrictions around managers and I think even nodes that are running DTR replicas, I think they have to be in the system collection. But that one was okay. And that's the basics of RBAC in Docker EE and universal control plane. It's all about the grant, and the roles bit? Super granular these days. Well, with that under our belt, let's go and take a look at image scanning.

Image Scanning
Alright then, we've got our secure on-prem registry configured at this address. And on it, we have got a repo configured, called challenger/web-fe. Then on Node4 here in the cluster, I've got an image called web-fe, sweet. Well, to push this image to the challenger/web-fe repo on DTR, we need to re-tag it. You see, that's how pushing Docker images works. You tag the image with the registry and the repo that you want to push it to. So, we go, "docker image tag" and it's web-fe, that's its current tag. First, we'll tell it the DNS name of the registry, this will be different in your environment, right? Just remember, it's got to match how you resolve your registry. Then we tell it the name of the repo. And we'll tag it as latest. Alright, we'll check that it's there, which it is, and before we push it to DTR, we need to log in, so that's, "docker login" and then we tell it where we want to log into. This is our DTR again, yeah? And then we just give it a set of creds for a user on that registry. So, I'll log in as me. Okay, which if we pop over here and look, we should see that I've got permission on the repo, hm. Okay, right, we've got the ops team here with read/write permission, and you know what? My account is a member of the ops team. Long story short, right? I've logged in with an account that's got permission on the repo. Now, hm, you know what? I was going to say let's see, but before we do that, let's check this. Right, we want image scanning on push. Basically, scan every pushed image, yeah? Well, now let's see it. So, we go docker image push then we give it the image tag, let's chance of a typo if I copy it, yeah? But that's going to push the image to this registry in repo. Okay, it's a small image, so if we come back to DTR, okay, scanned and clean. Cool, but we can dig in for a deeper report. Eh, okay, for this image, not much. I mean, it's a small image and it's clean, right? But you know what? If we push another image, so back here, I'm going to pull an old image from a previous course, one that I know is full of nasties, right? Now Docker's opinionated, right? So, because I didn't tell it which registry to pull from, it went with Docker Hub. Just give it a second or two. Okay, so if we tag that one now, and we'll tag this one as dirty. And then if we push that, and we'll copy and paste again, right? Ah, now this one's going to take a minute 'cause it's a bit bigger. So, we'll come back when it's done. Okay, we're back in the repo, but I've covered up the results 'cause I think you should probably be sitting down to see this, it's not pretty, and view discretion is advised, are you ready? Oof, that right there, ladies and gents, is a dangerous image. In fact, even more disturbing scenes to follow. Aw, oof, you do not want to be running your production off of that, and you know what? That's what I love about security scanning, but it's also what I hate 'cause with great knowledge comes great responsibility. I mean, would I dare go show something like that to my boss? Eh, maybe not, you know, 'cause there's a pretty good chance she's going to tell me to either fix it, or get my coat. So, security scanning, yes, a very powerful tool. But, one that can kick the proverbial hornet's nest. Right then, you know what? One last thing before we recap, and dare I say, the best 'til last? I don't know, decide for yourself. But application layer routing is what we're looking at next.

Layer 7 Load Balancing
I'm pretty sure that earlier in the course, we've seen at least a couple of examples of routing and load balancing in a Swarm. You know, where we deploy a service, publish it on the ingress network, and then we can hit any node in the Swarm and reach it. Well, we call that the routing mesh, or the Swarm-mode routing mesh, and it's a transport layer solution, great. Well, it's got a big brother or sister, called the HTTP routing mesh, which, in true UCP style, builds on top of the Swarm mode routing mesh, adding application layer intelligence. And yes, at the time of recording, it is only available to paying Enterprise Edition customers. Now then, before we demo it, with the Swarm-mode routing mesh here, every service gets a VIP, a virtual IP. This is stable, it's long-lived, and it masks the IPs of service replicas. So, let's say you've got a service with 10 replicas. Every one gets an IP and it registers that IP with the VIP. Then, when DNS resolves that service name, it resolves it to the VIP, and the VIP then round-robin's across the 10 replica IPs, brilliant. Also though, again, sort of behind the scenes, any time we publish a service on a Swarm-wide port, that port is actually published on a special system overlay network called ingress. Put all this together, and it's what makes it possible to hit any node in the cluster on that service port and get redirected to a valid service replica. And it's pretty dope, only, it's ignorant of higher layers. Remember, it's all transport layer stuff down in around Layer 4. Anything higher in the stack, that's a mystery, and that's where the HRM comes in. Add a high level, the HRM, like HTTP routing mesh, that lets you publish multiple services on the same Swarm-wide port. And then in order to route traffic to the right service, it inspects the HTTP host header. So, a simple example that we'll build here. We've got a UCP cluster, and we deploy two services, red. service and white. service. And they are both published on these exact same two ports. So, both on each port, yeah? Then when traffic hits the cluster, the HRM steps up, reads the HTTP headers and says, "Oh, red. company. com, you go to this service. Or maybe it's white. company. com, "well, you go to this one. " And it is a fairly beautiful thing. Talk is cheap though, let's do it. So, we're here in UCP, and you know what? Don't worry if the UI has changed a bit. The fundamentals won't have. Well, we come under admin settings here, and we find the routing mesh. But turning it on is just this tick box here. Now, I mean, we can configure different ports if we want, but that's the gist. One click for on, and choose a couple of ports. Sweet. Now then, under the hood, this has created a system service here, called UCP HRM. And if we look at its config, we can see that it's got a couple of ports, right? And no surprises, but it's the two that we said when we turned it on. And see as well how it's connected to the ingress network. Speaking of networks, okay, well first up, here's the UCP HRM network, it's an overlay, and it's scoped to the Swarm. But also, here's the ingress network. That's also a Swarm-scoped overlay. And these two are critical to how the HRM works. But you know what? With all of this in place, we're ready to deploy an app. So, we'll deploy red first, use this image, and I think for this one, we'll say five replicas. Now then, under Network, the application actually listens on 8080. Yes, we want to publish it on the ingress network, and we don't need a port here 'cause we're going to use the ports we configured the HRM for. Right, well then, we want to add a house name based route, this is the magic, right? Now for this demo, I'm going to take the easy option, but in the real world, that's probably going to be HTTPs, but we're going to be hitting this one on red. company. com, and that looks good. Okay, a little red warning about attaching it to the HRM network, but you know what? We, hm, no, actually, nearly forgot, we need to give it an environment variable. So, this app accepts an environment variable, called TITLE, and we'll call this one red. Sweet, that's going to go and spin up five replicas. So, while that's going on, we'll do the same for the white service. Give it a name, tell it to use the same image, you know what? This time, we'll just leave it as a single replica. But then for the network, so it's the same app, so it's 8080 again. Yes, yes, yes, and a house name rule, HTTP again, and this time, white. company. com. Confirm that and put it on the HRM network. Oof, nearly forgot the environment variable again. We'll call it WHITE this time. Okay, so while that's creating, let's stick this up here. Two services, Red and White. Red has five replicas, and White has one. Alright then, let's go try this out. So, if I go red. company. com here, and before I hit Return, remember a few things. I've got name resolution configured for this. So, if you're following along, you're going to need your own name resolution. It can just be a host's file, right? But it's resolving this DNS name to the IP of a node in the Swarm, any node, right? Or maybe a load balance that it's pointing to your nodes. But the Red service here has got five replicas, and we gave it the title, RED, so here we go. Okay, we've got RED here, and we're racking up the replicas, five, right? Okay, and then these five handling or future requests. Brilliant, we found the right service. But we should be able to come up here and change this to white, and before we hit Return again, remember, this one's going to be called WHITE, and it's only got one replica, yeah? Let's see. Bingo, WHITE and one replica. And that's really cool. Two services, on the same cluster, on the same port. And Docker knows how to route requests to the right one based on the host header up here. Brilliant, decent Layer 7 routine, let's do a recap.

Recap
Okay, first up, plenty of companies offer competing products to what we've shown here, and we've only shown you a small part of what the Docker products can do. Moral of the story, there is a much bigger world out there, but at least now, you've got an idea. Anyway, Docker EE and the stuff that we've shown here are commercial products. That means they cost money, but they come with support contracts. Well, Docker universal control plane is a graphical container management tool designed for ops and DevOps. It sits on top of the EE version of the Docker engine, which in turn, sits on top of just about anything you'd like. Now, we didn't get specific about the lowdown stuff here, right? But we're talking about cloud and on-prem. Check with Docker though for the latest support matrix. Anyway, UCP has this notion of a cluster, managers and workers, and if you poke beneath the surface, it is all Swarm, which makes sense, right? Swarm's enterprise grade, and it's secure. Anyway, the net result is a fancy web UI that can do things like RBAC and security scanning, and of course, deploying and managing Swarm and Kubernetes apps. But we also talked about Docker Trusted Registry and on-prem or any virtual private cloud registry. So, a safe place to store and manage your application images within the safety of your own environment. Now, with the Docker stuff that we saw, it is all about that tightly integrated set of enterprise grade tools. A container as a service platform, if you will. But all tested and certified, and wrapped in a support agreement to help you sleep at night, sweet. Well you know what? This is pretty much where the train stops. But before you jump off, stick around for maybe just two or three more minutes while I throw some ideas at you about where you might want to go next. Because it's a big old container world out there, and a little nudge in the right direction might make all the difference.

What Next
What Next
Oh my goodness. What a journey. Well, this train's about to pull into the station. But you know what? It is a big old station with a mind-boggling number of places you can go next. So to help you out, just let me give you two or three ideas. First up, we've got more container courses right here on Pluralsight. If you've just taken this course and maybe not the container management learning path then that is a cracking place to start, right? I mean, I've regularly got people saying, "Hey thanks, Nigel, I loved your Docker course. " And I'm like, "Well, great, but which one? " And you know what, so often when I say that, they're like, "What? There's more? " So yeah, in case you didn't know, there's more. And not just from me. So go check them out. I've also got a Docker book. So if you do books, check it out. Now, I mean, yeah, there's going to be some content similarity. Docker is Docker, right? It'd be a bit weird if a Docker book had totally different content to a Docker video course. So if you get the book expect some similar content. Docker, yeah? But if you like books and maybe you want something to reinforce what we've talked about here, check it out. There's also meet-ups like all over the place. These are local gatherings of people interested in Docker, and you get all kinds of people. All different levels, right? But always great people. So get yourself along to a meet-up and get involved. I mean, man, I'm excited for you. Get to a local meet-up. There's DockerCon as well. So two to three days of Docker, Docker, Docker. I'm always there, and they're great places to learn and network. So if you do go, right, and you see me? Come and say hi. Honestly, I would be gutted if you saw me, but you decided not to say hi. Anyway, then there's the exam, Docker Certified Associate. If exams and certs are your thing, have a look at that. Now, I know this course wasn't a study guide for the exam, right? But we did touch on a bunch of the topics. And I reckon, right, with not a lot more effort, yeah, I reckon you'd be not far off ready for it. Then there's kubernetes. If you're going to be deploying containerized apps, at scale and in the Cloud, you're going to be doing it with kubernetes. And lucky enough, so much of what you've learned here about Docker is applicable to kubernetes. In fact, since forever, kubernetes has used Docker to run its containers. Now, that's kind of changing at the moment, okay? These days, the kubernetes container runtime is pluggable. But, it's looking for all intents and purposes like the default runtime going forward is going to be containerd. And we know containerd, right? And that it came out of Docker. In fact, Docker itself these days is running containerd at the lower levels. We covered that, right? So you're in great shape to start your kubernetes journey. And you probably should start one of those, right? It is the hot technology at the moment. I mean, calling it the Linux of the Cloud is not an exaggeration. Well, guess what, right? To help you out with kubernetes, we have got a course: Getting Started with Kubernetes. And it is exciting stuff. Oh, and I've got a book as well. So yeah, kubernetes. I recommend you check it out, definitely. You know what, right? The main thing is keep learning. I'm serious. The future is bright. So, thanks for sticking with me. I've had a blast, and I really appreciate you being here. And, honestly, I hope-- Well, I guess not only that you've enjoyed it, I mean, I do hope you've enjoyed it, but I really hope that you've learned what you set out to. I've tried really hard to make it fun and informative. So, congrats on getting to the end. Feel free to give me a shout. Now, look, I can't fix all of your hard problems, alright? I've got more courses that I'm working on, and I'm raising a family. But I would love to hear from you, so say hi. And that's it! Good luck.
