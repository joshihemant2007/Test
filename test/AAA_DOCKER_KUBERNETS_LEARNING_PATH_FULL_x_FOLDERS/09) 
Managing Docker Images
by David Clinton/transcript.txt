At the core of successfully running Docker container deployments is a thorough knowledge of the creation and administration of Docker images. In this course, Managing Docker Images, you'll explore image creation and administration best practices. First, you'll explore all the main image management options currently available. Next, you'll discover how to build, store, and share images through Docker Hub and Docker Cloud. Finally, you'll learn how to host a private image repository using Docker Registry. When youâ€™re finished with this course, you'll have a foundational knowledge of installing, running, and securing Docker images that will help you as you move forward towards managing entire Docker container clusters.

Course Overview
Course Overview
The speed, flexibility, and power of container technologies have in only a few short years made them among the most popular deployment platforms around, and Docker containers have been leading the way. Getting yourself a good seat on this flight will require you to master a few key design concepts, and then familiarize yourself with the rather wide range of available administration and hosting tools that are currently out there. I created this Managing Docker Images course to introduce you to the design and function of the image and how it works within the larger Docker ecosystem. You'll learn how images are created, stored, shared, and deployed as containers. You'll also see some image architecture best practices and how you can secure and validate images as they're moved across in secure networks. To give you a better sense of the choices you've got, I'll take you on a tour of some image administration tools provided by Docker Inc itself, like Docker Cloud, and then you'll dive a bit deeper as you learn how to host and manage your own private image repository using the open source Docker Registry. Docker is a really great tool, or better, a really great set of tools to quickly building highly-reliable development and production environments, and the Docker image is, in many ways, the cornerstone on which it all hangs. Why not join me as I pull together all the pieces?

Introduction to Docker Images
Introduction to Docker Images
Hi, I'm here to talk to about Docker Images. Along with other container technologies, Docker has significantly changed the way many public facing application services are provided. Containers are fast, extremely versatile, and incredibly good at squeezing every last ounce of value out of your physical hardware. Since so much of the Docker advantage hangs on the proper design and creation of images, this is one part of the technology you'll really want to understand well, so stick around. I'm going to assume that you're already familiar with the basics of installing Docker on a workstation or server and firing up at least a simple container. If you're not so sure you're quite there yet, no problem. There are a few courses her on Pluralsight that can quickly bring you up to speed. One good place to get caught up is my own Using Docker on AWS course. Now about this course, here's what's coming. I plan to introduce you to the basic structure and function of images within the broader Docker ecosystem, and at least briefly touch on the various services currently available for managing images and making them available so that Docker can do all the Dockery stuff that will prove so useful. I'll show you how to acquire, build, and share images and talk about Docker image best practices. We'll build our own Docker Registry, a locally-hosted repository for Docker images so that teams working within a private network can efficiently share images with each other. We'll also learn about how images can be secured in transit so we don't end up pulling or pushing images that have been maliciously compromised, and finally, we'll go just a bit deeper into a couple of newer image- administration tools, like Docker Cloud. Now, let me describe my work environment just a bit. I'll be working from a terminal on a virtual instance of an Ubuntu 16. 04 Linux machine running on VirtualBox, but that's not nearly as significant as you might think. In fact, Docker now runs happily on any of the three big operating systems. So once you get it nicely installed, the Docker engine will work pretty much exactly the same way no matter where you're using it. If you don't mind though, there is one small Ubuntu trick that I will use. By default, running Docker commands from a terminal requires administrative privileges, which can be inconvenient over long stretches of time, or insecure, or both. So I'm going to edit the /etc/group file and add my user who happens to be named Ubuntu to the Docker group. From now on, my regular user will have automatic access to Docker-related resources on my system. By the way, I've created a page for the course on my bootstrap-it. com website. It's at bootstrap-it. com/docker-images. This page has all the lab setup information, configuration and script files, along with the many commands we'll be using throughout this course. Please use the page as an easy resource to avoid having to type out everything yourself and also as a review tool. One of the things I added to the page is a simple script you can use to really quickly install Docker Engine onto an Ubuntu or Debian machine. If I'm using a cloned VirtualBox VM, the script can get me from 0 to a fully-provisioned Docker machine in just a bit more than 2 minutes. We're all set to start, but first, a simple question. Just what is an image? It's a software file containing a snapshot of a full file system. Everything necessary to launch a viable virtual server is included. An image might consist of just a base operating system like Ubuntu Linux or the tiny and superfast Alpine Linux, but an image could also include additional layers with software applications like web servers and databases. No matter how many layers an image has and how complicated the relationships between them might be, the image itself never changes. When an image is launched as a container, an extra writable layer is automatically added into which the record of any ongoing system activity is saved.

A Survey of Key Docker Image Management Tools
In this clip, we're going to discuss some image-management services. It's nice that the Docker Toolbox is full to overflowing, giving us all kinds of choices. To some degree however, distinguishing between the many choices and figuring out which is best for your needs can be a bit confusing. I'll try to clear things up, at least a bit. When you install Docker on your Linux, Mac, or Windows workstation, or any cloud-based server for that matter, the key to it all is Docker Engine. Docker Engine negotiates for access to shared hardware resources with the operating system kernel and provides a daemon through which you can launch, manage, and shut down containers. While we'll soon see how a number of external tools exist to help you visualize or abstract some of your more complicated Docker tasks, the truth is that there is really very little you can't do using only the Docker Engine. The first place you're likely to come face-to-face with Docker Engine is on the command line when it responds to commands beginning with docker, like docker images that will list all of the images currently available on your local system. The ways you build or download and then consume images are all managed by Docker Engine. At least on Linux systems, important data is kept in the directory var/lib/docker. Drilling down a couple of levels to the layers directory shows us existing image layers represented by these hashes. It's Docker's storage drivers like the aufs unification filesystem that provide the organization allowing layers from multiple sources to be stacked up and utilized as though they are all part of a single object. This is how Docker can instantly know exactly what's already available locally that can be put to work on new projects without the need to download or load the same objects over and over again. Whether or not you'll notice a difference right away, the time and resources you'll save over the long term thanks to this architecture will be significant. You can think of Docker Hub as a GUI front end to the docker push and pull commands you run from the command line on your local workstation. Besides creating and deleting repositories and managing which users can access your images, there really isn't all that much you can do on the Docker Hub website, well besides create and manage organizations that is, oh, and research too. Clicking on the Explore link at the top of the page presents you with a couple of pages of the most popular official repositories, including the NGINX web server running on Alpine Linux, Ubuntu, and Docker Registry for building your own image repository environments within a Docker container on your own local server. We'll definitely come back to this one later in the course. I'll click on a repo busybox, which is so small it makes Alpine Linux seem bloated, where we're given the docker pull command we should use from our local Docker Engine to get the image. A bit further down the page, we get a description of what BusyBox is and what kinds of things you can do with it, and then some sample commands and Dockerfile elements. One more thing about Docker Hub, you're allowed an unlimited number of public repositories for free, but since many projects require at least some protection for your intellectual property, most people will eventually require more than this single private repo you're allowed on the free tier. Extra private repos are billed by volume. Clicking on the Get more link at the top of the screen will show you how much financial punishment you're likely to face. Docker Cloud is a resource administration service that you manage through your browser. What Docker Cloud does do is efficiently and visually coordinate all the resources you want to run. You can store your images on Docker Hub within a Docker repository, run your own Docker host running within the local infrastructure using their bring-your-own-node feature. Similarly, you tell Docker Cloud where you want your containers to run, whether it's your Amazon Web Services, Azure, or DigitalOcean account, or your own workstation, assuming that it's got Docker Engine installed. The idea is that you install a cloud agent on whatever you're using for the host node, and the agent will act as a control interface, carrying out any instructions it gets from Docker Cloud. The agent actually runs as a number of Docker containers, but you never need to actually see all that if you don't want. From now on, you'll only need to deal with the browser console, using it to select images and deploy them to deliver services through node clusters. We'll see how all this works in a later module. The Docker Cloud module is quite similar to Amazon's EC2 Container Service. ECS itself is really nothing but an administration interface that coordinates images stored either in Amazon's own EC2 container registry or in a Docker Hub, and launches them to an EC2 virtual machine instance that has ECS agent installed. I would say that the two key differences between Docker Cloud and ECS are that Amazon's service is obviously focused more narrowly on integrating with AWS-based resources, and that the ECS paradigm is a great deal more difficult to understand, but then I guess I have an interest in saying that, as it will make it all the more likely that just as soon as you finish this one, you'll run right over and take my Using Docker on AWS course, which spends most of its time on ACS. Just two more services to discuss, Docker Trusted Registry and Docker Registry. Docker Trusted Registry, DTR, is like the Docker Engine you run on your own workstation in that it can be used to store and manage your images. However, in two ways it's much bigger than that. First of all, DTR is designed to be deployed as a network server, much like Docker Hub, but under your complete control. And secondly, it's closely integrated with Docker Datacenter. Datacenter, like Docker Cloud, is a browser- based administration tool for orchestrating complex combinations of container resources. Unlike Docker Cloud, Datacenter must be hosted on your infrastructure, whether on a local server, or within a cloud platform like AWS. Both Docker Trusted Registry and Datacenter are commercial enterprise products. I'm not going to spend time in this course discussing the Docker-trusted registry because that's already been nicely handled by Elton Stoneman in his Getting Started with Docker Datacenter course, here on Pluralsight. But I do plan to spend a lot more time on Docker Registry without the trusted. Docker Registry is a free toolset that you install locally to create and manage your own images in a private environment. DevOps operation might need to manage images within a private network, support a continuous integration deployment workflow, or deploy a cluster of nodes built on your customized image. That's it for our Docker Images introduction. It's time for the review of what we've seen so far. We saw that docker images will list all images available locally, docker build will create an image, and docker data is kept in the var/lib/docker directory. Docker Hub is a public repository of images, both official vendor-supported images and others built by people like you. Docker Cloud is a hosted browser-based administration interface for managing your own Docker-related resources. Amazon's EC2 Container Registry is a repo way you can store your own images that's closely integrated with Amazon's Docker host EC2 Container Service. The Docker Trusted Registry is Docker's own premium image repository. Trusted Registry is part of Docker's enterprise service, Docker Datacenter. Finally, Docker Registry is Docker's open source repository that's designed for active, private network deployments. In the next module, we'll play around with Docker Engine to pull images from Docker Hub, create a few of our own images using Dockerfiles, and explore some best practices used to get the most out of your images.

Managing Images Using Docker Hub
Core Docker Image Management Principles
If you want to make full use of Docker's various image management tools, we'll need a bit more insight into the basics of the way the Docker image ecosystem is designed, how images are pulled from and pushed to remote repositories, how to build your own images, and what is the stuff of which images are actually made? But first, I'll talk a bit about why images are so important to Docker and how that relationship shapes the way we build them. Among the key features that attract so much attention to Docker are speed, and what I'll call predictability, both of which owe a lot to the core design of the Docker image. A great deal of Docker's speed is due to the way images are divided into independent layers, each of which can actually be shared among multiple images. Among other things, this means that depending on what you've already got on your local storage area, pulling new images from remote repos can sometimes require downloading very little new data. We'll talk more about how layers are added to an image later on. Now, what about predictability? Docker lets you launch containers that provide identical environments no matter where they're hosted and no matter how many times they've been launched before. This can be really useful if you're sharing your application with colleagues, working remotely, or if you're deploying your application live to production. Either way, you have a keen interest in knowing that all the code and software and system dependencies are there exactly the way they were when you created them. Basing your environment on Docker images is way better than shipping your applications with painstakingly documented instructions on how it should be installed and deployed, or even working from snapshots of virtual machines. A Docker image is the documentation, and with a single command can become the machine. So by predictable, I mean that a Docker image will reliably deliver exactly what you expect it to, that is, you can be confident that nothing affecting your container's behavior will change between leaving your workstation and reaching the rest of the world. We'll talk more about what you have to do to ensure this happens a bit later in this module. Now, however, it's time to work through some of the basics.

Building and Pushing Images
In this demo, I'm going to show you how to find out what images are available both locally and on the Docker Hub, how to peek inside an image to learn about the layers of which it's made, how to write a Dockerfile that precisely defines a new image, and how to build the image from the Dockerfile and confirm it worked out the way we wanted by running it as a container. You list the images that are currently on your local system with docker images. There's a lot of output here so I'll put it on a slide. We're shown the repository name, tag, image ID, history, and size. Docker search will look through Docker's primary online repository, that's Docker Hub, for freely available public images whose names and descriptions match your search terms. My search for an image based on the Ubuntu operating system with the Go programming language environment preinstalled, returned a nice variety of results, including an official image, which, as we can see, has all kinds of star ratings. Those two points are important, as you really want to avoid taking chances with shady images that could have malware embedded. So remember, official is good. When you find the image of your dreams, you can incorporate it into your system, either by referencing it by name in some kind of script like a Dockerfile, or by pulling it directly from the command line. I'll pull what's got to be one of the smallest operating systems known to mankind, Alpine Linux, the barebones security-conscious distribution that's often used as the base for Docker images. Docker images shows us that Alpine is there and just how small it is, less than 4 MB. I'll run docker history to take a look at what's inside. Now I'll admit that in this case, history tells us very little that we hadn't already seen using docker images, but there is one item of interest, the CREATED BY column, which appears to be the invocation of the sh shell that runs an ADD operation against that file. Let me illustrate what's going on by running history again, but this time against the Ubuntu image that's already on my system. This one is half a dozen lines. The file that was the target of this ADD operation at the bottom is a whole lot bigger than the one we saw with Alpine. But then you'd expect that considering how much more functionality comes with Ubuntu. The missing image names is nothing to worry about, it's a product of the recent transition to the new content addressable storage model, adopted with Docker version 1. 10. Let's build our own image. I've written this simple Dockerfile with all the ingredients needed to create an image of an Apache web server on Ubuntu. The FROM line tells Docker that the image will be based on the Ubuntu 16. 04 image, which will take up the first layer of our new image. The two RUN lines will execute apt-get update and apt-get install to install Apache. I used apt-get rather than the newer apt, as apt is potentially less reliable in a scripted setting. ADD will copy a file I'm going to create in the current directory called index. html to the web root directory, var/www/html/. This will make the file available to users browsing to the web server URL. Of course, I could have saved the file to any spot on the file system. The CMD line makes sure that Apache will actually remain running when the image is launched as a container, and EXPOSE 80 will open up port 80 for web browsers. I'll use the nano text editor to create a one-line inspirational welcome page for my site. I'll save it using Ctrl+X and S, and then run docker build to create the image. Dash T associates the tagged web server to the image. Running docker images once again should show us our new image. I can run a container based on the image using docker run and the image named webserver. The -d tells Docker that I want to run the container detached, meaning I don't want to open a shell within the container. By default, the container will launch into the bridge network, so I'll run network inspect bridge to get the container's IP address. Here it is in the 172. 17. 01 subnet. Now I'll run curl against that address to see what kind of web server we're running. It looks like a nice and friendly web server based on the index. html page. Docker ps shows us our running container. However, since being friendly will only get you so far in the dog-eat-dog world of IT administration, I'll use docker stop to shut the container down. Well, we've created our own image based on a Dockerfile script, successfully run it as a container, and shut it down. One more thing, and we're done with our Docker images 101 section, pushing our new image to a Docker Hub repository so I can share it with my friends and family, although while it is cheaper than flowers, I'm not at all sure what my wife would do with it. I already have a free Docker Hub account, so I'll only need to run docker login to authenticate and connect with the repo. Let's run docker image once again to see our image details. Now I'll give the image a new repository name using docker tag, the current repo named webserver, and a new name based on my Docker Hub account name. This tag, by the way, has nothing to do with the existing latest or 16. 04 tags you see on the docker images output. With that, we can use docker push to upload the repository to Docker Hub. Once that's done, I can prove it worked from my repo page on hub. docker. com.

Docker Image Best Practices
If you want to make the most of Docker's predictability and speed, you'll need to adapt your operation to match some peculiarities in the Docker system. First of all, you'll want to create images that are as stable and reliable as possible. That means being very careful to avoid contaminating them with, well, unpredictability. So in general, you won't want to create an image from a running container using docker commit. There's nothing stopping you from doing that, by the way. I've got a container running based on Ubuntu, as we speak. I'll use ps to get its ID, and then run commit against the ID followed by a name I'd like to give the new image. It will work, but it's a bad idea. Why? Because once you've been playing around for a while, you have no way of being absolutely sure what file system or configuration changes have been applied, and you'll be unable to confidently reproduce the image. Building your images cleanly from Dockerfiles on the other hand gives you total control. Similarly, you can control your images by carefully managing which base images you use in the FROM line in your Dockerfiles. This might sometimes require you to ignore what's normally a fundamental principle of responsible administration, always go with the latest patched version. Docker certainly lets you do that, usually by adding a colon and the word latest to the distribution name for pull commands or FROM lines in your Dockerfile, but when you're managing dev and production cycles, that can sometimes end up causing you some serious discomfort. The problem is that even though the latest version is likely to be the most secure, it won't necessarily be completely compatible with the applications you've got running on it. Once the upstream developers have rolled over to a new version, a Dockerfile line reading FROM centos:latest will deliver a different software stack that could unexpectedly break your application. The solution is to hardcode the release version you want into your Dockerfile or pull command by specifying by number. This example will pull CentOS 6. 6. Of course, you'll have to deal with the possible security holes that might be opening up, but life is all about compromise, isn't it? A lot of Docker's speed is achieved by keeping images small. Docker Engine does its bit by sharing locally-stored layers between images wherever possible and through some really smart caching, but no one's exempt from doing their part in the Battle of the Bulge. I'm going to demonstrate something that I first saw on Red Hat Developers blog. It gets to the core of how Docker images are built. Each new run command in a Dockerfile will generate a new layer in the resulting image, and each new layer takes up extra space. Therefore, squeezing multiple commands into a single run line by using two and characters will actually make a big difference. We'll I'm not sure that's always true. I ran a few of my own experiments, and there were cases where the differences were negligible, but here's one where there's no doubting the results. I'm going to create two versions of the Dockerfile, build images from both, and then compare their sizes. First, I'll create a new directory, which allows me to use a new Dockerfile without messing with any of the others I might already have created. Now I'll paste the multiple line version. We start with Ubuntu 16. 04 as our base, then use apt-get to update the repository index, and then install curl, which we'll need for the next step. Now the next commands actually make no sense as it assumes that I already had a version JBoss's WildFly Application Server running on this machine, which isn't true. But the JBoss download happened to be the example from the Red Hat blog, and it still illustrates the point perfectly well, so I'll stick with it. But still, why should I actually use make dir to create a JBoss/WildFly directory tree you might ask? That's because every extra step, no matter how trivial, can become an extra layer. Just to explain what those commands will do, make dir creates a directory tree, I just mentioned, into which we'll eventually move the program files we'll extract. Next, I'll change directory to /tmp, and using curl, download the latest version of WildFly. Then I'll use tar to extract the archive, move it to its new home in op/jboss/wildfly, and finally remove the original archive. I'll save the Dockerfile and run docker build, tagging the image manylines. The dot at the end, by the way, tells Docker to look for a Dockerfile script in the current directory. I'll pause the video while Docker goes about its business, but I will note that Docker's use of existing layers and cache means that nothing will be downloaded unless absolutely necessary. Now I'll edit the Dockerfile, and by adding and characters, combine many of the commands into a single run line. If I mess something up and corrupted a command, it will exit with a code other than 0, and the docker build operation will fail, reporting the error as part of the command output. Either way, I'll save the file and run docker build again, this time tagging the image as one line. With that done, I'll run docker images to compare the image sizes. To make it easier to read, I pasted the output to this slide, and the difference is amazing. Just by including all the commands in a single run line cut the file size nearly in half. So the way we write our Dockerfiles can have a huge impact on size and performance. Before we go on, I'll just delete the two images we just created using rmi. As the blog post pointed out, this principle can apply in other cases. For instance, if you had the choice of basing an image on the latest version of a distribution like CentOS or an earlier version, but then updating within your Dockerfile, you really want to go with the latest release. That's because invoking the yum update, or dnf update, as it's more commonly done now, will add a significant size overhead to your image. It's review time. You didn't think you were getting off that easy, did you? We talked about how so much of the Docker advantage hangs on two principles, speed and predictability, and noted how important it is to build images with those two things in mind. We used docker search to find out what useful images were available on Docker Hub, and docker pull to download an image to our local storage. Docker history gave us an ultrasound view of the insides of an image. We wrote a Dockerfile script as a template for building an image that specified a base image and added a local file to the image's web root directory. I then pushed an image up to my Docker Hub account and took a look. Finally, to demonstrate how you can control the size of your images, I created images from two separate Dockerfiles, one with separate run lines for each system command, and the other with all the commands compressed into a single line. In the next module, I'll install and operate my own private Docker repository using Docker Registry. Don't miss it!

Installing a Private (And Free) Docker Registry
Installing Docker Registry: The Container Method
Unlike Docker Trusted Registry and Datacenter, both of which I briefly discussed a while back, Docker Registry is one of the core open-source Docker projects and will, by definition, always be available for free. Because Docker Registry servers are installed locally, they can be both really secure and really fast. Even if you decide to install your Docker Registry server on a cloud platform like AWS, because you control it, you can easily restrict access to a secure VPN, or at least to clients able to find their way in through firewall rules, like those of this AWS security group. It may somehow be possible to torture a Windows or a Mac PC into running Docker Registry, but I can't imagine the whole thing being worth the effort. For most of us, working with a virtual Linux server on VirtualBox is probably a cleaner solution. I will note that if you're using a Mac that's already got Docker running, then it might not be possible to launch VirtualBox at the same time. I'm going to use Ubuntu, but I'll also note any differences between what I'll be doing and the way it's done on CentOS or Red Hat as we go. Whatever distribution you're running, you'll need to make sure that port 5000 is open on the server. By default, Ubuntu ships with all ports wide open, so that won't give us any trouble, but if you're using CentOS or Red Hat, you'll need to open the port manually. You can do this with this firewall-cmd and add-port=5000. The second command with the --permanent flag ensures that this rule will persist through a reboot. If your server is running on a cloud platform, then naturally you'll need to open up port 5000 on their security group. There are two very different ways to install Docker Registry on a server. One is by installing the Docker Registry package through your package manager. The other approach involves pulling the Docker Registry image and running it as a container. To be honest, I'm not sure which approach is preferred. On the one hand, Docker documentation seems to get the container option more visibility, but I've also read that container-based registries don't scale quite as well, who knows? Either way, while I'll show you how to install for a package manager in the next clip, right now I'll quickly pull the image from Docker Hub. How to pull the Docker Registry image won't be a struggle for you to figure out at this point. There is no reason why we shouldn't request the latest version, whatever the version number might be. The -p flag on my run command opens up port 5000. I could also add a -d to detach myself from a container, which is probably the way you'll normally go. Doing it this way, without the -d, will add you to the shell so you can keep an eye on pulls and pushes by reading the ongoing system output. The downside to this is that bailing out by hitting Ctrl+C will close the container. That's the container method. Coming up next, getting it done with the package manager.

Installing and Using Docker Registry: The Package Method
Now, as advertised, I'll get Docker Registry going using the Debian Ubuntu apt software package manager. For CentOS machines, just substitute yum for apt. We'll make sure that our installation is successful, which may prove a bit more complicated than you think, then push an image to the registry and confirm that everything is working. I'll start by updating the local apt index so we'll be sure we're getting the most recent version available from Debian repositories. Then it's just a matter of apt install docker-registry, and we should be done. Since Ubuntu 16. 04, like recent versions of most Linux distributions, uses the Systemd process manager, I'll start it up with systemctl start, and confirm it's running with systemctl status. Whoops, we've got ourselves a bug. As it turns out, this version of Docker Registry is still a bit behind, and there's been a problem with Docker establishing appropriate permissions for certain configuration files during installation. That's what's behind all this. The bug was patched more than a year ago, but the patched version hasn't yet found its way into all the upstream repositories. By the time you read this, everything might well be working fine, and I would recommend that you try the apt install version first. If you do have trouble though, here's what we're going to do. I'll browse over to the Docker Registry page on the Debian repo pages. The best way to find it is to search for something like debian download docker. From there, I'll scroll down and click on the AMD 64 link since my VM is 64-bit. The quickest way to get this done is to find a download link, right-click, and get the URL, and then back on the Docker server, add it to a wget command. That will download the package. To install it, I'll run dpkg -I, and the name of the package we just downloaded, apt will do the rest. Now we're back where we should've been a few minutes ago. I'll start up Docker Registry once again, and check its status. This time it looks fine. I'll check the Docker Registry version once again to confirm that we're working with something a bit more up to date, and then use systemctl enable so it'll launch every time the system boots. Okay, we're finally ready to take our registry out for a test drive. I'm going to download a really small image from Docker Hub by running hello-world. When that's done, I'll use docker images to see what's new, just an image called hello-world. I'm going to push a copy of this image up to my Docker Registry, which is distinct from the images stored locally by my Docker Engine. Docker knows where to send an image when it's pushed by looking at the address label. The address label is actually contained in the tag you assigned to the image. I'm going to give the hello-world its address tag through docker tag. Since it's running on the same machine, localhost is the address of the registry, but you could also use an IP address or a domain name. Five thousand is the port you'll use to access the registry, and hello-world:latest is the image name. Docker images shows us that we now have two images in our local collection, the original hello-world image and the one we just tagged. I'm going to push the tagged image, (Typing) which seems to go without trouble. To confirm our success, I'll remove the two local images using rmi for remove image and -f to force it in case either image is in use by a container. I'll pull the image back into our local system using its full tag and name. There's no way that this one could've come from Docker Hub, not only because the address we gave it is all wrong, but because it came so quickly. Of course, we were able to pull this image from our registry only because we happened to know its name. It would certainly be helpful, especially once registry activity starts heating up, to be able to see what's available at any given time. As strange as it seems, the most direct way to get a list of images in a registry that I've found so far, is by using curl against the V2/_catalog file in the Docker Registry file system. This will also work from any machine with network access by specifying the registry's address. Nice! We've got a working registry all set to serve images to clients near and far, well, nearly all set. Before we're really done, we'll have to make some decisions about where the images will be stored, and we'll really need to give some attention to securing our images as they move back and forth between clients.

Docker Images Storage
Building your own Docker Registry is all about storing and managing your images on your own terms, but that also makes it your responsibility. You are now the here, where the buck stops, and you're the one who's got to make sure your images remain reliably available over the long term. So then it makes sense to spend at least a couple of minutes talking about just where and how those images are going to be stored. By default, a package-based Docker Registry will store its images and directories within the var/lib/docker-registry/ tree. Now this is a big tree as it's taking us quite a few levels down before we finally get to the repositories directory, which contains our hello-world image. But wait until you see where the container-based Docker registries put your images. I'm not going to drill down there manually, but it is important that I explain what it all means. You'll notice that the first directories, var/lib/docker, are the same directories on a host machine used by the package-based registry. That, more or less, is where all Docker resources are kept. However, the next level down is volumes, which are data objects for holding and managing Dockerfiles. You can actually create and populate your own volumes and mount them into containers as they launch. In fact, I think I'll show you how. Docker volume create will build an empty volume using, by default, the local driver. You could also import driver plug-ins like Flocker, which you could then specify with the -d flag. Once it exists, a volume can be added to a container as part of its run command. So what does of all of this have to do with us right now? Well, Docker Registry stores its images within a volume on the host machine. Thus, this nutty hash of a title is the name of the registry volume. Everything below that is the more or less normal directory hierarchy you'd expect, leading eventually to our images. And just why is this important? Because if you don't know where your images are kept, how are you going to be able to back them up? Didn't of that, did you? In many cases and for many reasons, it can make sense to outsource your storage. Perhaps you'd rather have Docker Registry save your images to a bucket on Amazon's S3 Service, that way, you can rely on Amazon's robust reliability and security while, as often as not, save yourself some money. Well it can be done, and not only using S3 but other cloud services like Azure Blob Storage, Swift, and Google Cloud Storage. Walking you through the process is a bit beyond the scope of this course, but Docker's storage driver API was built for just this purpose, and there is reasonably complete documentation for each of the available platforms. Let's review. You learned how Docker Registry can be installed either through your yum or apt package manager, or as a Docker container. You point an image to your registry by tagging it with the network address of the registry and the port through which it can be reached, which is usually port 5000. By default, installed registries store images beneath the var/lib/docker registry directory, while container registries use a Docker volume beneath var/lib/docker. Finally, you can also have your registry store images on cloud platforms like Amazon's S3, using the storage driver API. That's storage taken care of. But there's one more thing you'll need to master if you want to roll your own image repository, security. How can you be sure that there's no one messing with the images flying back and forth over network connections? That's the subject of the next module. See you there.

Securing Your Images in Transit
Docker Registry and Security
Our Docker Registry is indeed running and patiently waiting for our clients to start pulling and pushing images, but there is a bit of a problem. Unless you happen to be on the same host machine on which the registry is installed, you won't have access. That's because Docker Registry only accepts activity coming over encrypted connections. No encrypted connection, no pulling and pushing for you. Let me show you how it works, or actually how it doesn't work. I'll use curl to list the images as we did earlier, but from a remote machine with Docker installed. Then I'll try to pull an image down to the machine. It's a no go. The registry was expecting an HTTPS request. Why is Docker Registry built this way? It's all about elementary security. Even if you're only planning on sharing the registry among local clients, all unencrypted traffic between any two hosts will be plainly visible to man-in-the-middle attackers. If you've got anything sensitive built into your images, and most of the time you probably will, then that data will very likely soon be part of the public record. Okay, so to make our registry at all useful, we'll have to find a way to enable proper encryption. In this module, I'm going to explain how to use the certificates issued by a recognized certificate authority to encrypt registry traffic moving back and forth between your clients. In fact, I'll show you three different ways to apply certificates, each with its own advantages and disadvantages. To make it easier for you to try out encryption within test environments, I'll also demonstrate using self-signed certificates. Deploying self-signed certificates will take a bit of extra work, but it's not nearly as hard as you might, at first, imagine. Finally, we'll talk about a second important aspect of image security, configuring login authentication for your Docker Registry. Let's get started.

How to Apply a Certificate from a Certificate Authority (CA)
To make this work, you'll need to have a certificate for your domain issued by a certificate authority. This can be the same certificate you use for your website traffic, so the odds are that this won't require any extra work on your side. If the domain you're planning to use doesn't already have a certificate, you can now request free certs from Let's Encrypt. Not sure how that works? Well I just happen to have a course here on Pluralsight that can help, Linux Encryption Security. Once you've got your cert, you can copy the. crt and. key files you're given to their own directory. You can also leave them where they are and point to their location in the next step, your choice. I will note that if the package you've been sent by the CA includes an intermediary file, usually ending with. pem, then you'll need to combine the. crt and. pem into a single. crt file. With your cert files in place, you're nearly there. If you're running Docker Registry using the registry container, then you can incorporate the certs into your run command like this. Let's work through the syntax. We're already familiar with docker run and -d to detach from the container at start, and -p to specify the access port. Restart always tells the system to launch the registry on reboot, and --name tells us the name to use when identifying the running container. Now we get to the new material, -v defines a volume, which can really be any location containing data you'd like to include in the container. In this case, the location will be based on the present work directory and specifically include any files in the directory called certs. You can obviously point to whichever directory where your cert files are currently living. Dash e refers to environment values. These two lines point directly to the. crt and. key certificate files. The last line is the instruction to launch the image called registry. This assumes, of course, that the name of your domain is stuff. com, something that's highly unlikely, but works well as an example. That's it! Once the container loads, both the server and its clients will automatically be using the certificate to encrypt requests and responses for both pulls and pushes. You can also package your cert files into an image using a Dockerfile. In this example, we use the registry images as base, add the certs directory and its contents to /home on the container, and set the certificate and key environment variables to use our keys. It will also open port 5000. There's one more way to use certificates, and this one will work for both container-based and installed registries. Docker Registry configuration can be controlled by a file called config. yml that's kept in the etc/docker/registry directory. As you can see, the file is divided into sections like storage and http. I'm going to add a subsection to http called tls, which, by the way, stands for transport layer security. Then I'll add a value for certificate, which will be pointed to the stuff. crt file within a certs directory that doesn't yet exist, and another value for key aimed at a. key file in the same new directory. But right now, I'm just doing this to show you how it's done, but I will actually use this configuration in the next clip. If you're working with an installed registry, then all that's left is to restart the service. If you're running the registry from a container, then you'll need to add the file as a volume along with the run command.

Working with Self-signed Certificates
Okay, we're finally ready to try this out, but I'm going to do it using a self-signed certificate, so I'll have to jump through a few hoops to properly set up the lab. Even though using self-signed certs is definitely not normally recommended, it will have two benefits in our case. One, it's something I can easily demonstrate even on my local test environment, and two, it's a great way to help you visualize how the certificate process actually works. First, I'm going to use local DNS settings to enable the use of a stuff. com domain for machines on my local network. I will then generate a self-signed certificate for stuff. com and install the cert files on my Docker Registry host. Finally, I'll copy the. crt file from my registry host to each Docker client I want to have access to the registry. Let's get going. On Linux machines, requests for network domains are first sent to a local file called hosts that lives in the etc directory. Now there's an entry in hosts matching the requested domain name, then the request will be forwarded to the associated IP address. If there are no matching entries in the local database, then the request will be sent out to public internet DNS databases. What I'm going to do is add an entry to the host file on this machine that I'd like to use as a Docker client. The IP I'm using belongs to my Docker Registry server. I would normally do this on all the machines on my local network to which I'd like to give domain name access, including the registry server itself. Now, I'll move over to the machine that has Docker Registry installed, the server. I'll create a directory beneath my users home, called certs. I'll then run this openssl command to create cert key files and save them to the certs directory. If you want to learn more about what this command is actually doing, check out my Linux Encryption Security course. But what's important for us is that I've named the crt and key files stuff. You'll be taken through an interview where your domain profile is established. Since we're self-signing here, the only value that really matters is the final question about the common name you're giving the domain. This one has to be accurate and will be stuff. com in my case. You'll remember how I added cert file values to the config. yml file earlier. Now we'll just take a quick second look to make sure they match the files I've actually created. Looks good. To make these changes take effect, I'll restart the Docker Registry server. The last thing I need to do on the server is securely copy my crt file to the client, scp is a great tool for this purpose. Let's move back to the client. I'll rename the stuff. crt file to ca. crt and then create a directory tree beneath etc/docker, starting with certs. d with stuff. com:5000 beneath it. I'll need to get admin powers to do this through sudo since it's in the etc hierarchy. Two more steps. The first is copying the ca. crt file to the new stuff. com:5000 directory, and the second is restarting Docker, not Docker Registry since our machine isn't running DR. We can test the whole mess out by seeing if we can pull an image from the remote registry. Whoops, forgot to type docker. Let's try that again. Success! Before you get too carried away with your well-earned victory celebrations, I should tell you that setting up encryption will have an effect on the way you access Docker resources on your registry host. For instance, using curl to get a list of images the way we did in the previous module will no longer work. However, adding https to the URL should help, unless that is, you're using a self-signed certificate as I am, rather than one from a certificate authority. As you can see from the error message provided by curl, if you're absolutely desperate, you can still make it work by adding the --insecure flag. Just remember that the information will be moving through the network without any encryption at all.

Configuring User Authentication on Docker Registry
Having used encryption to secure the transfer of images into and out of our Docker Registry, we've made it much more likely that our data won't be intercepted and abused. But how do we know that the users pushing and pulling into the registry are our users? How do we know the person on the remote machine didn't get there illegally? The most obvious first step to solving that problem is to enforce password-based authentication, and here too, Docker has got you covered. Relying on the certificate files we're already using, we'll generate user accounts and passwords and configure DR to require a login before anyone can access the registry. I'll begin by creating a directory called auth within my user's home directory on the DR server. Next, I'll run the DR registry image as a container for the express purpose of adding the user name and password combination to a file called htpasswd in the auth directory. I used two right arrows so the operation will append the new data to any records that might already exist. As it turns out, there's no such file yet, but this is a good habit to get into all the same. Using one right arrow will overwrite all the current contents of the file with the new text, something you usually won't want to happen. This will, if necessary, pull the registry image from Docker Hub and populate the htpasswd file with only the user name and an encrypted version of the password I specified. There are, of course, other ways to generate htpasswd files, in particular Apache's htpasswd program from the apache2-utils package. But this is a Docker course, right? So we'll need to tell DR about the new password file by once again editing the config. yml file. I'll add a new section called auth and add these three lines. Make sure you provide the correct path to the htpasswd file we just generated on the path line. Since we've added to the config. yml file, we'll need to restart DR before the new settings will take effect. I'll head over to a Docker client and try pushing a local image to the registry. Docker images tells me that there is an Alpine Linux image in our local collection. Docker tag will address it to our stuff. com server. I'll try docker push, but remember, we haven't provided the basic auth credentials. Well I'll be, it didn't work! We can fix that pretty fast. We'll use docker login, but rather than logging into my Docker Hub account, I'll specify the stuff. com server, and when prompted enter my login credentials. Once that's done, I'm able to push images to my little heart's content. It's time for some review. We learned how Docker Registry expects remote requests to use TLS encryption, which can be provided either through the certificate files issued by a recognized certificate authority, or through self-signed certificates generated locally. You tell DR about your cert files in any one of three ways, when running a container, through the -e environment argument, as part of a Dockerfile configuration, or through the config. yml file that can be added to a container as a volume at run time. If you decide to use a self-signed certificate, you'll need to copy the. crt file to each client that'll be connecting. The. crt file should be named ca. crt and saved to the etc/docker/certs. d/stuff. com:5000 directory. And finally, we learned how to set up password authentication to our DR server through generating an htpasswd file and adding the htpasswd configuration to the config. yml file. Up next, I'll look at a few more image-management tools.

Other Image Managing Tools
Digital Signing for Images Through Docker Content Trust
In this final module, I'll introduce you to a couple more important tools for managing Docker images. We'll begin by returning to the subject of security, which in the IT world at least is a subject you can never really leave. In particular, I'll demonstrate how to enable Docker Content Trust to validate images as they move between hosts, and then we'll take a brief tour of the commercial administration tool we mentioned awhile back, Docker Cloud. Digital signing is a way for people at two ends of a data transmission to know that the transfer has not been intercepted or corrupted. When both the sender and the recipient possess matching encryption keys, metadata, like hashes, associated with files, can be exchanged and verified against known copies. When a client wishes to push a new image to a trusted registry, the image must first be signed using the client's private key. Anyone with a matching public key will subsequently be able to confirm the integrity of an image copy by decrypting the signature. Key and signature administration is handled by Docker Content Trust through the Notary utility. However, from a client's perspective, most steps of the process are managed for them invisibly by Docker Engine. Besides having to authenticate with a passphrase, all other command line tools will work exactly the same as before, with the one small caveat, Docker Content Trust won't work at all on a client until you enable it. All that's needed for that to happen is to export the appropriate environment variable to your system. Here is how you slay that dragon. To permanently enable content trust, simply add the export DOCKER_CONTENT_ TRUST=1 line to your etc/profile file. While we're there anyway, it's always a good idea to add a comment line describing what we're doing, (Typing), save the file with Ctrl+X, and apparently nothing at all has changed. That's because etc/profile is only read at the start of a new shell session, so you could now either log out and then log back in again, or simply export the value from the command line to this specific shell session. Echo shows us that this time we're set. That should keep us happy until we log in again. Let's try it out. I'll try to pull an unsigned image of redis in a repo called xataz, I really hope I pronounced that correctly. I don't imagine there's really anything wrong with this image, by the way, I'm just using it as an example of how things work when data hasn't been signed. So let's fire off a request to see if anything blows up. Aha! We're getting a remote trust data does not exist error. Now if I were to disable content trust by exporting it with a value of 0, and then try pulling the image again, it will work. Docker puts me in the driver seat all the way. You should be aware that this protection is not only active for command line requests, but also for from entries within Docker files. That's pulling. How about pushing a new image up to a repo? Well, why not try it for ourselves? I'm going to log in to Docker Hub and then give a new tag to the swarm image I've already got available locally that will point the image up to my Docker Hub repository. As you can see, I'm prompted for a root key passphrase. This is because I previously pushed images from another content trust-enabled machine and was prompted at that time to create passphrase. I should also note that the first time you log in, a root key file is created and saved to the. docker/trust/private/root_keys directory. If you expect to work from multiple machines, especially if you use VMs a lot, then you should save the contents of the root key and of your root passphrase in a secure password vault of some sort. To make this work, obviously, I previously copied my root key file from my password vault and recreated it in the root keys directory of this VM. In any case, I'm going to paste my root passphrase in at the prompt and then enter a different and new passphrase that will be used for the new repository key. Once that's in, Docker Engine and Content Trust will do the rest. While Notary will usually act as Docker Content Trust's silent partner, you can also work with it directly from the command line. For more complicated cases, especially relating to the administration of Content Trust servers, you'll probably have no choice but to dig deep into Notary's functionality and make a study of it. I'm not going to go there in this course, but I will just note that the Notary CLI is available through regular package managers like apt. You can use the CLI to learn about existing images, both local and remote. This list command will display the hashes from the Alpine image on Docker Hub. Besides listing hashes, you can also initialize, add files to, and publish your own collections.

Image Resource Management Through Docker Cloud
As I mentioned way back at the beginning of the course, Docker Cloud offers a different approach to administrating your Docker resources, well different, but not too different. In fact, there does seem to be some overlap between Docker Cloud and Docker Datacenter, and that may be because Docker Cloud, originally called Tutum, was actually adopted by Docker when it was already a fairly mature product, so it's possible that Datacenter and Cloud have all been parallel. By the way, Cloud is definitely an important tool for integrating your entire code-to-production workflow, so I'm going to spend a few minutes introducing you to it. In this demo, I'm going to show you around the Docker Cloud GUI, install and work with the Docker Cloud command line interface, and briefly explain the functions of stacks, services, and containers. We'll begin at the beginning, the Docker Cloud home page. You'll notice that I'm automatically logged in. My Docker Hub account covers me here as well. The first link in the left panel is to the cloud registry, or, as it's called here, repositories. I'll click on Create and create a new repo. There's not much to this, just give it a name and, if you want, a description and decide whether it should be public or private. So far, it looks suspiciously like Docker Hub. In fact, I strongly suspect this is just a different front end to the same service. One thing that you don't get over in Hub is the option of making this repo a target for code builds in your code repository. I haven't currently got any associated repos to make that work. Great, we've got ourselves a repo. Now what? Well I think it's about time I told you about the Docker Cloud CLI. It works with the regular Docker Engine, and if you're working from a shell that's logged into Hub, you're also logged into Cloud, but it installs through python pip. I'll install pip and, just because it's the right thing to do, upgrade it to the latest version. I'm obviously skipping through some of the really fun parts where I get to watch the packages downloading. When that's done, I'll use pip to install Docker Cloud. Let's see what you can do with this new toy. The CLI works much the same way as Docker Engine with the obvious difference that you start with the prefix docker-cloud rather than Docker. Dash -help gives us an overview of top-level commands. Docker-cloud repository inspect will tell me about the myrepo I just created, not a lot going on there, especially considering as I haven't yet added an image. I'll change that right away. For this step, I'll use the old Docker CLI to tag my busybox image with the name of my account and myrepo. Still with Docker, I'll push the image to the repo. Did it work? Why not see for ourselves by running repository inspect once again? The rest of what we're going to see goes a bit beyond the scope of the course on Docker images, but it's obviously important to know how all these parts fit together. The shiniest images in the repo won't do you much good if you don't know how to deploy them. The Stacks page allows you to upload or enter your own yml scripts that launch integrated services. The sample code I'm pasting actually comes from Docker Cloud documentation and nicely illustrates a deployment that includes special Docker Cloud images of a load balancer based on haproxy, the Docker Cloud Quickstart Python webserver, and a Redis image. We can also push a stack file by way of the CLI. Clicking Create on the Services page takes us to the first of three categories of infrastructure templates. The first category is called Jumpstarts and includes prebuilt frameworks like Redis, Elasticsearch, or MariaDB, on which I'll click. There's a lot of configuration you can do here, but effectively you've got the basic structure of a pretty sophisticated deployment just waiting for your final touches and the launch command. You can also search for images online and any images that might be associated with your Docker account. Once a service is actually started, you can track and manage it from the Containers page. There's nothing running right now. We've got just one more step before heading for the exit, Cloud Settings. Here's where you can enter authentication data for your cloud platform or source code management accounts. This means that using Docker Cloud to pipe code and administrate your AWS or other cloud resources is pretty much trivial. Of course, so is spending lots of money running lots of cloud resources. So do remember to properly monitor whatever cloud stuff you start up. It's time for some review. We learned how Docker Content Trust uses Notary to protect images in transit through the use of encrypted signatures and keys. You enabled DCT by exporting it to your environment with a value of 1. Once enabled, DCT will prevent you from pulling un-signed images and require authentication before allowing you to push. Notary can also be installed and used directly from the command line. Docker Cloud has its own image repository, sometimes called Cloud Registry. The Docker Cloud CLI is installed through Python pip and can manage a wide range of administration duties remotely. Finally, the Docker Cloud browser interface is organized by stack, where you can upload yml scripts, services, where complex deployments are defined, and containers, where running containers are managed.

Course Review
That's the end of the line for this course. Before we go, why not take a short look at some of the key things we saw? We learned about the way Docker images are organized by layers with the operating system, applications, and data each on its own in a separate layer that's writable to the container at run time. We re-introduced ourselves to the core Docker CLI tool, Docker Engine, and quickly got up to speed on building and managing images from both the command line and Dockerfiles, and we mentioned some of the key image management tools, including Docker Trusted Registry and Docker Registry. We also saw how each layer of an image can be shared among multiple images and containers, making downloads and launches much quicker, playing a big part in the overall Docker value proposition. We learned how to search for and build images, and how to dig into the history and structure of the images we've already got, and we spent some time exploring some of the best practices for building images, including convincingly demonstrating that extra layers can often translate to extra bloat. Turning our attention to Docker Registry, we learned how to get the free private repository tool running, both through a package manager and using a purpose-built image. We pushed and pulled images to and from our local registry, and saw where all the repo data is actually hosted on the server file system. Security should always be a prominent part of any resource deployment, and Docker images are no exception, so we learned how to incorporate encryption certificates into our Docker Registry, either by adding them to the registry container at run time, through the Dockerfile, or as part of a config. yml file. We added the use of a signed key infrastructure to the mix to allow trust between image publishers and consumers. And finally, we toured briefly through the Docker Cloud resource management interface. All in all, I was excited about some of the power and functionality I've seen in Docker images, and I hope that you've come away from this course with at least as much knowledge and respect for the tools as I have. Don't forget to make use of the Docker images lab set up page I created over at bootstrap-it. com/docker-images. I hope to see you all around.
