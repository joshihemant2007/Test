It's easy to run new and old applications in Docker, but you can't put containerized apps into production without monitoring. In this course, Monitoring Containerized Application Health with Docker, you'll learn how to implement effective monitoring for Linux and Windows containers. First, you'll learn how to gather and visualize metrics from containers using Prometheus and Grafana. Next, you'll see how to add metrics to your application, and export metrics from the Java and .NET runtimes and from the Docker platform. Finally, you'll explore how to build an effective dashboard with a single view over the health of your whole application. When you're finished with this course, you'll be ready to add monitoring to your application and move confidently to production.

Course Overview
Course Overview
Hey, how you doing? My name's Elton, and this is Monitoring Containerized Application Health with Docker. I've been running containers in production since before Docker even got to version one, and that experience has shown me that monitoring is one of the biggest advantages that Docker brings, both in production and in development. Containers are the best way to run new and old server applications, but before you go to production you need to understand a new approach to monitoring, one that works when you have dozens or hundreds of short-lived containers. This course gives you that understanding. I'll explain how monitoring works in Docker, how you expose metrics from your containers, and run other containers to collect the metrics and visualize them in a friendly dashboard. You'll learn how to build three levels of monitoring into your dashboards, so you can see what's happening in your applications, in your containers, and in the Docker platform itself. I'll be using Java apps in Linux containers and. NET apps in Windows containers, so you'll see how you can make monitoring consistent across different technology stacks, and I'll be running on single Docker servers and on Docker Swarm, so you'll learn how container monitoring works the same way in every environment. By the end of the course you'll understand how to add effective monitoring into your own applications using industry standard tools and techniques, so stick with me, and in just under 3 hours you'll learn all about monitoring containerized application help with Docker.

Architecting Monitoring for Containerized Applications
Course Intro and Module Overviews
Running apps in containers is easy. You can run brand-new microservices apps in containers and you can migrate 10-year old monolithic applications to run in containers too. The number one barrier that stops people pushing containerized apps to production is monitoring; knowing how your app is performing when it runs across hundreds of containers on dozens of servers. Welcome to Monitoring Containerized Application Health with Docker, a Pluralsight course which will help you get over that barrier and move containerized apps to production with confidence. My name's Elton and I've been running Docker in production since 2014, which is pretty much forever in the world of containers. Over the next three and a bit hours I'll be sharing my knowledge and experience of monitoring those production applications with Docker using both Linux and Windows containers, using an architecture like this. Here's how the course is going to work. In this first module I'll show you what monitoring looks like in a containerized application, walking through the architecture, showing you what metrics you want to track, and the technologies you can use to track them. There's plenty of demos coming in this module, which will show you where we're going in the course. Then I'll focus on the core technology for collecting and monitoring data, an open source metric server called Prometheus. I'll show you what Prometheus does, how to run the server in a container, and how it integrates with the rest of your application. Next I'll drill down into the type of metrics you want to collect to get a full view of the health of your system. Runtime metrics are statistics collected by the operating system and your application host, things like memory usage, CPU load, and a number of web server requests. Those give you an important view into how hard your containers are working. Application metrics are custom statistics that are relevant to your application. You may want to track how many times a particular type of event has been processed, or how many API calls you're making. Those give you a more detailed view of what your application is doing. Then there are container runtime metrics that come from the Docker platform itself, things like the number of containers running, and the number of failed container health checks. Those are great for giving you a detailed view of your application, which might be hidden in higher level, aggregated metrics. And then I'll show you how to plug everything together into a dashboard, which gives you a single view of your whole application health, what the application is doing, what the containers are doing, and what Docker's doing to manage the containers. By the end of the course you'll understand how to implement production-grade monitoring into your own containerized apps, building a dashboard like this, and you'll be ready to go live. First of all, I'll walk you through the architecture of monitoring Dockerized apps.

Understanding Monitoring for Containerized Apps
The first thing you need to understand is that monitoring apps in containers is different from monitoring apps running on servers. You may be used to monitoring applications with centralized tools like Nagios in the Linux world and SCOM in the Windows world. There you have agents running on the application servers, which push metrics to a central server, and that curates the data based on the type of apps running on the server. You can use tools like that with containers, but it doesn't really make sense. Typically containerized apps are more dynamic. Containers stop and start in seconds, so you can scale up and down easily. Container lifetime is much shorter than server lifetime, and you usually have a lot more of them, so traditional monitoring approaches don't work so well, and containers all have the same look and feel, whether they're running an old. NET app in a Windows container, or a new NodeJS app in a Linux container. You can monitor them all in the same way, using new lightweight tools. That's great for two reasons. First you get a consolidated view over every part of your app, pulling the same sort of metrics and displaying them in the same way no matter what technologies your app uses. Second, you can run the exact same monitoring tools in every environment. That's a huge step forward from today where your dev and test environments will have different monitoring from production, if they have any monitoring at all. It means devs can run the same dashboard that ops will use in production, so different teams can work together to make sure they capture all the information they need. To get there you need to learn some new technologies, and you'll need to change the way you package your apps in Docker, and if you want really detailed application metrics you'll need to add some code too, but these are tried-and-trusted approaches now. There's nothing hugely complicated, and I'll be covering it all in this course. This is what monitoring looks like in a containerized app. I have a bunch of containers running parts of my application, and every one of those containers provides a metrics endpoint. This is a REST API the container exposes, which provides a snapshot of all the metrics that that component is capturing. You can call that API and see the type of metrics I described in the introduction; current thread count and memory usage, the number of requests the component has served, and anything else that you want to monitor. My Docker servers also expose a REST API which provides metrics about the container runtime, and that's in the same format that my application containers use. Docker uses the Prometheus metrics format, because Prometheus has emerged as the most popular metrics server for containerized applications, so I also run Prometheus in a container. I'll cover Prometheus in a lot more detail in the course, but here I'll just say that its main role is to collect and store metrics from all the other components. The Prometheus container calls the REST APIs of the other containers, and of the Docker servers, on a schedule. Then it stores all the metrics it receives in a time-series database, which you can query to get data for a single point in time, or aggregated across a longer period. The final part is a friendly view over the data in Prometheus, which comes from a dashboard running in a Grafana container. Grafana is a fantastic open-source visualization technology, and it works very nicely with Prometheus. I'll have a custom dashboard running in Grafana, and Grafana lets you have many dashboards displaying the data from Prometheus in a user-friendly view. Next I'll show you how that all looks when it's running in Docker.

Demo: Monitoring Containerized Applications
In this demo I'll show you how all the monitoring components in a containerized solution look. You'll see the data that the application containers expose, and the metrics from the Docker runtime. You'll have a quick tour of Prometheus, and see what the final dashboard looks like in Grafana. I'm running my application in a Docker Swarm cluster with Linux and Windows nodes. This is actually a Docker Enterprise cluster, which gives me the Universal Control Plane UI, but you can do everything I'm going to show you with standard Docker Swarm mode and the command line. I have multiple services running, which form different parts of my application. This is a Java web app and this a. NET web app. Both of those are deployed for high availability, running across two containers each. The other services are my Prometheus server and Grafana server, each running across a single container. This is the. NET app. It's a very basic shopping website. All you can do is add items to a cart and click checkout. This is running in a Docker container on Windows. It's not an exciting app, but it's not meant to be, this course is focused on what's happening behind the scenes. There's also a Java app which is the internal stock management system behind the public shopping website. It just has functionality to order more stock. The Java app is running on Linux containers in the same Docker cluster as my. NET Windows app. That same Java web app has an additional endpoint, which is the REST API exposing metrics. This is the simple plain-text format which Prometheus uses. There's a key-value pair for each metric, and the comments tell you a bit about the metric. My Java app provides metrics from the Tomcat web server, including requests per second and amount of CPU usage, and also from the JVM, showing the heap size and the thread count, as well as runtime metrics, my Java app exposes some custom application metrics. I've got tracking here for when a session starts and when stock gets ordered. I have a similar setup in my. NET application. Here the app exposes a metrics endpoint on a separate port. I can see the requests per second served by IIS, and the heap size and thread count for my. NET worker process. These are coming from standard Windows and. NET performance counters. I also have an application metrics endpoint in the. NET app, capturing the number of active sessions, number of checked-out carts, and the total number of items ordered. I'll be looking the Docker platform metrics, and how Prometheus grabs everything in the next few demos, and for now I'm going to skip ahead to the end result, the Grafana dashboard. I have Grafana running in a Linux container in my cluster, and here's my dashboard. This is the final view where I see the aggregated stats for each part of my application. I've set up the dashboard to show my Java app on the right and my. NET app on the left. The top row is application metrics that's showing how often users are taking the actions which I explicitly track in my apps. The middle row is application runtime metrics, so this is CPU and memory usage from Tomcat and the JVM for the Java app, and from IIS and Windows Performance counters in the. NET app. The bottom row is the Docker metrics, the stats coming from the container platform itself. That's showing the number of nodes in my cluster, 4 online and 0 offline, and then some container breakdowns. I have a graph of container states showing the total number of running, paused, and stopped containers across the cluster. I also have the number of running containers on each node, the instance here is each Docker server in the cluster, and the number of failed container health checks, again, broken down by node. You can see this single dashboard gives me a really thorough view of the health of my app. It shows me what's happening in the app, what's happening in the containers, and what's happening in the container platform. I can use this to find trends and spot anomalies, to verify everything is okay with the app, and to identify and troubleshoot problems. This is exactly the sort of monitoring that's going to give you confidence to put your containerized app into production.

Introducing Prometheus and Grafana
The architecture I'm using for monitoring my app has three logical components; a metrics provider in each container and each Docker host, a metrics server, which reads and stores those metrics, and a visualizer that queries the metrics server to build the dashboard. There are different technologies available for the metrics server and the visualizer, but I've chosen Prometheus and Grafana for a few good reasons. Firstly, they're well established and solid technologies that companies like SoundCloud and PayPal have been using in production for years. They're also the most popular technologies in this space. When you read blogs and watch presentations on container monitoring, you'll typically see Prometheus and Grafana, so they're a good choice because they're very widely and very successfully used. Most major programming languages have Prometheus libraries, which you can use to provide metrics from your app, so Prometheus can call into them and pull the stats, and of course the Docker runtime itself provides metrics in Prometheus format. Prometheus and Grafana have public images on Docker Hub, which are regularly updated. They're maintained by the project teams, so you can run them in containers without having to build your own image. They're also both free open-source projects, which feature in the Cloud-Native Compute Foundation's Cloud-Native Landscape, a list of technologies proven to work well with cloud-native applications. I think it's safe to invest in Prometheus and Grafana because they're so widely used and they have all the functionality you need to build really useful monitoring into your solution, and you'll see in the rest of this course that the amount of effort you need to get going with Prometheus and Grafana is pretty minimal. You'll likely spend more time analyzing your app to see which stats you need to collect, than you do implementing the metrics. Next I'll take a closer look at what Prometheus gives you, so you can see why it's such a powerful and useful product.

Demo: Monitoring with Prometheus
In this demo I'll drill a bit deeper into Prometheus. You'll learn how Prometheus knows where to find the containers it's going to pull metrics from, you'll have a first look at the UI in Prometheus itself, and you'll see how to query metrics using the Prometheus Query Language, PromQL. First of all, Prometheus is configured with a simple YAML file. Here's the configuration for my application, and the important section here is the scrape_configs. Prometheus is a polling server. It reads metrics by making GET requests to target URLs on a timed schedule. I have jobs configured for my. NET and Java application metrics, then for the IIS and Tomcat web server runtime metrics, and then for Docker itself, with my nodes split by managers and workers. I'll cover those job definitions in more detail in the Prometheus module, so you understand how to deal with multiple instances of containers and Docker servers. The other parameter that's relevant here is the scrape_interval, which is how often Prometheus calls the metrics APIs. My default setting is 10 seconds, so all the Java and. NET apps will be polled every 10 seconds, but the Docker job overrides that so Docker is polled every 15 seconds. All the polling happens in the background. The Prometheus server runs its own database to store metrics, and it also has a basic UI to check on the setup and query the database. I'll browse to port 9090 which will take me to the Prometheus UI running in the container. From the status menu I can see the target list, and this shows me all the scrape targets I have configured, so that's my cluster manager and workers, and the application jobs with the state and the last time the Prometheus scraped them. In the graph view, there's a dropdown with the list of all the metrics Prometheus has collected and stored so far. The application, runtime, and Docker container metrics that I've set up are stored here, and Prometheus also stores its own metrics, which can be very useful for checking everything is working correctly. The scrape_samples_scraped metric is a record of how many metrics Prometheus has stored from the jobs I've configured. You can see that the application jobs just expose a few metrics, the runtime jobs expose a few more, and the Docker jobs expose hundreds of metrics values. Prometheus uses a time-series database, so it stores the historical values for all the metrics that it scrapes. I'll switch to the scrape_duration_seconds metric, which tells me how long it took Prometheus to read from each target, and switch to graph view. There's a line for each job in the graph, showing how long each scrape took, and I can shorten the timeframe here to get the most recent view. Prometheus has its own query language, which you can use to filter and aggregate data. Again, I'll be covering it in more detail later, but for a quick demo I'll use the max() function over that same metric, and the graph switches to showing the longest scrape recorded for each time interval. The Prometheus UI is useful, but pretty basic. It doesn't let you combine graphs into a dashboard with multiple views over your data, but it is a good tool for building up complex queries to get the result you want, and then you can plug those queries straight into Grafana.

Consistent Monitoring with Containers
You've seen how Prometheus is a polling server that calls metrics APIs to fetch statistics. It's down to you to build those metrics APIs into your application containers so that Prometheus has something to read. You'll learn how that's done in later modules, and it's really straightforward. It's a simple architecture, but it's also a clever way to get health information across your whole application. Every component in your app will have its own metrics endpoint, which provides the raw data. You expose just the metrics, which are relevant to each component, like CPU and memory usage, and web server requests per second, and the number of active sessions. You can collect similar stats for completely different technology stacks and expose them in a consistent way to Prometheus. So my. NET Windows container and my Java Linux container both have those key web metrics, even though the underlying implementations are very different. Collecting the metrics should be a cheap operation, and calling the REST API should be cheap too, and of course you control how often each API is called centrally in the Prometheus configuration. All that means you can run the monitoring side of your app in different configurations for different environments without making any changes to your applications. Devs might run without monitoring at all, and they can do that by spinning up the app containers without Prometheus or Grafana, but the metrics endpoint is still available, so they can check the stats manually. In a performance test environment you can turn the monitoring up to 11, scraping the metrics every second if you need to. Then in production run at a more modest level, scraping between 10 and 60 seconds, depending on the component. The key thing is that you can scale up and scale down your level of monitoring just with a configuration change, and you can turn up the monitoring when you need to without impacting the performance of your application. I'll get to the detail of that in the next few modules, but before I move on there's one last demo to show you the kind of stats you get from Docker itself.

Demo: Docker Platform Metrics
In this demo I'll show you some of the metrics you can get from the Docker platform, including container status, server information, and, when you're running in Swarm mode, details about the nodes in the cluster. The great thing about monitoring containerized apps is the consistency you get. Every component provides a metrics endpoint which tells you how it's doing, like my Java runtime and my. NET runtime, and it's the same with the Docker platform. On one of my Windows Docker servers I'll browse to port 50501, which is where I'm exposing the Docker metrics, and I get a very similar view to the metrics endpoints in my containers. The response is in Prometheus format, so I get key-value pairs telling me things like the number of containers in each state, and the amount of CPU and memory the host server has. My swarm manager has the same metrics endpoint configured, and it also shows container and engine details. And the manager also has metrics about etcd, which is where all the cluster state is stored, and the gRPC calls served by the manager, and the state of the nodes in the swarm. That's all scraped by Prometheus using the configuration that you've already seen, and in the UI I can work with those stats. Something simple like the number of containers running across all the nodes in the swarm is a useful headline statistic to see. This query gives me the number of containers in each state on each node. I can filter on running containers, which gives me the breakdown for each node, and then I can sum that to get the total across the whole cluster. Metrics from the Docker platform are important to have in container applications which are self-healing, when Docker is stopping and starting containers for you. I'll switch back to the Grafana view, and these are the aggregated compute stats for my Java and. NET applications. These are at such a high level they could be concealing a problem. If my app keeps crashing and Docker is constantly spinning up new containers, you wouldn't see that in this view, you'd just see application and runtime metrics which might look okay. Adding the container stats here would show that spike in container creation, so I would see that something is wrong even if Docker is doing a good job in maintaining the service level for my application.

Module Summary
In this module you've seen what application monitoring looks like when you're running in containers. There is a cost in moving away from the traditional monitoring tools that you've used, but the huge benefit is consistency. Every part of the app gets monitored in the same way with the same technologies, and you can run the exact same monitoring tools in every environment. The core of this is adding a metrics API to each of your containers, and enabling metrics in your Docker servers, which provide the stats that you're interested in collecting. Then you run a metrics server in a container which collects those stats, and a visualizer in another container, which hosts your application dashboard. In the rest of this course I'll show you how to expose those metrics from your app, and I'll use Prometheus for the metrics server and Grafana for the dashboard. Those are great choices, but the principles I'll show you will be relevant to other monitoring technologies too. That's it for the overview, so let's get started. In the next module I'll show you how to run Prometheus in Docker, both in Linux and Windows containers, how to configure Prometheus to scrape metrics endpoints, including in highly-available environments with multiple containers and servers, and look more closely at the types of metric Prometheus records and how you query them. That's coming in Collecting Metrics with Prometheus, the next module in Monitoring Containerized Application Health with Docker.

Collecting Metrics with Prometheus
Module Overview
A metrics server is the central point for collecting and storing monitoring data in containerized applications. Prometheus is the most popular metrics server. It's open-source, cross-platform, Docker friendly, and extremely powerful, and in this module you'll learn all about it. My name's Elton and this is Collecting Metrics with Prometheus, the next module in Pluralsight's Monitoring Containerized Application Health with Docker. In the last module you learned how all the pieces of the monitoring solution fit together, and now you'll get started with the main component. I'll start by showing you how to run Prometheus in Docker and explaining why the metrics server should be in a container alongside your application containers. The Prometheus team provide a Docker image for running on Linux, but not for Windows, so I'll show you how to use the standard Linux image and how to package your own Windows Docker image. Prometheus is driven by a simple configuration file, which you've already seen briefly, and in this module you'll see exactly what you can configure, and also learn the options for providing your configuration to the Prometheus container. The last thing for this module is to cover the types of data Prometheus can work with. You've seen basic incrementing counters, and there are three other data types you need to be aware of to cover all the monitoring scenarios. I'll also spend a bit more time in the Prometheus UI to show you how to query those different data types. First let's understand why Prometheus should run in its own container.

Using a Container to Run Prometheus
Prometheus is a server application written in Go, so it's lightweight and cross-platform. You can run Prometheus directly on a server in your cluster or a separate server on your network, but it's much better to run it as a container alongside your application containers. That's partly for the same reasons that it's better to run any app in a container; the portability, efficiency and security you get from the Docker platform, but particularly for Prometheus, if you run it in a container it can be an internal component, which isn't publicly accessible. Containers in the same Docker network can access each other without their ports being published to the outside world. So when you run Prometheus in a container, you can keep the metrics endpoints in all your containers private. They only get used by Prometheus, so they don't need to be public. You can also keep Prometheus private, because it's only used by the Grafana container. Grafana supports basic authentication, so in production you just need to run Grafana over HTTPS and your whole monitoring solution has a level of security. None of the Prometheus endpoints or the metrics endpoints in your app are publicly accessible, so they can't be compromised. Being in a container itself also helps Prometheus to discover all the target components. You'll see in this module how you can easily configure Prometheus to scrape from components which are running in multiple replicated containers, and that's made easy because Docker provides the service discovery. Prometheus is a stateful application though, and you need to be mindful of how you're storing data when you run in a container. It's straightforward to run a single instance of Prometheus with a Docker volume for storage. That's going to be fine for a lot of production scenarios, but if you want high-availability for Prometheus you need to cluster it across multiple containers. That's not super-complicated, but it's more detail than I'll be covering here. To start with, we just want to get Prometheus running, and I'll do that next.

Demo: Running Prometheus in Docker
In this demo I'm going to run Prometheus in a Docker container. I'll start the simple way using the official Prometheus image, which runs in a Linux container, and then I'll show you how to package Prometheus as a Windows image, and run the same server in a Windows Docker container. Prometheus is an image on Docker Hub in the prom organization, which is maintained by the Prometheus project team. It's based on the minimal BusyBox Linux distro, and the current version comes in at a tiny 29 MB. At the time of recording this, there's only a Linux version of the image, which is fine to run in Docker on any Linux distro, or in Docker desktop on Mac and Windows in Linux container mode. I'm connected to a Linux server running Docker, docker version shows me the operating system and CPU architecture of the server. To run Prometheus in a container I can just do docker container run to start a new container, detach to put it in the background, publish-all to publish all the ports, and prom/prometheus:v2. 3. 1 for the latest Prometheus image. Docker pulls the image if you don't have it stored locally, and it will start the container in the background. In production you wouldn't publish the Prometheus ports, but it's fine for non-production environments when you're working out your metrics. That's it. Docker container ls shows me Docker has published the Prometheus port 9090 to random port 32768. I'll browse there and show you the Web UI. It's the same UI you've already seen, but this image has the default Prometheus configuration. There are no jobs set up to poll remote metrics endpoints, but there are a stack of metrics already here from Prometheus itself. You can use these to track memory usage in the Go runtime, or the performance of the time-series database. For Windows there is no official image, but there is an implementation you can use on the Docker samples repo on GitHub. This is the short link to get to that repo. The Dockerfile shows how you install Prometheus in a Windows container using PowerShell. The first stage installs 7zip which you need to extract the Prometheus archive. The next stage downloads the Prometheus archive for the specified version and verifies the checksum before extracting it. The final stage is the Prometheus image, based on Nano Server because Prometheus doesn't need the full Windows Server runtime. This copies all the pieces the app needs from the previous stage, and sets up the container, so it starts with a default configuration file, but you can override that with your own path. The image also uses a Docker volume for the Prometheus data path. That lets you keep the storage separate from the container lifecycle. This image is published on Docker Hub too, in the Dockersamples organization, but it's not an official image, so it doesn't get security scanning, and it isn't maintained by the Prometheus team. Here I'm connected to a Windows server running Docker, and I'll run the same command but with the Docker samples image name, so that's docker container run, detach, publish-all, dockersamples/aspnet-monitoring-prometheus. Docker container ls shows my Windows container has published the Prometheus port to port 32117, and I'll browse to that server and you'll see it's the exact same Prometheus UI. This sample image isn't configured to read the Prometheus metrics, so there are only the scrape metrics in here. You can see there are no scrapes so far, and in the target view the only configured target is unavailable. That's because the image is part of a sample project that expects to have a web application running, which I haven't started here. The configuration in this container is different from the Linux container, but Prometheus is a cross-platform app you can run in Docker on Windows or Linux, and it behaves in the same way. It's not much use having a Prometheus container that doesn't collect any metrics, so next I'll show you how to configure Prometheus, and how to run a container with your own configuration.

Configuration and Service Discovery in Prometheus
You'll run your Prometheus container in the same Docker network as the application containers that you want to monitor. You configure Prometheus to poll those containers as scrape targets, and set a schedule for how often Prometheus makes HTTP GET requests to the metrics endpoints in the containers. The target configuration for a job is the server host name or IP address and port. Containers use DNS to reach each other by the container name or service name, if you're running in swarm mode, so you just need to add a job for each application you want to monitor, and set the container name as the target. If you are running a swarm mode cluster, you'll likely have multiple replicas for each service, so there are multiple containers hosting each component. You want Prometheus to query each of those containers, and there's an easy way to do that. For a single Docker container, the container name gets resolved by DNS to a single IP address, so my Java container might be running at 10. 0. 12. 10 in the Docker network. In swarm mode I'd have multiple containers in the java service for high availability and scale. The service is still available at the DNS name java, but Docker also provides the special tasks prefix for DNS queries, so you can query DNS for tasks. java and have all the IP addresses returned. If you want to learn more about how load balancing and service discovery in swarm mode, check out my Pluralsight course, Managing Load Balancing and Scale in Docker Swarm Mode Clusters, which covers service discovery, as well as load balancing for Linux and Windows containers. When Prometheus polls a job with multiple targets, like many IP addresses for one service, it includes the IP address as a property, which it stores alongside the metrics value. That means you can aggregate metrics across all the containers in a service, or drill down into individual containers. This is simple, but very neat because you can run the same setup on a single Docker server in dev or test, and on a multi-node swarm cluster in production, with just a change to how Prometheus is configured to find targets. Both setups use the DNS name, but in dev it will resolve to a single container, and in production it will poll all the containers in the service. This flexibility lets you work out a generic configuration file, which you package with Prometheus in a custom Docker image, and use as the default for dev and test environments, and you can use the same image, but override the configuration for production. I'll show you both those options in the next two demos.

Demo: Configuring Prometheus
In this demo I'll look at configuring Prometheus to poll application containers and Docker servers with scrape configs. I'll build a generic config file that can be used in for single-node dev and test environments, and package that in my own Prometheus image. Then I'll show you how to run a container from that image with the default configuration. Here's my basic Prometheus configuration, which works well for minimal environments. I have jobs for the. NET and Java application metrics, and for the IIS and Tomcat runtime metrics. In the job configuration you tell Prometheus the path to the metrics endpoint, and also how to find the target. This config file uses static configs for the targets, which means you specify a list of hostnames for Prometheus to poll. That's fine when you have a single container running each part of the application. The target hostname here is just the container name, which Prometheus can access because it's also running in a container in the same Docker network. For scalable environments with multiple containers, you need a different approach, which I'll show you in the next demo. The Docker servers are listed here as jobs with static configs, and the target hostname is host. docker. internal. That's a special magic hostname which Docker resolves to give containers the IP address of the host machine where Docker is running the container. This configuration works fine with Docker for Mac and Docker for Windows, which set up that host. docker. internal address, so this config is suitable for development. It's good to package a working configuration in your image so devs can just do docker container run, and it works as expected, and I can use this configuration as that default by packaging my own Docker image for Prometheus. That's as simple as a Dockerfile, which starts from the official image. It's always a good idea to use a specific version in the image tag, so you know you're always building from the same base image. And then copy in the custom config file over the default one in the expected location where Prometheus will look for configuration. If you're running on Windows you'll need your own image for Prometheus anyway, but it's the same config file format, so you can do the same thing and copy your config file over the default one. I've built this image and tagged it with my own image name, psmonitoring/prometheus. In the Compose file for my solution I have services to run my Java and. NET apps, along with Prometheus and Grafana. This is a basic compose file that just has the core details of all the services. I also have an override file for devs working with Docker for Mac. This is for working on the Java app, and not the full stack, so the. NET app doesn't get run, and I replace the image name with Alpine, so the Mac doesn't try to run a Windows container. Then I'm just specifying the ports to publish. I'm connected to Docker for Mac here. You can see the client is Darwin, which is the OS X, and the server is Linux, which is the VM that Docker desktop runs. Right now there are no containers running. I'm in the directory with my compose file, so to start work I just run docker-compose, specifying the core compose file with the service details, and the Mac developer override file. Then use up -d to start detached containers, and here are all the containers running for my Java app along with Prometheus and Grafana. I'll browse to the app, check the functionality, and this is all working fine. Browse to the Prometheus container, and I can see that the configuration is the default that I packaged with the image, using static configs for all the targets. Prometheus can reach the Docker and Java targets just fine, but this isn't a hybrid environment, so there are no Windows containers and the. NET targets are unreachable. That's fine for me as a Java developer. I can work on the app and check the metrics in Prometheus. Here's the application metrics that show me how many sessions are running and how many orders have been submitted. I can even load Grafana, switch to the dashboard, and see the same view that the ops team use in production. There are no stats for the. NET app, as expected, but I can see what's happening with the Java app and with Docker just the same as in production. Just by running docker compose up I've got the whole java side of the application stack, complete with monitoring and analytics. Next, I'll move onto configuring for a production environment, where I'll have multiple nodes running in my Docker cluster, and multiple instance of each application container.

Demo: Service Discovery in Prometheus
In this demo I'll show you how to configure Prometheus to use service discovery, to find all targets to scrape in dynamic environments where containers are starting and stopping all the time. I'll show you that you can save that configuration in Docker Swarm, and apply it to the Prometheus container in your Docker Compose setup, and then I'll show you the whole thing running with Prometheus scraping metrics from multiple containers across multiple swarm nodes. My custom Prometheus image has the default configuration file with static configs, but I can override it. This is the production configuration file here. The jobs are the same, but the discovery part is different. Instead of static configs specifying a host name, I'm using dns_sd_configs, which is service discovery using DNS. Prometheus will query DNS with this setup, and it will store every IP address the DNS server returns as a target to poll. The DNS name I'm using has the special tasks prefix, and the name of the service running in Swarm. The Docker platform has a DNS server built in, and the tasks prefix means the DNS query will return all the container IP addresses. We'll see that in action in just a moment. So my. NET and Java metrics, and my IIS and Tomcat metrics, all use the same DNS service discovery with the tasks prefix, so Prometheus will scrape metrics from all the containers, and it doesn't matter which node the containers are running on. I'm using static configs again for the Docker servers, specifying the IP addresses of my cluster manager, and my cluster nodes. You can also use DNS or one of the other service discovery options in Prometheus for your actual servers, so you can choose what works best in your environment. In the production compose file I've got the configuration I need for my cluster. The Java app uses ingress networking to publish port 8080, it will run with 4 replicas, and there's a constraint to tell Docker this should only run on Linux nodes. My. NET app uses host mode publishing. Currently, I'm publishing all the metrics endpoints, so I can check the statistics directly. It will run across two replicas on Windows nodes. If you're not familiar with host mode and ingress networking, or VIP and DNSRR service discovery, then I cover all that in my Pluralsight course, Managing Load-Balancing and Scale Docker swarm mode clusters. The Prometheus setup is where I override the default configuration in the Docker image. I've saved the production Prometheus file as a config object in the swarm, and here I'm loading that config object into the container, so it gets surfaced as a file in this location. That overwrites the default configuration file, so Docker injects the config object into the Prometheus container. This is the name of that config object. In this session I'm connected to my swarm with two Linux and two Windows nodes. I'll inspect that config object with the pretty flag, and you'll see it's just the same YAML file, with the DNS service discovery configs for the services, and the static IP address configs for the nodes. I've already deployed the stack using the production compose file, and my services are all up and running. I'll browse to the. NET app and check, and it's all working fine, and I'll browse to my Java app and check this, and this is working fine too. Here's Prometheus running from my custom image with the production configuration applied from the Swarm. The Service Discovery tab shows all the targets Prometheus has found. Here there are four targets for the Java app, and these are the IP addresses of my four Linux containers. There are two targets for the. NET app, and these are the IP addresses of the two Windows containers. The targets tab shows all those endpoints are up, which means Prometheus is scraping the metrics endpoints of all those containers and of my Docker servers. Grafana is running in the cluster too, and here's the dashboard showing me the whole stack, with the. NET thread count for each of the Windows containers, the Java thread count for the four Linux containers, the container states across the cluster, and the running containers for each node.

Understanding Prometheus Data Types
Okay, so now you've seen how to run Prometheus in Linux and Windows containers, how to package your own default monitoring configuration, and how override the configuration in clustered environments when you're running at scale. So what about the data Prometheus is actually collecting? To get the most from your monitoring, you'll need to understand the different metrics types which Prometheus provides. Different metric types work in different ways, and support different query operations. The simplest is the counter. It's a basic counter, which can only be incremented, so counters always increase for the life of the container which is collecting them. These are useful for values which naturally increment, like the number of requests a component has processed, or the number of errors it has handled. Next is the gauge, which is like a readout at a given time and can go up or down. This is good for snapshots like memory usage, or the number of open requests currently being handled. So you would use a counter to record the total number of requests a container has received, and a gauge to record the number of in-flight requests, which could be 100 at one point in time, and drop to 10 the next. Counters and gauges get you a lot of what you want, and they're the easiest to use. My dashboard for this course just uses gauges and counters. Prometheus has a rich query language, and every time it samples a counter or gauge value, it records the timestamp, so you can identify trends or take averages from counters and gauges. You can also record more complex metrics using the last two data types Prometheus provides, histograms and summaries. These have similar functionality, in that they group together multiple samples of the same metric. You record the number of samples and the sum of the sample values, which lets you find distributions in your data. Histograms and summaries are useful for things like response duration, where you may have an SLA for the 95th percentile response time. You just record the raw data in your service, and query Prometheus to see if you're meeting the SLA. Not all Prometheus clients support the more complex histogram and summary types. It's good to know about them, they might fit well in your own scenario, but for this course I'm going to stick with counters and gauges, which you can use with any client library.

Demo: Counters and Gauges in Prometheus
In this demo I'll show you how counters and gauges look in Prometheus, work with the Prometheus query language, PromQL, and show you the different types if queries you can run on counters and gauges. We've actually already seen counters and gauges, which are the most common types of metric. This is the metrics API for the Docker engine. Each metric is listed with some friendly help text and a type description. This is the current container state metric, which is a gauge, and the values here are the current number of containers in each state. I have 0 paused containers, 37 running containers, and 17 stopped containers. Gauge values can go up or down. I'll remove all the stopped containers, so we can see that value changing. Docker container list takes a filter, so I can list just the exited containers, and the format string will give me just the current status message. If I change the format, I can print out just the ID field, so this is a list of the IDs of all stopped containers, and I can pipe that output into the docker container rm command to remove all the stopped containers. Now when I refresh the metrics page, the stopped count has gone down to 0. Further down there are some metrics about the HTTP server in Docker, which is the REST API. This metric the number of HTTP requests for the Prometheus metrics endpoint. It's a counter, so it's recording the total number of times the Prometheus has polled the server for metrics since Docker started. Prometheus is constantly polling for metrics, so when I refresh the page, this counter keeps going up. The counter will increment every time Prometheus scrapes metrics, until I restart Docker, when it gets reset to zero. I'll switch to the Prometheus graph view now, and we'll see some of things we can do with those metrics. Here's the Docker container states gauge. There are entries for every container state on every server, and these are the labels that identify them. The instance is the IP address, the job is the Prometheus job definition, and the state is the container state. I can filter on those labels with braces, so this will give me just the values for the docker-managers job, for running containers. I only have one manager, but in an HA environment I'd have three or five, and I can use sum to get the total number of running containers across all the managers. I'll add another graph and look at that HTTP request counter. There are labels for the instance and the job again, and there are also labels for the HTTP response code, the API handler that served the response, and the HTTP request method. Now I could have lots of services recording http requests with the same metric name, so I can filter the job name with a regular expression using the tilde to get just jobs that start with the word docker, and I'll also select the metrics from the last 30 seconds. Here you can see the Prometheus time series database in action. It's returning multiple metrics for each instance, with the timestamp when the metric was recorded. The values increase with each timestamp because this is a counter type which only ever goes up. Because those timestamps get stored, Prometheus can give me the rate of HTTP requests over a time period. I'll take a sum across all instances, and use the irate function to get the rate of HTTP requests per second over the last 5 minutes. I'll switch to graph view and zoom in on the last five minutes, and this is a nice visual representation of the total HTTP traffic being served by my Docker servers. Those are just some of the filtering and aggregation functions that Prometheus provides. There's much more to the querying language, PromQL, but even these functions are enough to build a rich dashboard like the one I have running in Grafana.

Module Summary
In this module you've learned about Prometheus, the metrics server which scrapes data from Docker and from your application containers and stores it in a time-series database. You've seen why it's good to run Prometheus in a container, and how to do that on Linux and on Windows. You can use a simple configuration file to set up the targets for Prometheus to scrape, and I showed you how to package a default config file in your Prometheus image, and how to override that at run-time. You've also had a quick introduction to the types of metrics Prometheus can work with, focusing on the counter and gauge types, which are simple, but very powerful. In the next few modules I'm going to dive deeper into the parts of your application stack that you want to monitor, starting with the runtime metrics. Those are metrics your application is already recording, like the operating system and host web server metrics. I'll show you how to build your Docker images to export those metrics for Prometheus to scrape. That's in Exposing Runtime Metrics to Prometheus, the next module in Monitoring Containerized Application Health with Docker.

Exposing Runtime Metrics to Prometheus
Module Overview
There's different types of information you can use to monitor your apps, and a lot of really useful stuff comes for free in the operating system and the application runtime. You can make that available to your monitoring server just by packaging an existing utility inside your container image. You don't even need to change application code. My name's Elton, and this is Exposing Runtime Metrics to Prometheus, the next module in Pluralsight's Monitoring Containerized Application Health with Docker. All the major application runtimes collect their own metrics; Java has JVM metrics,. NET uses Windows Performance counters. Web applications also have metrics collected by the web server. Tomcat and IIS record useful data about requests and responses. A lot of essential information is already collected by the runtime, you just need a way to expose it from your container. That's what this module is all about, showing you how to take advantage of monitoring data that's already been collected for you. I'll show you how to package a metrics exporter utility in your application containers. That exporter reads the metrics your app runtime and operating system is already collecting, and makes them available in Prometheus format as a metrics endpoint. Some of the stats you've already seen being collected by Prometheus have come from exporting these runtime metrics, and this module's going to show you how to do it. I'll cover the specifics of Java and Tomcat, and. NET, and IIS, and you'll also learn the principles, which apply to any runtime that collects metrics. I'll start by talking about exporter utilities.

Using a Utility to Export Runtime Metrics
You know now how monitoring works in containerized applications. A central monitoring server, like Prometheus, polls all the containers and collects their metrics. To support that you need to add a metrics API to all your application containers. This could be an app that's currently in development, so providing Prometheus support is a feature that can be added in an upcoming release, but for existing apps that are no longer being actively developed, that's not going to be an option. It doesn't mean those apps can't be monitored in Docker, just that you need to make use of metrics that are already being collected. The idea is to package a utility application in your container image, alongside your original app. The utility just reads the runtime metrics the app is already collecting, and exposes them on an HTTP endpoint, which Prometheus can call. That utility is called an exporter, and this is a common pattern in Prometheus to collect metrics from apps which don't have their own metrics API. The exporter is a generic application which collects metrics for a particular runtime. So a Java exporter will provide a set of JVM statistics, and a. NET exporter will provide a set of Windows Performance Counter metrics. There are many exporter utilities available for various applications and runtimes. some are official tools supported by the Prometheus team, others are community projects. You can use an existing exporter, or you can build your own and make it configurable, so you can plug it in to any existing app that you're Dockerizing, and make all those metrics available to Prometheus. This approach does mean that your containers do more than one job. The container hosts the application, and it also hosts the exporter utility. That means if you want to change the exporter you'll need to deploy a new version of your app too, and your exporter needs to be reliable, so it doesn't starve your application of resources or even bring the whole container down, but we're talking about a pattern you can use with legacy applications where changing code to add Prometheus support isn't an option. In this scenario the additional exporter utility provides a huge amount of benefit, and the risks can be mitigated with testing and Docker health checks. So next I'll show you how it looks to package an exporter utility alongside your application in a single Docker image.

Demo: Exporting Java Runtime Metrics
In this demo I'm going to export runtime metrics from my Java web app. I'm going to use a third-party Prometheus exporter and package it into my application image. I'll build a version of the image with the exporter set up, and run the app in a container, so you can see the JVM and Tomcat metrics that the exporter provides. Here's the Dockerfile for my Java web application. It's a multi-stage Dockerfile where the first stage is the builder, which compiles the app from source using Maven. The second stage is the final application image, which starts from the official Tomcat image based on the minimal Alpine Linux, and packages the compiled application from the builder stage. Now I do have the source code for this app, but I don't want to write code to expose generic metrics that could work for any app, so I'm going to add a utility, which exports the web server and JVM stats to a metrics endpoint. This is the community Tomcat exporter on GitHub, and here's the short URL for the repo. This exporter works really nicely and gives me all the core information that I need. The installation steps, in this case, are a bit fiddly. There isn't a single archive I can drop in. I need multiple downloads, so I've got another Dockerfile here, which captures all those install steps and builds a Docker image, which just contains the exporter utility. I've built that exporter image in the normal way, with docker image build, and pushed it to Docker Hub. That's a public image now, which anyone can use to add the Tomcat exporter to their own application image. To do that, I'll go back to my Java application Dockerfile. I'll set the working directory to be the Tomcat web application root, and copy all the JAR files from my exporter image. If you're not familiar with this COPY syntax, it lets you copy files from an earlier stage in the Dockerfile, or from a separate image, which is what I'm doing here, copying files out of the psmonitoring tomcat-exporter image. I'll also copy the servlet WAR file from the exporter image into the metrics folder in webapps, so when I browse to the metrics endpoint, the response gets served from the community exporter. I don't need to change anything about my application. I'm just bundling a new dependency from a separate image, which gives me all the runtime metrics for my app. This is a Linux image, and I'm connected to a Linux Docker engine in this session. I'll run docker image build, tagging the image as psmonitoring java version 1, and set the path to the Dockerfile. Everything comes from the cache because I've already built this image, but you can see the steps here copying the exporter files from the tomcat-exporter image. Now version one of my Java web app has the original application packaged with a metrics exporter. I can run it locally to check that, using docker container run, -d -P to detach the container and publish all the ports, and then my v1 image name. Docker has mapped the container port to local port 32771, so I'll browse there, and we'll see what we've got. This is the Java app, which is working as usual, and now I have a new metrics endpoint, which is served by the Tomcat exporter utility. These are all the stats available for Prometheus, showing the runtime metrics for the container with counters and gauges for JVM stats and Tomcat stats. We get all those metrics for free, and next I'll have a quick look at the most useful ones.

Monitoring Java Web Apps in Linux Containers
I'm packaging two components in my Java container image now. There's the original web application, which is a WAR file plus all the dependent JARs, and there's the metrics utility, which is a separate WAR file with its own dependent JARs. They both run in the same JVM in the same instance of Tomcat. The metrics utility is a generic component that exports metrics from Tomcat and the JVM, and because it's running in the same JVM as my app, I get all the metrics for my own application. There are a whole bunch of metrics available from the community Tomcat exporter, and it's open source, so you can add to it if there are more that you need. These are the highlights. For the JVM you get CPU time in seconds, the age of the process, memory usage, the number of Java classes loaded and unloaded, details of the Java runtime version, JVM threads in use, and JVM threads deadlocked. There are a whole lot more, but this is a great list to start with, giving you insight into your Java app and helping pinpoint the source of any problems. On the Tomcat side you get counts of active, rejected, and expired sessions, along with the average session time, and the number of active connections. You also get the number of bytes sent and received by the Tomcat request processors, and the count of requests and errors. There's also details of the version of Tomcat running. Again, this is all great stuff to see what's happening and start diagnosing any issues that you find. I've deployed my metrics utility in a pretty simple way, which means the metrics endpoint is listening on the same port as the application endpoint. It would be better to have the metrics on a separate port, so I can keep that private to the Docker network. The Prometheus container will be able to access it, but it wouldn't be publicly visible. You can do that with a more advanced server. xml configuration, which you package in your Docker image, but that's a Tomcat detail, which I won't cover here. Now I have runtime metrics for my Java application, and I haven't had to change any code. Next I'll do the same for my ASP. NET app.

Demo: Exporting .NET Runtime Metrics
In this demo I'm going to export runtime metrics from my ASP. NET web app. I'm going to use an exporter utility and package it into my application image. I'll configure the image to run the exporter at startup, and then run the app in a container, so you can see the IIS,. NET, and Windows performance counters that come from the exporter. Here's the Dockerfile for my. NET web application. This builds a Windows Docker image, and it uses the same approach as my Java Linux app. The first stage is the builder, which compiles the application from source, and the second stage uses Microsoft's ASP. NET image as the base, and packages the compiled application from the builder stage. Just like the Java app, I'm going to use a community exporter to get the ASP. NET metrics out of my app - this is from a repo on Docker samples, and here's the shortened URL to get there. The exporter app is a. NET console application, and the Dockerfile just builds the source code and packages the app on top of Microsoft's. NET runtime image. That exporter app is available as a public image on Docker Hub, so I can copy it straight into my application. Back in the Dockerfile I'm going to add a new environment variable for the path to the exporter. It's good practice to capture things like this in a single place in environment variables, so you define them once and use them in scripts and in the rest of your Dockerfile. Now I'll switch the working directory to that path, and copy in the exporter application. This is the syntax which copies the file contents from the public image on Docker Hub into my image. The exporter utility uses a configuration file, specifying which performance counters to collect, and this is a standard IIS config file, which comes from the exporter, Docker image. Then I need to change how the container starts. I'll use PowerShell for the entrypoint, so I can run a script on startup, and add the default startup script in a command directive. This starts IIS, then makes a local call to the web app, which starts the worker process for ASP. NET, and then runs the exporter app as a background process. Then it runs servicemonitor, which is Microsoft's utility that comes in the ASP. NET image and checks IIS is still running. One more thing and I'm done. I need to expose port 50505, which is the port where the exporter app hosts the metrics endpoint. I don't need to change anything about my app setup. I'm just adding and configuring this extra utility. I'm connected to a Windows Docker engine in this session, so I can build my. NET image. I'll tag it as psmonitoring netfx v1, and provide the path to the Dockerfile. The first part of the build comes from the cache, but the changed parts get built, so here's the exporter app setup coming in. The build is done, and next I'll run a container from the image, using the usual flags to detach the container and publish random ports. Docker container list with the last flag gets me the most recent container, and the output shows me the app port is mapped 59739. I'll browse to that and check the app, and this is all working fine. Now the metrics port has also been published, and this is on port 54426. I'll switch to that port and browse to the metrics endpoint. Here are the IIS performance counters and the. NET counters, together with standard Windows performance counters like the percentage processor time for the w3wp IIS worker process. That's runtime monitoring for my. NET app, exporting performance counter metrics, which were already being collected in the container. Next I'll look at what it means to run the exporter utility in the background, and the type of metrics those counters provide.

Monitoring ASP.NET Apps in Windows Containers
I've added a metrics export utility to my ASP. NET container image. There are two separate components running in my Windows web container now, the ASP. NET worker process running through IIS, and the metrics exporter console app, which runs in the background. IIS and ASP. NET write metrics to Windows Performance Counters, which are available for other processes to read. That's how my exporter app, which is running in a separate process, can read the ASP. NET worker process metrics. There's a downside to this approach, where the container is running two components. Docker monitors the foreground process in every container to make sure it's still up and running. The foreground process in my container is actually service monitor, Microsoft's utility, which is there to check on the IIS Windows Service. If the IIS service goes down, service monitor exits with an error, and Docker knows the container is stopped. Then the container platform can start a replacement. Docker doesn't know anything about the background exporter process, so if that stops the container keeps running. My app is available but without any metrics. I could fix that by running the exporter as a Windows service and having service monitor watch both services, so if either of them stop then the container stops. That's not difficult, but it's a. NET detail, which I won't cover here. The metrics I get from the ASP. NET exporter are the key performance counters. For. NET I get the memory heap size for all the Garbage Collector generations, the large object heap size, and the time spent doing garbage collections that can all help diagnose memory leaks. For the worker process I get percentage processor time and current thread count, together with memory usage for the working set, and private and virtual bytes. All useful for seeing if the ASP. NET worker process is using more resources than it should. And for IIS, I get requests per second, and the percentage of responses with HTTP status codes other than 200 OK, so that's 403s, 404s, and 500s. There's also the current and maximum file cache memory used. These stats are good to check that your website is responding correctly. So I'm getting useful stats surfaced from my Java and. NET application containers, and next I'll show you how I use them to build the runtime metrics for my dashboard.

Demo: Dynamically Scraping Runtime Metrics with Prometheus
In this demo I'm going to start analyzing runtime metrics in Prometheus. I'm going to deploy my application to Docker Swarm, running a hybrid cluster with Linux and Windows nodes. I'll show you how to deploy a distributed solution like this as a single Docker stack. Then I'll use the Prometheus UI to build up queries over my Java and. NET runtime metrics, so I have those query expressions ready to plug into my Grafana dashboard later in the course. Here's the basic compose file for my app. Here I just have the Java and. NET services defined, along with my custom Prometheus image. In the production override file I apply the Prometheus config from the cluster, and publish port 9090 for the Prometheus UI. I'm connected to my Docker Enterprise cluster here, with two Linux and two Windows nodes. I've already created the Prometheus config object in the Swarm, which is set up with the DNS service discovery that I walked through in the Prometheus module. I want to deploy my app as a Docker stack, which means I need to join the compose file and the production override file into a single file. I can use docker-compose to do that, specifying the two files and running the config command. With config, compose joins the two files together, validates the content, and writes it back out. So I can capture the combined output in a file called docker-stack. These warnings from compose tell me there are parts of the file that only apply in Docker Swarm mode, which is fine because that's what I'm using. And here's that combined output file, which has the image names from the compose file and the environment details from the production override file. I deploy the whole thing with Docker stack deploy, passing the compose file name and the name of the stack. Incidentally, you can deploy to Kubernetes the same way in Docker Enterprise. You use the simple compose file format for your app definition, but deploy to Kubernetes which is supported alongside Docker Swarm in Docker Enterprise, but that's for another course. The services are all created, and when I check in the UI I see two. NET containers, which are running on my Windows nodes, four Java containers, and one Prometheus container. I'll browse to a Windows node and check the. NET app, order some products, check out, and this is all working fine. Now I'll browse to a Linux node and check the Java app, order some more stock, and this is working fine too. So now onto Prometheus. In the Targets page I can see the Docker endpoints are up, but the Java application metrics are down. That's because version 1 of my Java image only has runtime metrics, the application metrics come later in the course. The Tomcat runtime metrics are up, only two out of four, so two of my Tomcat instances are still starting up. On the. NET side, again, there are no application metrics because I haven't implemented them yet, but the IIS runtime metrics are all up. While I'm on the targets page, it's good to point out that Prometheus is a resilient server. It doesn't break if endpoints aren't available. It will keep trying to reach them, and it will collect metrics from every target that is up. On to the graph page to start working through my queries. First I want to know requests per second for my. NET app, which is from an IIS performance counter. Prometheus has values for both the Windows containers, and this is just showing the most recent value. Using irate function, I can get a rate of requests per second over the last 12 hours. I'll switch to graph view and zoom to the last 5 minutes. There's one line for each container. That's useful, but for a headline graph I just want a total, which I get with the sum function. I'll use the without clause to exclude all the labels that distinguish different values, so I'll just get a single combined sum, and this is the total requests per second across all containers for the. NET app. I want to see how hard the ASP. NET processes are working, and I get a good idea of that looking at the thread count. Again, I get one result for each container, and the graph view shows that more clearly over the last 5 minutes. I do want to see this at container level, so I can see if work is being evenly distributed among all the containers. And the next important metric is memory usage. This is the. NET heap size for Generation 0. or a more in-depth dashboard I might want to include older generations too. Again this is one result for each container, and I can take a sum without the instance labels to get a total value. This graph will show the heap size growing and shrinking when there's a garbage collection. Those are the key things I want to know for my. NET app, and I can get the same headline stats for my Java app, even though they're coming from a very different source. For request rate I can use Tomcat's processor request count. I get a value for each container, but also for each port, and I don't want the admin port, so I'll filter just on port 8080. Now, just like the. NET app, I'll use irate to get the rate of change over the last 12 hours, and sum that without the instance labels to get a single value showing rate of HTTP requests. This is a different metric source and a different query, but the same information as I've got in my. NET app. For the thread count I can get this from the JVM. There's one value for each active container, and when I graph that it gives a rough idea how the work is being distributed between containers, and for memory usage, I have the JVM bytes used, which gives values for each container, and for heap and non-heap memory. I'll filter for the heap, and sum the response without the instance labels. This will show me if I have a memory leak in the Java app. So I can produce the same sort of information from Prometheus for my Java Linux containers and my. NET Windows containers. I'm running in a production-like clustered environment, and Prometheus is using DNS service discovery. When I scale my services up or down, or when failing containers get replaced, Prometheus keeps scraping metrics for all the containers, so this is perfect for a dynamic, self-healing environment.

Module Summary
In this module you've learned about runtime metrics, operating system and application host data, which is already being collected in your containers. They give you key information on how hard your app is working, so you know if it's getting overloaded. You saw how to make those metrics available to Prometheus by adding an exporter to your Docker image. I used a Tomcat exporter for my Java app, and a Windows Performance Counter exporter for my. NET app. I also showed you how to work with those metrics in Prometheus, using the UI to build up queries to get graphs covering HTTP requests, CPU thread count, and memory usage. You saw how this monitoring approach abstracts the implementation details, so you can surface the same types of information from completely different technology stacks. In the next module I'll look at a deeper level of monitoring, application metrics. Unlike runtime metrics, which you get for free, with application metrics you explicitly record the events you care about in your own code. You'll see how to do that using the Prometheus client libraries for. NET and Java, and see how to query those application metrics in the Prometheus UI. That's in Exposing Application Metrics to Prometheus, the next module in Monitoring Containerized Application Health with Docker.

Exposing Application Metrics to Prometheus
Module Overview
The great thing about monitoring containerized apps is how much detail you get from such a wide range of sources. You can build a single dashboard, which tells you what's happening in Docker, what's happening in your containers, and exactly what's happening inside your applications. My name's Elton, and this is Exposing Application Metrics to Prometheus, the next module in Pluralsight's Monitoring Containerized Application Health with Docker. So far I've looked at generic metrics which your application runtime collects for you, things like Java JVM statistics and. NET Windows Performance counters. Those are great to show you how hard your application is working, but they don't tell you what's happening inside your app. This module is all about custom application metrics, collecting specific information that you care about, that's relevant to your app, and making it available to the rest of your monitoring stack. Maybe you have an event driven architecture, and you want to record metrics about the number of events published, the number handled, the number successfully processed, and the number of failures. You can do that and you can group events by type, so you can see exactly how many of each event type is in each status. Or maybe you connect with a lot of external services, and you want to record how many calls you make, what responses you get, and how long the calls take, and again, you can record that for each service. You can even record business metrics, like how many new customers have been registered, how many sales have been made, and the total sale value for a period. You expose all these application metrics to Prometheus, which means you have a single monitoring system for all these different types of data, and you can build different dashboards for different users, all being powered from the containers running in Docker. I'll start by showing you how to make these application metrics available to Prometheus.

Recording Metrics in Your Application
You've seen how to add a Prometheus endpoint to your app by packaging an exporter utility alongside the app, so you have two processes running in your container, your application, and the utility, which exports runtime metrics at an HTTP endpoint. Writing application metrics needs to be done in your own application code, so you're going to have a metrics endpoint hosted by your app. You can include this as an additional endpoint, specific to application metrics, and leave your runtime metrics in a separate endpoint, or you can include runtime and application metrics in the same endpoint, and remove the exporter utility. I'm going to use separate endpoints in this course, because it's more flexible. Separating the endpoints means you can have different scrape jobs in Prometheus with different time schedules. Hosting that metrics endpoint in your app is really easy. All the major languages have a Prometheus client library, which you can just include in your project, and it provides the metrics endpoint for you. You'll also use the client library to record your application metrics. The logic to record metrics is simple too. You start by creating the metric with code like this. This creates a counter with the name events_processed_total and the description, events processed. The other strings are labels, which you've already seen, but I'll explain in much more detail in this module. You keep a reference to that counter object, and when you have a new event to record, you increment it with code like this. The Inc method increments the counter by one, and you can also increment by an explicit integer. The value of that counter is stored in memory in your application. It doesn't go anywhere until Prometheus scrapes the endpoint when it collects the sample along with the timestamp of the scrape, and stores it in the time-series database. Okay, just a bit more theory before we get to the demos, because you really need to understand labels. When you collect metrics for monitoring, you need to store them at the lowest level of granularity, so I don't just want a total number of events processed, I want to know the number of events of a particular type, in a particular status, handled by a particular container. That's where labels come in. I can have a single Events Processed metric, but store more granular data using labels. I have one label for the event type, one for the processing status, and one for the host, which is the container ID when I'm running in Docker. I record the data at the lowest level, so I increment the counter with a specific set of labels, the host name, event type, and status. Prometheus stores data at that level, but lets me easily aggregate, so I can get a count of event types for any status and any host, or I can drill down and find a count of failed events by host to see if one container is behaving badly. Metrics and labels are part of the Prometheus API, so they work the same way with any client library. Next, I'll show you how it looks in Java.

Demo: Adding Application Metrics to Java Apps
In this demo I'll add custom application metrics to my Java web app. I'll show you how to add the Prometheus client library with Maven, how to create and register metrics, how to use counters and gauges, and how to provide the Prometheus endpoint as a servlet. Here in my Java web app I've made a couple of changes. First in the Maven dependencies, I've added the packages for the Prometheus client. This is an official client, which is provided and supported by the Prometheus team. The simpleclient package is for capturing metrics, and there are several options for exposing the metrics endpoint. I'll be running in Tomcat in my container, so I've included the servlet package, which will publish a metrics servlet. Those packages give me access to the io. prometheus. client namespace, which has the Counter and Gauge classes. I've imported those already into my index class. First I'll create a Counter object using the build method on that class. There's a fluent API here, so I in the same call I name the metric, give it some help text, specify the labels, and register the metric with the local Prometheus client. Creating the counter is an expensive operation, so it's done as a static final member, and I'll use the same object every time I work with this counter. This metric is for recording session status, like the number of sessions started and the number of orders placed. The status label is for recording different types of status. Those metrics will only increase for the life of the container, so this is a counter metric. I'll also create a gauge, using the same approach with the build method from the gauge class. This is recording the number of active sessions, which can go up and down, and for that you need a gauge. The only label here is the host name, which will actually be the container ID. That's very useful for identifying which container generated the metrics, if you need to connect to the container or fetch its logs to investigate a problem. Okay, so when a user hits the page for the first time the sessionInit method fires. Here I'm going to increment the session gauge. I specify the label, which is just the DNS name of the container recording this metric, and call the inc method. I don't need to store the current value, or worry about multiple threads incrementing at the same time. The client library takes care of all that. I'll also increment the total number of sessions started in the counter. The two labels here are the host name and the status name. This is the number of started sessions. Then I just call inc again, and the client library increments the metric. Labels are really powerful in Prometheus, but they're just strings, so you need to be careful about how you use them. The correct order of the labels isn't something the client can enforce, because they're only strings, so you need to make sure you specify labels in the same order that you used when you created the metric object. The next session status I want to record is when users submit an order. This is in the submitOrder method, and it's a call to increment the session counter, specifying labels for the host and for the order-submitted status. To get this level of detail in your monitoring, you need to make these sort of code changes, but the end result is a very high level of visibility. In a real app I'd also hook into the user logging out, so I could decrement the active session gauge, but what I have here is enough to capture some useful stats. To provide the metrics endpoint, I need to set it up in the web XML. First I'll add the metrics servlet, using the class from the client package, which gives me the exporter. Next I'll map the URL, so the app-metrics endpoint gets served by the exporter servlet. My Dockerfile for the app compiles the source code and then packages it, so I'll build a new version of my Java image. In this session I'm connected to a Linux Docker engine. Then it's just docker image build as usual, giving a tag with version two this time, and specifying the path to the Dockerfile. I've tested this build before, and so everything comes from Docker's cache. Now this version of the image has application metrics available. When a user hits the app-metrics URL, the exporter handles the request, and returns the current values for all the metrics that I've registered in my app. That's how Prometheus will read and then store my session gauge and counter. Next I'll show you how to do the same for a. NET application.

Demo: Adding Application Metrics to .NET Apps
In this demo I'll add custom application metrics to my ASP. NET web app. I'll use the community Prometheus client from NuGet to create and register metrics. I'll also show you how to use counters and gauges, and how to host the Prometheus endpoint in an ASP. NET website. I've already added the Prometheus client package from NuGet into my app's dependencies. This is a. NET Standard package, which means you can use it with full. NET Framework apps like this, and also. NET Core apps. The client library contains all the metric classes and also the exporter endpoint, and it has its own dependencies for Protocol Buffers and JSON. The metrics I want to collect will all be from the homepage, and here I've added the Prometheus using statement. First, I'll create the session counter. This is a private static member because of the cost of instantiating the object, and I can reuse the same object for all instances of the class. The constructor parameters here are the counter name, the help text, and then the labels. I'm using labels for the DNS hostname of the server, which will be the container ID in Docker, and the status of the session, and I'll also create a gauge which I'll use to track active sessions. Here the parameters are the metric name, the help text, and the labels. You need to be careful with the. NET client because the labels are just string arrays, and you need to make sure you reference them in the same order when you create the metric object, and when you use it later. When a user first hits the site I generate a session ID and assign it to session state. This is where I'll increment the active sessions gauge, which just has a label for the host recording the value, and I'll also increment the session counter for this host and for the started status. The client library is much the same in. NET as it is in Java and in all the other supported languages. You create the metric object once, keep a reference to it, and then call inc to increment the label that you're measuring. When a user adds an item to the cart I want to track that, which is in the button event handler. Again it's a simple increment call, specifying labels for the hostname and the added-to-cart status. Finally, when a user goes to the checkout it fires this event handler, where I'll increment the checked-out session status counter, and that's the end of the session, so I'll decrement the active session gauge. That's all the code I need to record my metrics, so now I just need to expose the metrics endpoint, which I'll do in the global application class. The. NET Client library collects a whole bunch of metrics by default, including. NET memory and CPU usage from performance counters. I already have that in my runtime metrics exporter, so I'll keep this endpoint clean by clearing out those default collectors, so the only metrics which are registered now are my custom application metrics. Now I just need to start an instance of the MetricsServer class from the client library, specifying the port to listen on. I'm using a different port from my runtime metrics exporter, so they don't clash, but for a real world application I'd make this configurable, so I can set the port when I run the container. That's all I need to do, so now I'll build a new image version. This is a Windows image and I'm connected to a Windows Docker engine here. So it's just docker image build with a new v2 tag for the image and the path to the Dockerfile. This Dockerfile has a multi-stage build, which compiles from source, so you'll see the MSBuild messages as the app compiles. The next stage of the build comes from the cache up until the file copy command, which copies the latest published website from the builder stage. And that's done, so version two of my. NET app now has application metrics available for Prometheus. There are many client libraries you can use with Prometheus, and different options for hosting the metrics endpoint. Before I go on to deploying the new version of my app and scraping the application metrics, I'll quickly cover some of the other client options.

Prometheus Client Libraries
I've shown you how to add application metrics to your app by instrumenting the code using a Prometheus client library. There are a whole lot of client libraries you can use. They do vary in implementation details, but they all follow the same basic process. In your app code you start by creating your metrics objects, one object for each counter, gauge, summary, or histogram you're recording. The metric is created with a name, help text, and optionally, a set of labels to stripe the values you record. I used hostname as a label for all my metrics, and in other metrics I also have a status field for recording variations. You record the data for your metric by incrementing counters, by incrementing or decrementing gauges, and by observing values for summaries and histograms. In each case you include the labels you're working with if the metric has any labels. Then in your application startup you make the metrics available through a server endpoint. The client libraries typically include an HTTP server implementation, or they integrate with your application server to provide the metrics endpoint. You've seen the Java and. NET implementations. There are official libraries provided by the Prometheus team for Go, Python, and Ruby, as well as Java. The client libraries don't all follow the same semantics. Here's an example in the Go client. Metrics are created and then registered using the core Prometheus class. Values are recorded using the WithLabelValues() method, and then the metrics endpoint is exposed using the standard Go http class, with a handler that's part of the client library. Compare that to the Python client. The registry of metrics collectors is an object you use explicitly, and you supply the registry to use when you create a metric. You use the labels() function to distinguish label values, and there's a start_http_server function to host the metrics endpoint. You'll need to get familiar with the client library for your language, and there are lots more community libraries, as well as the official ones, for. NET, and Node. js, Erlang, Haskell, Rust, all the major app platforms are covered, so you should be able to get metrics into your app now. In the next demo you'll see how to get those metrics out and into Prometheus.

Demo: Scraping Application Metrics with Prometheus
In this demo I'm going to deploy the updated Java and. NET apps, which have application metrics built in, upgrading my stack in Docker Swarm. Then I'll fire some traffic at the containers and show you how Prometheus metrics let you aggregate statistics or drill down using labels. Here's the compose file for my app. All I've changed is the image tags to use version two of the Java and. NET applications. The production override file is pretty much the same. The java definition hasn't changed. In the. NET definition I'm publishing the metrics ports just so I can check them directly if I need to, and the Prometheus definition publishes the UI port and maps the config from the swarm. I covered all that in the module, Collecting Metrics with Prometheus. I'm connected to my swarm here, with two Linux and two Windows nodes. My version one stack is already running, and I can use docker stack ps to see the services in the stack. I've got v1. NET containers and v1 Java containers. I'll upgrade the stack, but first I need to join the new compose and override files together. I'll use docker-compose, just like before, specifying the core compose file and the production override. Then use the config command to join and validate the files, and capture the output in docker-stack. yml. Now the stack YAML file has the v2 application images. In Docker Swarm you upgrade a running stack using the docker stack deploy command, passing the compose file to use, which is my stack file, and the name of the stack. Docker uses the compose file definition as the desired state, compares it to the running state, and create or updates services as necessary. Now when I look at the containers in the stack I see there are v1 app containers, which have shut down, and the replacement v2 containers are running. I'll switch to the UI where it's easier to see what's happening, and in the services I can see 2 containers running the v2. NET app and 4 running the v2 Java app. I'll browse to one of the Windows nodes, and here's the. NET app. It's the same application, but when I load the page and click the buttons the app is now recording those events as metrics. Same with the Java app. It looks the same and works the same way, but the metrics are being recorded behind the scenes. I'll go back to my Windows app and check the original runtime metrics endpoint. This still shows all the same. NET, IIS, and Windows metrics that you learned about in the runtime metrics module, and now I also have an application metrics endpoint listening on port 50506. This response comes from the metric server in the. NET client library, surfacing the metrics I record in my application code. And that's the same in the Java app. I have the original runtime metrics at the metrics URL, and the new application metrics at the app-metrics URL. Now these metrics are empty right now, even though I've been using the app, which could mean Prometheus hasn't scraped the metrics yet, or it could mean that this time I'm hitting a different container, which hasn't had any requests yet. Remember I'm using ingress publishing for the Java app, so Docker is load-balancing requests across all the containers. I'll have a look at the Prometheus UI to see where we are, and actually only three of the four java targets are up, even though all 4 containers are running, which means the Tomcat startup must be very slow in one of the containers. The. NET Prometheus client only returns metrics if any have been collected, so only the container that I've hit is returning stats. To get more data coming through all my containers, I'll send in some HTTP requests. This is a simple CURL statement, which is looped 20 times the fetch content from the Java app. I'll run that a couple of times, and now this command does the same for one of my Windows nodes, hitting the. NET app 20 times, and I'll also do it for the second Windows node. I'm using host mode publishing for the Windows app, so I don't get load balancing across the containers. You can watch my Pluralsight course on load balancing in Docker Swarm to learn more about why that is. Now I'll refresh the Prometheus target list, and all the Java endpoints are up, all the. NET endpoints are up, and I'll have some application metrics to look at. So back to the graph view. My application metrics are all here. You can tell them from the other metrics because they're using a different naming convention. This is the active session gauge for the. NET homepage. If I look at the latest values, I get one entry for each container. Prometheus adds the instance label, but that's an IP address, which isn't very useful. My own host label has got the container ID, which is much better. I can filter to just one host, and use the ID to go back to Docker if I need to track down one badly performing container. Using one metric and having multiple labels to distinguish values lets you pick out individual details, but also to easily aggregate. I can take a sum of active sessions using the without clause to ignore all the labels. This gives me a total value of all active sessions across however many containers I have. Labels are even more useful in the session status counter. When I query the metric I get values for each status on each host. I can see the container starting 2AD is the one I browsed to with Firefox, where I added items to the cart and checked out. The other container starting 058 just has the session started value from the CURL requests. I can filter just on the added-to-cart status, and filters are cumulative, so I could also filter on one container ID, and of course, I can sum across a filtered query, so I remove the labels I'm not interested in, and I get a total value for items added to carts across all containers. The Java app metrics work in the same way, but Prometheus is scraping more containers this time. The active sessions gauge returns a value for each of the four containers. I can filter on the host label to get the value for a single container, and I can sum across all containers by excluding the host, instance, and job labels, and lastly, the session status counter in the Java app. I get values for each host and status label. I can filter on the status to get just the number of orders submitted, and sum that filtered query to get the total order count across all containers. The Prometheus UI is just showing the latest value for this query, and in the return it includes the status label, because I haven't excluded that label in the without clause. When we get onto Grafana later in the course you'll see that it's useful to get this metadata from Prometheus to add information to your dashboard. That's it for the demos, but before I wrap up this module I'm going to cover some best practices for storing application metrics, so you can build instrumentation that really works for you.

Instrumentation Best Practices
Application metrics are a way of instrumenting your app, so you can see what's happening inside. You want to capture metrics so they can have a high level representation in a dashboard, but the detail is at a low enough level so that you can dig into and diagnose any problems that you find. In Prometheus labels are the key to this, and they affect how you need to think about metrics. Say you have a website or a REST API and you want to record metrics about HTTP responses. You want to know counts for response status codes, so if your 200 OK count suddenly dips, and your 500 error count increases, you know there's a processing problem, or if the 401 and 403 count suddenly spike, maybe your app is under attack. At first glance, those look like different metrics, like http_responses_200_total and http_responses_500_total, but if you record them as different metrics in Prometheus you make it difficult to work with them. If you want to show a graph with both values, then you end up having to query multiple different metrics. Instead, you should record one metric called http_responses_total and have a label for status_code. Then you instrument them by incrementing the relevant label. Don't go wild with labels though, each label is actually a separate time series in Prometheus, which costs processing power and disk. Aim to keep the number of labels under 10 for your metrics. As to what you record, that depends on your app of course. You should err on doing more instrumentation rather than trying to capture just the key details, because it's much easier to delete a graph you don't use than it is to do another release which just add some more metrics, and here are a few guidelines. Any external interactions with other systems should be instrumented, so you know how often your app is communicating, whether the communication succeeds, and how long it takes. Any workflows that involve multiple processes should be instrumented at each stage with a correlation ID label, so you can track progress through the system. Any business facing metrics, which need reporting on, should be instrumented, so you can build a live dashboard rather than sending out historical reports. And look at all the logging that you currently have in your app. Hopefully you have lots. Every time you log something happening that's potentially something you want to instrument, so you get a count of how often those things happen, and this is a great piece of advice direct from the Prometheus website, try to instrument total counts for each type of log entry, info, warn, error and so on. Then you can compare the numbers between environments or between releases. If there's a big discrepancy you'll see that very quickly, and you might identify a problem in your app. When you start implementing app metrics you should head to the Prometheus docs website where there's a whole section on best practices. Here's the short link for this URL. There are pages here that go into detail on naming, suggestions of what to instrument, how to choose between metric types, and much more.

Module Summary
This module covered application metrics, the level of detail where you record what's actually happening inside your app. You learned how to add custom metrics to Java apps using the official Prometheus client, and to. NET apps using the community NuGet client. Both client libraries also provide ways to serve the metrics endpoint from your app for Prometheus to read. I used a servlet for my Java app and a custom metrics server on a dedicated port for my ASP. NET app. There are plenty of other options, and you've seen there are client libraries for pretty much every major language. I covered best practices for instrumenting your apps, and showed why it's important to record metrics at the lowest level of detail, using labels to distinguish variations of the same metric instead of completely separate metrics, so you can aggregate and graph values in Prometheus. There's one more set of metric to record before we can build a fully detailed dashboard. We are running our app in containers after all, and we want to show information about what's happening in the Docker platform. I'll cover that in the next module. I'll show you how to enable metrics in Docker on Windows, Linux, and Mac, and look at the kind of statistics you get, from the individual engine metrics, to details about the Swarm. Then I'll show you how those metrics look in Prometheus. That's in Exposing Docker Metrics to Prometheus, the next module in Monitoring Containerized Application Health with Docker.

Exposing Docker Metrics to Prometheus
Module Overview
In a containerized solution, everything runs in containers, all the components of your monitoring architecture, as well as all the parts of your app. It's critical to have insight into what the Docker platform is doing to manage those containers, if you want to be confident about going to production. My name's Elton and this is Exposing Docker Metrics to Prometheus, the next module in Pluralsight's Monitoring Containerized Application Health with Docker. The Docker engine has a built-in feature to export metrics in Prometheus format. It provides all the key metrics you need to monitor your containers in production, and the feature works in the same way across all versions of Docker. You can enable metrics in Docker Engine on the server, and in Docker Desktop on Mac and Windows, so you get the same consistency across all your environments. There are metrics covering the engine and containers, and in Swarm mode there are additional metrics about the cluster. There are even metrics about image builds, so when your CI process is all running in Docker, you can add monitoring to your CI servers using the same tools you use for application monitoring. In this module I'll show you how to enable metrics in Docker Desktop on Mac and Windows 10, and Docker Engine on Ubuntu and Windows Server. I'll look at the key metrics you get from the Docker engine and the Swarm, and I'll scrape those metrics and query them in Prometheus. I'll start by looking at how the Docker metrics work.

Working with Docker Platform Metrics
The first thing to understand is that Docker collects metrics in the engine. Docker has a client-server architecture, where the engine runs in the background and exposes a REST API for clients. The Docker command line is one client, but there are UIs and SDKs that also use the Docker API. All the clients are interacting with the engine, and it's the engine that collects and exports the metrics. So when you run docker image build from the CLI, the Docker Engine records metrics about that build. Metrics are in Prometheus format, so you can add Docker monitoring to your app dashboard using the same tools that you use for your runtime and application metrics. Right now in Docker 18. 06 the metrics feature is flagged as experimental. That means you need to explicitly enable it. It doesn't mean the feature is in beta or that it's unstable. Docker used that experimental flag to make it clear that the API isn't confirmed yet. In the case of metrics, that means the exact metrics exported may change by the time the feature graduates from experimental, but actually that's been pretty stable for the last few releases, so you're unlikely to lose any of the metrics I'll be showing you, and because experimental features are all opt-in, you can enable metrics without turning on any other experimental features. Docker have enterprise customers using experimental mode in production, with just the metrics feature enabled, so it's safe to use. In this module you'll see how to enable metrics on Docker Engine on Ubuntu Server and Windows Server, but I'll start with the metrics on Docker Desktop.

Demo: Enabling and Using Metrics in Docker Desktop
In this demo I'll show you how to enable metrics in Docker Desktop on Mac and on Windows, and show you how to access the metrics endpoint locally. Then I'll look at the key metrics that Docker records about the engine, about image builds, and about containers. This is Docker Desktop running on the Mac. For that you need OS 10 Yosemite or higher. Click on the whale icon and open preferences to configure the setup, and it's the daemon tab where you configure the Docker engine. I've got experimental features checked, which means the engine will run any experimental features I opt into. For metrics I need to switch to advanced mode, which gives me the actual JSON configuration that the Docker engine uses. You enable metrics by specifying the address of the endpoint with the metrics-addr property. I've set mine to listen on port 50501 on all local IP addresses, but you can use any free port. The metrics endpoint is in standard Prometheus text format, so I can browse to the localhost and see what's there. The first thing I see is a 404. There's no response handler set up for the server root. You need to use the metrics path, and here I've got all the Docker metrics covering builds, containers, and here there are details about the engine itself. I can see the OS is reported as Docker for Mac, and the OS type is Linux. Remember that the Docker Engine is running inside a VM here. The CPU count is 2 and the memory available is 4GB, and that matches how I've configured the VM. In the Advanced tab of the preferences, you can allocate CPUs and RAM to the Docker virtual machine, and this shows that the setup matches. Now I'll switch to my Windows box. Docker Desktop requires Windows 10 with at least the Professional edition, because you need Hyper-V to run the VM for Linux containers. The configuration is the same. Open the whale and click settings. Now in the daemon tab, again, I've got experimental features checked, but I need to switch to advanced mode to add the metrics address in the JSON config. It's the same metrics-addr value you need to set, and I'm using the same port. I'll browse to the endpoint at the metrics path, and it's the exact same set of stats that I saw running on the Mac. This is Docker in Windows containers mode, so I'm not using the Linux VM and the Docker engine is running directly on the host. Down here in the engine details, I can see I'm on Windows 10 Pro, and the CPU count is 4, and the memory available is 16GB, which is the full power of my laptop. If I switched to Linux containers mode, then the metrics would come from the Linux VM that Docker runs, and I'd see that in the engine details. The engine details metrics are static, they're just there for information, but the others show what's happening now in Docker. At the top there are image building metrics, lying at all 0 at the moment, so I'll do some builds, and that will update them. I've got Dockerfiles here which are a mixture of valid and invalid syntax. Some of these will deliberately fail when I try to build them. I'll build them all with PowerShell, so I'll start by getting a list of the file names, and then I can pipe that file name list into the Docker image build command using a temporary tag, and specifying the path to the file, so this will try to build all the images. Don't worry about the contents of those Dockerfiles. They're all in the course materials if you want to see them, but they don't do anything particularly interesting. When I build I've got two errors here, an unknown instruction error, and an empty Dockerfile. I'll repeat that loop to get some more stats, and now switch back to the metrics. I'll refresh and now I've got eight builds triggered, which is the total number, and then two failures with unknown instructions. This is really useful for developers in the desktop and for CI environments when all your builds are running in Docker. Container metrics are here in real-time too. I have three containers running, a basic website, and two containers from badly-performing images. That slow-start image has a custom startup command which sleeps for a few seconds, so every container run from this image will take a while to start. The failed-healthcheck image has a healthcheck command which will always fail. This is valid syntax, so the image builds, but the healthchecker. exe app doesn't exist inside the image, so the healthcheck will error every time it runs. In the metrics I can see those three containers running, and I have a histogram here showing container actions. I haven't focused on Prometheus summaries or histograms in this course because they're a more advanced topic, but you can see here how they work. This metric records how long container actions take in seconds. There's a label for the type of action and the start action records container startup time. There are buckets for each time range, so less than half a second, less than 1 second, and less than 10 seconds. These are cumulative counts, so I've started five containers, one started almost instantly, another 2 took between 1 and 2. 5 seconds to start and a further 2 took between 2. 5 and 5 seconds. Docker records this in a histogram so you can easily query Prometheus to find percentiles, something like the 95th percentile startup time might be useful. I also have the healthchecks in here. One of my running containers has a constantly failing healthcheck, so the engine has fired 88 healthchecks so far, and they've all failed. The check runs in the container every 2 seconds, so when I refresh the failed count jumps again. These metrics are really useful on the Desktop, so dev can check that any changes they make don't impact containers in production. Next I'll walk through the most useful of those metrics.

Understanding Docker Engine Metrics
You enable metrics in Docker by switching on experimental mode, and adding the metrics-addr configuration flag, to specify the port where Docker exposes the metrics endpoint. The types of metric you get from Docker broadly fall into three areas; the engine, the builder, and the containers. These are the core metrics which you get for all the Docker runtimes, Docker Desktop and Docker Engine running in standalone mode or in swarm mode. On the engine side you get information about the host, like the number of CPUs and the amount of memory. That's useful in virtualized or managed environments when you want to see exactly what resources Docker has been allocated. You also get low-level details on the version of Docker running, including the CPU architecture, operating system version, whether the OS is Windows or Linux, the Docker version, and the exact build of the Docker engine. Image building metrics are really useful in a CI environment where all your builds are running in Docker. You get metrics for the number of builds triggered, and the total failures with a label for the reason. This isn't a CI/CD course, so I won't go into more detail here, but if you're not using Docker for CI you really should look into it. It massively simplifies your build workflow. Probably the most useful, though, are the container metrics. You can see how many containers are in each state, running, stopped, and paused, and you can get a breakdown of how long container actions take, so you can track if containers are taking a long time to create. And the last really useful metric is on healthchecks, the number of container healthchecks Docker has fired, and how many have failed. This has to be mandatory information in production, but it's really useful to have visibility in other environments too. If devs make a code change, which means the healthcheck for a container constantly fails, that's not something they'd necessarily see in a single-node environment because Docker will keep that container running, but that problem means constant container restarts when you get to the swarm. In swarm mode you get all these engine metrics for every node in the cluster, and the master nodes also emit metrics about the swarm. I'll show you how metrics work with Docker Engine on Ubuntu Server and Windows Server in swarm mode next.

Demo: Enabling and Using Metrics in Docker Swarm
In this demo I'll show you how to enable metrics in Docker engine on Ubuntu Server and Windows Server. I'll show you the metrics that Docker exposes about the engine and about containers, and also the additional metrics about the Swarm cluster. Docker Engine on server operating systems uses the same JSON configuration format as Docker desktop, but you need to edit the file directly. On Linux that config file is typically at etc/docker/daemon. json. Here on my Ubuntu manager node I've set the experimental flag to true, and set the metrics addr value to be the same port I always use. I have these same settings configured in my other Ubuntu server in the Swarm. This is the place for other Docker configuration too, so I have a local Docker registry running on HTTP instead of HTTPS. Docker requires SSL for registry communication, so if you're using HTTP in a local environment, you need to explicitly add the address of each insecure registry that you want to allow. When you change the config file you need to restart the Docker engine. How you do that differs between Linux distros, but on Ubuntu you'd run service docker restart. These are still my local Docker Desktop metrics, and I'll browse to my Ubuntu server to make sure the engine metrics are coming through. This server is a lot busier than my desktop, so I can see a wider range of container start times. Out of 67 containers, 39 started in under a second, and another 21 started between 1 and 2. 5 seconds. There are a few outliers here, which took over 2. 5 seconds. Maybe I'd want to check those out in a test environment before I went to production. I have all the engine details showing I'm running Docker Enterprise version 17. 06 here on Ubuntu 16. 04 with 4 CPUs, and about 4. 5GB of memory. This node is a Swarm manager, so I also get all the swarm metrics. This is the internal node ID and the Swarm ID of the cluster, and this is the count of nodes striped by state, so I have four nodes ready and none down. The Windows nodes in my cluster are running Windows Server 2016 Core with no UI. To enable metrics for Docker I need to use the same config file. On Windows the location is c:\programdata\docker\config\daemon. json. It's the same format for every version of Docker, and I've got the experimental flag set to true, and the metrics-address specified. Just like on Linux, if you edit this file you need to restart the engine, which you do on Windows by restarting the Docker Windows service. Again, I have these same settings configured in the other Windows server in my swarm cluster, so every node is configured to export metrics. I'll browse to that Windows server now at the metrics URL, and it's the same set of metrics we've seen everywhere. This consistency for monitoring across all operating systems and every environment is a real step forward. The engine details show I'm on Windows Server 2016, with 2 CPUs, and 2. 5 GB of memory. The swarm stats are here too, but because this is a worker node it doesn't have all the information that a manager node has. I have the node details with the server ID and the Swarm ID, and the node status counter is there, but all the values are 0 from a worker node. That's something to be aware of when we start collating these metrics in Prometheus. As well as the node status, the manager has a lot more metrics on the internals of the swarm. There are a suite of metrics about etcd, which is the distributed data store the cluster uses. There are metrics on the Go runtime where the Docker engine is running. There are metrics for Go Remote procedure calls, which is how the Swarm nodes communicate, and there are metrics for Docker engine's REST API. Some of these metrics are pretty low level, but they're useful at times for troubleshooting. Metrics about the Swarm are one area that's being added to in subsequent releases, and I'll talk about some of those metrics next.

Understanding Docker Swarm Mode Metrics
Docker running on server operating systems is configured with the daemon. json file in the same way as Docker Desktop. There's no friendly UI, so you need to set the experimental flag to true and the metrics-address value in the JSON file. The file lives in etc/docker on Linux distros and in C:\ProgramData\docker\config on Windows. In swarm mode you need to enable metrics on every server because each engine collects its own metrics. So when you're looking at container statuses the individual values come from each node, and you'll use Prometheus to aggregate and graph across them. In addition to the engine metrics, the manager nodes also export metrics about the Swarm. These fall into three areas, information about what the Swarm is doing, information about how the Swarm is performing, and low-level information about the internals of the cluster. The cluster internals give you metrics on the Go runtime which Docker uses, the http usage of the Docker API, and details about etcd. Etcd is a distributed key-value store which Docker uses in Swarm mode to reliably store data across the cluster. You rarely need to dive into this level of detail, but if your cluster is running at full capacity, checking out these metrics could tell you if there's a bottleneck somewhere. Swarm performance metrics are more useful day-to-day indicators of the health of your Swarm. The most basic metric is one of the best, how many nodes are in the swarm, and what status they're in, so you can see as part of your application dashboard if there's a serious issue with your cluster. If your graph showing the average response time of your app suddenly shoots up, and you can see in the same dashboard that half of your swarm nodes have gone offline, then you have a pretty good idea where to start investigating. My swarm is running Docker 17. 06, and there are more swarm performance metrics in the latest 18. 06 release. You additionally get metrics to see how long it's taking for internal swarm transactions to complete, and also how long it takes for containers to start running, once they've been scheduled to start on a particular node. These can be useful to identify if there's a communication problem between your manager and your worker nodes. There's additional information too on what's happening in the swarm. Metrics like the total number of config objects and secrets stored in the swarm, and the number of running services give you a good idea of your swarm's utilization. Application dashboards really just need high-level information about the Docker platform, so you can identify any serious issues. It's nearly time to start building that dashboard in Grafana, but I'll finish up this module with a demo looking at these Docker metrics in Prometheus.

Demo: Scraping Docker Metrics in Prometheus
In this demo I'll start querying Docker engine metrics and Docker Swarm metrics using Prometheus. You'll see how to configure Prometheus to scrape from manager and worker nodes, and how to aggregate data across the swarm. Here's the default Prometheus config file I'm using in my Prometheus image. I've got jobs for my application and runtime metrics, and also for my Docker metrics. I'm using separate jobs for Docker Swarm manager nodes and worker nodes. In single-node environments, like dev or test, these will both point to the same target. You can run a single-node swarm and get the full set of metrics, or you can run the engine in standalone mode, in which case the additional swarm metrics will be empty, but it's important to have a consistent set of jobs in all your environments. The manager job doesn't really make sense in development environments, and it would be easy to delete it, but then my Grafana dashboard would break in dev if I had queries that selected by the manager job. In production I have the same set of jobs, but the targets are the IP addresses of my real manager and worker nodes. You don't have to use IP addresses. You can use DNS hostnames, as long as Prometheus can reach them from inside the container. Onto the browser. My metrics are running here on my Linux node and on my Windows node. This is the UI for my Prometheus container where I'll start looking at metrics across the whole swarm. So I'll start with that CPU metric. This is just a count of CPUs per node, not hugely useful by itself, but this is where the breakdown by managers and workers is useful. If I sum this by job, then I get the total CPUs for all my worker nodes and the total for all my manager nodes, so this gives me an idea of the processing capacity of my cluster, and I can do a similar thing with memory capacity. This is the memory in bytes for each node. Bytes isn't very readable, but I can do simple math in PromQL and divide that to get a readout per node in gigabytes. And now I can sum that without instance, which is the same as summing by job, in this case, and see the combined amount of memory across managers and workers. The container state metric shows a count of containers in each state on each node. On a detailed dashboard that might be useful, but for a higher level I maybe just want to see the sum by state, which shows me the states across the whole cluster, or I could sum by instance to see how many containers are on each node, and then I can filter this job to include only the workers, so this show me the total count of my own application containers on each worker node. Swarm node info is sometimes useful if you want a quick way to find the external swarm ID for a node by its IP address, but swarm_manager_nodes is the more useful metric, which shows nodes by state. All the nodes return values for this metric, but only the managers have any actual data. The worker nodes don't know about each other. So I'll filter this to the manager's job, and that gives me the count of nodes in each state. I can sum this by the state label, and that gives me a cleaner output where the value for the state is shown, but not the instance or the job name. These are all pretty simple Prometheus queries. I'm still not doing anything with the more complex summaries and histograms, but this will give me a very useful insight into my swarm cluster.

Module Summary
In this module I covered the last type of metrics that you want to collect for containerized applications, metrics from the Docker platform itself. You've learned that the Docker engine can collect and export metrics in Prometheus format, and that it's currently an experimental feature. I showed you how to enable metrics in Docker Desktop on Mac and Windows, and in Docker Engine on Ubuntu Server and Windows Server. For all those types of deployment, Docker collects the same engine metrics, covering image builds, containers, and the engine itself. In swarm mode there are a whole lot of extra metrics, covering the basic setup of the cluster, what's running in the swarm, and low-level details, which can be useful for tracing problems when you're pushing your swarm to the max. You also learned how to monitor the cluster in Prometheus, scraping from all the nodes and aggregating across them to get high-level statistics about the swarm workers and the managers. That's it for setting up your metrics sources, and in the next module I'll look at building all the data we've seen in the course into a Grafana dashboard. You'll see how to configure Grafana, how to build up a dashboard, and how to package your dashboard into a custom Docker image. That's in Building Dashboards with Grafana, the next module in Monitoring Containerized Application Health with Docker.

Building Dashboards with Grafana
Module Overview
The goal of building monitoring into your application is to have a single place where you can see at a glance how well your app is performing, and if it's not performing well then see where the problem is likely to be. That's your application dashboard, where you show all your key application, runtime and container metrics. My name's Elton and this is Building Dashboards with Grafana, the last module in Pluralsight's Monitoring Containerized Application Health with Docker, where we'll tie together everything we've seen in the rest of the course. In this module you'll learn different approaches for hosting your application dashboard, and you'll see how to build and package the custom dashboard for our demo application. I'll show you how to run Grafana in Linux and Windows containers, how to connect to Prometheus as a data source, and how to put together visualizations from different Prometheus queries to build up the dashboard. I'll start with a quick walkthrough of how Grafana works. It's a web application which runs queries against data sources and visualizes the results. Each query is represented in a visualization called a panel, and the panels in a dashboard can all use different data sources. Grafana has built-in support for querying time-series data sources like Prometheus and InfluxDB, as well as relational databases like MySQL and SQL Server, so you can build a dashboard that queries across all the parts of your stack. You can have multiple dashboards in Grafana which cover different apps, or different levels of detail for one app, and you can configure a playlist so Grafana will loop through a set of dashboards, which is ideal for showing project status on the big screen. Grafana itself is pretty easy to use, because most of the complexity is building the data source queries, which you'll most probably have done before you get to your dashboard. In this course I've been using PromQL in the Prometheus UI to work through all the queries I want to visualize in my dashboard, so now I just need to plug those into Grafana. Before I do that, I want to cover the approaches for running Grafana in a containerized app, so you can see what your design and deployment options are.

Approaches to Running Grafana in Containerized Applications
This is the monitoring architecture I walked through at the beginning of the course, and I've been building it out in all the modules up till now. This is a nice approach where the project stack and the monitoring is completely self-contained. I have a dedicated Prometheus instance for collecting metrics and a dedicated Grafana instance for my dashboard. That makes it very easy to deploy the same set of components in every environment, but it does have a drawback in larger companies where there are lots of projects all following the same design. Then you end up with dozens of Prometheus instances and dozens of Grafana instances, you need to manage all those deployments and work with lots of URLs for different dashboards, and you can't get a consolidated view over your whole estate. The alternative architecture is to have a centralized monitoring stack, which is shared between all projects, so you have a Docker network running Prometheus and Grafana containers, and all the application containers which host metrics endpoints are connected to the monitoring network, so Prometheus can reach them. This has the advantage that you can scale up Prometheus and Grafana to support multiple projects, and you have a much lower management surface to administer. The downside is the monitoring components need to be highly resilient and scalable because they're shared components now, and it becomes difficult to support automated deployments of the monitoring tools, because they need to support the release schedule of multiple projects. It's also much harder to have the same setup deployed in every environment, so you can't really have devs running the same monitoring as in production, and then you end up losing one of the key benefits in monitoring containerized apps. My preference is for hosting separate monitoring components for each project, which the project teams own. That means sharing the admin burden across all the teams, but it gives everyone the freedom to own their monitoring, and run it at the scale they need. It also has the big benefit that your Prometheus and Grafana deployments can be completely automated, with your own application's configuration and dashboards, because they won't be monitoring any other workloads. That's the approach I'll build out in the rest of this module, but first I'll start by running Grafana in a container and reading from Prometheus.

Demo: Running Grafana in Docker on Linux and Windows
In this demo I'll get Grafana running in Docker, using the Grafana team's image for Linux, and a custom image for Windows. Then I'll show you how to connect Grafana to Prometheus running in a container, and how to import a dashboard into Grafana from the community library. I'm starting with Linux and I'm connected to a Linux Docker engine here. I'll start by creating a Docker network for Grafana and my Prometheus containers. Grafana can use a lot of data sources, but I'm only using Prometheus in this course, so I'll start a new Prometheus container. I'll run it detached in the background and publish the port so that I can see the UI. Then I'll attach the new network and give the container a name. This will be the DNS name of the container, so Grafana can reach Prometheus, and I'm using the official Prometheus team's Linux image. That container started, so now I can run Grafana. This will be a detached container and I'll publish port 3000, which is the Grafana UI. I need to attach it to the same Docker network as Prometheus, and then this is the Grafana team's Linux image on Docker Hub. Now I'll browse to Prometheus and just make sure it's running, which it is, and this is the default Prometheus config where the server is scraping itself. Now I'll open up Grafana at port 3000. The default image has a user set up with username admin and password admin. When I log in with that account, I should really change the password, but I'm going to skip that for now. Here's the homepage on a new Grafana instance. The timeline here tells me what I need to do, so I'll start by connecting a data source, which is a data store for to Grafana read from. There are lots of choices for the data technology and lots of types of database; Elasticsearch is a document database, MySQL and SQL Server are relational databases, influx OpenTSDB, and Prometheus are all time-series databases. The data source tells Grafana what type of database it's using, and how to connect. I'll give this Prometheus source a name and specify the URL for the API. I can use the Prometheus DNS name, which Docker will resolve to the container IP address, and the same port as the Prometheus UI, 9090. All the default options can stay, so I'll save and test this connection, and the green block here tells me Grafana has successfully connected to Prometheus. I can write a custom dashboard to query Prometheus now, or I can import a standard Prometheus dashboard like this one. There's a community site on Grafana. com where you can share dashboards, and this one reads from the Prometheus server metrics and gives a lot of low-level information on what Prometheus is doing. I won't go into any detail on these metrics, but I'll show you that you can set the time range for the whole dashboard here, and you can also set the dashboard to automatically refresh. I've published the dashboard for this course up on the Grafana site, so I can import that here. I'll go back home and then manage dashboards, and from here I can select Import, and I need either the URL or the unique ID of the dashboard. Mine is here on the Grafana. com site, and the ID is 7193. So I'll import the dashboard, and now I need to select which Prometheus data source to use, and I only have one here. Now click import and this is the final dashboard that we'll be building up in this module. The data is all empty because I don't have the application containers running, but we'll get to that in the next demo. Before that I'll show you how to run Grafana in a Windows container. Like Prometheus there's no official image for Windows, but there's an up-to-date version on the Docker Samples repo with the same short link as before. The Docker file is pretty simple, it just downloads Grafana, verifies the checksum, and then extracts the ZIP file. Here I'm connected to a Windows Docker engine, and I'll run the dockersamples version of Prometheus. I don't need to create a Docker network on Windows, because by default all Windows containers are connected to the same NAT network, but I still need to specify a container name for Grafana to find Prometheus through DNS. Now I can run Grafana. It's the same command as on Linux, publishing the same port, but using the dockersamples Grafana image, which is called aspnet-monitoring-grafana, and tagged with version 5. 2. 1. I'll browse to Prometheus on my localhost. This is Docker Desktop on Windows 10 which does the localhost mapping. The sample image config uses a target which I don't have running, but Prometheus itself is working just fine. Now I can open port 3000 and load Grafana. It's exactly the same release of Grafana now running in a Windows container. I'll sign in with the admin account credentials, and ignore the advice to set a proper password. Now into the home page, and the experience is identical. I can add a data source, specify the type as Prometheus, and call it Prometheus, and then the URL is that same URL, connecting to the Prometheus container by its name. That's all working fine, so I'll import my application dashboard. The ID is 7193, and I need to select the Prometheus data source, and you'll see my dashboard load up. It's empty, but it's working fine. So you can run your dashboard in Grafana in Docker on Linux or Windows and use the same dashboard and the same Prometheus setup. Next I'll talk a bit more about data sources, dashboards, and panels before I go on to build my application dashboard.

Working with Data Sources and Dashboards in Grafana
Grafana has a plug-in architecture for data sources, and it ships with support for a lot of major databases built-in. You build your dashboard using visualizations in panels, and for each panel you specify the data source to use and the query to execute. Metric data almost always has a time element involved. You want to see the number of new customers in the last day or in the last 60 minutes. For time-series databases, like Graphite and Prometheus, Grafana takes care of the time part. You set your dashboard view for a particular time range, and Grafana adds the time restriction for you, so you can keep your queries simple. Grafana can also work with relational databases like SQL Server where data isn't always recorded with a timestamp. You can still read that data as a time series, by retrieving a date in your select statement and using one of the built- in query functions in Grafana. The $__timeFilter function tells Grafana to add the time range for the current dashboard to the SQL query, and there are lots of other functions available along with user-defined variables that you can add to your dashboards to make them completely flexible, so users can change what they're seeing in the dashboard without any changes to the queries. Grafana uses the native transport and query language for the data source, adding custom functions for standardized ways to build visualizations. So for Prometheus you connect the data source to the Prometheus API, and you define panel queries using PromQL. For SQL Server you supply a connection string for the database and write queries in T-SQL. For Elasticsearch you connect to the API and use Elastic's Query DSL for panel queries. Those panels can have different visualizations. The most common is the graph, which shows multiple data series with a legend to identify the graph lines or you can show the values in a bar chart. Then there's the Singlestat Panel, which shows a single number, either as a simple value, or as a gauge with green, amber, and red markings based on thresholds that you set. For larger datasets you can display a table, but these tend to crowd the dashboard with an overload of information, so they're better for a drilled-down display than for an application overview. Heatmaps are a great way to show trends across a large dataset over time, where color coding is used to show the frequency of values, but for a general audience they will need some explaining. The other major feature of Grafana is alerting. You can set up alerts in Prometheus too, but because Grafana can work with multiple data sources, it's a good idea to use Grafana as the single place for your alerts. You set up rules on your panels, which trigger notifications when metrics exceed thresholds. Notifications can be email, slack, or a call to a custom webhook. Okay, that'll do for the Grafana walkthrough. It's a powerful product and I won't be using all the features in this module, but you will see how easy it is to build a very rich dashboard, which is what I'll do next.

Demo: Building Your Application Dashboard in Grafana
In this demo I'm going to finally build the dashboard that you've seen so many times in this course. I'm going to start with an empty dashboard, and add all the different visualization panels. I'll show you how to populate the panels from Prometheus queries, and how to organize individual panels and the whole dashboard. I've switched back to my hybrid swarm in this session, and I have my stack running with all the latest image versions, so I have Prometheus reading application and runtime metrics from my containers, and Docker metrics from the platform. I also have Grafana running. In the browser I'll load Grafana at port 3000. My existing dashboard is already there, but I'll start a new one for this demo. I need a data source for that, and I already have a Prometheus data source set up with the same configuration that I showed you earlier. I'll run the test here to confirm that it's all working correctly, and it is, so my Grafana container is connected to my Prometheus container. I'll go back to the home view, and here click to create a new dashboard. This is a new, empty dashboard. I don't need to connect the dashboard to a data source because each panel will connect to its own source. These panels are the types of visualization that Grafana provides. There's a standard graph, single number, tables, and heatmaps. There's also a styling element here, so I can add a row, which is where I'll group related sets of metrics. I'll start with a row for my application metrics, and that means I can show or hide all the app metrics as one unit. Now I'll start with the real visualizations. I'll add a graph for the active sessions in my. NET shop application. The new visualization just shows a dummy graphic, and I click the panel title to get to the menu, which has an edit option. There's a lot you can configure here, but don't get overwhelmed. Most of this is for more advanced tweaking. As a minimum I need to select a data source, which is Prometheus, and then paste in my query. I'll use all the queries that I've worked through in previous modules. This is the sum of active sessions across all hosts, instances, and jobs. When you enter the query, Grafana connects to the data source, and now this is a live view of the data. I only have one graph line here, so the legend isn't really necessary. You can tweak the legend, which I'll do for other panels, but here I'll just select not to show it. Now I need to set a panel title so people know what this data is, and this can be any friendly name. Mine is Customer Sessions. Now return to the dashboard and my panel is here. It's using the 6 hour time range, which applies to every panel in the dashboard, and my app hasn't been running for 6 hours, which is why most of the data is empty. I'll add a new panel to show the total number of customer orders. This is a single number, which Grafana calls singlestat. The visualization just shows a random number to start with. Click to edit the panel, and I'll give it the title of Customer orders. In the metrics tab I'll use the Prometheus data source again, and paste in my PromQL query. This is the live metric value now, and I'll go back to the dashboard to add some more panels. The next few panels are more of the same, so I'll skip forward to where I've added them in, and then look at arranging them in Grafana. Grafana has a flow layout, so you don't explicitly place panels, they just snap next to other panels on the same row. I'll resize the stock order panel, and then drag it down next to the management sessions. These are from queries to my Java application metrics. I'll shrink the management sessions and move these panels over to the right. Now I'll shrink my cart items count, and reposition it, and do the same with my customer order total. Designing the flow layout is a bit fiddly, but you can arrange things how you want them. That's where I want my panels, but when you resize you find the layout may not be correct. I can't read the panel title here, so I'll edit it and use a smaller title, just orders will do. And I have the legend showing in my management session count, which isn't very helpful, and it overflows the panel size, so I'll go in and hide the legend. That looks better, so now I'll drag all those panels into the Application Metrics row, so I can move them all around together, and then save my dashboard. I've got lots more to do, but I'm only using graphs and single stat values in my dashboard, so there's nothing new to see and I'll skip forward. Here's the completed row for my runtime metrics, with all my IIS,. NET, Tomcat and JVM stats. You can shrink the rows in Grafana, and then use the grabber on the end of the row to move them around. There's one more row to add, for my Docker metrics. I've included the legends here so I can see what the graph lines relate to, and this is the core information about the containers and the swarm. I'll move those rows around, expand all the panels, and this is my full dashboard with all three levels of data. All the panels are using the same time range, so I can switch this down to the last 30 minutes, and here are the latest stats. When your application is mature, you'd probably have your dashboard set to show the last 24 hours, and you'd ignore the actual scale of the graphs. They're all measuring different things, but what you care about is the trends, and you'll quickly see anomalies when you get familiar with how your graph lines usually look. I've built this dashboard manually and it's stored inside the Grafana container on my cluster. That's no good for a repeatable deployment, so next I'll show you how to build a Docker image which has the dashboard built in and ready to use.

Demo: Packaging a Custom Grafana Image with Your Dashboard
In this demo I'll show you how to export the dashboard so you can share it, and how to add a read-only account in Grafana, which shows the dashboard when you log in. Then I'll cover how to build a custom Docker image with your own application dashboard. I'll start the quick and easy way by committing the running container to an image, and then show you the proper way using a Dockerfile, which automates the user and dashboard deployment on top of the standard Grafana image. Now my dashboard is done and saved in this Grafana instance, I can share it to other dashboards, or export the dashboard definition. When I do that I get a JSON file I can use later to automate the deployment, or I can post it up on Grafana. com. I'm logged into Grafana as the built-in admin user, which means I can edit the dashboard and data sources. For ordinary users, I want them to have a read-only view, which I can do by creating a new user account in the server admin section. I'll add a basic user with a password here, but if you have real users you can send them an email invite through Grafana, and they can set up their own accounts. You can group users into organizations and set permissions for a whole set of dashboards, but the default permission for a new user is view-only, which is just what I want. I'll log out of the admin account and log back in as the new viewer user account. The user sees the normal Grafana home page, which isn't very friendly. You can set a specific dashboard as your home page. First, you need to add the dashboard as a favorite, then in the Preferences view you can choose any favorite dashboards as your homepage, so I'll choose my application dashboard. I'll log out and then log back in again to check that it works as expected, and now the homepage is my application dashboard. I don't need to know how to navigate Grafana to see the dashboard, and this is all read-only so I can't accidentally or deliberately break anything. So now I have a running Grafana container which I've manually configured. I obviously don't want to go through all that manual configuration to import the dashboard and set up the user every time I do a release, so I want my own Docker image with the setup packaged into it. There's an easy way to do that. I can look at the running containers and get my container ID for Grafana, which is 11a5. The docker container commit command takes the state of a running container and saves it as a Docker image, I'll call mine psmonitoring/grafana:v2, and it's done. Now I have a Grafana image with my data source, my dashboard, and my user account all set up. I can use that image in my compose file, and whenever I deploy the stack I'll have the dashboard ready to go, but committing images isn't a very good approach. It means more manual work every time I want to do an update. So when the Grafana team release a new version, or they update the underlying operating system image, I need to run a container from the new image, manually set everything up, and then commit it again. It's much better to automate all that with a Dockerfile. I've got a few supporting files alongside my Grafana Dockerfile, and these are all about automating the setup of my custom image. The Dockerfile starts from the Grafana team's image, and then I copy in this datasource-prometheus YAML file into the data source provisioning directory, copy the dashboard provider, YAML, into the dashboard provisioning directory, and the ps-monitoring-dashboard. json file into the dashboard directory. I'll go through those files in a moment. These are special directories. When Grafana starts up it looks for files in the provisioning folder and will apply any configuration that it finds in there. My dashboard JSON file is just the file I exported after I set up my dashboard. Then this provider file tells Grafana to configure any dashboards it finds in that directory, which is where I copy the JSON file. My dashboard needs a Prometheus data source, which is configured here. This is just the same information to connect Grafana to Prometheus that I set up in the UI. Those standard provisioning files all get set up by Grafana automatically, so when the container runs it will have my data source and my dashboard already provisioned. There's no similar way to provision user accounts, so instead I'll use the Grafana API, and I have a shell script here that does just that. This copies in the script and runs it when I've built the image. The shell script is pretty simple. It starts the Grafana server, and then it makes CURL requests to the Grafana REST API to create a new user, and then to set the homepage for the user to be dashboard ID 1, which is my custom dashboard. For Windows users, I have a Windows dockerfile, which does exactly the same setup. I start from that sample Grafana image, then copy in the provisioning files to the same locations. I have one set of provisioning files and one dashboard that I can use for Linux and Windows images. Only the init script is different, because the Windows version uses PowerShell. Logically, the script does the same things; starting Grafana in the background, then calling the REST API to create a new user, and set their homepage to the application dashboard. The PowerShell version is more wordy, but then it's easier to read than the CURL commands. That's it. When I run Docker image build on either of those Dockerfiles, I'll end up with a fully provisioned Grafana image with my dashboard and my user account ready to go. The last thing to show you is my final Docker compose setup. The core compose file has the Java and. NET apps, my Prometheus image with the default configuration, and my Grafana image with the dashboard set up. In the production override I have constraints to use Linux or Windows nodes for the application services. Then I have the production config for Prometheus and the port publishing for Grafana. The Mac override file is for Linux developers, where I just replace the. NET image, so the whole stack starts, but there is no. NET app. And for Windows developers, I replace the Java image, and I'm using the Windows versions of the Prometheus and Grafana images. The build section here means devs can just run docker-compose build to package up all the Windows images. And that's my whole monitoring solution that I demoed back in the first module using custom images based on Prometheus and Grafana with my own configuration applied and Docker Compose files which let me run the same monitoring stack in every environment from production to development.

Module Summary
That's as far as I'll take Grafana in this course. You've seen where it fits in the monitoring architecture for a containerized application, running in a container in the same Docker network as Prometheus and the application containers. The approach I've used is to run a custom deployment of Grafana packaged with my application dashboard and a read-only user account, so you can deploy the whole stack from Docker images and be up and running with the dashboard with no manual steps. You've seen that Grafana is a powerful visualization server which connects to multiple data sources to show metrics in different ways. Grafana respects the connection method and query language of the data source, so you don't need to learn a whole new language, but it does provide custom functions, so you can standardize how you execute queries. I showed you how to run Grafana in Docker using the Grafana team's Linux image, and how to package Grafana in a custom Windows Docker image. Then I built out the dashboard by adding panels, connecting them to Prometheus, and pasting in the PromQL queries that I've worked through in other modules. Lastly, you saw how to package a custom Grafana deployment into your own Docker image, so when you run a container it already has your data source configured, your dashboard deployed, and a read-only user set up. The dashboard is the final part of monitoring containerized applications, so now you should be ready to go and add metrics to your applications and deploy confidently to production, but before you go, I'll end the course by recapping all the key points and recommending how to get started adding monitoring your applications.

Course summary
Thanks for joining me on this Pluralsight course, and I hope I've covered monitoring in enough detail for you to get started. I've covered the architecture of monitoring containers, and how it's different from server monitoring, and I've walked through Prometheus and Grafana in enough detail for you to see what they do and how you can use them. The rest of the course focused on collecting the metrics that you'll want to see in your dashboard. The highest value metrics are also the ones which will take the most effort, so I'll finish up here with some recommendations on what to do next if you already have your app running in containers, and you want to add monitoring. The Docker platform metrics are the easiest to bring into your solution. You need to run Prometheus and Grafana in containers, and you need to set up your Docker servers to export metrics, but those are all simple, low risk tasks. That gives you basic information on container states and failed health checks, and the nodes in your cluster, which you can deploy as your Phase 1 monitoring without deploying any changes to your application containers. Next up is runtime metrics, which should still be straightforward if there's a Prometheus exporter for your application runtime. I've shown you community examples for Tomcat and IIS which give you key information about the web server and about the compute resources being used by the JVM and. NET inside your containers. You'll need to set up exporters in your Dockerfile to get those runtime metrics out of your containers, but you shouldn't need to change code. So this is a fairly straightforward, low risk piece of work, which adds a lot of value because it shows you how hard your containers are working, as well as top level stats like HTTP response status for websites. That can be your Phase 2 release, which you may tie in with the next planned application release. The last type of metrics are custom application metrics, which mean adding code to your apps. You'll need to get familiar with the Prometheus client library for your language and do some analysis to figure out the best types of things to instrument. I gave some guidance on that in the module. This part is the most work, and it means changing code, so it's also the most risk, but it gives you the chance to capture really useful metrics, which can give insight into your app across different stakeholders, from business users and IT ops to developers. So if you can find the time and the budget to do it, this is going to be the most beneficial to do as Phase 3, by which time you'll have all the stats that you've seen in this course. And with that, it's the end of the course. I hope you found it useful. Please do leave a rating and feel free to get in touch if you have any feedback. I've got stacks of other Pluralsight courses you may find helpful, and I record more every year. My name's Elton and thanks again for joining me for Monitoring Containerized Application Health with Docker.
