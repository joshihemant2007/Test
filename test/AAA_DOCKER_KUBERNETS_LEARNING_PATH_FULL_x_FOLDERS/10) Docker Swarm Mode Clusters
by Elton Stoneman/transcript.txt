Docker swarm mode is a production-grade container orchestrator with built-in features for load-balancing and scaling your applications. In this course, Managing Load Balancing and Scale in Docker Swarm Mode Clusters, you'll learn how to deploy and manage applications in swarm mode for high availability, high performance, and easy scale. First, you'll learn how load balancing and service discovery works in swarm mode. Then you'll learn how to scale your services and your swarm - with Linux and Windows nodes. Finally, you'll learn how to run multiple applications and maximize the use of your cluster, and how swarm mode supports production maintenance and deployment. When youâ€™re finished with this course, you will have the skills and knowledge to run performance reliable apps in production with Docker swarm mode.

Course Overview
Course Overview
Hey, how are you doing? My name is Elton and this is Managing Load Balancing and Scale in Docker Swarm Mode Clusters. I've always found swarm mode to be a powerful container orchestrator that's easy to use, and I've been running it in production since it was released in 2016. If you're ready to take your containerized apps to production, but you need to understand how the orchestrator helps you with load balancing, scale, updates, and maintenance, then this course is for you. Over the next 2 hours I'll show you how a service discovery and load balancing works in swarm mode and the configuration options that Docker gives you. I'll show you how to scale up your services and your swarm and how to get the most out of it by running multiple apps and fronting them with a reverse proxy running in a container. I'll show you how to take nodes out of the swarm safely for maintenance and how you can configure your services with rolling updates and automatic rollbacks, which makes your deployment fast and reliable. And my demo solution uses a mixture of Windows and Linux containers, so I'll be showing you how all that works with a hybrid swarm, made up with a mixture of Linux and Windows nodes. By the end of the course, you'll understand just how powerful swarm mode is and you'll be comfortable moving your Dockerized apps into production knowing that you can scale and manage them easily.

Understanding Load Balancing and Service Discovery
Introducing Load Balancing and Scale in Swarm Mode
Hey, how are you doing? I'm Elton and welcome to Managing Load Balancing and Scale in Docker Swarm Mode Clusters. Wow, that's a long title but we're going to cover a lot of ground here. This course is going to give you a thorough understanding of how to run scalable, highly available containerized apps in production using Docker. Here's what this course is about. It's all about helping you get the most from Docker swarm mode, which is the clustering technology built into Docker. Docker running in swarm mode is a production-grade container orchestrator, it's how you run containers across multiple servers and get service discovery, scale, redundancy and self-healing from the platform. Docker Swarm mode is one orchestrator in a landscape that includes Kubernetes, Mesos and Service Fabric. Swarm mode has a couple of great advantages - it's built into Docker, so it's super easy to set up. It uses the simple Docker Compose syntax to define deployments, so you use the same manifests in all your environments. And the components of swarm mode are all open-source, but it's the same platform you get with Docker Enterprise Edition, so there's consistency across all your environments, right up to production. You're going to learn a lot in this course, but there's a few things you should know before we get started. I'll be using Docker on Linux and Windows, and I'll work with containers, images and the Docker command line. I won't go into detail on any of that, so if some of these terms are new to you, you should check out Nigel Poulton's Pluralsight course Getting Started with Docker. This is what I will cover. In this module I'll be talking about load balancing and service discovery - how external traffic gets into the swarm and to your containers, and how containers can reach each other internally within the swarm. The next module covers scalability. I'll add Linux and Windows servers to my cluster and show you how load balancing and service discovery works in a hybrid swarm. You'll see how to scale the up the nodes in your cluster and the services that run your application. The next module is about making full use of your cluster - showing you how to run multiple apps on one swarm. I'll cover request routing, to get traffic to the right applications, and I'll also talk about stateful applications and how you can manage storage at scale. The last module is about running in production. I'll show you how application updates work at scale in a load-balanced swarm, and how the platform supports maintenance. Docker gives you the tools to get zero-downtime for your application updates, operating system patches, and even server upgrades. I'll be using a simple app for the demos throughout the course, which is just a web front end that talks to an API. I'll be adding capabilities and extending the deployment in each module to show what you can do with swarm mode. That sounds good to me. If you're happy running containers on a single box now, by the end of this course you'll be ready for prime time, running on multiple servers in the datacenter or any cloud. I'll get started right now, and look what Docker swarm mode offers for load balancing.

External Load Balancing: Ingress and Host Mode
Your swarm mode cluster is made up of several servers running Docker, and your applications run in containers on the servers. Load balancing is about making sure incoming requests get to the right container, and that the load is shared between containers to get maximum use of the cluster. Swarm mode does that by binding container ports to the cluster, and it gives you two options for doing it. The first is the simplest, it's called host-mode publishing, and it binds a container port to a server port. Incoming server traffic gets directed straight to the container to handle. That's simple, but limited. Your cluster could be fronted by a load balancer which directs traffic to any node, so it could send traffic to a node which doesn't have a container running, so there's nothing to handle the traffic and the user gets an error. You can get around that by running a container on every node in the cluster, but that container has exclusive use of the server port, so you can't scale up and run multiple instances of the container on each server. There are uses for host mode publishing, but swarm has a much more flexible option - the ingress network. Ingress networking is the default in swarm mode. It works in a similar way, but the container port is published to the whole swarm, and every node listens on that port, whether its running a container or not. Incoming traffic can reach any server. If the server isn't running a container listening on that port, the swarm silently redirects the request to a node which does have a container listening, and it sends back the container's response from the original server. That means you can use a simple external load balancer that directs traffic to any node, and the swarm will intelligently handle it. It also means you can run multiple containers for your app on each server, because the containers don't exclusively lock that server port. So if your app is built to scale horizontally, you can just run as many containers as your cluster can handle, and Docker will share the traffic between them. That's the basics of load balancing in swarm mode. Next I'll show you how it looks with the default ingress network, and then I'll go on and look at host mode publishing.

Swarm Services with Ingress Networking
Here's the Docker compose file for my demo app. There are two services - an API and a web application, and they both plug into the same Docker network. In this file I've just got the core definition which applies to every environment, and I have override files for extra settings that apply to different environments. I've taken the build settings out into a separate file, because I only need them when I build the images, and this keeps my compose files clean. The local override file is for running the app on a single machine, so developers would use this on their dev box. The web app listens on port 80, but devs want to keep that free so the local override publishes to port 8085 on the host. Version 1 of the web app consumes the API, and the URL is configured with this environment variable. The API isn't publicly available - but devs may want to work with it directly, so in dev the API port 5001 also gets published. I'll run this on a single Docker setup just to show you the app. I'm using Docker for Mac, which has compose installed, and I just run docker-compose, pass in the two files - docker-compose. yml and the local override file, and run up -d. Compose starts the containers in the right order. I can open the browser and point to localhost on port 8085 and here's my demo app. It's not exciting, but it's got enough going for it that we shouldn't get bored looking at it during this course. It's a password generator, and the web app makes three calls to the API, which generates random passwords. The web app shows the hostname of the server that responded here, and it also shows the hostname of the API server that generated each password. Both components are running in Docker so these server names are really container IDs, and I just have one of each container, so I can refresh as often as I like and I'll always get the same IDs. The API port is published, so I can curl localhost on port 5001 and fetch the password resource. This is the raw JSON response from the API container - and, of course, it's the same server name, which is container id d51. Compose also created my application network, and if I look at the details I see this is a bridge network, which is the default for new networks when you're running on a single Docker server in Linux. Now, I also have a simple swarm running in virtual machines on my laptop, which is my test environment. This compose file is the override for that test environment. It's very simple, it just specifies different port publishing, so in test the web is published on port 80 and the API isn't published at all, it's an internal service only accessible to other containers. The URL where the web app reaches the API is the service name. In this terminal session I'm connected to the swarm. My swarm is secured with TLS certificates, so these settings point my local Docker client to the certificates and the host for the swarm manager, and tell Docker that this connection needs TLS. In swarm mode you deploy your application as a stack, which is a set of services, that get distributed around the swarm as containers. I can use the compose files to define my stack, but unlike the docker-compose, the stack doesn't support override files, it needs just one single file to deploy. So I'll use Docker Compose to combine the basic compose file and the test override file. The config command joins the files together and validates the combined content, and then writes it out. So I'll capture that in a stack file. This is a command you can easily add into an automated deployment, and it's a nice way of using override files to keep your manifests clean and reusable, but still deploy to a swarm. Here's the combined output which I'll deploy as a stack. The deployment is with docker stack deploy, passing the path of the compose file with the full manifest, and giving the stack a name. That creates the services, and my swarm just has one manager node and one worker node - and the containers could be running on either of those. I'll dig deeper into this setup in a moment, but for now I'll just connect to the test URL and check that this is working. Here's my web app, and on the password page I get the responses from the API. My app is working in swarm mode with no extra configuration, and next I'll have a close look at what's running and show you the difference between ingress mode and host mode.

Swarm Services with Host Mode Networking
So my app works, but where is the traffic going? The port spec in the test override file just exposes port 80, so Docker's using the default ingress network, and it takes care of routing incoming traffic to the right node. The test URL I'm using is swarm-m1-0 which is just an entry in my hosts file for one of the swarm VMs. So when I browse to m1-0, I'm hitting the IP that ends 101, which happens to be the swarm manager. I can browse to the worker node, which is swarm-m1-1, and I get the application response from the same set of containers. The other default in swarm mode is to run each service in just one container. Docker service ls shows me that, each of my services has a replica level of one out of one, which means the desired number of containers running the service is one, and we have one container running. It doesn't matter whether the container is actually running on node 0 or node 1, I can hit either server and swarm's ingress network routes the traffic to the container. The web app is always served by one container and the API responses from the other container. Just remember these host names for a second - the web is served from 917 and the API is served from 586. The other networking option that I talked about is host mode publishing, and I'll switch to that to show you how it's different. That's defined in the mode property of the publishing spec, and in the test override I need to use the expanded port definition, setting the mode to host. Published is the host port to listen on, which is port 80, and target is the container port to send traffic to, which is also port 80 for this app. Now the website uses host mode publishing, so I'm bypassing the ingress network and the web container binds directly to the port of the server where it's running. I'll need a new deployment manifest to update the application, so I'll repeat my docker-compose config command to generate a new stack file, and this one has the host mode publishing settings. And now I update the app just by repeating the stack deploy command. Swarm mode works like Docker compose when you deploy a stack, and the swarm compares the running services to the stack definition and makes any changes to get to the desired state. Now if I look at the service list, both are replicated with one container. Docker service ps shows me those containers, and for the web app there's one container running and one which is shutdown. The shutdown container is the original deployment, which has been replaced, and the running container is on node m1-0. I'll refresh the browser pointing to node m1-1, which isn't running the web container, and the app is no longer working. There's nothing listening on port 80 on this node to serve the app, and I'm bypassing the ingress network so Docker doesn't do anything clever with the traffic. It sends the request directly to port 80 on node m1-1, there's nothing listening, so the client gets an error. If I browse to node m1-0, that is running the web container and I do see the website. The web app is still connected to the API container, so I can see those generated passwords. If you've been paying extra special attention, you'll see something else too - this web app's being served from a different hostname - this is fb9 instead of 917, which is the new container I got when I changed the service definition and deployed. But it's the same hostname in the API - 586, like it was before. The API definition hasn't changed in the new stack deployment, so Docker has left the original container running. Changing the publishing mode affects traffic coming into the cluster, but doesn't affect how traffic is routed between containers, and I'll I'll talk about that next.

Internal Load Balancing and Service Discovery
So the publishing mode configures how Docker handles incoming traffic. Ingress mode is the default, and it means the swarm takes care of load balancing. Traffic can come into any node, and Docker will route it to a container - that could be on the server that got the request, or any other node in the swarm. The alternative is host mode, which you use by specifying the port spec. With host mode, Docker doesn't route between nodes on the swarm. It will send traffic into the container if there is one listening on the published port, but if there's no container running on the node that receives the traffic, the request will fail. Host mode publishing means you need to take care of load balancing - you need an external load balancer which does its own healthchecks, so it can be sure to send traffic to nodes where there are containers listening on the right port. Host mode publishing moves those concerns to your infrastructure, but there are some scenarios where it's useful, and you'll see that later in the course. That's how external traffic routing works in swarm mode. You have a couple of options for routing traffic between containers too. In my application manifest, the API service doesn't publish any ports because it's just an internal service, used by the web app but not by any external consumers. Two containers attached to the same network can access any ports the other container exposes. You don't need to publish ports for internal services, which is a very simple way of securing different parts of your app. My API here is only visible within the Docker platform to containers which are attached to the same Docker network. How do containers find each other? Well service discovery is built into the platform, and it works in the same way in swarm mode as in standalone mode. Containers refer to each other by the name defined in the compose file. Docker has a DNS server built in. Containers query that with a container name, and Docker returns the container's IP address to use. In swarm mode there are two options for configuring how the container's IP address gets resolved, which changes how networking works when you run containers at scale. I'll talk about those options next, and then show you how they work.

Service Discovery: VIP and DNSRR
Here's a simplified picture of my app running at scale, where I have one web container and three API containers. The web app is configured to find the API using the DNS name password-api. That's the name of the service in swarm mode, but there are three containers running the service, and ultimately the web container just needs an IP address for one of the API containers. The two options for service discovery are VIP and DNS round-robin. VIP is the default, which makes use of the IPVS load balancing feature, which is part of the Linux kernel. VIP discovery means each service gets a virtual IP address. Docker's DNS resolution always returns the same virtual IP address, and the consumer uses IPVS to load balance between the containers in that virtual IP. The other option is DNS round-robin, DNSRR for short. DNSRR means each container in a service has its own IP address, and the DNS resolution does the load balancing. It round-robins between the containers in the service, so each DNS query will get a different response. At the network level, DNSRR is layer 7 routing, and VIP is layer 3/4 routing. VIP is more efficient in a couple of ways. It's more performant at the networking level, because it runs at a lower level in the networking stack and uses kernel-mode load balancing. It can be more performant at the application level too. Most application stacks do DNS caching, to save on lookups to the DNS server. That's fine in VIP mode because there's only one service IP anyway, so the app can cache that as long as it likes. But in DNSRR, the app might cache just one of many container IP addresses. At best that skews the load balancing, and at worst the app could fail. Docker swarm takes care of service levels, so if I lose a node in my cluster and that takes down one of the API containers, Docker will start a replacement on another node. In VIP mode, the service IP doesn't change, the old container is no longer part of the service, so it won't get any traffic, and the new container is part of the service, so it will get its share. In DNS round robin mode, the web app may have cached the IP address of the old container. It will try and connect directly to that container and fail, and it will keep failing until the DNS cache in the app container expires, when it will call back to Docker's DNS server and get a new container IP address, which it might cache for an unsafe amount of time.

Swarm Services with VIP and DNSRR Discovery
I'll show you how a service discovery works next, and then we'll be ready to wrap this module. My demo app is running with one container in each service. I want to show you the service discovery options, so for that I need multiple containers. Later in the course I'll cover scale in more detail, but for now I'll just run docker service update and set the replica count to 3 for my password-api service. The output here shows the containers starting up, and Service converged just means the service now has the desired state. Docker has created two new containers for that service, which could be on any node in the swarm, and as I'm using the default VIP mode, they will all be using the same virtual IP address for the service. The service task list shows me the three containers are all running, so they're all available for the load balancer. Two are running on node 1, and one is running on node 0. Here in the web app, I'll refresh a few times and we'll see the API responses come from different containers. The web container is resolving the service VIP from the service name, and then Docker is routing requests between the API containers. The original 586 container from the previous demos is there, and the two new containers are equally sharing the traffic. I can see how that looks inside the service by listing the containers running on this node, and using docker container exec to connect to an interactive shell session on one of the containers. My container image has ping installed and when I ping the API service, I get a single IP address back - which is the virtual IP that spans the three containers. My image doesn't have extra tools installed, so I can't use dig to query the DNS server, and I'll need to use a different approach to look closer. I skipped over this in the compose file, but the test override specifies that the application network is attachable, which means you can run containers and have them join the network, even though they're not part of a swarm service. That's set up in the test environment specifically for debugging scenarios like this. Docker swarm created the network when I deployed the stack, and you can see it's an overlay network. Overlay is the default driver in swarm mode, and it means containers can reach each other from any node in the swarm. I'm going to run a container in interactive mode, connecting to the application network, and the image I'm going to use is sixeyed/Ubuntu-with-utils. This is one of my public images on Docker Hub. It's based on Ubuntu and has useful tools like curl, ping and dig installed. This new container is connected to the app network, so I can run dig password-api. The DNS response comes from Docker and it's the virtual IP address of the service. I can repeat this as many times as I like and it's always going to give the same response. I can also use curl to make an API call, and if I use verbose mode it will tell me the IP address of the responding server. The URL is http://password-api port 5001/password, and I'll get a JSON response. This is from IP address 10. 0. 0. 4, and the hostname is c-e-f. When I repeat I'll get the same IP address, which is the service VIP, but a different hostname, 1f5, because this request is routed to a different container. And again, because the VIP is being load balanced across all three containers, now I get a response from the original 586 container. That's VIP service discovery, which works really well, and now I'll show you how DNSRR is different. The type of service discovery is specified in the compose file, and I'll add a section to my test override file for the password-api so I can configure that to use DNSRR. You do that in the deploy spec with the endpoint-mode property which I'll set to dnsrr instead of the default - vip. To deploy that change I'll repeat my docker-compose config command to rebuild the stack file, and check that the stack file now has the new definition for the API - using DNS round-robin mode. Now I'll repeat the docker stack deploy again, and that will recreate the API service with a single replica, so I'll manually scale up to 3 again. For users of the web app, the behavior is pretty much the same. I'll refresh and the password suggestions come from different containers, but not quite so evenly spread as with VIP mode. Sometimes successive requests are routed to the same container, whereas VIP mode balanced the requests equally across them all. Service discovery is DNS round robin now. I'll run the utility container again to see how it's different. When I run dig I get a DNS response with all three container's IP addresses, 3 4 and 5. Some DNS client libraries will understand that and do their own load balancing, but if not they'll just take the first response. So Docker randomizes the response order - here 10. 0. 0. 3 is first, now 5 is first, 5 again and now 4. Service discovery works in the same way for the consumer, they just query the DNS name and get the response back. But for high-throughput services, the consumer's DNS cache can have a big impact on load balancing, making the distribution less uniform with DNSRR than with VIP. That's all I wanted to show you for this module, so I'll wrap up now and let you know what's coming next.

Module Summary
In this module I've walked through load balancing and service discovery in swarm mode. You've seen how ingress and host mode publishing affect external traffic coming into the swarm. You use the port specification in the compose file to switch between ingress and host mode. Host mode just means the swarm publishes the container's port to the server port. You can't scale up to run multiple instances of the container on each server, because the port is exclusively used by one container. And if you're running fewer containers than there are servers in the cluster, you need an external load balancer configured with health checks, so it doesn't route traffic to a server which isn't running a container, otherwise Docker doesn't do anything with the request and it fails. The alternative is the default. Ingress mode means all nodes in the swarm listen on the published port, and Docker load balances between the containers running the service. That means you can run multiple instance of the container on each server, to scale horizontally. And if you run fewer containers than there are servers, the swarm will route traffic intelligently to nodes which are running the containers. You can have a basic external load balancer which sends traffic to any node and let Docker take care of managing it. That's load balancing for external traffic. Internal traffic between containers is also load-balanced in swarm mode, and that's configured with the service discovery endpoint mode. The default is VIP which means the service gets a virtual IP address, which is returned by the DNS server running in the swarm. When containers access the service VIP, it's IPVS in Linux, which takes care of load-balancing and sends the traffic to one container. VIP is the default, and the alternative is DNS round-robin. In DNSRR mode the service doesn't have its own virtual IP address, instead the DNS server does the load balancing by returning one of the container's IP addresses, using a round-robin approach. Now you should have a good understanding of load balancing and service discovery, both inside and outside of the cluster. I haven't talked yet about why you would choose between ingress and host mode publishing, or VIP and DNSRR service discovery, but I will cover that as we go through the course. The next module is all about scale. I'll show you how to scale your cluster by adding more nodes, how to scale your services to run on many containers, and how you can use service configuration to work with load balancing. I'll also add Windows nodes to my cluster and show you how service discovery works with a new version of the demo app that mixes Linux and Windows containers. So thanks for watching this module, and join me for Scaling Services in Swarm Mode, the next module in Managing Load Balancing and Scale in Docker Swarm Mode Clusters.

Scaling Services and Nodes in Swarm Mode
Introducing Scale for Services and Nodes
Hey, how are you doing? I'm Elton and this is Scaling Services and Nodes in Swarm Mode, the next module in Managing Load Balancing and Scale in Docker Swarm Mode Clusters. Now you understand how to manage the traffic getting into the services running on your swarm, and how to manage traffic between services, this module is all about scale. I'll cover the different options for scaling services in swarm mode, that give you high availability for your applications, and high throughput. Scale, load balancing and service discovery are linked together, and I'll show you how the options work in different combinations. Scaling services lets you get the most out of your compute power, but when demand increases you'll eventually need to scale out the cluster. I'll cover that by adding nodes. I'll join Windows and Linux worker nodes to my cluster, which gives me a hybrid swarm that can run a mixture of Linux and Windows containers. Then I'll show you what happens when you have services running at scale and you add more nodes. I'll be using an evolution of my demo app to show off these features, so before I get started, I'll walk through how the app has changed.

Evolving the Demo App Using Windows Containers
The original version of my app had a website which talked to an API, all running on Linux containers. For version 2, the devs want to make the API publicly available, and they're also using a new password generation algorithm. That algorithm apparently uses a core feature of Windows, so the API needs to run on Windows Server. That's fine because I can join Windows nodes to the cluster and run the API in Windows containers. The networking model in swarm mode is overlay networking, where you have a Docker network that spans all the cluster nodes. Any containers attached to that network can reach any other containers using the same service discovery methods that we've already looked at. I want to take advantage of that to make this update in two stages. First I'll deploy version 2 of the API on Windows containers alongside version 1 of the web application, running on Linux containers. The API will still be private at this point, and we can check we're happy with the new API features. Then I'll go ahead and make the API public and release version 2 of the web app which uses the public API. Windows doesn't have the same rich networking stack as Linux does at the moment, so there are some restrictions on Windows containers that don't apply to Linux containers. Those restrictions are all around port publishing and service discovery, so the hybrid app is a good way to demo all the options for load balancing and scale in swarm mode. I'll start in the next demo by updating my password API to use the new Windows version.

Adding Windows Nodes to the Swarm
I've switched to a new demo environment here running in the cloud, because there's a limit to how many VMs I can run on my laptop. I have a single manager running on Linux, two Linux worker nodes and two Windows worker nodes. This swarm is running on Azure and I've used the Docker EE deployment from Azure marketplace to set it up because it's Tabular really easy way to deploy a hybrid swarm, but to build your own hybrid swarm, you just need Docker installed on all the nodes, initialize the swarm on the manager, and then get the join token to run on the other nodes to join the swarm. The command is the same for Linux, Windows, x64, ARM and IBM mainframes so you could build a very hybrid swarm if you need to. I'm calling this my pre-prod environment, and I have an override file with the service configuration. The password web service has a deployment section here, with a placement constraint. That tells Docker I need to run this service on Linux nodes, because it's a Linux image which needs to run in a Linux container. The swarm can work that out for itself, but if you add the constraint then it saves a little bit of time at service creation. Plus your compose file is really your deployment document, and it's good to make it explicit, so I'm also specifying ingress mode for the port publishing, even though that's the default. The password API is set up to run on the Windows nodes, also using a placement constraint, and I'm using DNSRR for the endpoint mode. You know what that means from earlier in the course, but I'm not making that as a deliberate choice. Remember service discovery uses VIP as the default and that uses the IPVS feature of Linux, and Windows Server doesn't currently have its own version of IPVS. So VIP is out for Windows containers, and you have to use DNSRR. There's no port publishing yet, because I'm going to keep the API private for the moment, but the web containers can still reach the API containers by the swarm service name, even though they're running on a different operating system and the ports aren't published. Back in the main compose file, I'm using version 2 of the API image, which is the Windows version. I'll use Docker-compose again, passing the base compose file and the override file for my pre-prod environment. Then I'll use the config command as before to build my stack file, and just print out the file to check it has all my deployment sections. This is my first deployment to pre-prod, so when I run docker stack deploy, Docker will create the network and the services, deploying one web container on a Linux node and one API container on a Windows node. Docker service ls shows me those services are fully replicated, these UCP agent services are part of the Docker Enterprise Edition deployment. This is the public DNS name for the external load balancer over my Linux worker nodes. When I browse I see the same good old demo app, and in the password page I get the v2 passwords. The UX is the same although you might see the passwords are a bit more complex and the API is now version 2. This API hostname is from the Windows server container, there's only one right now, and the web hostname is from the Linux container. The load balancer could send this request to any of the Linux servers, and it will always be routed by the ingress network to this container, which will always be routed by DNSRR to the one Windows API container. So the app is looking good, but this module is all about scale, so next I'll look at my options for scaling the services.

Scaling up Windows and Linux Containers
You define scale in the compose file using the replicas property, and it's easy to scale both parts of the app at the moment. The web service is using ingress networking, so I can run more containers than there are Linux nodes. A replica level of 4 gives me 4 containers across the nodes. Swarm mode spreads containers around the nodes by default, so each Linux node should have one web container, and some will have two. In pre-prod I have that load balancer over the Linux servers, so I can hit any server and let the ingress network take care of routing the request. I can overprovision the API too, running three containers across my two Windows nodes. This is still an internal service right now, and service discovery will use DNSRR to balance between the containers running on the Windows servers. I'll deploy as usual by repeating the docker compose config command, and use cat to check the new config is all in the stack file. Now I'll run docker stack deploy to push those changes to the swarm. Scaling up is pretty fast so when I check the services - yep, these are running at full scale already. Now when I browse to the Linux load balancer, I see different hostnames here, because the API requests are being load-balanced between the Windows containers. This is DNS round robin, so how effective the load balancing is depends on the DNS caching inside the web container. If I refresh then I can also hit a different web container, but not every time. Between my browser and the web container that serves the request, there's a combination of browser keep-alive settings, the external load balancer and the ingress network deciding the route. But this looks fine, so I'll update the deployment and make the API public. Version 2 of the web app uses the public API, so I'll update the image version ion the core compose file. In the pre-prod override, I'll add a port spec to the API service. I'll need to use host mode publishing here because Windows currently doesn't support ingress networking, although it's likely to have support in the next update to Windows Server 2016. I'd like to publish to standard http port 80, but I can't because ports are swarm wide and 80 is already being used by the web app. So I'll publish port 8080 and that needs to target the API port 5001 inside the containers. Host mode publishing is a problem, because I have three replicas specified but only two Windows nodes. But I'll leave that as it is and show you just what happens. Version 2 needs to use a different URL for the API. In the new version is the client's browser makes the API call and the website uses JavaScript to render the response. The URL is the external load balancer across the Windows nodes in my cluster, using port 8080 will send API requests into the Windows containers. Now the same deployment steps as before, recreating the stack file with docker compose and checking that the new spec is there ready to go. Docker stack deploy will update both services, because the service definitions have both changed. The service list shows me the update is happening, the web service is on version 2 and it's rolling out. Three replicas are up out of the desired four. I'll come to rolling updates later in the course. The API service is also rolling out an update, currently at 2 out of 3. I'm going to skip forward a few moments here and look again at the services to check the deployment status. The web service is fully deployed now, 4 out of 4 replicas are up. The API service is still not fully deployed, only 2 out of the 3 tasks are running. Why has that happened? I'll tell you that next and show you what to do about it, but I'll end this demo just by checking the application is working. So I'll refresh the browser, and the UX is just the same. But now the browser is fetching the web page from one of the Linux containers, and the API responses direct from the Windows containers. Even with an incomplete deployment, the swarm is keeping my app running correctly.

Understanding Service Modes: Replicated and Global
I've scaled up my application by running multiple replicas of the services. That's specified in the compose file with the replica property of the deploy section. This only applies in swarm mode, if you run this with Docker compose on a standalone Docker server, it will ignore this section. I've overprovisioned the web service, running four containers across three nodes. That's fine because I have ingress networking, so the swarm routes traffic between the containers. I could scale up to a dozen replicas, or down to two replicas and the app would work in the same way, it would just support different levels of traffic. The API service is a different story. I've over-provisioned again with three replicas, but when I deployed I only got two. That's because of the host mode networking. Remember from earlier in the course that host mode publishing means the swarm publishes the container port to the server port. That's an exclusive use of the port, so no other process, and no other containers, can listen on the same port. Effectively that's an implicit constraint for the service, tasks can only be created on nodes that have port 80 free. My two Windows nodes meet that constraint, but when the first two tasks are created there are no more nodes that do meet the constraint. The third task goes into the pending state, waiting for a node to be available, which is running Windows and has port 80 free. Unless I add more Windows nodes to the cluster, the third task will stay pending indefinitely. That's a limitation in host mode publishing, which will stay as a limitation until Windows supports ingress networking, which should be coming soon. Check my blog at sixeyed. com for updates on that, and other posts about Docker and Windows. Until then, there's an alternative way to get failover and scale for Windows containers, which I'll cover next.

Replicated Services with Ingress Networking
Replicated services are a perfect match for ingress networking. You can run as many tasks as you need to meet your expected load, and the swarm load balances for you. You can scale up or down just by changing the replica count, and even if servers go offline, the swarm maintains the service level. We've seen that doesn't work for host mode networking. You can't overprovision services to run multiple tasks on each node for scale, but you can configure the service differently to get exactly one task on each node, which does give you high availability. Services are deployed as tasks on the swarm, which is just another name for containers. By default the service uses replication mode, which means you define the service level independent of the cluster size. You specify the number of tasks to run, and the swarm will maintain that service level, provided that there are enough resources in the swarm. The alternative is a global service where the service level is linked to the size of the cluster. With global services, the swarm will run exactly one task on each node. That's a perfect match for host mode publishing. So I can get failover for my API using Windows containers by making it a global service. That also gives me scale, because I'll have many containers serving requests, but the scale is limited by the number of servers. To get more scale with a global service, I need to add more nodes to the cluster. I'll switch to global mode next and show you how it works.

Global Services with Host Mode Networking
Global services are specified in the mode property of the deploy section. Global mode means the replica count is ignored, so I'll take that out of the API definition to keep my specification clear. As before, I'll generate a new stack file and check the changes. The API is global now, so I can deploy this with stack deploy. Except that I can't. The service mode is a core property which you can't change with a rolling update. You need to remove the existing stack and redeploy it. On the plus side, that's simple because it's just docker stack rm to remove the stack, although Docker won't remove the application network if there are containers using it, so if it takes a few seconds for your containers to shut down, you'll need to repeat the rm command to remove the network, and then the deploy command to redeploy the whole thing. The beauty of Docker means this will be the same stack running, but there is downtime when you remove and recreate a stack, so it's good job I'm walking through the options in pre-prod before I go to production. I'll check the service list now and this is what I want. Four out of four tasks for the web service, and two out of two tasks for the api service. The swarm works out the desired number of API tasks is two, because there are two nodes in the swarm that meet the placement criteria. So even though this is a global service, the placement constraints still apply, so this is only global across Windows nodes. I'll switch back to the browser and refresh a few times. This is all working, but now both the API and the web app are dependent the browser's keep-alive settings, and the cloud load balancer, as well as Docker's service discovery, so load-balancing is not so spread out. The web requests are sent to any VM in the Linux load balancer, and then the swarm routes the traffic to one of the Linux containers. The API requests are send to any VM in the Windows load balancer, and the container on that node responds directly. One big advantage of host mode publishing with global services is that they automatically scale when the cluster resizes. I'm going to scale up my pre-prod swarm and add another windows node and another Linux node too. I'll come back when they're ready. My servers are read and the node list shows me I now have one Linux manager, three Linux workers and three Windows workers. The service list shows me the API service is now running 3 out of 3 tasks. So when the new Windows node got added, the swarm realized the global service should now run three tasks and ran another API container on the new Windows node. The web service is still running 4 out of 4 tasks, because that's the replica count that I specified. And if I look at the tasks, I'll see they're on the original 3 servers, the swarm manager, and worker nodes 01 and 02. The new Linux node 03 isn't doing any work, because replicated services don't automatically re-balance when a node gets added. If I want to rebalance the service, I need to do a new deployment to change the replica count. So I'll change the replica count to 6, and I'll also add another placement constraint. I have plenty of capacity in the swarm, so I can keep my application workloads off the manager and keep it free for management duties. This constraint says the service should only have replica tasks running on worker nodes. I'll deploy that new configuration to pre-prod by first building the stack file and checking the output has the new settings. Now it's docker stack deploy to make the change, and if I'm quick enough we'll see the rollout happening, so now there are 4 web replicas out of the desired 6. A fews seconds later and the service is fully replicated. When I check the service tasks, we see they are running across the three worker nodes, but it is a bit difficult to see with all the shutdown containers listed in the output. The servcie ps command has a filter option and I can set that to only show me replicas where the desired state is running, which shows any live and pending containers. And here we see the service is balanced with two containers on node 01, two on node 02 and two on node 03. So my new server is sharing the workload and the manager is free of application containers. The new version of the app is running with high availability and scale across multiple Linux and Windows nodes in my hybrid swarm. That's all the excitement for this module, so I'll wrap up now and tell you what's coming next.

Module Summary
This module was all about scaling services and nodes in your swarm cluster. Scaling nodes is super easy, you just need servers or VMs running Docker in the same network as your existing swarm and make sure the nodes can reach each other on the swarm ports. Then you just get the worker join token from the manager, and run docker swarm join on the new nodes. It's the same process to add managers to your swarm, just using the manager swarm token, because in production you need at least three managers for high availability. Swarm nodes can be any operating system and architecture that Docker supports, and you manage them all in the same way through Docker. But container images are built for a specific OS and CPU architecture, so it's good practice to set constraints in your compose file, so the swarm only selects matching servers for the service image. My final deployment used a replicated service with ingress networking for the Linux web app, and a global service with host mode networking for the Windows API, both of which are now publicly available. I'm using host mode publishing for the API because that's Docker on Windows, which doesn't support ingress networking. That's likely to change soon, but for now you can still get high availability and scale with host mode services by making them global. Global actually means global-within-any-placement-constraints, so you can restrict global services to an operating system or a CPU architecture, or to any arbitrary label you apply to your swarm nodes. Global services are useful for any scenario where you want to run exactly one container on each node. When news nodes join the cluster, they'll also spin up a task for the global services. That's not the case for replicated services, where you explicitly set the number of tasks to run, and that's independent of the cluster size. If you add new nodes to the cluster, the swarm doesn't automatically rebalance any services. To do that would mean stopping healthy containers and spinning up new ones, which could potentially damage your application. Services do get rebalanced if a node leaves the cluster, and it's task containers are no longer running. In that case the service level will drop below the required number, and Docker will spin up replacement containers across the remaining nodes. In that case your app has already potentially been damaged by containers stopping, so Docker is repairing the failure. That's it for this module. In the next module, I'll how to get more density for your swarm by running multiple applications across the cluster. I'll look at using a proxy for request routing, and dealing with stateful applications. So thanks for watching, and join me for Managing Request Routing and Data Storage, the next module in Managing Load Balancing and Scale in Docker Swarm Mode Clusters.

Managing Request Routing and Data Storage
Running Multiple Apps on Your Swarm
Hey, how are you doing? I'm Elton and this is Managing Request Routing and Data Storage, the next module in Managing Load Balancing and Scale in Docker Swarm Mode Clusters. In this module I'm going to show you how to get the most from your cluster, how to run multiple applications, and make them all publicly available on standard HTTP ports. And I'll also cover stateful applications, and show you how to run a stateful app at scale in the swarm, so you get redundancy for your data storage too. Right now my swarm is hosting a stateless web app and a stateless REST API. One is running on Linux nodes and the other one on Windows nodes, so I have two different public entry points, using different load balancers. I want to have a single entry point to the cluster, so I can configure my public DNS CNAMEs to one load balancer address, and have the swarm route application requests for me. Swarm mode doesn't have native functionality to support that, so in this module, I'll be deploying a proxy in a container, technically a reverse proxy. That will be the public entry point for all the apps on my cluster, and it will route requests to the right set of containers. Swarm mode does support Docker volumes, so you can have stateful services deployed at scale. But the swarm doesn't automatically replicate data between nodes, so your application needs to manage replication. I'll be showing you that in this module, using Nginx for the reverse proxy and Elasticsearch as my stateful application.

Using a Reverse Proxy to Front-end Your Applications
You've already seen in this course how the ingress network receives incoming traffic and routes it to containers, but that works on the port level. You can't have multiple swarm services listening on the same port, just like you can't have multiple containers mapping to the same port on the host. As you add more applications to your cluster, you need a different way of routing requests and for web apps, you can use the host domain name in the incoming request. Docker Enterprise Edition provides this out of the box, with a feature called HRM, the HTTP routing mesh. I won't cover that here, but if you're looking at Docker EE for your production deployment, HRM is a great feature. You can get a reference architecture from Docker at this URL, which covers what it does and how to use it. I'm using EE for my demos, but it's built on swarm mode and the features I'm showing you are the same in the free Docker CE platform. So I'll need to manually set up HTTP hostname routing. I'll use Nginx which is an open source, hugely performant HTTP server, you can use it for serving websites, or you can use it as a reverse proxy for other web applications. Right now Nginx powers something lke a third of the websites on the internet. It's easy to use and very robust, as well as fast. It's been taking market share from IIS and Apache at an accelerating rate ever since it was released. So if you haven't used it, it's a pretty safe bet. I'll run Nginx in a container and set up routing rules based on the incoming HTTP host name. Nginx will be the only externally published service now, my other apps are internal services which get routed to by Nginx. Before I deploy Nginx though, I'm going to run another web app on my cluster, so we can get a real idea of the problem that request routing will solve. I'll be using Elasticsearch, which is a document database, as the stateful app in this module. There's a web front end you can use to run analytics over data in Elasticsearch. That UI is called Kibana, and I'm going to run that in my cluster too. I want the password web app, the password API and the Kibana app to all be publicly available. Next I'll deploy those new components so you get a good idea of the problem, and then I'll introduce Nginx.

Adding a New Stack - Elasticsearch and Kibana
My hybrid swarm is up with one Linux manager node, three Linux worker nodes, and three Windows workers. I'm already running the password stack, which has the web site and the password API as services. I'm running version 3 of the API, which has analytics integration built in. And I also have a new Docker-compose file here which describes the new analytics application. I have one service for Elasticsearch, and another one for Kibana. These are public images on Docker Hub, and they're based on Linux. So in the pre-prod override file, I have the placement constraints to put them on Linux worker nodes. For now I'm just running one replica of each service, and I don't have any volumes yet so the data in Elastcisearch isn't being stored safely. Kibana is the public web UI, and it uses port 5601. I'm publishing that using the ingress network, so I can get to the Kibana container from any Linux node. The analytics app is meant to be a shared component, so containers from different stacks will use it. Services can be connected to multiple Docker networks, and that's how the password API will send data to Elasticsearch. The analytics components are connected to a network, which I've already created on the swarm called shared-analytics-network. In the pre-prod override for my original app, I'm using the same external network for the analytics-net, and in the core compose file the API is connecting to password-net and analytics-net. That means the API containers can reach the Elastcisearch containers using the same DNS-based service discovery that we've already seen in the course. I'll deploy the new app in the same way, using docker-compose config to build the full stack file. I'll deploy this as a stack called analytics. Check the service list, and here are the new services, Elasticsearch and Kibana, with a single replica each. Now I have the new services running, so my password app will log analytics to Elasticsearch. Each time the API generates a password, it sends some details to Elasticsearch, like the length of the password and whether it contains special characters. The app is still working correctly, and the API is public in this version, so I can browse to the Windows load balancer on port 8080 and see the raw JSON from the API. To use the analytics app, I can browse to the load balancer for my Linux nodes on port 5601 and see the Kibana web UI. Again, this is using the ingress network, so it doesn't matter which Linux node gets my request, the swarm will route it to the Kibana container. This is a completely new installation of Kibana, but it's already connected to Elasticsearch using the default setup in the Docker images. The new data is in the index called passwords, and here I can see all the entries coming in from the API, and this shows me how heavily the API is being used, and it could be the basis for seeing if the password generator is complex enough, or if we need to tweak the API again. So I've got the density here, running multiple apps on my cluster. And I haven't put any resource constraints on the services, so the containers will use as much compute resource as they need, and as long as they don't all peak at the same time, these apps happily coexist. But the end user experience is not good. I have different domain names and ports for the different applications. I can use my own domain name and set up CNAME records in DNS, so I could have analytics. sixeyed. com for Kibana, password. sixeyed. com for the web app and api. sixeyed. com for the REST API, but each CNAME needs to point to a load balancer, so I need to know if the app is running on Linux or Windows containers. And two of the apps are running on non-standard ports, and that's no good for a public service. Some corporate firewalls will block web traffic on anything other than port 80 or 443, and even if it's not blocked, it's not acceptable to have a public service running on some random port.

Deploying Nginx as a Swarm Service to Proxy Other Apps
So next I'll run an Nginx service as my entry point, so all the apps will be available on standard ports, using my custom domain names. Here's my Nginx configuration. I won't go into too much detail here, but there's a server block for each application running in my swarm. The server name is the public DNS name, so that's how Nginx routes traffic to the right destination. The destination is specified here in the location block, where every request gets passed onto another URL. This is the server block for my password web application, so the external server name is password. sixeyed. com and the proxy destination is password-web. That's the name of the swarm service which is running my password web containers. Nginx is running in containers in the same Docker network as my other applications, so the proxy can route traffic to the service by name, and one of the containers in the swarm will respond. There's a server block for the API, which proxies the password API service, and a block for the analytics app, which proxies the Kibana service. The external endpoints all use port 80, and then route to the relevant port in the swarm service. The Dockerfile here packages my custom configuration on top of the official Nginx Docker image, which is all I need to do to run Nginx as a reverse proxy for all my applications. I've got another compose file for my Nginx proxy, and this is connecting to a network called dmz-net. In the pre-prod override file I use the ingress network to publish port 80, and Nginx is a Linux image, so the placement constraint makes sure this runs on Linux nodes in my swarm. The network in pre-prod is an external one called shared-dmz-network. The proxy needs access to the password web containers, the API containers and the Kibana container, but it doesn't need access to Elasticsearch. So in my analytics compose file, the Kibana service is connected to two networks, DMZ and analytics. Elasticsearch is only connected to analytics, which means if my proxy gets compromised in some way, the attacker can't get from there to Elasticsearch, so my database stays safe. The pre-prod override uses the same external DMZ network as Nginx. And in the password app compose file, the services are plugging into the DMZ network as well. I've removed the original password-net network, because these containers don't actually need to communicate. In the override file I'm using the shared external networks, and I've also removed the port specifications. These are internal services now, they're only accessed through Nginx, they're not publicly available. And that also means I can run the Windows API with multiple replicas rather than as a global service, because it's not exclusively using a server port, so I'm not limited to one container per node. The network definitions have changed in all the compose files, so I'll deploy the password stack again, by using docker compose to combine the base compose file and the override, and deploying the stack which will recreate the application containers and plug them into the DMZ network. Same again for the analytics API, which will plug Kibana into the DMZ, and finally now the proxy stack which deploys my Nginx service. Those stacks are all deployed, the shared networks are there as overlay networks on the swarm, and the services are all up and running. I've configured my external DNS provider with CNAMEs that match the server names in my Nginx configuration. All the CNAMEs are pointing to the Linux load balancer in my swarm. Now when I can browse to password. sixeyed. com, and the request goes to the load balancer, then to an Nginx container, which internally routes the request to a Linux web container, and I see my site. Similarly with the password api, which is configured at api. sixeyed. com. When I browse to port 80 on that domain, a Linux node gets the request, and Nginx routes to a Windows container on port 5001. And lastly for Kibana, I have analytics. sixeyed. com configured, which is Nginx routing to a Linux container on port 5601. Kibana itself is connecting to Elasticsearch running in another Linux container, and that could be on the same Linux node or a different node. So now I have multiple applications running and externally they're all mapped to friendly DNS names. Those are all served by the same Nginx service, which routes traffic internally to other containers in the swarm. And the apps are segregated with Docker networking, which is an easy way to isolate and protect the workloads. Next I'll look at how I can run my database service at scale for redundant storage.

Stateful Apps and Docker Volumes in Swarm Mode
The entry point to all my apps now is the Nginx proxy service, which I can run at scale using ingress networking. All my application services can run at scale too, using VIP or DNSRR load balancing, internally. Everything is redundant now except my database, which is running in a single container. That container doesn't even use a volume for storage, so if the container exits for any reason and the swarm spins up a replacement, the data will be lost, even if the new container runs on the same node as the previous container. I can run the Elasticsearch service with multiple replicas which gets me high availability for the service, but I also want to make sure the data storage is safe too, so I'll use Docker volumes which have a separate lifecycle to containers. When you create a service in swarm mode with a volume attached, Docker creates a volume on every node, so it can schedule a container anywhere and that container will find the volume. But the swarm doesn't replicate data between volumes, they're all separate pieces of storage. To get redundancy for your data, that needs to happen at the application level. Stateful apps need to have their own replication logic if you want to run them at scale in containers, so when one container writes data to its local volume, that data gets replicated by the other containers to their local volume. Elasticsearch does have replication built in, so if I configure it correctly I'll get redundant storage for my stateful app. This is the scenario I want to support. I want Elasticsearch running as a Docker swarm service across three nodes. When all the replicas are running, I get scale because any container can handler requests. If one of my servers goes down, or a container exits, my service is still available from the other containers. Elastcisearch takes care of replicating data, but I need to be careful about how I configure it. Elasticsearch will manage the database cluster itself provided it can locate all the instances running Elasticsearch, so I need to make sure the replicas all have their own IP address.

Configuring Elasticsearch to Run as a Scalable Service
I'll use swarm functionality we've already seen to support this, using DNSRR for service discovery means the containers each get their own IP address, and if I run 3 replicas constrained to Linux worker nodes, I should get one container per node, each with its own Docker volume for storage. I'll show you that next. Here's my updated analytics compose file. The Docker setup is all done in the deploy section of the Elasticsearch definition. I'm running it as a replicated service, with DNSRR service resolution and constraints limiting it to Linux worker nodes. That means I'll have a single container on each Linux node, and when containers lookup the Elasticsearch service by name, they'll get container IP addresses by round robin. Now I also need to configure Elasticsearch to run as a replicated service. I do that by overriding the startup command in the Docker image and providing my own command with a bunch of configuration settings. Here I'm telling Elasticsearch there should be three nodes in the cluster. Elasticsearch uses the same node terminology as Docker, but the Elasticsearch nodes are actually running inside Docker containers in the service. And here's the clever part. Normally in the discovery part you would list all the hostnames or IP addresses of the servers running Elasticsearch. But I can't do that because they'll all be in containers with dynamic host names and IP addresses. So instead, I specify a single host, which is the name of the service, Elasticsearch. When Elasticsearch starts up, it will query DNS to get the IP address of the host called Elasticsearch. It will keep making queries because it expects 3 nodes, and because I'm using DNSRR for the service, Elasticsearch will ultimately get a list of all the container IP addresses, and it will initialize the cluster across all the containers. I'm also specifying a volume here mapped to the storage location Elasticsearch uses. Swarm will create the volume with the default settings, which in swarm mode means each node will have its own local volume. That gives me redundant storage for the service, because the Elasticsearch cluster replicates data across all the containers, and each container is using a persistent volume on its local server to store the data. Okay, so I'll deploy my analytics stack, which will update the Elasticsearch service definition and scale up to three replicas, one on each worker node. That's all pretty fast and if I check the volume list we'll see where those volumes are. The volume is named after the stack, so here there's analytics_es-data on worker 02, and that's a local volume. There's another volume on node 01 - same name, but this is a different storage area on a different server. And the same on node 00. The manager doesn't have a volume for the service because it doesn't match the constraints, so it won't be used to run Elasticsearch containers. I've got a lot running on my swarm now, but the proxy is still only a single container, so I'll update that before I check the availability of the solutions. Here in the proxy pre-prod configuration, I'll specify three replicas of the proxy service. This is already using the ingress network, so I don't need to do anything else, just generate the new stack file using docker compose, and then deploy. That updates the service, and in the list there's now two replicas, and now three. So the proxy has high availability. To see what's happening in my Elasticsearh cluster, I want to check the logs for the individual Elasticsearch containers. You can do that with the command line, but seeing as my swarm is using Docker EE, I'll switch to the UI, Universal Control Plane, which makes it much easier to manage. Here are all the services, and from here I can drill down into Elasticsearch and see the containers. Here's the stopped container, which was the original deployment, and the three in my new deployment. I'll select this container and check the logs. You can see here, this container is the Elasticsearch master, it's added the two other containers when they started, and then when all three had synced data, the cluster goes into the healthy green state. I'll look at the logs for the second container and that shows me it started after the first one, and found the first container as a master. And if I check the logs of the third container, we see the same situation. So the Elasticsearch cluster has configured itself in containers, and I'll check the app is still working. Here's the password web app which still looks good, and check the API and that's still responding correctly, so now Kibana. Kibana's talking to a whole new Elasticsearch cluster, so I need to set up the index again, which is passwords, and now I'll only see those password details that have been generated since this latest deployment. It's all looking good, and next I'll show you that this really is highly available.

Proving Failover by Killing a Swarm Node
By removing a node from the swarm while the apps are being used. I have two browser windows open here, one pointing to the password web app and one to Kibana. I've put together a simple Kibana dashboard that shows me the total number of passwords generated, and a breakdown of the password length. I'm using a Firefox plugin to automatically refresh the password page every 2 seconds, so each time the website refreshes that generates more passwords, so the API sends more data to Elasticsearch. Kibana has its own auto-refresh every 5 seconds, and when Kibana refreshes the numbers on the dashboard will go up. Now I'm running at scale so these requests could be actioned by any container, but the response and the user experience should be the same. In the next module, I'll show you how you can gracefully manage your cluster for production maintenance, but here I'm going to show an unexpected server failure. My swarm is running in Azure and I have a scale set for the Linux worker nodes. If you're not familiar with Azure, this is a set of VMS that all have the same base disk and startup scripts. There are my three Linux worker nodes, and I can simulate a failure in this server, which is swarm node 02, by deallocating it. Like the warning says, this is effectively a power down of the VM. I'll switch back to the refreshing pages as the node goes out of service, and we'll see something interesting. The web app and API are highly available with multiple replicas running, and those apps keep running without any service interruption. Kibana is only running in a single container, and that happens to be running on node 02, which is in the process of shutting down. There is a service interruption here, because it takes a few seconds for the swarm to realize that the node is out of service. The end user sees an error page because Nginx can't reach the Kibana container, which has now gone offline. So the swarm starts a replacement Kibana container on another worker node, and by the time I refresh the browser, that container is running, Nginx finds it and service has been restored. The password count has jumped and it's still increasing, so Elasticsearch must have been running while Kibana was offline, and the storage has been preserved even though one of the Elastcisearch containers went down with node 02 as well. I'll check the service list and we see that everything is running at full scale. Any services which had containers running on node 02 have had replacements started on the other nodes now. When I run a node list, 02 is still part of the swarm, but Docker knows that it's down, and that's what triggered the new container creation on other nodes in the swarm. Now my swarm is running multiple applications across Linux and Windows containers, with high availability and redundancy, even for the stateful database application. So I'll wrap up this module now and tell you what's coming next.

Module Summary
In this module I've covered request routing and data storage, which is all about getting the most from your swarm cluster, running multiple applications, stateful or stateless, with high availability. I started by deploying a new analytics stack to the cluster to show the limitations of ingress and host mode networking. If you run multiple applications on your swarm, they need to run on different ports, and you potentially need to map DNS CNAMEs to different load balancers, which isn't a viable approach. So I added a reverse proxy as the single public entry point to the cluster, and configured the proxy to serve applications based on the host name of the incoming HTTP request. That means all my DNS entries point to a single load balancer, and all my applications can use the standard HTTP ports. The proxy is also a great place to put core features like SSL termination, logging and caching, and it means all the application services are internal now, so they can use VIP or DNSRR without affecting how you can scale them. As you add more apps to your cluster, you'll have some which store state and you need to think carefully about how you support that. You can use Docker volumes in swarm mode, but they'll use the local driver by default which means each node gets its own volume and the swarm doesn't replicate the data between nodes. That means to get redundant storage, replication needs to happen at the application level, so your app needs to be cluster aware and it needs to synchronize data itself across all the nodes. I showed that with Elasticsearch, running it as a replicated service with DNSRR discovery. That setup puts a container on each worker node, so they each have their own dedicated volume, and I configured Elasticsearch so it would find all the other container IP addresses from the swarm's DNS service. So far in the course you've learned about load balancing and service discovery, scaling services and nodes, and now request routing and storage. In the next module I'll cover the production lifecycle of applications and the cluster. I'll show you how load balancing and scale in swarm mode supports rolling application updates and automated rollback, and how you can provide maintenance windows by managing the nodes in the cluster. SO thanks for watching this module, and join me for Supporting Production Maintenance and Deployments, the next module in Managing Load Balancing and Scale in Docker Swarm Mode Clusters.

Supporting Production Maintenance and Deployments
Understanding How Load Balancing and Scale Support Production
Hey, how are you doing? I'm Elton and this is Supporting Production Maintenance and Deployments, the next module in Managing Load Balancing and Scale in Docker Swarm Mode Clusters. There are two things that scale and load-balancing provide to your application landscape. The first is the ability to deal with whatever load you need, which Docker provides with replicated services. If you have a spike in traffic you can scale up by adding more containers, and if your traffic profile is well known, then you can automate that. The second is high availability, running at scale means you have redundancy. If you lose servers from your cluster, Docker automatically brings up replacement containers on the swarm to maintain your service level, and if you're using the ingress network then the load balancer is smart, it only routes traffic to containers which are up and healthy. In this module I'll dig into more detail to show you how that helps you support production environments, dealing with maintenance and deployments without loss of service. There are three key features that you need to know about when you move containers into production, and I'll cover them here. The first is server modes, which let you take a node out of active service, but leave it as part of the swarm. That covers you for maintenance. The second is rolling updates and rollbacks, which let you do a safe, staged, automated deployment of your application. I'll show you how Docker swarm does those updates in a scaled environment, and how you can configure the behavior. images, the healthcheck is functionality built into the image that tells the Docker platform how to test the application is healthy. Docker can replace unhealthy containers in swarm mode, and that helps keep your applications running and your deployments working. First I'll look at the server modes and the maintenance cycle.

Drain Mode for Worker and Manager Nodes
You treat your container platform as a single entity, so you deploy services and stacks to the swarm and you don't know or care which actual servers the containers will run on. But you do care about the containers running on a server if that server needs maintenance. That could be an operating system update that requires a reboot, or an update to Docker. In a virtual environment it could be part of moving the VM to a different host, or in a physical environment it could be a hardware update or planned infrastructure maintenance. I showed you earlier in the course that if a node goes down, Docker swarm realizes it's not available and spins up replacements for any lost containers on other nodes. You can do that in a planned manner by putting servers into drain mode. Nodes in the swarm can have a role of manager or worker, and they have an availability setting of active, pause or drain. Active means they're normal members of the swarm, they can have containers running, and they can have new containers scheduled. Pause means any containers that are running are left as they are, but no new containers will be scheduled on the node. And drain means no new containers are scheduled, and any existing containers are gracefully shut down. In a high-availability scenario, that means the service level of your app goes down, so the swarm schedules replacement containers to start on other nodes. Now that graceful shutdown is a best-effort from Docker. Containers get sent an interrupt signal, which should tell the app to shut down, but if it doesn't complete within the expected time, then the container gets killed. So Docker swarm gives you the mechanism to avoid service disruption, but whether it works depends on how your apps are coded. There's another use for drain mode, which is isolating your manager nodes so they don't run any application containers. Swarm managers by default are eligible for running user workloads, so service tasks can be scheduled on managers. That's fine for small swarms with a modest workload, you can run multiple managers for high availability, and have them running your application containers too. But swarm managers are quite chatty and if a manager is starved of resources because it's running lots of intense containers, it might drop the connection to other managers. That's not good, so for a high-performance cluster, your managers should be manager-only.

Setting Nodes to Drain Mode and Checking Service Levels
Drain mode is pretty simple to show, so I'll demo that next and show you the implications and then we'll move on to application updates. Here I have a production-grade cluster running, with 3 managers, 3 Linux workers and 2 Windows workers. I've deployed the latest version of my app landscape, and scaled it up to make sure there are plenty of containers running on all the nodes. In the compose files, I've removed the role constraints for the web application, the proxy and for Kibana, so containers can run on the managers for those services. Elasticsearch does have a worker role constraint, because that's currently restricted so it doesn't run on manager nodes. I'll check the replicas for the password web service, and again it's a bit difficult to see what's running where with the default output. Service ps accepts a format option where I can restrict the data that gets shown, so I'll just list the replica name and its node. You can see that web containers are running on all the manager nodes, as well as all the Linux worker nodes. The first thing I'll do is switch the masters to drain mode. The command is docker node update, setting availability to drain. For the manager nodes this can be a permanent state, and it's safer than relying on deployments to have role constraints that keep the managers free. I'll do the same for manage 01 and for manager 02. Now the managers are all in drain mode, but my service level is still 100%. As the nodes went into drain, Docker stopped the containers and started replacements. I'll check the web app again, and it's difficult from this output to see whether containers are running or stopped. So I'll add the CurrentState field to the format string, and here we see that the containers which were running on managers have all been shutdown and replaced with containers which are running on the workers nodes. I'll check the apps and verify they're all still working. Here's the password web app which is looking good, and then the api, and that looks fine too. And then lastly Kibana, and it's all working well. Now let's say I need to take a Linux worker node out of service to do some maintenance on it. I'll have a look at the tasks running on node 02, and the node ps command accepts the same format string, so I can show just show the name and the current state of the tasks. Other than the UCP agents for Docker EE, there are containers for Kibana, Elasticsearch, the proxy, and the web app running here. I can run the same docker node update command to put worker 02 into drain mode. The node is still part of the swarm, it's status is Ready rather than Down because the managers are still connecting to it, but the availability is drain, so there won't be any containers running. I'll check the service list and my apps are all at 100%, which is correct for my setup, but it's actually not really what I want. Remember that Elasticsearch is setup to run across 3 replicas, excluding the manager nodes, and each replica will use a local volume on the server. Let's take a look at the tasks for this service to see where they're running. Now I have two Elasticsearch containers running on node 01. Docker's done what I told it to do, but these two Elasticsearch containers are writing to the same volume on the server, so they're going to get their data stores mixed up. So the service level in Docker doesn't necessarily tell the whole picture, because Docker doesn't know what the app inside the container is trying to do.

Manually Balancing Services After Nodes Join
If I carried on running like this my Elasticsearch data would get corrupted. So in the next demo, I'll fix that up before we carry on. So it turns out my service configuration for Elasticsearch isn't restrictive enough. This tells Docker I want three replicas on Linux worker nodes, but actually I want at most one container on each node, with a total of three across the cluster. So I'll change the service mode to global, which gives me one container per node. In a real environment I might have a fluid cluster with nodes coming and going, but a core set of nodes with large disks that I wanted to use for stateful services. In that case I'd add a label to those nodes, and add a label constraint in here so the services only run on those servers. We've seen you can't change the type of a running service, so I'll remove the analytics stack, check that I'm in the right directory here and rebuild the stack file using docker compose. This now has the global setup, so I'll deploy it again. Check the service list and we have 2 out of 2 replicas in the global elasticsearch service, that's all of the available Linux worker nodes, because node 02 is still in drain mode. The other services are running at 100%, because the replicated services have new tasks running on the other workers, and the global service running my stateful database app only expects 2 tasks because there are only 2 active nodes that meet the placement criteria. Again, Docker thinks the Elasticsearch service is working fine, but is it? I'll switch back to the web UI and check the logs. There are two tasks in the service and in the logs for the first one I can see that it's found the master node which means this one will join as a slave. The other container is the master node, and the logs here show the slave being added, but there's another message saying that the master is expecting 3 nodes but only has 2, and that's going to delay the cluster coming online. I'll see that in Kibana, the UI tells me Elasticsearch is red and it can't connect to fetch data. The API is still working correctly, and the password app is still working correctly, but having 100% service availability doesn't necessarily mean the apps in the containers are all running correctly, which is especially true of stateful apps. Anyway, let's say my maintenance is done and I'm going to bring node 02 back online - which is just docker node update, setting availabilty to active. The service list gets updated and now I have 3 out of 3 Elasticsearch containers, it's a global service now, so when node 02 rejoined it had a container scheduled to run on it. The web app is still running, and the API is running, and if I refresh Kibana and connect to the passwords index again, we'll see this is all working again too. I'll check to see what containers are running on the node now that it's active. So I've got one Elasticsearch container and the UCP agent for Docker EE, and the rest are all shutdown. The replicated services are at 100%, but all the Linux tasks are running on two worker nodes, so the swarm is not balanced. It's the same when a node rejoins the swarm as when a new node joins, the swarm doesn't automatically rebalance running services. We can see that with this service. At the moment the two Linux workers are running three containers each. When node 02 became active again, if Docker wanted to rebalance the service, it would need to run two containers on each of the three nodes, but to do that Docker would have to kill two healthy containers to start replacements on the new node. The swarm doesn't take potentially destructive action like that, so if you want to balance the services then you need to do it manually with scale. You've seen me doing that in this course using the compose file, and you can also use the docker service update command to specify a larger replica level. I'll scale the proxy service up to 20. That will create new containers, and I can see if I list the tasks for the service, with my format string to make it easier to read, that most of the new containers are running on node 02, because it had the most availability. Now I can scale back down again, which will remove containers. For explicit scaling operations, Docker does try to balance the swarm, so now when I check with the filter condition to give me just the running containers, I have 12 tasks, with 4 running on each node. And of course my application, API and analytics endpoints, which are all routed through the proxy, are still working. Drain mode is the same for managers and workers, and for Linux and Windows nodes. It's a very powerful way of managing downtime without losing service, provided your applications can support it, and for stateful apps you'll need careful considerations of placement constraints and scale to get that.

Understanding Rolling Update Configuration in Swarm Mode
Servers need to be managed, but if you have sufficient capacity in your swarm then you can drain nodes without interrupting any of your running applications. It's the scale and load balancing features that we've already looked at in this course that power those updates, but it does need support from your applications too. Apps need to gracefully shutdown when containers are being stopped to make sure client connections don't get broken. And stateful apps need to reconcile their data stores when they come back online in new containers, to make sure the distributed storage is intact. Server maintenance is an occasional task, but the same principles, again, come in for application updates. You run your app in multiple containers for scale, but that means you have multiple instances of your application running. And when you need to update to a new version, you need to update all those instances. Docker swarm does that for you with a rolling update process. When you upgrade a service from v1 of an image to v2, the swarm takes down the v1 containers one at a time. Then it spins up a replacement container running v2 and checks that it starts up correctly. If that container is OK, it moves on to the next v1 container until the whole service has been upgraded. How many containers get replaced at a time, and how long Docker waits to check the new containers are working are all configurable settings for each service. You can tweak those to give you an automated update, following the canary deployment model. There are six settings to configure rolling updates. I'm going to cover them all because you'll be doing a lot of application updates and you need to know how this works. Update-failure-action is the easiest, if the update fails, you can choose to pause the rollout so you can investigate, or rollback the rollout to the previous image version, or continue the rollout anyway. The default is pause. Update-max-failure-ratio tells the swarm how many new containers can fail before it considers the whole update to have failed and invokes the failure action. The default is zero which means any failing containers will cause the update to fail, and this value is expressed as a fraction, so to allow 10% failure tolerance you set it to 0. 1. Failure is determined by the update-monitor setting, which is the amount of time a new container has to start up correctly. The default is 5 seconds, so if a new container isn't running successfully within 5 seconds of starting, it's deemed to have failed. The other values define how the rollout happens. Update-parallelism lets you take multiple v1 containers down in a batch and replace them with a batch of v2 containers. The default is one task at a time. Then update-delay specifies the time period to wait between updates. The default is zero seconds, so when one task, or a batch of tasks, has been successfully updated, the swarm moves onto the next one straight away. Lastly is update-order, which lets you configure updates to create new v2 containers before removing old v1 containers. The default is stop-first which means the old containers are stopped before the new ones are created, but you can reverse that with start-first. There are similar settings to configure how rollbacks happen, and that gives you a lot of flexibility to set up the deployment model that you want for each of your applications. Next I'll show you the basic update and rollback actions, and then I'll use a custom configuration and apply an application healthcheck too.

Pushing a Rolling Service Update and a Manual Rollback
My password web application is running across 10 containers, and it's app version 2. I have a new release to deploy which is version 3. I can deploy this with the default rollout settings using docker service update, passing a new tag in the image parameter. That will start a rollout, when I originally deployed my app as a stack from this compose file, and if I run a command to update the service, then what's actually running will be different from the spec in my compose file. And my spec is meant to be the desired state. If I update to version 3 and later someone deploys a stack update, it will revert the web app to version 2. SO I'll make the change here in the compose file and my policy will be the compose file is the source of truth, and updates must always be done through the stack. In reality the docker-compose config and docker stack deploy operations would be part of an automated pipeline. Let's just check that no-one has deployed an update, and the web app is running on version 2, so that's as expected. Docker stack deploy will either create a new stack or update an existing one. If I look at the services, the web app is running 10 out of 10 containers, but I'll check the task list for the web app and we see 3 tasks are on version 3, and the rest are on version 2. This is the rolling update happening now, with v2 containers being taken down and replaced with v3 containers, one at a time. So I'll browse to the app and refresh lots of times, you'll see some responses come from v3 containers, and other come from v2 containers. I'm refreshing too fast here for the API responses to load. Docker swarm stores the current spec and the previous spec for every service, which you can see with docker service inspect. The latest update status is completed now, the previous spec uses image version 2, and the current spec is image version 3. Storing this information is how swarm powers automated rollbacks. My update succeeded, which means all the v3 containers are running correctly, but there could be a bug in the app which we didn't discover until we went live. In that case running a service update command is going to be faster than changing the compose file and doing another deployment, and safer too. The rollback flag actually does a rolling update, but it automatically returns the service to the previous spec, so I don't need to know what the last version was. The progress shows me that the update is being rolled out across the 10 containers. Browsing to the app now, we see v3 containers responding to start with and then more v2 containers, and within a few seconds, the service has converged and all the responses are coming from v2 containers. Just to confirm, the service list for running containers shows they're all on v2 and they've all been running for a minute or less so these are new containers. Rollback is a great way to safely and automatically roll back an update where the deployment has succeeded, but there's an issue with the new release. I've used the default update and rollback configuration for this app.

Adding Healthchecks and Pushing a Service Update with Automated Rollback
But, in production you'll usually want to tailor the config for each service, and you'll definitely want healthchecks in all of your Docker images. I'll show you that next. I've updated my pre-prod compose file to specify how updates should work. For the API, Docker will update 2 containers at a time, and wait 5 seconds between each update. If there's a problem with the rollout it will pause, which is the default value. The web app is more conservative. I rollout 3 containers at a time, but I wait 8 seconds in between to check the status, and if more than 30% of the new containers fail, then the update will automatically rollback. I'm using version 3 of the web app, which is the same as version 2, just to show you that you can do a deployment which changes how updates work while also doing a service update. Right now the web app is on v2 and the API on v3. I'll do the usual deployment steps of generating the stack file and checking it through - this has my custom update config, and sets both the app and API to v3. So I'll deploy that as an update to the stack and check the service list. The service level is 100% and as the web app is replacing 3 containers at a time, we should see v3 responses coming through. And yes, here we are, web app v3 and API v3. I'll inspect the service, and the latest update completed in 27 seconds. I have 10 containers, which is 4 batches of 3 with an 8 second gap after the first 3 batches, so it's taking under a second to rollout each batch. The final update is to version 4 of the images, which have Docker healthchecks. In the Dockerfile you specify how containers should start. This is my password API which starts with a dotnet command to run the REST API. Docker starts that process when it runs a container, and it checks that the process is still running. That gets used in the failure check for a rolling update, if the process fails to start, that's flagged as a failed container. But that's only a basic liveness check. The process could be running, but that doesn't mean the app is working, and that's where the healthcheck comes in. This is a Dockerfile instruction that gets built into the image. You can specify how it should run, this one has an interval of 5 seconds between checks, but it waits 15 seconds for the app to start before it fires the first check, and it will do 3 retries before reporting that the app has failed. And then you specify the command that executes the healthcheck. Docker runs this command inside the container, so in this case every API container will have a healthcheck that executes every 5 seconds, which runs this PowerShell script to call the API on the localhost. If the HTTP GET Request returns 200 then the app is healthy, and the command returns 0, which tells Docker the healthcheck passed. If the response isn't a 200, it could be a 503 if the app is maxed out, or if the call errors, the command returns 1, which tells Docker the healthcheck has failed. If a healthcheck fails three times in a row for one container in this service, Docker will replace it with a new one. If a healthcheck fails during a service update, then Docker can stop the rollout or start a rollback. The API is fine, but the web app has a bad image in version 4 - the healthcheck is configured so it will always fail, with just one retry and 2 seconds between checks. That means the update will fail and we'll quickly hit the maximum failure threshold. When I deploy this new version of the spec, Docker will successfully update the API to version 4, but the web update to version 4 will fail and it will roll back to version 3. I haven't changed the update configs for this release, but the base compose file is using v4 of the images. I'll generate the stack file with Docker compose and check it has the right spec - which it does, so now I can deploy. When I hit the website now, right away I get v4 of both the API and the web app, but soon the web containers will fail their helthchecks, the update will hit the failure threshold and will automatically roll back. And yes, now we're on version 3 of the web app - but the API is fine, so that stays on V4. The services are still at 100% replication, but now I can be confident that my apps are really working for the end user too, because I've given Docker enough information to manage deployments for me. The password web service has a latest update status of rollback completed, which tells me there was a problem, and sure enough if I check the tasks for the service, the v4 containers are shut down and the v3 containers are running. With healthchecks in the Dockerfiles and custom update configs in the compose files, you can use Docker swarm to make your app self-healing as well as scalable and highly available.

Module Summary
This module covered the swarm features that will get you through your normal production maintenance tasks. You saw how to take swarm nodes out of service by putting them into drain mode, which is good for temporary server maintenance and something you might consider permanently for swarm managers. Going into drain mode means swarm stops all the containers on that node and makes it unavailable for selection, so no more containers will be scheduled to run on it. But the node is still part of the swarm, so when you're done with your OS update or server reboot, you can put the node back into active availability and it's ready to run containers again. But making a node active is the same as adding a new node to the swarm, Docker doesn't automatically rebalance replicated services, so you need to manually scale them to achieve that. But it does automatically run a container for global services on the node. You also saw how swarm mode does rolling updates when you deploy a change to a service or a stack. It takes down containers one at a time and replaces them with containers matching the new spec, typically that's a new image version. You can configure how many containers are replaced in each batch, how long Docker should wait to check the batch is healthy, what level of failure can be tolerated, and what to do if the failure tolerance gets exceeded. It's easy to configure an update spec that gradually deploys your app and immediately rolls back the update if the new version starts to fail. I also showed you that Docker only checks the container startup process is actually running when it checks for failures, unless you add a healthcheck. The healthcheck goes into the Dockerfile and tells Docker a command to run to check if the app is actually working correctly. Healthchecks are a must for production container apps, and you'll benefit from having a custom update and rollback config too. And that's the end of the course. I've covered all the aspects of load balancing and scale in swarm mode, so now you know how to configure service discovery with DNSRR and VIP, how to use load balancing with the ingress network, and external load balancers with host mode publishing, how to scale replicated services and run containers on every node with global services, how to run multiple apps on your swarm and route requests with a proxy, how to approach dealing with stateful apps and now the production lifecycle. I hope you've enjoyed the course and found it useful. Thank you for watching, and there's lots more from me on Pluralsight, so I hope to see you again. My name's Elton and this was Managing Load Balancing and Scale in Docker Swarm Mode Clusters.
