Docker simplifies running software, and now with the release of Windows containers, you can run just about any software with a consistent set of commands thanks to Docker. In this course, Getting Started with Docker on Windows, you'll learn how to use containers in Windows environments, both Linux and Windows containers. You'll see how to use Docker for Windows on workstations and how to use the Docker Engine in server environments. Along the way, you'll see how containers simplify finding, downloading, installing, starting, stopping, and uninstalling software. You'll also see how container isolation provides security and simplicity. You will learn how to build your own images and how to run command line apps, web servers, databases, and other apps in containers. Finally, you'll learn how to easily orchestrate complex applications with docker-compose. By the time you're done with this course, you'll know how to use Docker to simplify software management.


Course Overview
Course Overview
Hi, my name is Wes Higbee. Welcome to my course, Getting Started with Docker on Windows. Windows containers are hot off the press, and in this course I'll show you how to get started using Docker on Windows to manage both Windows containers and even Linux containers. First we'll take a look at installing Docker on both Windows client SKUs and also server SKUs, and in the process we'll understand a little bit about the difference between Docker for Windows and Docker on Windows. Next we'll take a look at how we can simplify running command-line applications inside of containers. We'll then see how we can host static web sites inside of a container, and in the process we'll learn a bit about building our own images. Then we'll take a look at databases and containers. Databases like SQL Server on Windows and MySQL on Linux. And then we'll take a look at how we can compose both web applications and databases with Docker Compose, and quickly spin up rather complex environments, all with a single Docker Compose command. By the time you're done with this course, you'll be much more confident using Docker, understanding the value of containers, and you'll likely be excited to put a bunch of this to use in your own work. Before taking this course, you should have some understanding of using command-line environments, especially the command-line on Windows. All right, let's get started.

What Is a Container?
Installing Software Is Often Painful
Last weekend I went to a wedding reception, and when I got there I was taking my jacket off when the host came into the room and asked if anybody could help with a technology problem. I remained quiet for a few minutes until I realized that nobody else was going to speak up. Turns out the host had a USB drive that had a video on it that they wanted to play for everybody at dinnertime, a video from people that couldn't attend. So it was pretty important. And when I took a look at the thumb drive on the computer, I realized that the video was probably just in the wrong format. It was an m4v file and Samsung TV seemed to only support mp4, so I knew right away if I just converted the file, it would probably work. Of course, the host had an ancient MacBook, so converting the file was easier said than done. First off, I did a good old Google search for FFmpeg, and it came to this site, and I looked for a link to download, and here's a button here to do that, so I clicked that, and then I saw a big Download button right here, so I hit that, only to realize that, of course, I was just downloading the sources. So I came back to the page then and I saw some packages over here on the left, and I saw an Apple icon, so I hovered over that and noticed down below I had a couple links show up, and I was looking at this, scratching my head thinking well, this is an older computer, so I don't think the static builds are going to work, so I hopped into the builds for a 32-bit and a 64-bit for OS 10. 5 and above, and I was presented with this page. And I scrolled down and I couldn't really figure out what was what. I tried the one Download that was here right here at this Download Now button, though I was worried this might be a link to some malware. Turned out that wasn't the case, but it didn't work either, so I saw over on the right-hand side a link to the archive here, and then on the archive page there were hundreds of different links. Eventually I got down to this section about halfway down, and I noticed that there were a couple of older builds that I could try, and I think that this version here on the left-hand side eventually worked. Probably took me about 25 minutes to figure all of this out. It's amazing that in the 21st century software can be such a pain to simply install on a computer. Now you might be wondering, what does this have to do with Docker and containers? Well, turns out at the core of all of this buzz around Docker and containers is a simple notion of simplifying how we interact with software. Simplifying installing it, discovering it, starting it, stopping it, uninstalling it, even building it. And that's what we're going to take a look at in this course. And I've got a little secret to share with you. If you already know how to use software, install it, run it, uninstall it, et cetera, then you already understand Docker. We just have to get you familiar with some of the terms, and how software works in the context of Docker and containers. And perhaps the best place to start is just to take a look at some of the hassles we have when it comes to dealing with software. Let's look at that next.

Challenges Discovering Software
So my first question is to you, what are the biggest challenges that you face when you're using software or developing software or anything related to running software? Will you take a minute and scribble down maybe the top four or five things that come to mind? Chances are you don't think about this much, but there are several prominent steps involved in getting a piece of software under your computer and then executing it. And if we step back and take a look at the overall picture of what it takes to install software and run it, we can start to understand some of the areas where we have problems, and then we can take those areas and see how Docker and containers can help us out. These days we find our software online. And you may not even think about that, but there's a process that you have to go through to discover the software that you would like to use. Even software you know how to use, like FFmpeg, you may not know exactly where to go to get the particular version of FFmpeg that you need to run on a particular computer, like an old MacBook. Once you find software, you download it onto your computer in some format that allows you to then extract it locally, or run some sort of installer, and then after it's installed, you can go ahead and kick it off to execute it, to load it into memory, and the CPU then runs through your program, so that you can interact with it. So there are at least four primary steps here, four different areas of pain when it comes to dealing with software. Let's drill into these areas and see what some of the hassles are. When it comes to finding software, the first problem we face is just where to go. On some platforms we have places that we can go to look for new software, for example, we have App stores, predominately on mobile devices, but on some desktop devices as well, but quite often we don't have an App store for a given platform, or it just doesn't cut it, the good stuff's just not there. And then some of these platforms like Linux we have package managers that are really good at helping us find software to install, but oftentimes a software is out of date on package managers, it's an old version. And even if a package manager on one distribution of Linux has a package, say, for example, CentOS, that doesn't mean that another distribution like Ubuntu is going to have that same package. And, of course, on Windows we don't really have a package manager, we kind of have an App store, though that's not really that popular yet. On Windows for the most part, and even on Mac and some other Linux environments, we spend a lot of time on standalone websites to find software, and it's a website that oftentimes put together by the group of people that created the software, so it's unique to that piece of software, as you saw with the FFmpeg site. Now beyond just where we go to get our software, it can be helpful to have information, metadata and statistics about that software that can help us make a decision about whether or not we want to use the software. Stats like popularity, number of downloads, number of issues that people have, the last time the software was updated, all of these things can help us make decisions, but oftentimes because of the inconsistency of where we go to get our software, on standalone websites, for example, we don't get a lot of this information, so we can't use it to make a decision. We also have trust issues. What's this standalone website that I'm downloading from? Is it really the site for the people that created the software, or is it some malicious person? Sometimes we don't even notice that we're downloading over plain old HTTP and not using a secure connection to get our software. You may also be wondering if the software itself is secure. What does the code do? And then when it comes time to download the software, what's the availability like? How much bandwidth does the site have? Will the site be up when I want to install the software? And then if we have to pay for the software, how do we pay for it? And it seems 9 times out of 10 when you have to pay for software, you're going to a standalone website that is unique to that piece of software, the payment structure, the payment means, everything about paying is unique to that piece of software. And then, of course, there's a license involved too that's unique to that piece of software and how you get the license into the software is unique as well. There are some exceptions to that when it comes to App stores, but that's about the only place we see a consistent means of paying for software.

Challenges with Software Installation
When it comes to installing software, each application is like a snowflake, it has its own special process you have to go through to get it installed. Of course, the first concern when it comes to software is, will this even run on my operating system? Is it cross-platform? Does it support my OS, my OS version, and build number, does it support my CPU architecture? And of course for geeky people like us that understand technology, these are questions that we can answer, but can you imagine the host of that party that wasn't familiar with computers, could you imagine her finding the right version of FFmpeg to download and then to use to convert the video? Good luck! So we have to worry about, am I on a Mac, and I on a Windows machine, Windows 10 Anniversary Edition 64-bit? And even when something's compatible, then we have to worry about the format. Some software comes as source code that we have to build on our own. Some software comes as a standalone executable, it's wonderful. Some software comes as an executable, plus some libraries that are needed, but the libraries are bundled with it, perhaps in a zip file or some other installer. And, of course, for optimization sake, a lot of software comes with an executable, and then also dependencies on shared libraries or a runtime that has to be on the system, for example, the Java runtime or the. NET runtime. Now a lot of times we don't have to worry about all these formats, because software comes with an installer that takes care of this for us, or perhaps a package manager takes care of this for us, but sometimes we have to manually put the software together, and so then these concerns actually matter. But installers are a double-edged sword, because we oftentimes wonder what exactly did the installer do? What did it put on my computer? It's like magic to install software this way; we don't learn a lot about what's going on because of the convenience. On the flipside, at some point we might want to uninstall the software, or maybe update to a new version, and of course then we have to worry about how do we do that? And in a lot of cases, updates and uninstalls are specific to an individual piece of software. And, of course, we've built tools to help us with this problem, tools in the configuration management space, but if you ask me, they've only automated something that's a big problem, they haven't really solved the underlying problem. Because at the end of the day, installing software is just way too complicated.

Challenges with Running Software
Once we've got software installed, then we have to worry about how do we operate it. Is there helpful documentation that helps us figure out how to actually operate the software? Where's that documentation at? When's the last time it was updated? Oftentimes there are inconsistencies if we even do have documentation. There are many things that are undocumented, and there are many things that are just out of date. In order to run software, we have to know where it's at. Do we have to change into some obscure directory on our system, or was the software added to the PATH as part of an installer, or do we need to add it to the PATH by hand? How do I start the app, how do I stop it? Is it a single process, are there multiple applications that came with an installer? Or perhaps is the app meant to be run as a service and then if that's the case, how do I set up the service on a Mac, on Windows, on Linux? Do I have to write some service configuration file like for System D or upstart? Do I have to write that by hand to get the service to work? A lot of times with services, the people that make software, don't even provide configuration files for the plethora of service managers that are out there that we might use. So then we're stuck Googling for articles or blog posts to help us do this, and we're learning from sources that we don't know if we can trust or not. Software that we pay for then has to be licensed, and how we get that license in, like I said, that's unique to each piece of software. Before we run some software, we have to make sure we have dependencies, and a lot of times installers don't take care of installing dependencies, we have to either do that by hand, or we have to hope that we have a package manager that points out that there's a dependency that's missing. For example, the. NET runtime, if we pulled out a. NET application and we don't have the. NET runtime, that. NET application won't work. And then sometimes those dependencies have dependencies that are needed as well. For example,. NET Core on a Mac machine needs an updated version of open SSL, so that's another transitive dependency that we have to worry about. And, of course, when it comes to running the app, do we trust it? After all, the traditional model of running an application on our computer means it has access to everything, our drives, network drives, our local network, other applications running on our computer. And, of course, one of the biggest problems that we have is just dealing with breaking changes. Perhaps an update comes out to the OS that breaks something in our application, or perhaps we used some shared library that's updated. For example, in the past, the. NET framework had in place updates at times that could break functionality that our application depends upon.

Containers Are About Software Not Virtualization
Obviously there are many challenges running software, but what exactly does that have to do with Docker? Well, turns out that most of the time when people hear about Docker, they hear about it in the context of an alternative to virtual machines. If I just put Docker into a Google images search here, right away you can see in the top results that there's a comparison here of containers versus VMs. And, of course, these discussions lead people to believe that Docker and containers are some sort of replacement or alternative virtualization, but it's not. While containers can be an alternative to virtual machines, they're not about virtualization, and I think this analogy or this comparison can be quite misleading. Containers are really about software at the end of the day. Some people say software delivery, but I think that we can just say overall that containers are about running software. And if we come from this angle and we start to think about things in terms of the challenges that we have operating software, it's much easier to understand how Docker and containers work. So, as you work through this course and you're trying to understand these new concepts, if you ever struggle with them, try to come back to the simple basis of running software and find an analogy that can help you out. For example, we'll talk about images with containers. Well, an image is basically just a packaged up application, so it's a lot like the zip file here that we download when we want to use a piece of software. So let's dig into how Docker and containers are related to software, and let's learn a little bit more about containers.

Installing and Running MongoDB as a Traditional Application
So in order to understand a little more about this thing called containers, let's step back and let's take a piece of software, MongoDB in this example, and let's just walk through the process of getting it installed locally. Now I picked it because it's a pretty simple installation process. Nonetheless, it showcases the steps that we need to take to perform this manual installation. And then we're going to relate that to running MongoDB inside of a container, and then we'll be able to compare the two processes to learn a little bit more about containers. Now, I don't expect you to follow along just yet. This module is conceptual. In the next module I'll help you get this up and running on your local computer, I just want you to soak in and start thinking about the differences between installing this with and without a container. So the very first thing I might do is hop over to Google and I might search for MongoDB so I can find where it is that I can go to get MongoDB. And, of course, I come to MongoDB. com then, and then in the upper right corner I see a link to download. So I can click on that then, I can then scroll down here, and I see some choices that I'll have, so I'll use a community server version, though. There are other versions of MongoDB, the enterprise server, and I'm on Windows right now, so I need to pick that; otherwise I might need to pick Linux, OS, or another operating system, and then here I can choose what version of MongoDB I'd like to download, in which case I'll start with the 64- bit version with SSL support. So I can then grab this MSI, and then save that, and while that's downloading, I noticed a link that I might want to look at on the previous page, so I'll go back to that. There's a link with installation instructions here. So I might want to refer to this to find out a little bit more, and if you scroll through here, you'll see discussion of 64-bit versions, I believe they'll talk about the old 32-bit version, and there are some steps in here that we can take to set up MongoDB. And I believe there's a mean of downloading a zip file and just running MongoDB in the extracted folder that comes out of the zip file, but I believe this installer, then, this MSI, probably does a little bit more for us. So let's run this and see what happens. And, of course, we walk through and click Next without reading anything. Looks like I have some choices. Let's choose Custom to learn a little bit about what's going on here and expand out this tree here. Looks like there are some choices for the server and client components, and some other tools that I might like to use. I'll just leave everything checked for now. One thing I'm really curious about, because I've never used this installer with MongoDB, I've always done this via the zip file, I'm wondering if this sets up for the service for you as well, that's always something that I've had to set up by hand in the past, so let's just click Next here and find out what happens and install. Of course, we can go get our coffee now. This'll probably take a little bit of time. I'll pause the recording until this is done. Surprisingly fast, actually. I'll click Finish then, and so far I haven't needed any of these instructions, so I'll just minimize this and forget about that. And let's see what happened to our system, because this is that piece where we don't always know what exactly got installed. I'm curious if there's a service for MongoDB, because I know that that's something you typically use, unless you're running it for testing purposes. I'm not really seeing a service in here, though. So how about we hop back over to the documentation, and let's take a look at what it takes here to actually get this server up and running. So there's a section now where I need to set up the MongoDB environment, and it wants me to use this path here, so this is the default installation path. Let's go check that out. So inside of this location, we have Mongo the client utility and MongoD the server utility. If I look up a directory, looks like that's about it, so the bin folder. So this actually isn't much different than installing this from the zip file that I'm used to in the past; looks like just a server component is in here, and then 3. 2, and then bin, so there's the client there. Okay, so now I understand a little bit more about what's going on. So I have to come back over to the docks here and it wants me to make this directory for \data\db. Of course, this is a bit weird. This is not a Windows-like syntax for a path, there's no drive letter in front of this path, so I'm really confused about where it might want this default location at. How about we hop back over, and let's open up a Command Prompt here, and let's just run the MongoDB process and see what happens. Maybe it'll tell us where it wants things at. Okay, so the Mongo server process exited, and the reason is because there's no data directory, and here we go, it is C:\data\dn, so let's go set that up quickly. So we can make that directory \data\db. Just so happens you don't have to specify a drive letter to be able to create a directory. But that's something that most users don't necessarily know. Okay, so now let's clear out the screen here, and let's try running MongoDB again. Okay, this time it's asking us for access to open up a port, so we'll go ahead and allow access. And it looks like our database is waiting for connections on port 27017, which is the default MongoDB port, so let's try and connect to this. I'll open up another command window, and of course, I'll change into the directory where we installed at. You can see the mongod. exe client in this folder. Oh, and by the way, if we want to hop back over to the documentation and follow along here, we're done with step one now, step two is starting the database server, we did that already. Step three is connecting, so here is the instruction. We just need to run mongo. exe. So, mongo. exe. Okay. Looks like we have something here that's working. I could perform some MongoDB command that I know of, like showing the databases. And you can see we only have a local database, so nothing too exciting here, but we've set up MongoDb on our computer now, we've done that by downloading a copy of it, running the msi installer, and then performing a few steps to get the server up and running, and then using the client here. So this application actually has two applications inside of it, both the server and the client component. Now let's compare doing this with a container with Docker instead.

Finding Software on Docker Hub
Now let's see how we can run MongoDB in a Docker container. So, I might come out in this case then and type in mongodb and docker, and taken right away to this page on the Docker hub, hub. docker. com/_/mongo. This is how I can get access to MongoDB to be able to run it inside of a Docker container, and you can see this is an official repository, which means that Docker Inc has blessed this repository as a good way to run MongoDB. So let's go ahead and take a look at using this to spin up a local MongoDB server. If I scroll down here, I'll see some instructions, and there's quite a bit to this, and of course, there's going to be a learning curve here. But at some point down below you'll see some instructions about how to use this MongoDB image. But let's step back and let's compare what we're doing here to our process that we looked at before. So, thus far, we found MongoDB, and this time instead of going to the standalone MongoDB website, we are now looking at Docker Hub. So Docker Hub is how we discover software with Docker. Next we want to get a copy of MongoDB from the Docker Hub, and when we do that, we need to download something just like with normal software, but instead of downloading software, we pull software, and to do that we use docker pull. It's basically the same thing, you could think of it as docker download. That brings something down to our computer, but is not a zip file, it's not an MSI like we saw with the MongoDB installer from the MongoDB site, instead it's what's known as an image. So we get this image on our computer, basically you can just think of that as our application, the image contains our application, and anything else that comes with it, so all those dependencies as well. It's just a special package for applications. And then at some point we'll want to use that image, but before we get to that point, before we run our app in a container, let's go ahead and do this docker pull thing and get our image downloaded.

Downloading Software with docker pull
Let's hop over to the command-line, and let's go ahead and download a copy of MongoDB, and in this case, I'm also specifying a tag here of Windows Server Core so I download a version of MongoDB that runs in a Windows container. We'll talk more about that later. So I use docker pull to pull this down, and you'll see a bunch of output here talking about downloading and extracting our image. So give this a moment. This can take awhile the very first time you pull this because of some of the dependencies that this image has. We'll talk more about that later as well. So don't be surprised if it does take awhile to pull this down. Okay, that's done now. You'll see pull complete on a bunch of different items here with these weird identifiers. And you'll see in the output that we successfully downloaded an image from MongoDB. This is quite a bit like downloading a zip file, or an MSI to install software. So now that we have that image on our computer, we want to use it to run MongoDB, and to do that, we need to take that image and run it somehow. And running it involves a container. Now normally when we're installing software, we would extract the zip file to install the software onto our computer somewhere, so we would extract the zip file or maybe run the MSI installer, in the case of MongoDB we ran that installer and it created that directory for us with the contents of the installer, and then we could use that to then run MongoDB. The equivalent idea to installing software when it comes to containers is actually a container. A container is a lot like installed software. A better way to think of it might be as a stopped container, is like installed software, but a container just like installing software oftentimes involves setting up some files on disk, perhaps creating some config files, getting everything ready to be able to run our application. And of course, the next thing we can do then is run our installed software. In the case of MongoDB earlier, we ran the exe for the MongoDB server. Well, the equivalent term there over in Docker land is a running container. So let's go ahead and run a container with MongoDB in it.

Running Software in a Container with docker run
So when we're ready to take an image and run it as a container to use the software that we downloaded, there's only one command that we need to know, and that's that docker run command that we saw a bit ago on the Docker Hub instructions for MongoDB. The secret to all of this is that actually docker run is one command that can do everything I have on the screen here. I just wanted to break things up by steps so you could see a little bit more of what's going on and relate it back to the traditional process with installing software. But at the end of the day, we could have used docker run, which will also do a docker pull to download the software if we don't have it already. So let's see docker run in action to spin up a container, so to create a container first, which is like installing software, and then run that container, which is just like running an exe. So I'm back over on my server here, I can clear out the screen now, and I can use docker run, and I can specify mongo, and then I specify the tag windowsservercore for a Windows container, and then I can go ahead and kick that off. And hey, look at that, we've got the same output we had when we installed this with the msi installer, so we have our waiting for connections on port 27017, so our MongoDB server is up and running. Of course, this window is now locked up with the output of MongoDB, I'm attached to that running process for MongoDB right now, that's called attached to a container in the Docker parlance. So I'll need to open up a new Command Prompt. And now to connect up, I'm going to run this command right here, and we'll talk more about what this does later on. I just want to show you that we can use the Mongo client, which is the last option there with Mongo you can see on the end of that command. We can use this to connect to our server, and I can show the dbs here. And you can see we've got our local database, just like when we had this installed locally. And if I hop back over to the window for the server, down at the bottom you can see that a connection was accepted. For now, what I just did with docker exec to run the Mongo client to connect to the Mongo server, you could just think of that as running the program that we installed two separate times, one is running the server, and one is running the client. And you could imagine I could run more instances of a client, I could even run more instances of the server if I wanted to. Once we've got an application installed, we can launch it several times.

A Running Container Is Just a Running Application
These analogies are so important to helping you understand Docker that I've put them together in there just for the course, which by the way, you can access via this shortened URL. If you scroll down here, you'll see an analogies file, and I've outlined that traditional process for installing software manually that we just ran through with MongoDB first via the MongoDB website, and then I also outlined how that relates to what we just did with Docker to run MongoDB inside of a container. And I want to reiterate that we find software via Docker Hub. We then use docker pull to download the software, and it comes down as an image, and that image is just like a zip file, that's what I think of it. It's just an application packaging format. And then instead of installing the software, extracting a zip, or running an MSI, we create a container instead. Now this was done transparently for us by Docker when we ran docker run. You could also use docker create if you wanted to manually do just this step in the process. And, of course, when we use docker run we also then ran the container that we created, which means we then ran our MongoDB server. So that's just like running an executable. In fact, it's actually the exact same thing, because under the covers, even though we're using a container, we are running the exact same MongoDB binary, the exact same MongoDB exe, and we'll see that in a moment. I left one more little analogy down here about docker exec. Remember I said that that's kind of like running multiple copies of an application, either multiple copies of the same or multiple copies of different applications, all inside of this thing called a container. Now, I just said that when we ran MongoDB, the server component in the container, even the client component, I just said that it's exactly the same thing that we did at the command-line when we installed it the traditional approach. We're still running the exact same binaries at the end of the day. I want to show you that. So I'm back over on my computer where I ran MongoDB, here's the client connected still. We still have the server up as well. I'm going to open up another window here. Okay, I went ahead and split the screen out here. In the upper left we have the client connection, you can see my show dbs command. In the lower left we have the server. In fact, you can see docker run mongo:windowsservercore here, so this is the server down here, and then up in the title up here you can see docker exec, so that's our Mongo client. And then over on the right side, I have a new Command Prompt opened up, and inside of here I'm going to do something really freaky. I want to use docker exec again. Same exact command, except instead of Mongo, so you can see on the end I ran Mongo here, same exact command, I'm going to put PowerShell on the end here. When I brought down this MongoDB container, it also brought down a copy of PowerShell inside of it, and this is something we'll get into later on. But there's more applications in there than just MongoDB. It's really, really cool. So it's kind of like we just installed almost everything that would be on a traditional Windows server installation. Again, we'll get into more of that later on. The important thing is, I can run PowerShell inside of that container as well, just like the Mongo client, just like the Mongo server. I can run as many applications and processes in there as I want to, and it's exactly the same as running PowerShell locally. But what's really neat, the reason I did this, is because I want to run Get-Process inside of here, which is a command that'll show you the processes that are running. Let's take a look at what we have here. Now, this is a really short list. I might maximize this temporarily so you can see all of this. So we have our mongo client right here, and we also have our mongod server, so this is the exact same thing that ran at the command-line, the exact same binaries. And we also have some other components in here as well, these are Windows services, and then we also have PowerShell. What we're looking at right now with Get-Process are all the processes that are running inside of this container that we've created. So, you could deduce from this that a container is something of a process namespace, because I'm not seeing the rest of my processes on my regular system here. I have tons of processes on a typical Windows server, and they're not showing here. I can if I want to open up the processes on this machine here and go to the Details tab, and you'll see there are just tons and tons of processes running here. However, I want to point this out to you as well, take a look at these. I didn't run these locally, but look at this, we actually have mongo and mongod available here inside of the traditional Task Manager here on our computer. In fact, you'll see something called a job id, and this case, job id 172 seems interesting, I'm going to put a box around all of the processes that are marked with jobid 172. There's powershell, and then we have mongod for the server and we also have mongo the client that we're running. This red box outlines the exact same process as we see over here when we ran PowerShell inside of the container. The important thing to note here is we're actually running the same exes, the same binaries at the end of the day. And they're actually running on this computer as a regular application. So the thing I want you to take away from this clip is that an application that we've launched via a container, so a running container, is absolutely the same thing as if we had just launched that application traditionally by loading the. exe. At the end of the day we're still just dealing with software, we're still just dealing with applications. We just have a special new way to work with them, and that special new way to work with these applications is called containers. The tldr here is that a running container contains one or more running applications.

Stopping Apps in a Container Stops the Container
So let me show you what happens when I start killing things. So I'm going to kill, and I'll split the screen here first, so watch job id 172. First I'm going to come over and kill PowerShell, so that's this process right here, watch that process. Boom. You see it go away? Okay. And then I also had the client opened up. I could kill that, but I want to actually go and kill the server. So I'm down here in the server down below, docker run monto:windowsservercore, that's the server process here. I've got a question for you. What do you think is going to happen when I kill this? So what do you think will happen when I just Ctrl+C and kill the MongoDB server? Let's find out. So that was actually pretty interesting, nothing really happened to our processes. Ctrl+C, in this case, just seems to have detached me from that running container. So let's blow up a little bit bigger window here, and let's run a command called docker ps, which will look at running processes, and you can see here that we've got our mongod server here is still running, this is what this is telling us, this is a lot like that get process command that we just ran with PowerShell. I'll use another command called docker stop here, I'll specify this ID that you can see 27ef, I'll just start typing 27 here, that's enough. And then I'm going to split the screen again here, so I'm going back to the split screen here. I want you to watch the processes on the right-hand side. And now let's run this command. Did you see everything with 172 disappeared? So with the docker stop command, it's kind of opposite of the docker run command, it stops the running software. And you could see that, all those processes are now gone. Our client connection up above is gone as well. In fact, it looks like that Window's frozen.

Why Docker and Containers?
So the natural question is why? Why do we need a new way to manage software? Do you remember these lists? Let's quickly revisit these. When it comes to finding software, we have a variety of different ways to go about it, but oftentimes we're stuck going to standalone websites. With Docker, we have one simple place to go, Docker Hub, to be able to find the software that we'd like to use. And we can search for this software. For example, if I want to run IIS, I could search for that. And look at that, Microsoft has a repository, repositories hold images by the way, and this repository in this case has IIS in it. If I wanted to run maybe Apache, I could find that out here as well. Just about anything you can think of, if it's popular software, you are going to find a repository out here on Docker Hub that holds images that you can download to then run that software. I also mentioned that it's nice to have metadata and stats about software, and also it's nice to be able to trust software. Well, we've got this information, we have stats here about usage and how many people like the software with stars. If we click into a particular repository, we have documentation here, oftentimes giving us instructions we can run to use the software. And when it comes to trust, well, we have some official images, like we have these images from Microsoft or IIS, or we have this official image for Mongo that Docker Inc stands behind. And if you go to the trouble of logging in with a free account, and you go back to the MongoDB repository, for example, and click the Tags tab here, you can scan through a list of the results of scanning these images. So there's a security scanning service that's provided by Docker Cloud, which is an enterprise offering, but you can see these results out on Docker Hub for free. This won't work for every single image out on Docker Hub, but many of the official ones have this information, and you can go and find out what vulnerabilities have been found. And one of the really neat things about these images, especially the official ones, Docker has partnered with the people that develop the software, so in this case they've partnered with MongoDB to make sure that this image is the best way to be running MongoDB in a container, and they work together to make this an awesome image to provide awesome documentation and to make sure that these images are safe to use. And when there are problems, the people that create the software MongoDB can fix them, and you can rely upon a trusted image instead of needing to build this yourself, especially when it's an official repository. And, of course, we have one site with Docker Hub, a site we could actually mirror, where we could get all of our downloads for any software that we want. We don't have to go all over the place to download software. So we have one site to worry about with regards to availability, and when it comes to payments, well, Docker Store, which you can see the advertisement for up here, is a new way to browse through images and not just download free images, but also be able to pay for commercial software. When it comes to installing software, Docker and containers completely takes the mystery out of the process. As you saw here, we just download that image, and then all of a sudden we can run it. So the installation process was dead simple, and it's the exact same installation process regardless which piece of software you pull down. You no longer need to worry about whether or not software is cross-platform. You will need to worry about Linux versus Windows containers, we'll get into that more later on, but by and large, since you can run both Linux containers and Windows containers on Windows as we'll see shortly, you can run pretty much any software on your computer now, you don't need to worry about this. And you also don't need to worry about what format it comes in, that's all taken care of inside of the image for you. If you're curious, you can look at how that image was built, as we'll see in this course, but you don't have to. And you don't have to worry about the format. And of course the installation process, like I said, is exactly the same, it's just docker pull and then docker run. Actually it can be just docker run, it'll do all of it for you. There's no more manual installation or Package Mangers being out of date. And if you want to know what was installed, you can take a look at what's inside the images. So if I hop back here and go back to Docker Hub, I can scroll down here and I can take a look at what's called a Docker file, which for right now, the best way to think of this is just a script to create the image that we're pulling down, and you can see exactly what was run to create an image. It's just like a regular command-line bash script that you might have used in this case. Or if I go back, that was the Linux container version, I can scroll down here, and I can find one of the Windows Server Core Docker files to see how this was built as a Windows container. PowerShell commands, this is stuff I can understand. So, when I want to know what's going on, I have one place to go to figure it out. And when it comes to updating and uninstalling, Docker has a remove command that we'll see later on, that's the equivalent of uninstalling software with Docker, just docker rm, and it's all removed from your computer. Aside from the image, the image is still there in case you want to install and run it again. And when it comes to updates, well, you just pull down a new images. Images are tagged. So if I go back here to Docker Hub, if I click on the Tags here, you'll see there are various different versions, 3. 3, 3, 3. 0, so I can grab just a new image, I can actually specify the version when I'm pulling down an image, to pull down a particular version of software, and what's really neat is this is consistent across every piece of software you can pull down. You saw a bit ago with the FFmpeg site how ridiculous it was to try and find an old version of FFmpeg, well, now it would just be specifying the appropriate tag. Of course, I'd have to figure out what tag to use, but I wouldn't have to jump through some arcane website and then into I think another arcane website to find some package just for an old version of ack os, it would all be here on Docker Hub for me. When it comes to configuration management tools, you won't need those as much anymore with Docker. That Docker file we looked at a moment ago, it was a simple script, PowerShell, or Bash, or whatever it might be that you'd like to use, it's just a simple shell script to install software to build the image, as we'll see later in this course. So you won't have to rely upon obtuse configuration management tools to set up most of your infrastructure anymore. And, of course, suffice it to say I think you can understand that when it comes to running software, all of these problems vaporize as well. Perhaps one we didn't really talk about here is security and sandboxing, we will take a look at that a lot more, because Docker provides this thing called a container, and containers typically come with a lot of isolation, so much isolation that we don't really have to worry about what other software is doing on our computer, it can't affect the software that we're running inside a container, including libraries, so library updates aren't going to be a problem anymore. Where to run the software, where is it at, is it in a PATH, is it a service, how do we start it and stop it, docker run is the answer to all of those questions. So I hope you can see that Docker is a unique take on managing software, and there are many benefits that you've already seen, and there are a ton more that you're going to see throughout this course. I just wanted to touch on these now so you can start to see the reason why we would have a new tool to do all of this. Now let's get into how you can get your hands on this tool. Next let's take a look at how to install Docker. Join me in the next module for that.

Installing Docker for Windows
Module Intro
Now that we have a framework for thinking about containers in terms of software management, and in terms of just running applications on our computers, let's in this module help you get up and running with Docker, so that you can experiment with some of this and take a peek under the covers and learn some more yourself, so in this module, we'll take a look at installing Docker on Windows.

Installing Docker for Windows on Windows 10
The very first thing we're going to take a look at as far as installation is concerned is setting up a tool called Docker for Windows. You might notice there's also an option called Docker Toolbox. That's more of a Legacy set of tooling, though it's still perfectly good for a lot of scenarios. But Docker for Windows is the latest incarnation of how you can use Docker on your Windows computers. It's also worth noting that this is mostly meant for your desktop or workstation environments, it's not really meant for a server environment. We'll get into the server environment a little later. Okay, let's take a look at some of the requirements for Docker for Windows. First off, you need to be running a copy of Windows 10 or newer. And even though this says that you can use the November update, the second build of Windows, I'd strongly recommend that you use the Anniversary update, otherwise you won't be able to run Windows containers, you'll only be able to run Linux containers. And then in addition to that, the Hyper-V package or feature must be added to that Windows 10 computer, because Docker for Windows uses Hyper-V to be able to run Linux containers, as we'll see here in a bit. And interestingly, it also uses Hyper-V for Windows containers. We'll get into more of that later on. So this is one really neat thing about Docker for Windows that we're running out of Windows 10 desktop computer, we can run both Windows containers and Linux containers. I know we haven't got into those yet, but I think you at least get the point that that means we can run both Windows software and Linux software on our Windows computer. That's pretty darn cool. In order to run Hyper-V, you'll also need hardware virtualization, and it needs to be enabled on your computer. And to find out if you have this, you can come into Task Manager and go to the Performance tab, and then down in the lower right, you should see virtualization enabled if everything's okay. If this is not the case, you'll need to enable this, this is something you'll need to enable in your bios, and you'll have to find instruction specific to your hardware. I will say, though, that most modern CPUs support this functionality, so you should be okay, you just might have to turn it on. The next thing I would recommend, go into Windows Update and run all updates that you can find. Make sure your computer is completely up-to-date and not out of date, otherwise you'll probably have some trouble. Once that's done, then you can go about getting Docker for Windows onto your computer. And, of course, there's a nice guide out here on Docker. com, as well as some help troubleshooting any problems you have, so refer to this if you get into any trouble, but let's go about the process here of installing Docker for Windows, and first off, that requires that you download Docker for Windows, obviously. So this is the one piece of software that we'll have to go about getting the traditional way, because well, at least right now, we can't pull Docker with Docker. Right now you'll have two choices to download, there's a stable channel, and then there's also a beta channel. The beta channel has the latest experimental features enabled, whereas the stable channel is, well, it's exactly that, it's the last stable release of Docker, and then that last stable release of the Docker engine is bundled up inside of Docker for Windows, and you can get that here. In this course, I will be using the Docker for Windows beta, because you need the beta right now to be able to use Windows containers. Docker for Windows when it first came only had support for Linux containers, because Windows containers at that time were not yet done, they were still a work in progress, but now that Windows 10 Anniversary Edition came out, we have containers inside of Windows now. So in this case, if you're following along, grab the beta, and then we can go ahead and run that. By the way, take note that at the time of recording, I'm using Beta 29. 3, so you'll need to use a newer version, whether or not that's the stable version or the beta, that doesn't matter so long as it's 29 or greater. I'll minimize that window there, and we can focus on the installer here. Accept the terms and then choose to install. Grant that permission, and then wait a few moments. Once the installer completes, you can go ahead and launch Docker then. Join me in the next video where we take a look at that.

What Docker for Windows Installs
If you don't have Hyper-V enabled, the Docker for Windows application will detect this the first time that it starts up and it's going to recommend that you do this; otherwise, you won't be able to use Docker. So, go ahead and click OK. If you want, you could also enable this via PowerShell with the enable Windows optional feature commandlet, but we'll let Docker do this for us. Click OK then, and in a moment your computer will reboot. If you want to check the progress of what's going on, there's a little tray icon that was installed with Docker for Windows. And right now it's busy setting up that Hyper-V feature, and then in a moment we'll reboot here. Okay, my computer is rebooting now. And then when it comes back on, it'll go through the process of installing that Hyper-V feature. That'll take a moment, and then it's going to reboot again. And then when your computer comes back up, Docker for Windows will start up as a service. Again, we have this tray icon to interact with Docker, that'll eventually come up here. And then it's going to go through an initialization process to set up your computer for the very first time. And one of the things it's going to do is set up a Hyper-V VM that contains a version of Linux on it, so that we can run Linux containers. So, while Docker for Windows is starting up, and you can see the icon doing that, let's come and open up the Hyper-V Manager, the management tools were installed as well. Docker for Windows took care of adding those as well, so we can come in and take a look at our computer here, and we can see that there's a virtual machine, and it's running right now, so Docker for Windows is setting up this virtual machine, so take a little bit of time the very first time you run Docker for Windows to get this created. While this is creating, let's peek around and see a few other things that were set up by Docker. So first off, if you open up a Command Prompt, you should have a docker command now, this is the Docker cli. There's a whole bunch of subcommands. This is what I was using the last module to manage the container examples I was showing you. If you take a look at where this is at, so if you can find the location, and then we go ahead and open that location up, you'll see when we open that location we have a few other tools as well, we have docker-compose and docker-machine. We'll take a look at docker-compose later on. So these are other client tools that we have to interact with Docker, and if you go up a bit here, you'll eventually find some of the server components, for example, the dockerd service. All right, let's go see what's going on here. Let's take a look at the tray icon, and you can see that Docker is now running, so that's a good thing. That means that we can come back to the command-line, clear this out, and we should be able to run some commands to get some information about what exactly was set up with Docker, and one of the commands that we can run is docker info. This will give us a whole bunch of information about our Docker server. This is a great place to come to understand what exactly was set up when you're working with Docker, because as you move between different environments, there are different drivers, for example, that will come into play. So let's see what we have set up here. First off we have the server version set to 1. 12. 3. We don't have any containers running right now, and we don't have any images, and that makes sense, we just set things up. I've got a question for you. Can you figure out for our server, for our Docker server, what operating system is being used? Well, if you look down below you'll see that there's an OSType and this is set to Linux, and that's interesting. And that's because Docker for Windows, when you first install it, it will work with Linux containers, because that's the first type of container that it has worked with in the past, and that's what that moby linux vm was set up for, so that we could interact with a Linux Docker host here, a Linux Docker server, which is the server that we're getting this information from right now, and it's running a distribution called Alpine Linux. And you can take a look at various other bits and pieces of information about this server. In addition to getting server information, we can also use docker version to get some information about our client as well, and you can see that our client is also using version 1. 12. 3. So our client and our server are using the same version of Docker; however, our client you'll notice is running on Windows, and that makes sense, because our machine here is Windows. So, the client that we're talking about here is this docker command that we're running, and then the server, well, that's running over inside of the vm, so that's why that is Linux then and is different from our Windows client. And you'll also notice in this case that the client and the server are both marked as experimental, and that's because I grabbed that beta channel. If you had grabbed the stable channel, that would be false. If you come down to the tray here and click on the icon here, you can right-click and choose About Docker, and this will tell you about the version of Docker for Windows that you're running, and Docker for Windows is a wrapper around the client and the server that we just saw. It manages everything for our Windows environment here on our Windows 10 machine, and I'm running beta29. 3 at this point in time. Maybe one last thing we could take a look at, we could take a look at our processes on our computer, we can open up the Task Manager with Ctrl+Shift and Esc, go into the Detailed view, hop over to the Details tab, and inside of here you'll notice some processes, com. docker. whatever, there's a host of running applications here that comprise Docker for Windows. These allow us to communicate then with that moby Linux VM, transparently, almost as if we're able to run Linux containers on our local computer. And well, let's take a look at that next, let's take a look at running a Linux container.

Running Linux Containers on Windows
So now that we have Docker on our computer, let's use it to run some software, because that's what Docker helps us do. So, if you come to the command-line, and you run docker ps, this is a command that will show you running processes inside of Docker. That's how I like to think of it, that's what the PS comes from, the PS comes from the Linux Process Command. It's short for process status, hence, PS. And Docker uses the same command to show you running containers, which again, containers you can just think of as running applications, so this is like our list of running applications via Docker. And obviously we have none at this point, so let's take a look at setting one up. One of the canonical examples to start with is the hello-world image, so let's take a look at using this. I did want to point out in the docks here that normally you do get this dialog that pops up the very first time you install Docker for Windows. I haven't see this in the latest version. It might be a bug, or maybe it was removed, but just a heads up that you might see this as well to let you know that Docker is up and running for the first time. So we can come over and we can run docker run and then hello-world. Now I've got a question for you. Do you remember what this part of the command is? What is this? This is the image, so this is the image that we would like to download. We could have pulled this first with just docker pull. And then this is the image that we'll then take and use to create a container and run that container. So let's go ahead and kick this off here. And I want you to watch the output the very first time this runs. You'll see that we can't find the image locally, that's the Docker Engine reporting that back. The Docker Client here is connected up to the Docker Engine right now, and it's dumping out information from the Docker Engine. You can see that we pulled down this hello-world image. In the output then you'll see eventually that the pull is complete, so that's pulling the image in this case. And then down below, we have some output here, and this is what this hello-world container does, it's just a simple little process that runs and then exits right away. We could run this again then with docker run, and we get similar output, though notice this time right after we run the command, there's no output about downloading the image, because we already have it. Perhaps something a bit more interesting, though, would be a web server. So, docker run, and we can then plug in some web server, perhaps nginx. So I'm referencing the nginx image here, and then, since it's a web server and there's a port involved, I need to specify how I want that port mapping to work. So I'll map this port to 80 on the host to 80 in the container. I could change this to 8080 on the host if I want. I'll leave it at 80, though. So, nginx will be listening on port 80 inside the container, and we'll map that out to port 80 on our host machine, as if we had this nginx process running on our Windows computer, even though it's actually running in a Linux environment inside of a Linux container. It's all transparent and done for you with Docker for Windows, so that's pretty cool. Okay, so we can go ahead and fire this off then. Again, we can't find the image locally, so we need to pull it down, and this time you'll see there are three separate downloads that are happening. We have three separate items here. One of them is already complete, and the other two are running. I'll pause the recording until I have those downloaded. Now once a download's complete, then an extraction process happens. So we take that image that's downloaded, it's downloaded in a compressed format to obviously say bandwidth. And then the image is extracted locally, so that we don't have to extract it each time we want to access it. It's a two-phase process here. And those three items there, those are actually referred to as layers, so images are layered, and we'll talk more about this here throughout this course. Once those are downloaded, though, the nginx application then is launched, and it's launched via creating a container and then running that container. Now, of course, we don't have any output here to show us this, and I just happen to know that nginx doesn't show you any output here in this default nginx image that we're running. Instead I know we can hop over to a browser, and I can put in localhost and port 80. And look at that! We've got nginx up and running. Not too exciting. It's just a default page here. I'll split the screen though and put my terminal over on the left-hand side, and I can refresh over here on the right-hand side, and you can see that we have log output coming out from nginx logging out the requests that are coming in. For example, we can see that a Windows NT 10. 0 computer has made a request.

Docker Inverts Learning
So this is a full-fledged instance of nginx that's operating as a Linux container inside of that Linux VM, and it's all transparently mapped as if it's running here on our local computer. It's almost like we have nginx running on our Windows computer here. How awesome is this! Have you ever tried to set up nginx? Have you ever noticed that even something simple like nginx can take some work to get set up? We don't have to do any of that, we don't know anything about how to set up nginx. We just run one command, docker run, let me bring that back here, so I Ctrl+C to get out of that nginx server, we just run this docker run command, and we have an nginx server up and running. This ability to quickly launch software with one command and not need to know anything about how to set it up is what I like to refer to as inverting the learning process. So, Docker and containers, they lead to inverted learning. We don't have to know how to set up software to be able to use it; we can use it before we decide if we want to learn how to set it up, which is great, because maybe we use software and find out we don't like it, and then we don't have to waste time figuring out how to use it. And then when we're ready, we have everything we need to help us understand how to set it up, and that's that Docker file we saw before, and we'll take a look at that more throughout this course, but we can see what work was done to create the image that we have then run to use the software. This is so absolutely phenomenal. Docker has literally changed how I learn. Have you ever gone to learn something, to learn a new piece of software, and spent maybe four or five hours just trying to get it up and running, and then you're out of time to learn for the day, and then you never come back to learn about that software, because it was such a hassle to set up, and you never really want to get back to it, because it seems like it's a pain? Well, now you don't have to deal with that.

Stopping Containers Means Stopping Processes
Okay, I'm back over on my Windows machine here, and don't forget that I detached from that running container, and I used Ctrl+C to do that. Normally you should use Ctrl+PQ if you want to detach. I'm used to Ctrl+C for killing containers, though it doesn't always work in every scenario, so Ctrl+PQ might be the safer bet, though that doesn't always work as well. Either way, make sure you detach from that container. And then if we take a look here at docker ps, and let me make this big here, what do you think will show when I run docker ps now? Well, in this case, we have something in our list. So I said that I detached. That means that this process, this application, is still running. So we still have our nginx application up and running. And it was started from this nginx image we can see here. We can see the specific command that was run, though we didn't provide this, this was provided by default. That's part of the container that's created, the container is kind of like an environment that specifies what to run. So we didn't specify this, this was specified inside the container, and it came actually from the image, the image contained this specification that was then carried over to the container, and then this is actually what's running behind the scenes inside of our Linux container. This was created about 26 minutes ago, and it's been up for about 15. When you create a container, each container is given a randomly generated name, unless you provide a name. And then we can also see here, this wraps around a little bit, but we have port information, so you can see the ports header here that that's referring to. And we also have something really important that's the container ID. Each container is also assigned a randomly generated ID in addition to the name, and either of these two values, the name or the ID, can be used to interact with and control this container. So what we have here in the Docker PS output says that this container is still running, still listening on port 80. I can confirm that by refreshing the website, and it's still up and running here. I've got a question for you. If I want to terminate this web server, what command can I run? As we saw in the last module, we can use docker stop, and we can use this to stop our container, which means we'll stop our application. And then I need to specify some type of identifier. Can you take a guess at what I can use here? Well, I can either use that sick_pasteur, or I can use part of the container ID, and the neat thing about these container IDs, you only have to use enough of it for it to be unique here. So, bdf is enough here to be unique. If that stops successfully, you'll get the portion of the IDE that you specified here, and that's only because you could kill multiple at a time, and it's telling you which ones have been stopped. And then if I run docker ps now, what do you think will show? Well, in this case we get nothing back. And if I flip over to the browser, what do you think will happen when I reload here? Well, now in this case, our browser's just going to spin around until it times out, because the nginx server is no longer up and running. So we've stopped our server. Join me in the next video where I show you how to start the application back up.

Restarting Containers and docker ps -a
So if I want to start my nginx server back up, what do you think I need to do? Well, this is actually a trick question. There are a couple of ways we could approach this. You might be thinking, well, we should probably do a docker run again with nginx, and do the port finding and whatnot. But we don't need to do that, because if I run docker ps and -a, I can see here that my nginx container still exists. So this bd and nginx, and also I have my containers here when I was running that hello-world application, they still exist as well. Just like when you install software onto your computer, when you started up and then when you stop it, that doesn't uninstall it, so same thing with a container. When you create a container, you can start it and stop it, but that doesn't actually destroy or uninstall the container. So the container is still here, so we don't need to do anything to be able to use it again, except start it back up. Now I've got a question for you. What do you think we can do to start that nginx container back up? Take a guess at the command. Well, in this case, it's docker start, and then we can just specify part of the identifier. Or I could type out that whole sick_pasteur, that's a container name. Now I've got a question for you. How do you think I could check to see if that container is now running? Well, there a couple of ways I could approach this, and one of those is I could hop over to the browser and just refresh here and see if my site comes back up. By the way, you might want to force a refresh of your cache to get your browser to recognize this site as back up and running, and you use Ctrl+R and Edge here, otherwise that might look like it's still down. And then I can come back over to the command-line, the other way I could look at this is with just docker ps. And now we can see that our container is up and running, and specifically we can take a look at the status here over on the right-hand side that has been up for 56 seconds. So there's a difference here between docker ps and docker ps with a -a. -a shows us everything, both started and stopped containers. Without the -a, we just see started or running containers.

Removing Containers Is Akin to Uninstalling Software
When we remove a container, what are we doing in terms of traditional software management? Do you remember when I said that installing software traditionally is like creating a container? Well, then the reverse process, destroying the container, is like uninstalling software. Let's take a look at this. So I'm back over here at the command-line. I'm going to clear out the screen. And then I'm going to run a docker ps -a, just to show everything. And we have three containers right now. You could think of this as having three pieces of software installed on our computer. And in the case of the hello-world container, it's kind of like having two copies of it installed. That's perfectly reasonable. We could have downloaded a zip file, for example, and extracted it into multiple locations on our computer, so that's kind of like what it would be like to have multiple copies of software installed traditionally. Okay, so before we actually get rid of the nginx container, let's take a look at our images. So, docker images is a command that we can use to list the images that we've downloaded. Remember, images are just application packages that we download, a lot like a zip file. So, images are separate from containers. Containers are the installed applications. I want you to see, though, that when we're done we'll still have the image around, and that doesn't mean that the software's installed, it just means that we have the installer still on our computer, as if we had left it in our downloads folder. So first we need to stop our container before we can get rid of it, so bdf here. It's kind of like stopping software before we try and uninstall it. And if I run a ps -a again, you'll see now that all of my containers are exited. I can then use the docker rm command to remove or destroy a container, and then I can specify bdf here in this case. Again, that's coming from the container ID. I could also be using the name of the container as well. Maybe before I do this, I'll show you something here. If I remove the f on this, this isn't going to work, because I have two containers that start with bd, I have this one, my old hello-world one, and I also have my nginx one. Do you see how they both start with bd? So if I just specify bd, that's not enough to uniquely identify a container, so I need to specify the f on the end there to actually uniquely identify the container that I want. Now if I run a docker ps -a, what do you think will come up in the output? So you can see now we only have two containers. I can go ahead and remove the rest of these. If I want to remove the rest of these, what do you think I need to type in after the rm command? Well, I need to identify the IDs here, so I'll do c99 for that first one, and then I'll use amazing_bassi for the second one. I can mix and match the identifiers that I use, and you can see that both identifiers are printed out, which means both of them have been removed now, and if I do a ps -a, you can see we have nothing, and so a docker ps would work as well, nothing coming back. So we have no containers on our computer anymore, none that are running and none that are stopped. That means we've uninstalled all of our software that we had installed from Docker. If I clear out the screen, though, I've got a question for you. What do you think will show up when I run the images command? Well, in this case, I have both pieces of software still downloaded. I have the package for them downloaded. I do not have them installed, though. So, just like when we're installing traditional software, when we remove or uninstall that software, we still might have the installer available. We'd have to remove the installer if we wanted to get rid of it. We'll take a look at that next.

Removing Images Is Akin to Deleting an Installer
So after we uninstall software, we might want to get rid of the files that we downloaded to install it, if we wanted to perhaps reclaim some disk space, so to get rid of those zip files or MSIs or whatever it might be. Well, let's take a look at what this is like when it comes to images with Docker. So, over here at the command-line, I want to get rid of these images. I have a challenge for you, though. I want you to pause the recording, and I want you to see if you can figure out what the command is that we should use to get rid of these images. All right, so in this case, if I run the Docker command, I can use that to figure out what commands are available, what sub-commands, and there are a ton of them. But there's one in particular that can do the job that we need to do, and that's the rmi command, and it's right next to the rm command. So, rm is for containers, rmi, I for images, is for images. So I can clear the screen, run docker images quick and then docker rmi, and then what do you think I need to specify here to get rid of one of those images? Well, there a couple of things I can do. There is the IMAGE ID here, and then there's also the name, which is actually the repository that we pulled the image from, so I will use the repository in one example here, and I'll use the image ID on the other one. And again, I can use just part of that image ID, just like part of container IDs. And we get a whole bunch of output here. And the reason we get multiple lines of output, images are layered, as I mentioned before, we'll get into more of that later on. We have to remove all the various layers that are involved in an image to get rid of it, so if I do a docker and then images now, now you can see we have nothing on this computer.

Running the Docker Docs in a Container
Let's take a look at one more container, and in this case, we're going to pull from an image that contains documentation for Docker, which is just a really neat use case for Docker. Now there are a couple of ways we can go about finding this. I want to show you the search sub command with Docker, this allows us to search Docker Hub from the command-line. So with this command, I could type in docs as my search parameter, and I'll get back a long list of repositories that I might want to pull from. And, of course, this is a long list of repositories, and you would need to know what exactly it is that you're looking for, but I've found a name over here on the left-hand side in advance, something I know about, it's the docs/docker. github. io image that we're looking for. We could also hop out to Docker Hub, and then I could come out here and I could search for docs as well. Now I might need to scroll down a bit to find what it is I'm looking for, and here is the exact same repository. Remember, repositories hold images that we'd like to pull. We haven't got into this very much, but a given repository could have multiple tags that link to multiple images that are hosted inside of this repository. It's kind of like a github repository, and a github repository can have multiple tags or branches that point to different versions of a project. So these tags here point to different images or different versions of the software that you might like to download, and you can see here on the Tags tab for this particular repository that we have some older versions of the documentation that we can access, or we can grab the latest version here with the latest tag. And by the way, when you don't specify the version, you get the latest, so we've been getting latest thus far. One thing I do want to point out, this particular image is rather large, it's 1 GB, so I would skip this demo if you don't have a fast internet connection and you don't want to wait. So I can copy this whole slug here, and this is a bit different than what we've been working with, it has this docs/docker. github. io. The docs/ is a username or an organization out on Docker Hub, much like a username or organization out on GitHub, and then this is one particular repository, so if I actually click on this first part here, on just the docs, you can see that this user, this docs user or docs organization, has multiple repositories with different images that I can pull from. And then once you're inside of a repository, you can pick different versions with different tags here. So that's a little bit about how that works, let's hop back over to the command-line, clear out the screen here, and we can run a docker and then run, and then in this case, we also have a port as well. This documentation is a little website that runs. And we'll map 4000, because that's the port that this application listens on. We'll just keep that the same on the host as well. And then I can just paste in the slug for the repository I want, so there's the docs organization / and then this happens to be the repository. This second piece is like nginx in the past. And then if I wanted to, I could tack on latest here. I don't have to do that, though if I don't specify the tag, it'll grab whatever is tagged as latest. Now, of course, this will take a moment, so I'll go ahead and let that download. You can see there are a ton of layers in this image, and each of those need to download and extract. So I'll pause the recording until this is done. Once the image is done downloading, all the layers are extracted, the site will be up and running on your local computer as the container is created and run here, so you can see we have some output telling us that we can access the site on port 4000. So let's hop over to the browser, and let's open up localhost port 4000. And take a look at that, we have the Docker docs here. So, think about this for a minute. If you wanted offline access to the documentation for Docker now, you could just grab a copy of this image, have it on your local computer, and then you can spin up a container anytime you want to run the site. And then on your local computer, you'll have the same docs you'd have if you went out to docs. docker. com. See? They look exactly the same.

docker run -it and --name
So we have the site up and running, and if we hop over to the command-line, in the output of running this particular container, we can see that we have this message here that says that we could press Ctrl+C to kill or stop this web server. And, of course, if you type ctrl-c, we seem to exit out of the container and we're brought back to a prompt here, but let's see what actually happened with the container. So, what do you think will happen when I run docker ps here? Well, it seems like our container is still running, because remember, containers that are running come back with docker ps here, and you can even see in the status over here that the container is still up. So it's a bit misleading the output here, and the reason for this, and I want to point this out. It's a bit nuanced, but you'll run into trouble with other containers when you're starting to learn, so I figured I'd point this out. Let's go ahead and stop the container that we have running right now, and then I'll clear out the screen here, and let's go back up to that run command that we used, except this time instead of just mapping the port, let's also specify -it. This will open up an interactive terminal into the container. And while we're at it, let's take a look at another parameter. There's a --name parameter that allows us to name a container. So let's call this docs. And let's go ahead and fire this up now. This will take a moment to regenerate the site. Okay, it's back up and running. Hop over to the browser, and reload just to make sure that we're still up and running on localhost port 4000. It looks like we are. Now if I come back to the terminal and type ctrl-c, it takes a moment there. I see the ctrl-c characters show up, and I'm returned back to a prompt now. Now when I run docker ps, you'll see that nothing shows in here. So make sure you add this -it flag if you want to run containers in the foreground like this and see the output and be able to then stop the container when you're done with it. So what's happening here is with this -it flag, when we type ctrl-c, ctrl-c is actually sent to the running process inside of that container, which is the web server, and when that primarily running process in the container is killed, then the container is stopped effectively, because remember, a running container is just a running application. I also added that name flag. Let's run this again, except I'll need to change the name. I did not get rid of the container, so I'll call this docs2. Okay, this one's back up and running. One trick that will work with quite a few containers, Ctrl+PQ, if you want to detach but you don't want to kill it, so Ctrl+PQ drops me back to a prompt here. I can run a docker ps now, and take a look at that, we've got our name here, docs2. And if I run a docker ps -a, you can see docs right here for the first container that we created, and then we have the docs2 for the second one. So, the name allows you to specify a name that you can then use to do things like docker stop and then I could specify docs2 and not need to know that obscure container ID or the randomly generated name. Okay, we've seen quite a few different containers that are Linux containers, one last thing we'll look at in this module is to take a look at a Windows container instead. So let's do that next.

Switching to Windows Containers
So let's take a look at what it's like to run a Windows container instead. So if you come down to the tray, and you find the Docker icon that's in the tray that says Docker is running, and if you right-click on this, you'll pull up a menu here, and there are a bunch of different options, you can get information about the version, check for updates, you can even configure some settings here. So there actually happens to be a whole host of settings, and we'll look at more of these throughout this course. For example, how to share drives, so how you can map volumes into a running container, how you can set up networking and proxies, there are a bunch of bits and pieces of configuration in here for Docker. In addition to that, though, if you right-click, there's an option here to switch to Windows containers. Once you click on that switch to Windows containers, the Docker for Windows application will start the process of setting up Windows containers on your computer, and one of the very first things you'll need for this is a new containers feature. So this is a lot like the Hyper-V feature, it's another feature in Windows that we need to turn on, and this is the feature that was added in Windows 10 Anniversary Edition. So if you don't have the Windows 10 Anniversary Edition, you're not going to be able to use this containers feature. And of course, you need to enable it, you could do that with PowerShell, you can let Docker for Windows do that for you. And of course, it's noting here that our computer is going to restart after enabling that feature. So we'll do that just like with Hyper-V, and then you can come watch here, kind of see what's going on. It'll take a moment. After a moment, your computer will restart, and then your computer should come back up, in a moment Docker for Windows will start back up, you'll see that tray icon again. You can see here that Docker is starting up. And this time it's starting up in the Windows container mode. And eventually Docker will be back up and running. Now, something interesting happened to me, apparently it didn't actually switch over. I did have some network connectivity issues with my VM that I'm demoing in, so that could have been it, but I'll just show you this in case you have the same problem, if it doesn't switch, click it again to switch. Once the feature is installed, you shouldn't need to reboot then. And then just keep an eye on it, make sure it actually switches over. Once you've switched over, you can see now the option here shows Switch to Linux containers instead. So, that's a great place to check to see which mode the Docker for Windows application is in. Now, while that's setting up for the very first time, if you go into Programs and Features, I'll show you where these features are at that are being turned on. Go into Turn Windows features on or off, and then inside of this list, you should see Hyper-V is now added, and the management tools as well. And then also we have the Containers feature enabled. So this is another route you can go to turn these on if you want to, those are the features that Docker for Windows is talking about, and setting up for you. Now eventually Docker will start it back up, and I hate going to this little menu here, so if you want, you can right-click and go to Customize notification icons, and go to Customize, and then in here you can scroll down, and there's an option to select what icons you want to show up all the time. I'll turn on Docker for Windows, and maybe turn off some of the other things here, and now I always get that icon right here. And it looks like things are up and running now. You can see the option for switch to Linux, so looks like Windows containers are working now, so let's take a look at this. Join me in the next video for that.

Running IIS in a Windows Container
All right, so just like with Linux containers, we can open up a Command Prompt to work with Windows containers. Let's clear the screen here and do just a simple docker ps. We have nothing, docker images, we have nothing. You'll notice that everything is gone now and that's because we've switched over to Windows containers, which are completely isolated from our Linux containers. Even if I do a ps -a, nothing's going to show up in here. And for the most part this makes sense. Windows is not the same thing as Linux, a piece of Windows software isn't going to run on a Linux environment, and a piece of Linux software isn't going to run on a Windows environment, at least not without VMs as we've seen here. So we've switched from that Moby Linux VM that we saw, if I open up Hyper-V, we have that Moby Linux VM. We're not using that anymore, because we're using Windows containers, so that environment doesn't even have to be on right now. So before all the containers that we were running were inside of this VM, so all the images that we have are inside of this VM right now. If we hacked into this VM right now, we would see all of our old images and containers. That's okay, we'll put this away for right now. Instead let's take a look at what it's like to start up our very first Windows container. So, what exactly can we run? How about we get a copy of IIS up and running? So the Windows equivalent of nginx, let's load up the IIS web server, and if I search out on Docker Hub, you can see Microsoft provides an IIS repository that has images inside of it that will allow us to run IIS. And if we look at the tags here, you can see there are some different editions of this, and you'll notice that there's a stark difference in size between these different editions. So we have a nanoserver tag that's 310 MB, and then we also have this latest tag and these windowsservercore tags that are all 4 GB. It turns out there's two separate base images that we'll get into a little later on. One is based on nanoserver, and the other is based on windowsservercore. And of course, the size is a big difference between the two. So let's start out with that nanoserver version of IIS. So if I hop over to the command-line, I can type docker run here, and now I've got a question for you. What do I need to put in here to be able to run IIS inside of nanoserver, inside of a container? Well, let's work through this. So, we have the username of Microsoft, and then we have the repository name as IIS. So let's type that in, microsoft, and then iis. Now right now as this is, if I don't put anything else in and I hit return, what is this going to run? When we hop over to the web site, you see you can have this as a reference. Which of these tags will be used if I don't put one in? Well, by default the latest tag is used if we don't specify one, so we need to specify nanoserver. Do you remember how to do that? We put a colon and then nanoserver here. Now this is a web server, so what are we missing here? What else do we need to pass to this run command? Think back to what we passed with nginx. Well, we obviously need a port to be able to communicate with that web server, so let's come back here and let's specify a port, and let's just use port 80. Actually, how about we use 81, because we used 80 for that nginx container, and you might have a cache in your browser that could confuse you. So let's use 81 on the host and map it to 80 inside of the container. So this time we have a different port on the host, that's what that first part is. So this is on the host computer, our Windows 10 machine, and then inside of the container, the application is running as if it's running on port 80, and this is really weird, but we'll talk a bit more about this in the next module. Now one other flag I'll want to pass here, I'll pass -d. So instead of the it, so that we have an interactive terminal into this running IIS web server, instead we'll use -d to detach from this container. So we're going to create a container, we're going to run that container, but we're not going to attach to it. So it'll basically run in the background, that's what you can think of this -d option as. And let's name this so we can easily interact with it. Let's call it iis, and let's go ahead and hit Return to fire this bad boy up. Now of course this will take some time to download. Fortunately, it's the nanoserver edition, so it won't be nearly as bad as downloading that 4 GB Windows Server Core version. Once all the downloads are complete, they'll extract. I'll pause the recording until that's done. All right, and then when all is said and done and IIS is up and running inside of this container, you should drop back to a Command Prompt, and then from here, we could run a docker ps and we could see that we have our container running here, our container called IIS. Now before we go take a look at this, I want you to run this command. There's another image that we're going to be working with throughout this course that's going to take some time to download. I want you to just kick this off, unless, of course, you have a really slow computer or really slow internet connection and this would interfere with other demos, go ahead and kick this off and let it run in the background. So microsoft/windowsservercore, pull that image down. This is that 4 GB base layer that that 4 GB IIS image is built on top of, so I want to get that queued up and running in the background while we take a look at other things. So, let's hop over to a browser, and let's go ahead and open up a new tab here, and take a look at localhost port 81. Unfortunately right now we're not going to be able to use localhost to be able to access IIS. However, you might have a newer version of Docker for Windows where this issue is fixed, in which case maybe this worked for you. But if this doesn't work for you, this is expected actually, so as of version beta 29, this is expected behavior here. Instead we have to find another way to get to that port 81. So first, let's see how we can access this externally, because we can access this from another computer on our network. So I can grab my IP address here, a bunch of IPs, but here's the one that matters on my Ethernet adapter here, 0. 18. I can bring that over then, and I can hop onto my Mac here. So I'm on another computer on my network, I can access that port 81 externally. So the port mapping is working, we just can't get to it from localhost on the local computer. Another option we have, and this will be a bit weird, if you don't have another computer on your network you'll want to use this, you can go to the container directly. And that's probably going to sound a little bit weird to you, so join me in the next video where we take a look at that.

Containers Have Their Own Isolated Network Adapter
Okay, so let's hop over to a new Command Prompt, and we can take a look at docker ps here. We can see that we have IIS running still. So, another route we have to access a web server inside of a container is to not rely upon these port mappings, but to actually go directly to a container, and that's a bit weird. What exactly does that mean? Well, it turns out there's a lot of isolation when it comes to running processes inside of a container. So much isolation that it actually looks like, when you run a process in a container, it looks like that process is running on a completely isolated computer. That's why a lot of people like to compare VMs and containers, because the net effect at the end of the day is somewhat similar as far as isolation is concerned. So, long story short, what you need to know for now is that the container has its own network adapter, as if it were a separate computer, and if we ping that network adapter on that IP address associated with it, then we can get into the container that way as well. So how do we find this? Well, there's a docker inspect command that allows us to pass in a container ID, so iis, for example. So again, that's the container name or the ID that you passed to this. And when I run this, I'll get a whole bunch of information about this container, but toward the bottom there will be a Networks section, and inside of there is an IP address. This is the IP address, the IPv4 address, that's been associated with the NIC that's inside of, that's unique to, this container. So I can grab that then, and I can do a couple of things with this. First off, I can just ping this. So it's almost like I'm pinging another computer here, and it's really just a virtual network adapter that's affiliated with the container. And all the applications inside that container bind to this particular network adapter with this particular IP address, so I should be able to make a request to this. So let's make a new tab here, paste this in, an go port 81, and unfortunately, that doesn't work. Can you guess why? Well, the port mapping that we set up for 81 is just on the host. The application is actually listening on port 80 inside of the container, so that's what we need to hit, and look at that, we can now access our IIS server, and we don't need to have a separate computer to do it. So we can do either approach, we can go directly to this virtual NIC inside of the container and hit the IP address for it on the container port, port 80, or we can use another computer on our network, and hopefully by the time you're watching this, there is an update out that fixes this problem so we can just come to localhost port 81. Just like we had with Linux containers, this is a Windows container specific issue. Linux containers work just fine with localhost as we've seen. But now you've learned a little bit more about containers, because now you understand that there is something interesting going on with containers beyond just running software. So thus far I've talked about containers as just a means of running software. Well, that means of running software comes with it a ton of isolation, and we'll look at more of this in the next module. Part of that isolation is network isolation.

Key Takeaways
As we wrap up this module, I want to emphasize yet again that containers and Docker are all about helping you run software. It's all about making your life better. I've got this nice table here that relates various different pieces of traditional software management to the Docker Equivalent, and then also to the Docker Command that we can use. For example, finding software, that's Docker Hub. Downloading software, that's pulling an image. Installing software, well, that's creating a container. Starting and stopping it, well, that's running the container or stopping the container. And then uninstalling software, that's equivalent to removing a container. And then my favorite of all is what's the one command that you can use to do all of this, and that's docker run. Feel free to refer back to this table. This is great way to anchor your understanding of Docker. And continue to use this relationship to help you understand the nuances and complicated parts of Docker, so when you get confused, come back to this and see if there's some way you can reason about things. For example, with Docker Exec, I came up with that analogy on the fly that Docker Exec is kind of like running another instance of the application or it's like running a different application that was also installed. So, for example, MongoDB has both a server and a client, well, we might start up the server, and that would be the container that we run, and then we use Docker Exec with that container, then also launch the MongoDB Client inside of that container. In this module we talked about using Docker for Windows. And we did that on our Windows 10 desktop computer. I said, however, that you're probably not going to use something like Docker for Windows in a server environment, it's not really meant for that. In a server environment instead you'll just run Windows containers on a Windows server, and then if you have Linux containers, you would run those on a Linux server, it's what makes sense. You wouldn't want the overhead of a VM inside of a VM just to run everything on one machine. So, in the next module we're going to turn our attention to taking a Windows server machine and getting just the Windows Docker engine set up on it, and then taking a look at how we can run just Windows containers in that environment.

Installing Docker on Windows Server
How Docker for Windows Supports Both Windows and Linux Containers
In the last module, we got Docker for Windows set up on our Windows 10 machine so that we could use both Linux containers and Windows containers. In this module, we'll take a look at a typical setup on a Windows server where we just want to install the components to be able to run Windows containers. Because, come on, let's be honest, if you have Windows containers, you'll run those on a Windows server, and if you have Linux containers in a production environment, you're going to run those on a Linux server. There's no reason to have the overhead of trying to run Linux containers on a Windows server in production or vice versa. Now this might still be confusing. What I think we needed to do is step back and take a look at the architecture of Docker for Windows a little bit and make sure you understand the components that are involved with Linux containers and then switching to Windows containers. Part of the thing we love about Docker and all the tools that are coming out of Docker Inc, they simply make our life easy. We run that MSI installer for Docker for Windows, and we have all these components out of the box that handle Linux and Windows containers for us. But when we install software and it makes our life easy, there's oftentimes magic going on behind the scenes that we might like to know about to really understand what's going on. So let's peek behind the covers here and take a look at what exactly is making things easy for us with Docker for Windows. So let's say that we've got our computer here, our Windows machine, our Windows 10 desktop computer. When we install Docker for Windows, it launches a service that eventually launches a process that is a proxy process. So behind the scenes we have this proxy process on our computer, and also when Docker for Windows first starts up, it creates that MobyLinuxVM that we saw. Now in addition to this com. docker. proxy process, there are others, but these are the important ones. And then eventually we opened up a Command Prompt, and we ran some commands with that Docker cli. For example, we started up an nginx container with docker run. When we run this and we execute this command, the Docker cli is talking through that Docker proxy and then that Docker proxy is forwarding commands into that MobyLinuxVM to spin up an nginx container, and thus we have Linux containers. And then at some point, we want to switch over and start using Windows containers instead, so we send that command via that Docker tray icon to switch to Windows containers, and behind the scenes, the Docker daemon for Windows has started up. And if you were to peek under the covers, you would see this as dockerd. exe. In fact, I did that for you, I grabbed these process snapshots of all processes that have the word Docker in them, and I grabbed the snapshot when we were running Linux containers on top, and then I grabbed the snapshot when we were running Windows containers on the bottom, and you can see down in the bottom panel, we have this dockerd process, and that process is not up above. So this is created when we switch over to Windows container mode, and this is the Docker engine running on Windows. And then you can also see here we have some of the services for Docker for Windows in both of these environments. So we have com. docker. proxy and service, and then also Docker for Windows. Docker for Windows is that tray icon, and then com. docker. service is the primary service that is orchestrating all of this stuff that I'm talking about, and then that proxy, well, that's what we route messages through to the various different environments where we run containers. So once we've switched over to Windows containers, we have the Docker engine dockerd running locally, we can then open up a Command Prompt and use the exact same Docker cli to run Windows containers. So, for example, we could run an IIS container for Microsoft. And, of course, we send our message to the proxy, and then that gets routed through the dockerd Docker engine on the Windows machine, and it spins up an IIS container. And so now we have the Windows container side of things. And the important takeaway here is that there are two separate sets of components behind the scenes, one for Linux containers, and one for Windows containers.

Docker on Windows Architecture
Now in this module, we're going to be putting aside our Docker for Windows environment, so our Windows 10 desktop computer, we'll put that aside for a minute, and instead we're going to focus on setting up a Windows server environment. And really the big difference here is this Windows server environment will only have the Docker engine for Windows, it won't have Docker for Windows on it, so it won't support the components to run Linux containers, it will only have the dockerd engine, but that's all we need to be able to create Windows containers. And then on this Windows server, we'll be able to open up a Command Prompt and use the exact same Docker cli, and send messages directly to this Docker engine on Windows to be able to spin up containers, for example, an IIS container. So you can see that the architecture here on Windows server that we're going to be setting up is much simpler than what we had with Docker for Windows. And that's because we only have to support Windows containers here. So in this module, we'll simply be focusing on what I like to call Docker on Windows. Instead of the tool called Docker for Windows, we'll just look at what it's like to run Docker on Windows directly and that's it. So, really all we have here in this architecture is the Docker cli, so docker. exe, and then dockerd. exe for the daemon or engine. Now while I'm running this on a Windows server, if you only wanted Windows containers on any other edition of Windows, like Windows 10 your desktop computer, you could also set things up this way as well, and you could bypass using Docker for Windows then. So, Docker for Windows is a second product from Docker, separate from the Docker engine that runs on Windows. I know that can be a bit confusing, but start to separate that out in your mind and I think things will make sense to you then.

Using the MSDN Windows Container Docs
All right, so I'm hopping over to my Windows Server 2016 instance, and you'll be able to tell the difference between this and my desktop instance. The server has this administrator user, the desktop has my username of Wes, that'll be one way to help you tell apart these environments. Also, you'll be able to see down in the lower right here, this is Windows Server 2016 standard edition. And then also the PC name is Win2016 here. So this is a completely separate machine. In fact, I can show you what's going on behind the scenes by hopping over to my Hyper-V host here, and you can see I have two separate VMs for this course. I have my Windows 10 VM, and I also have my Windows 2016 VM here. So this is a bare installation of Windows Server 2016. I have not enabled any of the features that we need, so we'll walk through the entire process of setting this up, and we'll walk through the process in this example of setting up Docker on Windows, which is just the Docker engine. The first thing that I want to look at, if you come up to the gist for this course, which, by the way, you can access via this shortened URL here. Down in the Docker Setup Links, I have a couple of links to help you out. We have Docker for Windows docs, and then there's also a link to the MSDN section for Windows containers. We click on this, you'll find a lot of helpful documentation, and I just want to point out that you can keep an eye on this in the future, because if anything changes with the process I show you, this documentation will likely reflect that. It's also possible that this documentation might move over to the Docker docs at some point; I don't know, so keep an eye on both the Docker docs that we saw earlier and this site to figure out what the latest steps are if for some reason you have trouble with the installation I show you. Now I would like to point out, if you are going to use this set of documentation, there's one chunk that relates to Windows Server, and there's one chunk that relates to a desktop computer, a Windows 10 environment, sort of like the separation I had between these two in this course. And then if you scroll down here, you'll even see some links for deploying two different environments, so Windows Server versus Nano Server. And we'll be following along with the instructions here for a Windows Server environment, because that's what I have set up here, this Windows Server 2016 standard edition. I would like to say that you are not obligated to follow along with this part of the course. If you are not versed in setting up a VM with Windows Server or spinning something up in Azure, perhaps, or if you just don't want to go to all that trouble, you could just skip setting this up, and instead just watch what I do, or you could go ahead and try and run the Windows containers that I'm running on this Windows Server, you could just try and run those on your Windows 10 machine with Docker for Windows. They'll all run just fine, and for the most part, things will be the same. There will be a few little details that you won't be able to look at that we can look at on Windows Server, we'll get into that in a bit here with different types of Windows containers, but you'll still be able to follow along with all the demos. It's up to you if you do want to set up Windows Server, then you can follow along with these instructions.

Installing Docker on Windows
In a moment here, we're going to run through an installer. And we'll be doing this with PowerShell. And one of the things I just want to point out before we get to that is that if you don't have the latest updates applied to Windows, you're going to have an error as you can see here with this red message down below warning me that I need this hot fix or later to be able to work with Docker on Windows. So as a result of this, one of the first things you'll want to do is to run Windows Update. So come into check for updates and install any updates. Even though the containers feature is brandnew in Windows Server 2016 that was just released, there are updates that you need to be able to use it with the Docker engine. So I will go ahead and finish updating my server here, and then once your machine comes back up, if all the updates have been applied, you can go ahead and move onto setting up the Docker engine. And I actually want to open up PowerShell and run as an administrator. We are now ready to install the Docker engine onto our Windows server. Now, we could run through a bunch of individual commands to do this ourselves, we could download a zip file with the Docker engine and cli in it and manually set that up, but if you hop out to the docs that I provided earlier, you'll see that there is a module that we can install, and if we install this module, it will take care of installing Docker for us, so let's go ahead, and grab a copy of that. Come over to the administrative PowerShell session and paste that in, and let's let that run. While that's running, I want to show you that this module is available via the PowerShell gallery. And I've got the script opened up here for this module, let me back up and show you how to find that script, though. So if you go to the powershellgallery. com and you search with this box here for the DockerMsftProvider, you can then click on that, and then inside of here over on the left-hand side is a link to the project site, which is a GitHub repository. And inside of here you can see the actual module that you're downloading onto your computer, and you can spelunk what it's actually doing. Because while it's nice to have a module to set all this up for you, sometimes it's also nice to know what's actually going on behind the scenes. The next thing we need to do, we need to install the Docker package, and that Docker package is provided by this module that we just downloaded. So I'll copy this command here, come over to PowerShell, I'll go ahead and paste that in, and then before I run that, I'm going to add the verbose flag so we get a few more details printed out. I'll run that then. The first thing that this does is grabs a JSON files that describes where to download a zip file that contains a Docker engine in it and the Docker cli. It downloads that, and then it's asking you if it's okay to go ahead and install this. So, it's telling me here that I'm going to be installing version 1. 12. 2-cs2-ws-beta. And to this I'll go ahead and say A for all, so I'll approve all things that are going to be installed here. Now one thing about this, the engine that you're downloading with this setup is the actual commercially supported engine for Docker. Now, before I run this though, I told you that I wanted you to be able to see what's going on behind the scenes. I want to know what was downloaded here. Let's take a peek at that real quick. So if I come over to the module source code out on GitHub, you can see that there's a link here, https, grab a copy of that, that's what was downloaded. Open up a new tab, and go ahead and take a look at what that downloads. You can see in here that this is just a JSON file describing where to go to get the latest download for the Docker engine, the commercially supported engine. So go ahead and copy this nested link then, so basically making a call out to this JSON file, that PowerShell module does that, it downloads this and then it uses this URL to go ahead and download a copy of the CS commercially supported Docker engine. Go ahead and open up a new tab here, and paste that in, and you'll see it's just a zip file. Let's go ahead and just open up this zip file, and then inside of here you'll notice in the Docker folder there's only two items. We have the docker cli, and we have the dockerd engine, so this is a really, really basic setup process, and I hope you can wrap your mind around what's going on here. We're going to put the docker cli into our PATH, and then we're going to register the docker daemon as a service on Windows, and that's really all this PowerShell module is doing for us, but hey, it saves us the trouble of doing that. And it also has to actually enable that container's feature on Windows as well, so it does a few extra things. Okay, so that's what was downloaded. Let's come back and go ahead and confirm to install this, so hit A and Enter. You'll see that it is installing the containers feature right now. Again, we need that for the Windows container support inside of Windows. Containers are installed, it's now downloading that file that I just showed you, the zip file. It does a few things to verify that that zip file is okay, verifies the hash here, it goes ahead and unzips that zip file, and you can see here it's extracting the contents into C:\Program Files\docker. So let's take a peek at that real quick. And there we go, we have the docker. exe and the dockerd Docker engine. And then we also have this metadata. json file. Okay, back to the output here, and again, the reason why I added that verbose flag is so that we get all this yellow output; otherwise I wouldn't be able to explain all this to you. And next thing that it did was enabled the Docker service, so that dockerd. exe, that's set up as a Windows service now. So if I look for services, and just look for everything called Docker, you can see we now have a Docker service on our computer. If you find yourself more at home with the Services MMC, what I just queried with Get Services is the same thing as what we have right here, so we now have the Docker engine set up as a service. If you look at the properties here, you can see we're pointing at C:\Program Files\Docker and dockerd. exe, and we're running it as a service. And then it goes ahead and nukes the archive that it downloaded, so you can't see what it actually downloaded. And then, of course, it's saying that we need to go ahead and restart this computer to be able to use the Docker service, so let's go ahead and do that now. And then join me in the next video once your computer is done rebooting.

Running the Microsoft .NET Core Image
Once your server comes back up, go ahead and open up a PowerShell session or maybe a Command Prompt, and you should have access to the Docker command now, so that was added to your path. And then also if you use Get service and look for just services with the name Docker in them, you'll see our Docker service is now up and running. So now I've got a question for you. If I want to find out some information about both the version of my client and server, how can I do that with the Docker command? Well, in this case, I can run docker version to get some version information about both of those, and you can see I'm using version 1. 12. 2, and it's a commercially supported engine. And because we're on Windows, we have obviously the Windows architecture on both of these. So this is something we only did with the Linux containers back with Docker for Windows. When we switch to Windows containers, you would have seen the OS architecture on the server flip as well. So this is one way you can help figure out what exactly you're pointed to when you're running Docker commands. Because this Docker Client can actually point at different servers. So, Docker has a client server architecture where we have these daemons or Docker hosts running on various different machines, and we connect to those to control the containers running on that computer, kind of like remotely controlling applications on other computers. In fact, a good way to think about Docker as a whole, both the cli and the Docker engine, it's a front-end for helping you run containers. So it provides an API that hooks into the operating system mechanisms both in Linux and in Windows to be able to manage containers, which means to be able to run software, basically. And I threw the phrase for now on there, because I think in the future we might see that Docker becomes a front-end for running applications in other ways. For example, we might see Docker become a front-end for unikernels, but I'll stop speculating for now. Let's go ahead and run our first Windows container here, so I'll clear out the screen, and let's use a docker run, and how about we run a container with dotnet core inside of it. So there's a microsoft/dotnet, and then let's use the tag nanoserver, so let's pull this down this image and run it on our local computer and just see what happens here. I will go ahead and pause the recording until the download and extraction is complete of the layers inside of this image. Okay, the image has been downloaded, and while that's starting up, before we do anything with that, open up a new window here, maybe a Command Prompt so you can tell them apart, and as long as you have a fast internet connection and fast computer, go ahead and pull microsoft/windowsservercore. That's that other monstrous image that we will be using, so you might as well warm this up to pull this in the background while we're working on other things. Okay, so this is interesting. This right here is the output of running this particular container. Before I explain this to you, do you want to take a guess what happened here with this. NET image that we just downloaded? What happened when this thing started up? Well, in this case, when this particular container was run, it looks like we started up some sort of a prompt here, but then immediately bailed out of that and went back to our computer. So you can see this very C:\, but for whatever reason, we couldn't attach to that prompt, and instead we fell back out to our computer's prompt here. And just to confirm this, let's run a docker ps here, and maybe a ps -a, and you can see that the container that we created for microsoft/dotnet:nanoserver core, it was created about a minute ago, but then it exited right away, so it's not up and running anymore. And there's something interesting here in the command column that I'd like you to see, but it's truncated right now, so if we run that docker ps command again, and --no-trunc for no truncate, now we get the full output of that command column, though it's wrapped around here, so you can see we have c:\ and then we start out with windows and then, actually, it's wrapped offscreen here, we have c:\\windows\\system32 and then cmd. exe. What is this cmd. exe? Well, it's a regular Command Prompt. So we tried to open up a Command Prompt as the application that we are going to run inside of this container. However, we didn't attach to that Command Prompt, it just died right away. Do you know what we can do to be able to attach to that Command Prompt? Well, in this case, as we had in the past, we want to specify -it to open up an interactive terminal into the application, the cmd. exe, that is running inside of our container. So if we run this now, now things change a little bit, and we didn't fall out of that prompt that was created. So we're now at c:\, it's like we just opened up a Command Prompt. Join me in the next video where we take a look at what's going on.

Running a Command Prompt Inside A Container
Okay, so we are inside of this container right now that we create from the microsoft\dotnet image that we pulled down, the nanoserver version, and the screen is kind of messed up here. You'll notice this, I'm sure this experience will improve over time as Windows containers evolve, but right now the screen sometimes will get messed up. We have this black above for the Command Prompt, and we have the blue down below from our original PowerShell session. So, supposedly I downloaded a dotnet core image. So inside of this prompt if I type dotnet, which is the name of the new dotnet cli interface, I get some output here. This is the Microsoft. NET Core Shared Framework Host. So it seems like I have access to. NET Core here. Let's go ahead and exit out of this Command Prompt, kind of like as if we had a nested Command Prompt on our computer, and now I'm back on my host, and you can tell that because I have C:\Users\Administrator. So I'm back into my PowerShell session, even though the colors are messed up. Inside of here if I run. NET, nothing's going to happen, I don't have that command available. So only when I go into a container and run this. NET container do I have access then to the. NET Core CLI. And in this case, when I run this without specifying anything else, it drops me into a Command Prompt, which is another application that comes with this image, I know it's a bit weird, but images can install multiple pieces of software, and this Command Prompt then allows me to have access to the. NET Core CLI, which is in the PATH inside of this particular container. Let's peek around a little bit more inside of this container and see what else is in here, so that we can learn a little bit more about the isolation that's provided by containers.

User Space and Kernel Space
Getting a Command Prompt inside of this container that we just ran, this. NET Core container, is a bit unexpected, it might be a bit weird. It's still just a process, though. The Command Prompt is a process running inside this container. It just doesn't happen to be. NET Core like we might expect. However, we did see how we could get access to. NET Core inside of that container, but let's step back for a minute, and let's talk about images. We haven't talked about these yet. In doing so, I can help you understand how we have a Command Prompt available inside of that image as well. So I've got a question for you first, though. Back when we first talking about images, in terms of traditional software management, what's the equivalent of an image? Well, the image is an application package, so the equivalent would be perhaps a zip file or an MSI installer. It's our application code. It's literally the. exe at a minimum, and it might be more if there are other libraries or other files that come down with this, so we have some application package that contains our app. Well, when it comes to containers, there's a lot of stuff inside of an image beyond just the application itself. So let's step back and understand what comes inside of these images. And to do that, we need to talk about software architecture or software layers. So on a typical computer, we have hardware, the CPU, memory, USB ports, et cetera. On top of the hardware when the computer boots up, a kernel is loaded, and that kernel takes control of the computer, and it runs at a very high level of privilege with full access to all of the hardware. A kernel, though, is typically comprised of drivers and some sort of hardware abstraction layer, and many other components that help abstract away the hardware and built out APIs then for us to use in our software to interact with the hardware. And whether it's a Linux operating system or a Windows operating system, there is a kernel involved, and they're very similar in terms of how they abstract away the hardware and run at a privileged mode. And then on top of the kernel, the operating system that we install onto a computer also containers some libraries. For example, the Win32 APIs, or on the Linux side of things we have the POSIX APIs. In addition to libraries, we also have what I like to call as OS Apps. They're applications that come with the operating system, things we can run like a Command Prompt or PowerShell or even IIS, even though that's an optional feature that we have to turn on. And then on top of this, then we go ahead and install our own software, for example, we might run MongoDB, or we might run SQL Server, or maybe we add the. NET Core runtime or the Java runtime, and then we run apps on top of them, like maybe I have a MyApp. exe that runs on top of the. NET Framework. Even though I have drawn some distinctions here in the purple boxes, really these are all pretty much the same thing, they all reside together in what's known as user space. There's a division, even in the operating system itself, between the kernel and then everything that's not in the kernel. And everything that's not in the kernel is referred to as user space, and everything that's a part of the kernel is referred to as kernel space. So everything above is a user space. And all of the things inside the user space that we execute at a much lower privilege level than the things in the kernel. For example, applications like MongoDB can't directly access hardware on the computer, they have to go through the kernel to do that. And to do that, they use a series of what are known as system calls. And these system calls are typically made from libraries that are a part of the operating system. For example, libraries that are part of the Win32 API. So, MongoDB will call into the Win32 API, the Win32 API will then forward calls onto the kernel via system calls. Now in the past when we distribute software, we typically only install this very topmost layer, for example, we might just add MongoDB to our computer, and we saw that when we downloaded the zip file, it just had some executables in it from MongoDB. And then MongoDB relied upon other things being already installed on our computer, for example, these operating system components. So if we don't have these operating system components on the computer, we're going to have a problem; MongoDB won't be able to run.

Images Contain User Space
In order to avoid the problems inherent in not having the components on the system that you need for your application to run, containers and images change how we distribute software. Instead of just distributing the application like MongoDB, an image actually contains everything that's inside of the user space. All of these layers come down in the image. So you could imagine a box drawn around the user space here, and that's what our image would be. And, of course, then we can avoid the problem of having the wrong dependency or missing a dependency or having the wrong version of a dependency available. All the dependencies for an application, everything in the user space is packaged up with the application. And for simplicity sake, I could shrink this down, and we could just look at this image as a series of layers, which we've already seen. We saw that there were layers when we pull an image. They don't necessarily correspond with the layers that I've drawn in my picture here. If I make a slight change here, this looks a little bit more like what you might expect to see if you took a look at the images that we are pulling down. At the very bottom of the image, the base of the user space, is a base OS image. This contains all the apps and libraries and the user space for the operating system. And then layered on top of that is at least one more layer that typically contains the application or some other application dependencies, so that apps layer might have something like MongoDB in it. And this image then is stored out in Docker Hub, it can be stored in other places as well, but Docker Hub's an easy place to get images from, from repositories, and we can have a series of these images, for example, we might have one that is MongoDB. So that top layer is MongoDB, and then the base layer is the operating system Apps and Libraries, and that might be on the Windows side of things, that might be a base Windows image. On the Linux side of things, that might be a base Linux distribution, for example, Ubuntu, or maybe CentOS. So, the user space over on the Linux side of things is pretty much everything that comprises a Linux distribution. The file that a Linux distribution adds to the kernel, that's basically a Linux user space. And, of course, out on Docker Hub we could have multiple of these repositories with different images in them. We've seen how we can run nginx, IIS, and many other things.

You Can Run Any App in the Image
When we're ready to run an application, we just start out by pulling the image as we've seen. It's copied down locally, for example, from MongoDB here, and then we create a container to run one of the applications inside of this image. For example, we would take MongoDB and run that as a container. But the neat thing is, because that base layer has OS Apps as well, I could actually run something from that OS Apps layer, and in the case of a Windows image, we know we have a Command Prompt available as one of the apps inside of that OS layer, a CMD. exe, so we could use that and run that instead. Because again, a container is just a running process, and that running process could be anything that comes inside of the image, and there can be lots of things inside of the image. So that's how we can end up having a Command Prompt, even though we pulled down an image from MongoDb. And of course, as you know, we can pull multiple images to our computer, so I might pull IIS as well, and then I could be running MongoDB, and a container from the MongoDB image, and I could be running IIS, and a container from the IIS image. One thing that's important to note here is that even though I have multiple containers running, they all share the same kernel. So they all call back via system calls into the exact same kernel, and this is what leads to quite a few of the efficiencies with containers as compared to VMs, where VMs use separate kernels inside of each VM that's running. So that's a whole extra layer that has to be booted up, and it's an expensive layer that oftentimes takes a long time to boot up. Here with containers, we just have to start up a new process if we want to be running a separate isolated application environment.

Layers and Shared and Immutable
Now you might be tempted to think, wouldn't this be horribly inefficient to be pulling down multiple copies of the entire user space for the operating system? We saw that the nanoserver image was 300 or 400 MB, and the Windows Server image was 4 GB, that's a lot of space, that's really wasteful just to avoid some dependency mismatches. I've got a question for you, though. What can we do to avoid needing to have multiple copies of these entire images on our computer? Because as it turns out, we don't actually have to have multiple copies of these entire images, so how do you think Docker pulls that off? Well, the reason the images are layered is that that base OS layer is common amongst many different images that build on top of it. So, if we're running MongoDB in a Windows Server Core container, it shares the exact same base layer as IIS running in a Windows Server Core container. So there's no reason to duplicate that, so we actually have to have one copy of that image locally, and it can be shared amongst all the images we have locally, and more importantly, it can be shared amongst all the running containers, so we don't need multiple copies of it. This is extremely powerful. If we want to reuse these layers, though, we have to make sure that the contents are always the same. How do you think that Docker pulls this off? The image layers that we pulled down, when we use them, they're read-only, so we can't change the image layers that we pulled down, and that's how we preserve the ability to be able to reuse these to know that nothing has changed when we share a particular image layer amongst many different images and containers.

Controlling the Process That Runs in a Container
So if we go back to the container that we have that we're running the Command Prompt inside of, the. NET Core container, I'll go ahead and exit out of this container. I want to show you how you don't have to go into this Command Prompt to then run the. NET Core CLI. I'll clear the screen here. And then I've dropped the font down a little bit, so we don't wrap so much. I think it'll be helpful so that you can see what I'm doing. And I'm going to run the no-trunc option again with docker ps, so you can see the full path to the command, and you can see right now we're running cmd. exe. And if I pull up my docker run command again, and then modify this this time, right after the image name that we want to run, we can provide a command to run instead, so I could provide. NET to execute that, instead of cmd. exe. Cmd. exe just happens to be the default for this particular image that I'm running, but we can override that. So you can now see the output here from the. NET Core Shared Framework Host, and then I'm returned back to my PowerShell session, so the container has exited, and that's because the. NET CLI exits if you don't pass any parameters to it, because it has nothing to do other than show you the help information here. So if I wanted to, I could also pass some additional flags to this program, --version, which you can see up in the output above. There are some flags I can pass. So I can pass that flag as well. And now when the. NET CLI runs inside of that container, before it exits it'll show me version information. And then I'm brought back out to my PowerShell session. So, the. NET CLI that I execute runs until completion, and then, of course, when the process that is running inside of our container terminates, the container is gone now, so we just go back to our PowerShell or whatever prompt we had here on our host machine. Let me clear out the screen here. So we can provide any of a number of programs, as long as they're inside of the image, we can use them. For example, we could use PowerShell too, and so we could open up PowerShell inside of our container, just like cmd. exe. And because I have the it flag set on docker run, I should open up an interactive PowerShell prompt, and from here it's like I'm sitting inside of the container. And I did this way back at the beginning of the course to show you the processes that were running inside of the container. So, for example, now I could run Get-Process here again, and this shows me the processes inside of this container. A pretty short list. And, of course, if I exit out of this PowerShell console, that will kill the PowerShell process that is running inside of that container, so then that container is now stopped, and I'm back to my PowerShell prompt on my "host" machine. I'll call it host machine when I'm outside of the container, host machine meaning container host. And out here if I run Get-Process, what do you think is going to come back now? We get back all the processes running on the host machine, which is quite a few more than what was running inside of that container. We already talked briefly about this, let's talk more about this, though, and other aspects of what is called isolation inside of a container.

Defining a Container Host
I want to take a minute and define a term. You're going to hear me use the term host or container host a lot throughout the rest of this course. I might have already said this a time or two. This is simply a way of referring to the machine that you're running containers on. So, my Windows 10 machine here that I have run some containers on, that would be a container host. Or, my Windows Server 2016 machine that I've been running containers on, that's also a container host. And this is in contrast, so hear me contrast this with inside the container. And the contrast I'm trying to draw is the perspective of the entire machine versus the isolation within a single container. So, for example, inside of a container, the processes that you're going to see are only the processes that are inside of that container, and not all of the processes on the container host.

Containers Have Isolated File Systems Created from Images
We've seen a few things now that hint at some sort of isolation for our container. First off, I've shown you how the image delivers the user space. Well, another way to think about this is that the image delivers the file system that will be available inside of the container. And that's because the file system for the container is isolated from the file system for the rest of the machine. We haven't really seen this yet, so let's take a look at that first. We just saw how the processes are isolated, well, the file system is isolated as well. So I'll clear the screen here, and let me just list the contents of the C: drive on my host machine, and let's go ahead and open up a container, and we can use PowerShell again inside of our. NET Core container. And now let me list the contents here. Maybe I'll exit out here now since the screen cleared out and run the ls again, so you can compare the two. So, clearly we have different contents here. You can see BootStrap and PerfLogs exist on my host machine, but they do not exist inside of the container. Also, Resources is unique here. And then inside of the container, well, there's a unique file, a License. txt, so these actually look like physically different hard drives. When we start up a process inside of a container, the file system is completely isolated from the regular file system, and that file system comes from the user space provided by that image. That's why we have to have the whole user space inside the image, because that's the only files we have access to inside of the container. Now, this seems kind of interesting and weird, but I found a neat way to see this, to understand this. If you open up Computer Management, and if you go over to Disk Management, and then I went ahead and split the screen here, I've got my PowerShell prompt open at the top, and down below I've got the Computer Management with the Disk Management loaded. Now, right now I don't have any containers running, so right now you can see that we only have the Disk 0 in our computer here. However, if I run a container now and open up the PowerShell prompt again inside of that container, that will leave the container running for me, and look at that down below already, did you see Disk 1 added here? We have another disk added. And then when I exit out of the PowerShell prompt and kill PowerShell, thus killing the container, look at that, Disk 1 is gone. And if I run a copy here, Disk 1 is added back, and maybe I'll open up another Command Prompt, docker run, and -it, and then microsoft/, and then I'll run PowerShell inside this one as well, look at that, we've got Disk 1 and Disk 2 now. Okay, so I have my two containers running over here in separate windows, and each of these two containers have PowerShell running inside of them, so I have my two virtual hard disks here. Now I can do something really neat by assigning a drive letter, which I wouldn't recommend in a production environment, but I'll go ahead and add D here to one of these, and then let me open up the D drive on my computer, and let me open a new window here with the C drive. Look at that, I've hacked into that D drive that's a part of one of my containers, and I can see that this is completely different than the C drive on my host machine. I can even come to one of these containers, I believe it's this one, and I can move one of those files, maybe I didn't get lucky. Try the other one. And look at that, the file moved over here as well, so this is clearly the same file. I can even delete this file. And now on top when I list the contents here, that file is now gone. The important takeaway here is that when we run a container, it has its own isolated file system. It does not share files with the host or machine file system.

Namespaces Provide Isolation
We've seen how containers have isolated file systems, well, they actually have a lot of other isolated things as well. What all do you think might be isolated inside of a container? Well, if you can think of anything that you might not want to compromise on a system when you run an application, you'll probably be able to enumerate most of the things that are isolated. We have process isolation that we've seen, we have file system isolation, we also have network isolation. We saw a little bit of that when we talked about the virtual network adapter that's affiliated with each container, and using the IP address bound to that to be able to access IIS inside of a container. When it comes to Windows containers, the registry is isolated as well. Each container has its own registry. On the Linux side of the things there's quite a bit of isolation. There's processes, and it's called process namespaces over in the Linux world; in fact, all of the isolation over in the Linux world is called namespaces. And if you want to learn more about the isolation in Linux, you can come to the man page for namespaces, and you can see there's a whole host of things that isolatable. For example, each container can have its own host name. User and group IDs can be remapped for security's sake. A group of processes can be isolated. Mount namespaces provide the ability to have a unique file system per container. Each container has its own network devices, so just like we talked about on Windows with the virtual network adapters. Each container also has its own IPC, or interprocess communication, so the ability to communicate between processes. Just because these namespaces exist doesn't meant that they have to be used. Each of these are opt-in. For example, only recently did Docker support creating containers with user namepaces. Let's see some more of this isolation on the Windows side of things.

Processes Are Isolated
Okay, so I've opened the Task Manager on the right for my computer, so these are all the tasks that are running on my computer here in my Windows Server. And then on the left I'm going to open up a container again running PowerShell inside of it. By the way, I've closed down all my other containers that I had in previous demos. So I'll go ahead and run this container on the left-hand side, and take note over here on the right-hand side, if I come in and right-click and choose to select columns and pick this Job Object ID, if I sort again by Job Object ID, you can see all these processes with a Job Object ID of 404. Job Objects in Windows are somewhat similar to Linux PID namespaces. They're a part of isolating processes. So, long story short, that means that it looks like when I run Get-Process here, I only see a subset of the processes on the machine over on the right-hand side here, and that's because this container is running in a way that's isolated from the rest of the computer, so it cannot see these other processes on the computer. So it's a really good thing if we are running some software in a container that we didn't trust, it wouldn't have access to other processes on our computer, so it couldn't compromise the processes directly then. Remember I said there's a kernel space and a user space? Well, when it comes time for an application like PowerShell to ask what other processes are on the machine, it has to somehow make a system call or otherwise ask the kernel. And so the kernel controls the world that any of our user space processes see. So the kernel is basically just lying when PowerShell asks what other processes are running, it's lying and it's hiding from PowerShell the rest of the processes on the computer, and that's to provide some isolation and security inside of these containers. I want to give you another way to visualize this process isolation. So, when we start up processes on a host machine, there's typically some first init type processes, a part of the Windows operating system. And from it, then other processes respond, and when a child process spawns processes, they're nested underneath of it in a hierarchical fashion. So we can have more and more processes here. These processes are all assigned some sort of identifier from the perspective of the host. And, of course, as processes start and stop, there are gaps in these identifiers, because the numbers incremented, and so the numbers could be pretty much anything, and we've seen that with process IDs and the thousands in the examples in this course. The isolation that a container provides is a lot like just drawing a boundary around a subset of these processes, and we're grouping these processes together, All the processes underneath any of the processes that have been isolated here, they'll all be grouped inside of a container, and that container can't see the outside world. So, process 18, 26, 27, and 28 have no idea that 15, 16, 25, and 10 are running on this computer, and that's because they're inside of the container. However, other processes outside of the container on the host here, for example, process ID 1, can see inside of the container, so that how it works is a hierarchical relationship. Now, one thing I haven't shown yet over on the Linux side of things, the PID namespaces in Linux, not only do they group processes, a kernel also reassigns new identifiers to all the processes inside of a container. So in Linux, if you use ps inside of container, you'll get a whole new set of IDs that do not match what you have on the host operating system. We've seen on Windows, though, that the identifier doesn't differ, so this is a Linux specific difference in terms of isolation. So you might have 1 container here, but you could have another container somewhere else, for example, we could have a container 2 that just contains process ID 11. So we could have whatever grouping we want. And I will say that the containers we've seen thus far were mostly Windows containers, especially when we were digging under the covers, and Windows containers are going to be more like this container 1 in that they're going to have multiple processes, the base services that are a part of Windows run as services, so there'll be more processes in a Windows container. Conversely, over in Linux land, you will see a lot of containers that only have one process. In fact, you're going to hear people talk about a "best practice" being to only run one process in a container. I don't believe in best practices, so I don't think that there's necessarily a right way. I do want you to just be aware of this, on the Linux side of things you're going to see single process containers, and it might be kind of weird when people are talking about a "best practice" that's violated by every Windows container that's out there, because every Windows container has a bunch of services in it. Now again, over in this container 2, one thing I'll point out, on the outside is has identifier 11 to the host, that process does, but on the inside, it would have an identifier of 1, so the process inside the container would think that it's PID 1. Okay, so that's another way to visualize the isolation. In addition to security, having this process isolation also provides a lot of convenience, because we don't have to worry about things like port conflicts, which is what we'll take a look at next. We'll see how isolated networking can be our best friend.

Network Stacks Are Isolated
All right, so I've changed the screen around a bit here on the right-hand side, I have my PowerShell prompt opened up on the host machine, I am not inside of a container on the right-hand side, and on the left-hand side I'm still in that container I had a moment ago. So I'm going to come over to the right-hand side, and I'm going to take a look at my network adapters. And the most important of these is this Ethernet adapter over here, this is my connectivity out of my computer, out to the rest of the world, and you can see my IP address is 0. 15. Now, what do you think is going to happen when I run ip config over on the left-hand side here? So the left-hand is a container here. The right-hand is my host machine. Remember you can see C:\Users\Administrator, the default path here for my administrative account on this server machine? So the left-hand side, C:\, that's your hint that I'm probably in a container. Well, let's run this and find out. Is that what you expected? You can see that the IP address for the Ethernet adapter or the virtual network adapter inside of my container is completely different. It's on a completely different network here, 172. 20. 64. 1, that's not at all connected to this interface. However, you can see that there's another virtual adapter on my host machine that shares the IP address 64. 1 here, which is actually the gateway for my container. So, my container and the host machine can actually talk to each other over the network that's connected here. I'm not going to get into how all of that works in this course, that's a very advanced topic. It has to do with Hyper-V and virtual switches if you'd like to look into that more. The important takeaway here is we have a completely isolated network stack inside of our container, it's like we have separate network adapters from our host machine. So just like you could plug multiple physical network cards into a computer and you could decide what traffic it's routed through each of them and they could each have their own IP address bound to them on separate networks if you wanted to, because they could be hooked up to separate switches outside of your computer, well, internally to your computer you can also have virtual network adapters set up that are basically the exact same thing. And then we can have applications bound to different virtual NICs with the same result. It's like they're bound to separate networks. So that means if I want to run a web server on port 80 inside of a container, I don't have to care about a web server running on the host machine on port 80, unless I want to expose the container's port 80 out to the host machine, and then I might have a problem. This is extremely powerful when you're running software. One customer I had recently, we were re-architecting their continuous integration system for testing, and part of their testing process was spinning up a web server, and they were spinning up hundreds and thousands of these every time somebody checked in to test various different aspects of the system, well, of course they had a lot of concerns about port conflicts then to open up all those web servers and run multiple test runners to be able to run tests in parallel. So part of what we did was to containerize their testing, so we didn't have to care about the port anymore. We put the web server and the test runner inside of a container and no longer did we have to worry about that conflicting with anything else. It lived in its own little happy world where the test runner client could connect to that web server inside of the container and it didn't care about anything else that was going on on the computer.

Environment Variables and Computer Name Are Different
So we still have our container on the left-hand side. Now I'm interested in taking a look at the host name, because I know those are different on Linux, so let's look at what they are on Windows. How do you think I could check the host name from PowerShell here? Well, I can take a look at the environment variables on this computer. And for PowerShell, I love this, I can just do ls env:\, and I'll get all of my environment variables, so actually we'll take a look at the whole environment variable. And then I can do the exact same thing on my host machine. Except it looks like I opened a Command Prompt over here, my bad. So I will exit out of that, and it looks like here is a PowerShell prompt that's opened up on the host, ls env:\, and take a look at that, looks a little bit different, right? Couple of really important differences here. First off, the computer name over on the right-hand side is WIN2016, that was my server name, but over on the left-hand side it's some garbled up sequence of characters. So the host names are different. You can also see that we have some custom environment variables over here for. NET Core, so that's different. The environment variables themselves are different, we have added environment variables. If I look down below here, the username is ContainerAdministrator, which is different than Administrator over here on the right-hand side, and of course, my user profile directory is different as well. Next let's take a look at the registry.

The Registry Is Isolated
Okay, so we want to take a look at the registry now. I've got a question for you, how can we diagnose what's going on with the registry? Well, there are many ways we could take a look at the registry. I'm thinking the best way is probably to try and enumerate it with PowerShell here, so let me clear both of these screens out. So, again, another thing I love about PowerShell, I can enumerate items in the registry by just listing out the contents of the drive here for the hk current user. So, hkcu, on the left is a container and right is the host. Wow, quite a bit of difference. So we're looking at the hive for the HKEY_CURRENT_UsER in each of these environments, and there are quite a few less keys over on the left-hand side. Also on PowerShell I can take a look at Get-PSDrive and I can see that there is the HKEY local machine available as well, so let's take a look at that. And it looks like on the right-hand side I'm getting permission denied, but once again, we've got a difference here on the right-hand side up at the top here, and obviously our access is a bit different as well. The registry is kind of a big thing, so it's hard to see all the differences, but if you want, poke around, especially if you have worked with the registry in the past, and I think you'll be surprised what you see that helps show you how the container environment is isolated with its own registry.

Users and Groups Are Isolated
Now let's take a look at users and groups. So, on the left-hand side, again, the container, right-hand side the host. If I Get-LocalUsers, and then I'll run this command over on my container host, and it looks like we have the same list; however, that's not actually the case. Over on the right-hand side I'm going to make a new user here, New-LocalUser, I'll give it a name of Wes, and that has created it, even though I provided an invalid password, I will Get-LocalUsers again, and you can see I've got the new Wes user on the right-hand side, my host. If I run Get-LocalUser on the left-hand side, I don't see that user created. If I exit out of the container and recreate the container just to make sure, Get-LocalUser, the user is still not there. So, we seem to have isolated sets of users and groups. And even though we have some names in common, if I select the name and the sid, the unique identifier of each user, and if I actually put in select over here, if you compare the ids that come out of these two, you'll see that there are differences as well. The ends are roughly the same, and parts of the beginning are, but take the administrative account, for example, and take a look at the middle of the identifier, and you'll see that this differs. And, of course, this is really important, because the name doesn't matter, the id is what matters when it comes to granting security on the host machine. So we have a different set of users in the container as far as the identifier is concerned, the names might be the same. It'd be just like having us have our computer with different users and identifiers there, those users on the other computer then wouldn't have access to perhaps files that were restricted on the other computer, so this is really important to protect the host machine if for whatever reason a process in a container could break out, this is another mechanism that might help keep it from accessing things that it shouldn't be able to access on the host.

There Are Two Types of Windows Containers
We have looked a lot of isolation here with these containers that are running on our Windows machine, and we've just been looking at Windows containers. I would strongly encourage that you take the time if you know the Linux world, I would encourage you to try to spelunk the equivalent things over in Linux land and see if you can compare some of these things on the host VM versus a container inside of that Linux VM, or just if you have a Linux machine somewhere, put Docker on it and take a look at the processes on the host and compare it to the processes in the container, and use the ps command in that case. You'll see very similar behavior to what we've seen here with Windows containers. One last thing I want to talk about, though, when it comes to isolation there's a very special type of isolation available with Windows containers that is not available with Linux containers. It actually kind of breaks the idea behind Linux containers. Not necessarily in a bad way, though. I've glossed over this throughout this course, but there's actually two separate types of Windows containers. There are Windows Server containers and Hyper-V containers. And thus far we've actually worked with both of these, you just didn't know it. Everything I've been doing on the Windows Server machine, so everything from this module, has been using what's called a Windows Server container. And everything I did on the desktop in the last module with Docker for Windows, that was using a Hyper-V container. These two types provide different levels of isolation. So, Windows Server containers, they use isolation just like a Linux container. It's all isolation by virtue of the kernel line to the processes. Remember that user space has to ultimately call into the kernel space when it comes to system resources. In the process of asking for the list of processes, or what's my process information, or what's my host name, or any of those things that we were looking in terms of isolation here, for example, what's my disk drive, all of that information comes back from the kernel in such a way so as to isolate all of those things. But it's not really isolated. At the end of the day, it's all running inside of the exact same machine. That's why I was able to show you the process monitor, and I was able to drill and show you the processes running in a container. Right here you can see Job Object ID 496, so this is on the host machine on the right here, let me pull the container back up on the left-hand side. So this container on the left that's running PowerShell for me, if I take a look at the PowerShell process ID 7280 here, it's just got pow, sorry you can't see all of that, this is inside of the container at 7280, look at this, on the host we have the exact same process running. So this container has a process inside of it. I said containers were just about running software, and that process is running on our host machine. It just looks like things are isolated from the perspective of that process running in the container, but that's not actually the case, the process is just running on our computer. If you want another way to think about this, we've had virtual memory forever inside of operating systems, and this is a technique to isolate memory. It looks like we're using physical memory when actually we are mapped to virtual memory in an application. Well, the kernel now has a host of other means of isolating other resources besides just memory. And that's what all these new degrees of isolation are for. However, this only applies to a Windows Server container or a Linux container. Hyper-V containers have a different type of isolation. They have isolation by virtue of the hypervisor. So just to recap, we have two types of Windows containers, and each of these uses a separate isolation model. A Windows Server container uses process isolation, whereas the Hyper-V container uses VM isolation. And I would like to point out, even though Hyper-V uses VM isolation, you're going to see much faster startup times with the Hyper-V containers inside of a VM as opposed to a traditional VM, so you still have substantial benefits in terms of speed here with Hyper-V containers. And then just to help you out to remember where we can run these Windows container types at, if you have a Windows Server 2016 or later edition of Windows Server, you'll be able to run both of these container types. A client sku of Windows, you won't be able to run the Windows Server container type, you'll only be able to run Hyper-V containers. One of the big key takeaways here is that the things that we did to sleuth around with our Windows Server containers, we can't do that sleuthing on Windows 10 machines, because we need process isolation to do so. So just to reiterate that, when you run these Hyper-V containers, you're not going to see the processes showing up from the container on the host in the Task Manager. Just to prove this to you, let's actually take a look at that disk drive thing, because that's one of my favorite ways to figure out what's going on with containers. So let's open up Computer Management, Disk Management, I'll put that on the right-hand side. Right now we have this Disk 1 for this container over on the left-hand side, let me exit out of that container, you'll see it goes away. Now if I hit the up arrow and go back over, if I add a parameter to run here, and I specify a -- and then I say isolation=hyperv, if I run this container now, I actually have a problem. And that's because I don't have the Hyper-V feature enabled on this server. So join me in the next video to do that, and then we'll be able to see this type of container running.

Running a Hyper-V Container
Okay, so let's get Hyper-V set up, and in order to avoid confusion, I'm going to close most of these Windows that I have open, because we're going to reboot anyway. So let's close things down, and I've now opened up PowerShell here, I can now use install, and then WindowsFeature, and I can specify Hyper-V. This will take a moment. It will enable the feature, and to do that it's going to require rebooting the server. Okay, the feature is enabled, we need to reboot now. And I'll be back in a minute when that's done. All right, so my computer is back up and running, I'll open up Computer Management again. Okay, I've loaded that up with Disk Management on the right-hand side here, and then I'll open up PowerShell, and then I will search for my last docker run command. By the way, this one came up, and I don't think I showed you this option yet, so --rm, once a container exits, it will just delete it automatically. Right now we're leaving behind a bunch of stop containers. If you put this flag on, the container will be deleted once the process is done inside of it. Nice way to not need to clean things up later on. So, I've got my docker run statement back, and then I need to add my isolation flag. Isolation-hyperv, run that, watch the right-hand side. FYI, the first time you run this, it's going to take some time to initialize the components necessary for this Hyper-V container. It should be faster than on subsequent runs after the first container you run with Hyper-V isolation. Okay, finally up and running, and you can see on the right-hand side no disk drive added. If I take a look at Task Manager, hop over to Details, under Job Object ID, really I'm not seeing anything here that looks like a container. Though I do believe that these top two processes have something to do with how these Hyper-V containers operate. In fact, we can now exit out of this, and you see in the upper right those two processes are now gone. So there's a little bit of a hint that we have this Hyper-V container running, but for the most part, this container runs with a strong degree of isolation, and that's the point. If you need a container that's isolated more like a VM, you'd like the benefits of a container in terms of fast startup, and these nice images that are easy to pull down, then you can use a Hyper-V container instead to have a stronger degree of isolation to really protect your applications. I think it's possible that we might see Microsoft take this technology that they built and try to extend it to Linux containers at some point in the future, or maybe Docker will work on that with them, I don't know, because it's an interesting idea at least, it's an interesting idea to provide more isolation without compromising speed and convenience of distributing container images versus a history I've had with VMs being very slow to startup and unruly to distribute.

Key Takeaways
We've now come to the end of this module, and I hope you've enjoyed it. There are a couple of key takeaways. First off, if you just want to use Windows containers, well, you don't need Docker for Windows then, you could just install the Docker engine and the Docker CLI right onto your Windows machine. Obviously this is what you'd use on a Windows Server in production, but you could use this in development as well. In fact, if you want, you could have a nice VM set up and running with Windows Server in your dev environment, and then you don't have to switch back and forth between Linux and Windows containers with Docker for Windows, and maybe you have Docker for Windows directly on your Windows machine for your Linux containers, and then you have another VM for your Windows containers. We wrapped up the last module talking about how containers really simplify the process of managing software. One command, docker run, to do it all, download, install, run, start, stop, uninstall, et cetera. In addition to this, though, I want you to also take away that there's this separation between the kernel space and the user space, and because of how system calls work, and how the kernel operates, we have a strong degree of isolation with containers. Not only do we ship the entire user space in an image, but when we take this image then and create a container, it looks like the processes inside of that container are running on a completely separate computer, and there are obviously a lot of benefits in terms of security and simplicity, for example, not having port conflicts. So, two things with containers thus far. They simplify software management, and they also provide isolation. And it's this isolation that is the reason why people like to compare them with virtual machines. And as a result, when you start to compare containers to virtual machines, is there anything that sticks out to you about containers that might seem to be a benefit that we haven't touched on yet? Well, when it comes to spinning up a VM, that can take a long time, usually on the order of minutes if we're lucky. If we can get below a minute, we are doing really good. Do you notice how fast these containers started up, though? Aside from pulling the image, which is then cached forever, a container can start up in seconds, especially Linux containers. Windows containers are a little bit slower, maybe four or five seconds at times, but the Linux containers are lightning fast. Play around with that and think about the implications of that in terms of hosting applications inside of a container instead of a virtual machine.

Running Command Line Apps in Containers
Module Intro
One of the things I love most about Docker is the ability to run software without having to know how to set it up and without a lot of hassle and time, even if I did know how to set it up. I can just type docker run, and I'm off to the races. And, of course, this is great for server-side software like we've seen with nginx or IIS, but it's also great with command-line applications. Let's take a look at running a few different command-line apps in this module, and in the process learn a little bit more about Docker and containers.

Exporting Images with docker save
I like to pull things apart to figure out how they work. You saw this when we were looking at the process isolation, and now we're going to take a look at it in terms of understanding what's actually inside of a container image. I told you that the entire user space is brought down inside of a container image. But why take my word for it? How about we pull apart an image and see what's inside? And in the process of doing this, we'll need a tool that we don't have on our computer and we'll use a Docker container to provide that software. All right, so I'm back over on my Windows 10 machine, and I'm going to take a look at the images here. So we have microsoft/iis and Windows Server Core. That IIS image is a bit smaller, so how about we take a look at that one? We can use the docker save command to export an image, and save it into a tar archive. So I can take docker and save and then I can specify microsoft/iis and then :nanoserver, because this is not the latest tag, it's the nanoserver edition, that's why it's smaller at 875 MB. And then I need to specify a file to save this out to, and I'll call this iis. tar. This will take a moment to extract. Join me in the next video where we pull this apart.

Switching Docker for Windows to Linux Containers
Once that's done, we can start to pull this apart, but we've got a little bit of a problem. Windows has no notion of an ability to be able to take a tar archive and open it up by default, so we need some software to help us do this. And of course we could download something like WinRAR to help us out, but let's see how we can use a Docker container to do this. So over in Linux land, I know that just about every Linux distribution comes with a tar command-line utility that allows us to interact with tar files. So let's come back over to our Windows machine here, and let's use a Linux container to access a tar utility to be able to extract this tar archive. I've got a question for you though. How do I switch from Windows containers here over to Linux containers? Well, to switch, we need to come down to the tray, and then right-click and choose Switch to Linux containers. So I'll go ahead and click on that. And then I just need to wait for the whale to stop animating. Another way you can know what's going on, you can come to the command-line here, type a docker ps, and if you're getting an error back, obviously things aren't working quite yet. And in this case, you can see that we're missing a pipe to the docker_engine for Windows, so it seems like we're still in the process of transitioning here. All right, our whale is done animating, so things might be working. Now let's take a look at a docker ps again. Okay, I think things are up and running. So let me clear out the screen here and run a docker images, and you can see we now have our list of Linux container images. And we just had that last one that we used, the one for the docs. Join me in the next video where we take a look at a Linux distribution that can help us use the tar utility.

Is This Image Safe to Use? - Official Images and Automated Builds
If you hop out to Docker Hub, and you search for an image called Alpine, Alpine is a very lightweight distribution of Linux. It results in a very, very small Docker image, an image that's actually only 5 MB. Think about that in comparison to this 800 MB version of nanoserver, which is a small version of a Windows container. So there's not much in the user space of this Alpine Linux distribution, and that's why it's so darn small. If we scroll down here, you'll see that we have a series of choices for the version of this Alpine Linux distribution. We'll stick with the latest, but I just wanted you to see that sometimes tags are used to correlate to versions of software, or in this case, versions of a Linux distribution. Now, while I'm at it talking about running software on Docker Hub, one thing I haven't really talked that much about is how to be safe with what you're pulling down from Docker Hub. First off, you do have a lot of isolation with containers, but that doesn't mean that something couldn't break out. So you should be safe when you're pulling software down from Docker Hub. And one of the ways to be safe is to make sure that a repository is marked official. If it's marked official, this means that it has the blessing of both Docker and whatever group of people are responsible for making the software that this container image contains, so in this case, Alpine Linux, the group behind that, would also be behind creating this image. Now that doesn't mean that the image couldn't be compromised, but seeing the word official on here gives you some sense of security in using this image. Also, if you sign in, so if I go over to tags here, you'll see that there's nothing showing up as far as virus scanning is concerned. If you log in, though, and then go back to that page we were looking at, now you can see the result of the security scanning that's provided by Docker for you. Okay, and this one's green, giving me further confidence that this is okay. Of course, I'll need to check the latest tag, which is the one that I plan to run. Now one more thing you can check, if I just search for this image again, so go back to the search page here, in addition to having official images, there are also images that are provided by users, users like yourself. So you could go out and build an image with the Alpine Linux distribution in it, or like this person has done, you could add in the Java tools on top of the alpine Linux distribution, and this is the user that has done this. Now a couple of things you can check here, first off, you can check popularity. If a whole bunch of people are using this, hopefully it's not compromised. That doesn't mean that's the case, but hopefully. The other thing, though, that can give you some degree of certainty is seeing this word automated build. This means that this image was built out on Docker Hub. So a Docker file was provided and the image was built on Docker Hub with that Docker file. At times you might find some images that don't have this tag, and a great example of this is the Microsoft organization right now, because the Windows base images aren't being built out on Docker Hub right now. So if you scroll down here, you'll notice IIS doesn't have automated build after it, nor does nanoserver, nor does Windows Server Core. This means that this image was assembled externally and then pushed to Docker Hub. So, it's possible that somebody put some malicious content in here because the Docker file to build the image was not given to Docker Hub to build the image. We'll talk a bit more about building images later in the course, but just be aware that if it doesn't say automated build, the image was assembled externally, and that's something to be a little bit more worried about. I would in general, though, be worried about anything you pull, even if it does say automated build, even if it does say official when it comes time to deploy into a production environment or any environment where security is important, and that could be your development environment as well. Okay, so join me in the next video where we go ahead and pull down that Alpine Linux distribution.

Running a Shell with the Alpine Linux Container
Okay, so we want to use this official Alpine image, so let's go ahead and pull this down to our computer. So I'll start out with a docker pull, just so we can take a look at it in the list of images. Again, the latest tag will be used if I don't specify one. And look how fast that was. Let's take a look at Docker images now. Take a look at that, 5 MB, that's pretty awesome, as opposed to even the docs above, which are 3 GB. Okay, so docker run here, and then I'll use -it, because I want to open up a shell inside of this container, so I need this to be interactive. So what I'm going to do is run this alpine Linux distribution, and I'm going to run a shell inside of it with just sh, except I believe that's the default command that we'll run here, so let's just go ahead and use this docker run statement, and we should open up a shell inside of this container, so just a Bash shell, just like we have a PowerShell console or a Command Prompt over in the Windows containers we've been looking at. And it looks like I actually do need to specify command here, so I'll use sh for shell, and now I have a prompt here. Now, we'll take a moment to take a look at the processes here inside of this shell, inside of my Alpine Linux container. So I'll run ps here, and you can see that we just have two processes, we have PID 1, which is the shell, and then we have 6, which is just the transient ps command. So inside of a Linux container we don't have nearly as many processes running as we do in a Windows container, and this is that mentality in the Linux container of having only one process running, the shell in this case. Though nothing is stopping me from starting up another nested shell here, and if I run ps now, you can see our PID 1 is a shell and PID 7 is another shell that I've started up. I could add another nesting here, so I'm in a third-level shell. And if I run ps here, you'll see there are three processes running for shells. If I started exiting out of these, you can see there's only two now, exit out again, there's only one now. And you can indeed that that ps command is transient, the id goes up each time and then it disappears once the ps command is done printing out the processes. Okay, so we have this Alpine Linux distribution running. And we're just running a shell in the user space, that's it. And this shell in the user space is communicating with that Moby Linux VM's Linux kernel, much like our PowerShell or Command Prompts were communicating with the Windows kernel over on the Windows container side of things. Long story short, though, I have what we need, which is the tar command. But somehow I need to get access to that file that we exported that's over on my Windows 10 machine, it's not here inside of the file system anywhere. So somehow I have to get that file in here to be able to use the tar command on it. Let's take a look at that next.

Using Volumes to Share the Host File System with a Container
If I want to get files into a container, somehow I have to mount a drive into that container, that's how you can think of this. Just like if you plug a USB stick into a computer, that USB stick mounts as an additional drive. And the way to do that is with a thing called volumes. Let's exit out of this Alpine container, I'll clear the screen here. So, in addition to the image that's pulled down, we can also mount additional drives via this idea of volumes. And you should literally think of this as adding another hard drive to a computer or plugging in a USB thumb drive. Now the first thing we need to do, because we have a Windows machine and that Linux VM, somehow we have to get our Windows files into that Linux VM first. And there's a special setting for that over here in the tray icon for Docker for Windows, so click Settings, and then come over to Shared Drives. Now this is specific to the Linux container side of things, because it's going to be different with Windows containers. On the Linux container side of things, we have to get our C drive into that VM, and so that's what checking this box here does, it makes our C drive available magically to that Linux VM, so that we can then take our C drive and mount it inside of our container, or we can mount part of it, we'll just mount a subdirectory inside of it. In fact, you can see a little snippet down here of a command you can use to be able to mount part of your C drive into, and in fact, it's the Alpine Linux distribution here. So actually, let's take this command that's provided right here, this is wonderful, talk about serendipity. Click Apply to share that drive. You'll have to type in your password. Docker for Windows will store your password so it can use it so you can access that drive externally via Samba share. And then go ahead and close the window. Okay, so now let's go ahead and paste in that command that we copied. So this is a docker run. We have the remove flag, so that means a container will be removed once it's done running. So when we exit out of whatever process we're running in this container, this container will be removed. And by the way, the process that we're running, we're running the ls program here to list the contents of this data folder that we're mounting with a volume mount here. So what this basically says is hey, take my c:/Users drive, and you can use forward slash to do this mounting, and map it into the container as /data. So there's a colon between these two to delimit that this is the host portion, so c:/Users on the host, map it into the container at /data. Remember, containers have their own entire file system, they have their own view of the world, so we have to mount in this c:/Users at the location /data. Again, we're mapping this into a Linux container. Remember that Linux containers just use forward slashes for paths, so they don't have any c: drive letters. So, all of this is the volume mount here. And then I'm specifying the Alpine image, and then I'm going to run ls, and I'm passing an argument to ls here of /data, the folder that I'm mapped in. So let's go ahead and run this. And we list out what looks like the various folders for the users on my Windows host machine. But let's verify this. Let's go and list the contents of the directory above us, which will be c:/Users. In fact, I'll just do this this way, so it's a little less confusing. And then I'll also need to specify /A to list all files. So if you compare these two, you'll see that these files here map to the files or actually the folders we have up above. We have indeed mapped in our C drive, specifically just the Users subfolder. Okay, now join me in the next video where we map in our tar archive so that we can extract it.

Manipulating Host Files with Tools Only Installed in a Container
Okay, so let's go about getting things mapped in. Now this mapping that we have right now for c:/Users is almost what we need. We could then also specify the wes folder, which is where that IIS tar file is at. Oftentimes when I'm trying to get the mappings correct, I like to use the ls command or something like it, drr over on the Windows side of things, just to make sure that I have the mappings correct. And then if you take a look at the output here, you can see I have an iis. tar file. Now if you want, you can be really specific. So, for example, I could pull back my last command, and instead of just mapping the entire folder, looks like I pulled an older version here, I could map the individual iis. tar file itself, and that's the only thing that I would map over then. And now take a look at the data folder inside of our container. It only has the one file that we want to use. And this can be a really good idea to constrain the security of your volume mounts, so that a container doesn't have access to anything that it doesn't need. Because again, whatever's running in this container, whatever volume mounts you had, the process in the container now has access to this. So these volumes are added in addition to whatever comes down in the image They're just mounted into the file system that's provided by the image. Okay, so now let's take this, and instead of running it a less command, let's just hop into a shell, that way I can run a series of commands. I could also pass tar instead of sh here at the end of this docker run command and use the tar command. I also need to add the -it, I forgot about that. So I could pass tar here if I wanted to run tar and that's the only thing I wanted to do, but I'd like to just sit in the shell here and work with that file. So first thing I'll use is tar -tf, and then I can pass the iis. tar file in the /data folder, and this just prints out some information about what's inside of this tar file. So these are all the files inside of the tar file, and it's kind of convoluted. We essentially have a series of directories here, here's one of them, here's another one. You can see these directories here, which contain a series of repeating files. We have a VERSION, json, and layer. tar, so we have some nested tar files here, and each of these nested layer. tar files are one of the individual layers that makes up this image. So this image happens to have about seven different layers inside of here. And most of those layers are going to be coming from that base nanoserver image. We'll learn more about distinguishing layers and where they come from when we build our own images. Now, if I want to, I can extract the contents of that file with x and then f and then /data/iis. tar. So when you use a tar command, x is for extract. The t is to list the contents. And if you wanted to create a tar archive, you could use c for that. So, that very first character that you pass as a flag is what you want to do, and in this case, let's go ahead and extract to this tar archive. Now, in the process of doing this, I'd like to map it out, the data that comes out of that tar archive, into, say, an extract folder, so /data/extract. However, when I run that, I'll get a problem, because I have to make that directory first, so we'll make a directory /data/extract here, and now I can run my command again. That's going to take a moment because that's a rather large archive. All right, and then that completes now, and I could change into the data directory, go into the extract folder, and I could list the contents this way as well. And now I could go into some of these different files. For example, I could cat out this manifest. json file at the top here, and while hmm, that's a bit garbled, so maybe I want to install some tools to help me out with this as well. This is json data, and I'd like to format it well. I know that I have a tool called jq available over in the Linux world, but if I run this, you'll see it's not found, so I need to install this, and with Alpine I can just add a package with apk add and then --no-cache, and I could specify jq. So this is how I can install a package with Alpine Linux. This will pull down the jq command and add it in. And forgive me, I had a little bit of trouble with internet connectivity, I lost it in the middle of trying to pull that down. Okay, that time it worked, and now I could use jq on that metadata. json file. Sorry, manifest. json. Hmm, looks like I messed up how to use that command. There we go. Okay, so you can see that this has some json data in it. And you can see how easy it is to pull down this jq package, and I could start extracting out some of those sublayers, but you know what, I would like to look at this stuff on my Windows machine. And the one thing I did when I mapped in this particular volume, I only mapped in the one file. So right now on my host, I only have my iis. tar file, and I can't get any of these extracted files out. So this is why it's really important to think about the volume mounts you want to use, so that you can get files out of a container. So, let's go ahead and exit out of this container. I'm back here on my host machine. And if I list a directory here, you can see we still just have our iis. tar file. Join me in the next video where we can see how we can get files out of a container.

Getting Files out of a Container Back onto the Host
When I launched that container the last time, I was really restrictive, I only gave it access to that iis. tar file. I'd like to give it access to my whole wes directory. And I'm also going to change the directory I mount into, because I want to show you that you can mount into whatever you'd like inside the container. Just needs to be a valid Linux path in this case, or if it's a Windows container, it needs to be a valid Windows path. And then I'll just run an ls with /wes quickly, just to show you that that's working. Okay. Looks good. We have the same directory here as I have right here on the host. Now let's go ahead and clear the screen here, and I'll go back up to that command. Now this time I want to work with files on the host, so I want to get files out of the container, the files that I extract out of the tar file, I want to get those files back out on the host so I can work with them on the host here. So, I'm going to do a couple of things. First off, I'm going to open up this directory on the host. Okay, so on the left-hand side here, I have my terminal, I have my docker run statement, and on the right-hand side I have my wes directory with the iis. tar file. So I want to get the extracted tar file out onto my host computer, so I can take a look at it there. So I'll come back over to the Command Prompt, and then I could open up a shell in the container and manually execute commands, but how about we just run one single command here, tar, and let's start with just a tf to make sure that we have things mapped correctly, wes/iis. tar. That's this iis. tar file here. Let's just list the contents again. So I'll run one command and that's it. Okay, it's a bit hard to read here, that's okay, though. It looks like we printed out the list. Now I'll up arrow, and instead of t, I'll use x, and then I need a directory to write this out into, so I'll come over in my host here and make a new directory, and I'll just call this iis, and I'll specify that directory here over inside of the command that we'll ultimately run inside of the container. So, there's a lot going on here, but just remember that after the image name, so Alpine Linux is the image name, then we specify our command, and then all the arguments to it. So, really, you could just gloss over everything except this very last part. It's like I'm running tar here on Windows, and I'm specifying some flags to it to extract this iis. tar file into this iis directory. Okay, let's go ahead and run this bad boy. Now this will take some time again. It's a pretty big file. And then it would also help if I mapped things correctly. So you're probably wondering, what's going on here? I want to map this to /wes/iis. You, too, will make mistakes with paths, even after you've been working with containers for a long time. Especially when you're moving between Linux and Windows containers, it is hard to keep things straight. Okay, so this will take a moment. We can come over to this folder, though, while that's running, and take a look at that! We already have stuff on our host machine. So we can just sit here and watch stuff coming across. Look at that, we have more folders showing up, isn't that awesome? We have a shared file system between a Linux container and our Windows host, that is so, so cool. In fact, the long story short, the point I wanted you to take away from this module, it's like you can just run commands that you don't even have on your computer that could be Linux commands right on your Windows machine. I don't even have to open up a shell into the container, I can just fire off these commands as I go. For example, I told you that there are nested tar files inside of these layers, so let's go into e68 here. Okay, and then I'm going to come back over to the command-line. This will get a little bit convoluted, because this is long, but I've copied that big long e68 string. So this time when I run the tar command, I want to extract this layer. tar. So I need to point at that instead of the iis. tar. So, first I need to paste in that big long string, and then I need to do / and then layer. tar. And then wes and then iis and then we'll also put it into the same folder that's in already, / and then layer. And let's go ahead and make that folder over here. So this layer. tar will get extracted out into this layer folder by virtue of this command, if I actually put all the things in the right place here with the paths. Ah, yes. I'm missing the iis. So it's wes/iis/ and then there we go. This is probably a good case for popping open a shell and doing things that way, but I did want you to see that there's nothing stopping you from running a whole host of commands into a container and just running 1 command at a time that's really fast, I mean, look how fast it was to even extract that 800 MB tar file. And, of course, the smaller tar file only took a fraction of a second. And now I have this extracted layer here. We'll take a look at this in a minute. I want to show you that we could also shell into this container. So let's go up a few times and get the smaller form of this command, and let's just delete parts of this. So right after the image name Alpine, I can just type in sh to go back to using a shell this time. I'll clear the screen here, list out the directory, I'll change into wes and then iis. And now these directories match up on each side. And then let's change into the e1, now I have this nice tab completion. And if I list the contents here, let's make a directory called layer, and before I do that, I'll open it up on the right-hand side here. So I'll mkdir to make a directory. See it pop up right there? Now I can do tar, and then I can do -x for extract, f for file, and then I specify a layer. tar, and then I also need to specify -C and then layer as the folder. And if I take a look inside of here, look at that, we've got the files extracted out. Let's do one more just so that you have the hang of things if you want to follow along here. So, db, so actually I'll make this full screen here. I'll go up a directory, and I'll change this time into the db one, list the contents here, mkdir layer, okay, that's made the layer right here. I'll open that up. And then over here I'm going to run, I'll just up arrow to it, because it's the exact same command this time, so tar -xf layer. tar into that layer folder, but before I run this, I'll go back to the split screen so you can see the files being created on the right-hand side. And take a look at that, we have our files created now. I'm going to go through and extract each of these layers. Join me in the next video where we take a look at the contents.

Extracting the File System from a Windows Container Image Layer
I ran into a little bit of trouble extracting those layers. Long story short, I found out that the tar program inside of the Alpine Linux distro is having problems, and I found that the tar program that's delivered by the Ubuntu distribution in the Ubuntu Docker image works just fine. So, all I have to do is come over here and swap out alpine for ubuntu, isn't that awesome, and then just like that I can be up and running with a shell inside of Ubuntu instead of Alpine Linux. I'll wait a minute for these to complete, and once they're done, I should have a shell inside of an Ubuntu distro. So it's just a shell again. If I run the ps command, you can see we still just have a single shell process, much like our Alpine Linux distro, and I'll clear the screen here now. And because I set up the same volume maps, I can go into cd /wes/iis. Now one thing I did notice in this Ubuntu distribution, tab completion is not working for me. That'll happen at times. Sometimes the terminal behavior will be a bit weird when you're working inside of containers. There are ways to fix that, but I don't want to get into those in this course. So, for now, I will take this c6 layer. I found that this c6 layer has the heart of what I want to show you in it, there are other layers you can look at as well. I will, however, just change into the c6 layer's directory, and I will make a directory for the layer. So I can extract it, and I'll use tar, same command xf layer. tar, and then I will write out to that layer folder. Now, you'll get a bunch of messages about tar ignoring some unexpected headers. That's fine. If you come over to that folder for the c6 layer and come into the layer folder, and into the Files, you can just watch the extraction process here. This will take a little bit of time. This is about a 700 MB tar archive that's being extracted. But do you notice what's being extracted here? Let me go back up just so you can see where we're at. So here's that iis folder, the full path is C:\Users\wes\iis. I've gone into the c6 layer that we're inside of right now, the one that we are extracting the layer. tar file behind here, the layer that we're extracting right now in the shell process in the Ubuntu container. And here's that layer folder. And inside of here is a Files folder, and inside of here, what does this look like? Seems like we've hit the jackpot. This looks like a regular old C drive. Looks like our primary disk drive. This layer happens to have the bulk of that common operating system layer, the operating system apps and libraries that I was talking about. So back to this diagram. The layer I just found here pretty much has everything that I was referring to in the bottom two purple layers on the right-hand side, they're all rolled up into one here. And if I go into the Windows folder, and go into System32, here's pretty much the heart of all the operating system libraries and programs. There's even a cmd. exe right here. So feel free to spelunk around the contents of this image. As you can see here, I'll show you the full path again, this layer pretty much has the bulk of the entire operating system files. And this is how then we get this into our container into that special private file system that's set up for the container, it's copied out of this image more or less. I can even be crazy here and I could double-click on this cmd. exe. I would caution you perhaps not to do this on your regular work computer, because this will start loading up some libraries from this file system we're extracting here, but I can double-click and run this as a regular Command Prompt here on the host machine, even though I just extracted this out of the image. So, what I just did here, where I used docker save to save out the tar file of the entire image, and then I opened that up, and then I opened up the tar file for a layer, and extracted the contents, and then I ran a program here, this is exactly what the docker engine is doing behind the scenes, what I just did here, step by step by hand.

Running nmap in a Container
Before we wrap up this module, let's run a few other command-line utilities that start to showcase the power of grabbing software and running it in a container instead of needing to install it locally. So come out to the Gist for this course. You can do that via this shortened URL here. And then come down to the CLI apps file, and grab the raw version of this, and I'll zoom in a little bit for you here. And the very first thing I want to run is nmap, and you can see that there's a command here that will do this. Go ahead and copy this command, and then come over to a PowerShell prompt, and paste in this command, and we'll talk about this briefly. So, a couple of things here. This is using docker run, so we're going to grab an image in a moment here, specifically this weshigbee/nmap image. This is one that I've build. I basically just took Alpine Linux and installed the nmap package into it. We will talk about building images in the next module. And then I have a back tick character here, because I like to split these lines apart sometimes, mostly because now on this second line, it's like I'm just running nmap. If you gloss over everything that comes before this, I'm running nmap and passing some parameters here. If you just ignore everything else before this, it's like I'm just running the nmap command on my computer here, even though it's going to be pulled down from an image and run inside of a container, and never actually installed on my computer, aside from being installed inside of a container. Okay, so the flags I'm passing here will do a port scan across my network. The v flag is for verbose, and then I'm also scanning the 0. 0 network. You might need to adjust this for your network if you actually want to do a scan of your network, and I can just go ahead and hit the Return key here. Docker will pull down my nmap image, and then once that's done, and you can see this is actually a pretty small image here, about 5 MB in total, it will start up nmap and start to scan the hosts on my network. And then after a few moments, I've got some information about open ports on my network. And once the container is done running, the process exits, I could then run the command again and change some of the parameters and scan something else about my network, and I never have to go about the trouble of figuring out how to install nmap on my Windows machine, I can just use it inside of a container image that I know works, which means I can focus on the task at hand, scanning my network, and not on how to install software.

Converting Videos with ffmpeg in a Container
Okay, and one more example. Getting back to FFmpeg, which I talked about at the beginning of the course, how much of a hassle it can be to pull down a copy of it just to use it to run a simple video conversion. So, I've set up an example here. It spans a couple of different lines. We have the docker run with the remove to get rid of the container when we are done with this process, this FFmpeg process that we're going to run. I've set up a volume mounting with the current directory, and I mapped that into a /output inside the container, and that's just so that we can convert a video and write out the converted video into this output folder on our local computer, so we can access that when the conversion is done. So, the long story short here is you can focus again on just this last line here, and if you ignore everything that comes before, which is just the plumbing for Docker, we are calling FFmpeg, we're passing in this video right here, which is a little video of a turkey chasing my mom, and then we take that video, which by the way, you're just grabbing from online here, and we convert it into an animated gif, simply by typing the output path here and giving a file name as a. gif, and that will tell FFmpeg enough to go ahead and convert an mp4 into a gif, which is actually pretty cool. So if you copy this first line here, bring that over to PowerShell and paste that in, and then come back and copy the last two lines, don't copy the comment, and paste that in as well then, Docker will pull down the FFmpeg image that I have referenced here, and then once that's done, it will convert that online video into a local animated gif. I'll pause the recording until FFmpeg kicks off. Okay, the image has been pulled down, FFmpeg is kicking off here, all right it's converting now, and it looks like we have a prompt back, so if we take a look at that output directory, which was our current working directory, you should see that you have a Turkey. gif in that directory now. And then if we open up the current directory, we could take a look at that file. And take a look at that, the video's been converted. It doesn't look that good, the quality isn't that good, but hey, it's an animated gif now. If I had Docker on that computer that night when I was trying to convert that video, it would have been a lot easier to just pull down this image and use it to run the conversion process. I wouldn't have had to worry about installing it on the local computer.

Key Takeaways
There are two things that I want you to walk away from this module. First off, containers make it really easy to run software with a consistent set of commands. So you've seen now how we can run command-line applications easily without needing to worry about installing them, we can just use docker run and we're off to the races. This gives us a consistent story to run different types of command-line applications as we've seen. And then the other thing I'd like you to take away from this module, is that you can share host files with container processes, but by default you're protected. So by default, there are no connections from the container file system back to your host file system, you have to selectively enable those connections, and you do that using volumes. I do want to take a moment and recap how volumes work conceptually, just to make sure that you walk away from this module with a good understanding of how volumes are mounted into containers. So again, a volume is quite a bit like plugging a USB thumb drive, or maybe attaching another disk drive to an existing computer, thus giving us access to that additional file system. But let's step back a moment and just reflect that every computer has a C drive, and underneath that C drive are a series of folders and files that we work with when we're using that computer. And we might have other drives as well, but let's just simplify things down to a C drive. So that C drive is what our host computer has here, and then when it comes time to start up a container, we have a series of images that we have on our host. And we take one of those images, for example, nginx, and when we start up a container, when we create a container, we unpack that image into a file system. This is all done transparently for you behind the scenes. When we call docker run, we are creating this file system for our container. And as we've seen, that file system is completely isolated from the host file system. It has its own folders and files. And those all come out of the image. But we've also seen how we can now add references back to the host file system from the container, if, for example, we wanted to map one of the folders on the host into the container. So it's somewhat like having a shortcut back to the host to give us permission to access files on the host. For example, we might have some website files that we would like to host up in nginx. So if we start the nginx container, well, we might want to map in some of our host static website files, so that we can serve up those files from nginx. Long story short, this reference back to the host file system is a lot like taking the folders on the host and just mounting them as if they were directly under the given path inside of the container file system. And so we can use them as if it were just a local file system, even though it's a reference back to the host. So, keep in mind that there are two important pieces here when it comes to a container file system. First off, the only thing that the container file system has is what's inside of the image that you pull down. And then on top of that, you can layer in additional pieces by mapping volumes to the host file system. All right, let's move into building our own images next.

Building Images to Host Web Sites
Module Intro
We've seen how to use command-line tools now and we've even spun up web servers like IIS and nginx, but we haven't hosted any of our own content in those websites, so much like we did in the command-line module, in this module we're going to take a look at how we can host services inside of containers, and specifically how we can host a website. And in the process we're going to learn about how we can build our own images.

Mapping Static Web Site Files into a Web Server Container
For this module, I have a sample application that is a game of Solitaire, and it's just comprised of a set of static assets that can be loaded up into a web server and then you can play a game of Solitaire just by visiting the index. html page that's provided, and that's what we'll take a look at. Now, to get this into a container, for example, a container running nginx, we have a couple of a different choices of strategies that we can take. And you might start to think of these choices when you're moving into a production environment and you're wanting to use containers for hosting static website files. The first option we have is to pull an existing nginx image, create a container from it, and then use a volume mount to map files for our static website from the host into that container that we created. So that's strategy one. Strategy two, we'll see how we can actually copy files into the container file system. So instead of a volume mount, we will copy the files from the host file system into the container file system, much like you might do with ssh. With this copying approach, you could think of this somewhat as like using a cp to securely copy files onto another computer. And then the last approach we'll take a look at is how we can bake files into an image, and then we can pull down our own image that has our files already baked inside of it, so we don't have to worry about getting them into the container, we just start a container from that image, and everything's up and running.

Volume Mount Web Site Files
In the last module we saw we could take files on the host file system and mount them into the container, for example, that tar archive, and then we could take that file and do something like extract it. Well, in this module, we'll see how we can take a folder of website files and put it into an nginx web server container, and also an IIS web server container, so that the web server can host our files. So that's what we'll look at first here. So first I'm going to hop over to my Windows machine, because we're going to look at nginx first, and that is going to come in a Linux container. The first thing you'll need to do is go into the materials for the course and into the solitaire folder, and then inside of here is an app folder. This is the folder that we'll need to get into our nginx container. So go ahead and copy that, and put it somewhere where you know how to access it. So, I'll copy it into this C:\Users\wes folder. Okay, so I have my folder up, and I'm actually going to go up a level and grab the whole solitaire folder and copy that over here into my user directory. With that in place, we can come back to the command-line then and just take a look here. You can see there's a solitaire folder, I'll change into that, and I'll just list the contents here. We've got everything we need in place. By the way, I'll list the contents of that app folder, and you can see that there's an index. html page in there, that's what we'll be hosting with our website. So you could think of this app folder as the root of our website that we would like to share. So then help me through this command here. So, docker run, and I'll remove the container when we're done with the process inside of it, and so I'll set that up as interactive as well, so I can kill the nginx process, and it will remove the container then. Now, I want to run nginx. Do you remember what the image name is for that? The image there is just nginx, and this is the same one that we ran earlier in this course. Now, let's start out without the volume mount right now. Let's just get this up and running without our files in it first. Now can you tell me what I'm missing to be able to use this nginx container? Well, I need to set up a port mapping here so I can access the web server inside. So I'll go ahead and add a -p and 8080 for that, and I'll map into port 80 inside of the nginx container. Now if you're ever confused about what ports you should use, just hop out to Docker Hub and take a look at the description of the image that you'd like to run, in this case, nginx, and you can usually scroll down here and find some help about ports and volumes as we'll see in a minute here. So you can see down below that the recommendation is to map container port 80, which is where nginx is listening then, to well, whatever host you want on the port, but it's nice to see the container port here, the port 80, so let you know what you need to map to. Not every web server container is going to use port 80, though quite a few tend to, because again, that's isolated, so it's not a big deal to just use the common web port 80. Okay, I can just hit Return then to start that container up. If the image isn't available, it will be pulled down locally. Now in the case of nginx, we won't get any output once things are up and running, you'll just have this blank line after the status message here, or it'll just be blank entirely if you already had the image locally. So, let's hop over to our browser and take a look at this default nginx website. And there we go, we've got our website up and running without any of our own files. So let's go back to the command-line. Now what I can type in here to stop this container? Well in this case, I can just type Ctrl+C, which will kill nginx, and then the container will be stopped as well, and it will be removed. Let me clear out the screen there, and let's up arrow a few times, and let's get this volume mount in now. So I have that app folder right next to me right now, so I can type -v, and then app, except I actually need to specify the entire path to that app folder, so C:\Users\wes\solitaire, and then app. So that's on the host. Now I need to map the container file system path-- do you remember the delimiter that I use to separate these two paths? I use a colon here, and then I can map into the container file system, and this is a Linux container file system, so I'll need a forward slash here, but I don't know exactly where I need to map to, so this is another case where you should hop out to Docker Hub and take a look at the documentation. Usually there'll be some docs to specify what a typical volume mount will look like. If mounting files in is something you would likely do with a given image, and since nginx is a web server, that's a pretty common thing. So you can just copy the path right here, hop back over, and then we can just paste in that path from the website from Docker Hub. And I'll arrow over a little bit more so you can see the full command there. So let's go ahead and fire this off now, and as I said, once you have the image you'll just get a blank cursor. So, nginx should be back up and running now. Let's come back over to the website and let's just refresh here and see what we get. And take a look at that. We've got our game of Solitaire up and running, so we can drag the 6 over, we could play a game. But the good news is we have our static files mapped into the container. I just went over to localhost 8080 and I'm up and running here, just like that. So this is the first approach we can take, which is to mount the files from the host. Next let's take a look at what it's like to copy the files into the container's file system.

Modifying Files in a Running Container
All right, so let's go ahead and kill off the last container that we had, clear out the screen here, it's no longer up and running, so that's a good thing. Now let's come back to the command-line and let's talk about how we can copy file into that container file system. Keep in mind that the host file system is completely separate from the container file system. And without a volume mount, well, the container can't access files in the host. So another option we have is to physically take the files from the host and copy them over to the container file system, much like we might copy files across a network onto another computer. So this time what we're going to do is go back up to that first run of nginx that we had, where we didn't use a volume mount. Except this time I'm going to make some slight changes here. I'm going to get rid of removing the container if we were to stop it, and I'm going to switch from -it to -d, -d standing for detached, and the best way you can think about this is when you use -d, the container is running in the background, kind of like a daemon. But the -d means we're not attaching to it, so we won't see the output of that container, and as a result of that, we can't use the --rm anymore, that doesn't work with detached containers. So that means at some point we'll have to clean up this container when we're done with it. I'll leave the port mapping alone, and I'll also leave this as the nginx image, but I'm going to set a name here, and I'll need this so I can easily refer to this container, so I'll call this nginx. I'll start that up in the background now, and I'm returned back to a prompt here. I can run a docker ps, just to see what containers are running, and I can see my one nginx container is running right now. If I take a look at the website, and when I refresh it, I actually get the green background here, which is a bit weird. Make sure you use something like Ctrl+F5, so your browser isn't using the cache to access the files for this website. Okay, so we can see we have our default nginx web server up and running here without any of our files mapped in. So I'll hop back over to the command-line here, and I'm going to use a command docker exec to make some changes to this container. Do you remember what I said that the docker exec command allows you to do? Well one of the things you can do is you can run another process inside the container, and if you want, you can always get some help information for these commands as well. So you can see here this allows us to run a command in a running container, so we can start up another process inside of our container that already has a process. So what that means is while I have nginx running, and you can see that because the default command is nginx inside of this container, I could also do something like open up a shell inside of the container. So I could type in docker exec, and then -it for interactive, I need to specify the container id next, so nginx I'll use the name that I assigned, and then I also need to specify what I would like to run, and this case I'll run a Bash shell. Now, one thing I want you to avoid confusion with, I named the container nginx, maybe that wasn't the best name, don't confuse that with the fact that this container was also started with nginx. So, this nginx right here that I have of my command, it's referring to the container name, and you can see that in the docs here, it's a container that I'm referencing, not an image, so don't confuse those two. So what I'll do here when I run this is open up a Bash shell inside of the container, and I'll be attached to it thanks to this interactive flag. Now once I'm inside this Bash shell sees the container's file system. So If I want to peek around, I can do that. I'll have to use Linux commands, so ls for example, and we can see that we have a totally different file system here from the host file system. So I came in from Windows, which is completely different in terms of having a C drive, and into a Linux container with a totally different file system. Now a moment ago we saw from the docs that there's a folder here that we can use to access the website files. So I'll copy that, and I'm going to change into that here. And you can see we have an index page. I could move that index page to perhaps index2. html, and now when I come back to the browser, go back to my nginx web server and refresh, you can see we are missing the file now. So I can make changes inside of this container, changes to the file system, and obviously that's going to be reflected in the running web server. Join me in the next video where we go ahead and copy over into the file system our solitaire web app.

Copying Files into a Running Container
Okay so right now I'm inside of this Bash process inside of my nginx container. If I took a look at ps, for example, you could see that there's both the Bash process, and if I type ps x, you can also see the nginx process running. So I'm going to exit out of this Bash shell now, and I'll just clear the screen here. Now right inside of the solitaire folder I have the app folder that I'd like to copy in to the website root. Now I've got a small challenge for you. I'd like you to pause the recording and see if you can figure out what subcommand I can use from Docker to be able to copy these files in. Okay, so there's a Docker cp command that allows us to be able to copy files almost as if we're copying them between computers across a network. It's a good way to think about containers as almost a separate computer, because there's so much isolation that the file systems are separate that this is a good way to think about things. So it's like I need to take this folder on my computer on the container host and copy it into that other computer, which is really just a container here. So I can use docker cp, and then I first need to specify the source path, and this is on my container host. So that's the app folder. And make sure you put a \. on this. In this case, I want to copy the contents of app folder, and then I put a space here, and I need to specify the container and the destination path, so two separate parameters here delimited by a colon. So I first start off with a container name, so nginx in this case, or I could have used a container id, colon, and then I can paste in that destination path that I got from the Docker Hub documentation. Now before I do this, I wanted to quickly run a docker exec here, and in this case I'm going to execute another process in the nginx container to just list out the contents of this folder before we actually copy the files over, even though we've already seen this here a moment ago, I just wanted you to see those files there. I can come back then and I can run my copy command now, come back over here, and I will list out the contents again, and you can see that we have our files copied in indeed. So just like that I can copy files in, like copying files across a network, and just like that, the site is back up and running now that we've copied the files in. So these files live now inside of the container file system. They've been copied in, physically copied in. Next let's take a look at how we could bake the files in so that when we pull our own image, we already have the files inside of the container file system.

Baking Files into an Image from a Container
So our last option here is to bake files into an image, and to do that, we actually have to do what we already did here, what we just did, which was set up a container, copy in the files that we want into that running container, and then we can take that container and take a snapshot of it, much like snapshotting a VM, and that's how we create an image. So in our second option here, we already set ourselves up for the third option. So let's take a look at that now. So I'm back over here at the command-line. If I take a look at docker ps, we still have our nginx container running, and it already has the files in the file system. As we saw, we can take a look at that with the ls command. And you can see we still have our files in here. So now what we can do is use a command called commit. This is a lot like a get repository where you take a snapshot with a commit to save the changes you've made. Or it's a lot like taking a snapshot with a VM to save the state of the VM. It's a lot like taking a picture of something. You now have an immutable copy of it to be able to look at it anytime you want, and it has the state at the moment in time when you took the picture or the snapshot. And what's the docker commit command does. So you take an image and you start a container, which is what we did, we copied some files in, and then you take a container, so a container's changes here, and with docker commit, you actually produce a new image. So images are created from containers, and containers are created from images. It's like the circle of life for containers. So all we have to do here is type docker commit, and then you can see we need to specify the container, and then optionally provide a name for the new image. So I'll type in here nginx as the name, so that's the container we're snapshotting, and then we need to specify a name for this. So let's call this solitaire, and I'll also give this a tag, so the tag will be nginx. So this is the nginx version of Solitaire. You could imagine we have an IIS version of Solitaire. And there we go, just like that we have a new image. How can I go about taking a look at that new image that we just created? I can use the docker images command. And take a look at that, right there on top we have a new repository locally called solitaire, so that's our image name, and we have one particular image in it with a tag of nginx. So we created this from the nginx image that we pulled down, the latest tag of the nginx repository out on Docker Hub, we pulled that down, we took that image, we spun it up into a container, we added some files, and then we took a picture of that container with docker commit, and we now have a new image that we can use. So, let's take a look at this. Join me in the next video where we kill off the nginx container that we have right now and start up a new one that has the files baked into it.

Running a Container from a Custom Image
So right now we have a container running that we started up from the nginx image, and you can see that here. But we actually could leave that one up and running, we don't have to kill it off right now. Let's create a new container with docker run, we'll put this in the background as well with -d, and we'll map a port here, a different port this time, we'll go 8090 on the host, maps to 80 on the container, and this time what can I type in here for the image to be able to use that image that we just created? Well, the image was called solitaire, and the tag was nginx. So if I put that in there then, and I wanted to pull up the old docker run command that we used, so this is when we were using just nginx, this is now what we're using with our image. So you can see that the only difference here is this very last part, the image that we created versus the default nginx image that we were using before. So with that I can run that container. It'll be listening on port 8090. I can do a docker ps here, and you can see both of these are now up and running, and they're both using nginx. The only difference is we have some files baked into one, whereas we don't have files baked into another. So let's hop over to the browser. Now this is 8080, this is the one we had set up before. I'll make a new tab here and we'll go to 8090. And take a look at that. We've got our new website up and running in a brand-new shiny container, a container that was created from an image that had our website files baked into it. We can also run a docker exec here and take a look at the files. Now this time I didn't name my container, so I'll need to use something like the d4b here for the container id. I'll specify ls and then the website root here, and you can see we do have the files inside of this new container that we created from our own image. And we didn't map them in with a volume, we didn't copy them into this one, they came right out of the image itself. Let's take a little bit closer look at that image next.

Looking at Image Layers
Let's step back and talk a bit about what just happened when we created our own image. So, we started out with the nginx image, and the bottom of that image contains an operating system base OS image layer. In the case of Linux, that's typically the contents of the distribution that you are running on. And then on top of the operating system layer, there's at least one additional layer that contains the nginx components, all the files needed to run nginx. Now, I'm simplifying things and saying there's two layers here, but this helps reason about this. Let's hop over and actually take a look at the history of this nginx image. We can do that from the command-line. So over on the command-line, we can type in history and we can provide an image id. This prints out the history of how this nginx image was created. You can see a column that contains what looks like a series of commands. This somewhat looks like a script that we might have used to create this image. And this script is broken out over a series of lines. I do want to say, though, don't confuse each of these lines with a layer. Not every line here results in an actual layer in the resultant image. If we were to group things together, the bottom two most lines here form that base OS image layer that I was talking about. And in the rest of the lines above, logically speaking, form the nginx image, though there actually are two layers that result from the lines above that are part of that nginx image, but it's okay to simplify things and just think of this as one layer. If you want you can get a more detailed output with no-trunc here, though this is going to wrap all over the place. And you can now see the full script, for example, though it wraps over four lines here that's a part of setting up nginx. This would be a regular shell script that you might run. Now let me show you a place where you can find this history via Docker Hub as well. So if you come out to the nginx repository on Docker Hub and you come down to this section here with the tags, we're using the latest tags, so this is the Dockerfile that corresponds that, and a Dockerfile is a way of building up an image. And you could think of this as automating the commands that we manually ran to create our own image. So we had the docker run to create a container, we docker cp'd to copy files in, and then we docker committed to create the image. Well, a docker file is a way to automate all of that. This corresponds to what we saw with the docker history command, so you can see that this nginx image is based on debian, so that is the Linux distro. Next is the maintainer, which we also saw as a line in the history of the image. This is more or less just a label that's applied to the image. Then we had the environment that's set, so that corresponds to what we saw in the output in the command-line as well. And then here are the scripts that are used, so big long scripts we saw, this is a much more digestible format for what we saw with the history command. You could even see the expose here, and then setting nginx as the command that will run when this particular container starts. So the lines here in this docker file correspond to the lines that we saw from the history command. Now, keep in mind that a line in here doesn't necessarily correspond to an actual physical layer. Loosely speaking or as a rule of thumb you can assume that if there are file system changes, than an actual layer will be created for a given line. Now one last thing, this nginx container is based on this image right here, so I can copy this, I could come up and open a new tab and go to hub. docker. com, I can then paste in the image that I'm looking for here, and this is the image that served as the based for the nginx image, and you can come here and take a look at the Docker file for this. And here you go, you can see the two lines that were at the bottom of that history command. So just to summarize, you could think of this as one layer at the bottom for the operating system or distribution components, again, the user space of the operating system, the apps and libraries that are in the user space, and then the next layer up is typically whatever framework or application that you're using to host your own application. So I sometimes like to refer to that next layer or set of layers as the framework layer. And in this case, it's nginx. So when we created a container from that nginx image, that container is based on those bottom two layers, but then there's also a layer that's put on top that we haven't talked about yet. Each container that we create also has a very special layer on top where it can read and write files to, so that it can make its own changes to the file system that comes from the image, and that's how we're able to copy in our website files with docker cp. So we literally copied those into this very special container layer on top that is read/writable. Now one thing we haven't gotten into is how we take these layers and build a file system off of it, so let's talk about that next.

Union File System
So we have these various different layers that have various different files in them so that we can reuse common layers, but how do we go from layers to a file system for our container? And this is where something called a union file system comes into play. And long story short, you can think of it as the mathematical union operator. It takes different layers and it aggregates them up into a single view that the container sees. And that view contains that read/write layer for the container as well. So then as the container's writing files to that very topmost layer, like when we copied our website files in, those transparently become available via the aggregated view from the union file system. So the best way to visualize how this works, how these layers are collapsed down into one file system, is to assume that we have a container right now that has three layers from the image, so the bottom three layers here, the colored ones come from the image, and then it has that container layer on top, the read/write layer for the container. What this really looks like is an aggregated view with all these files kind of smooshed into one file system. This one file system that we see inside the container is what's known as a union mount point. There are a couple of interesting things about this. Each different layer can contribute files, and if you want to make a change to a file and a layer above, then the file in that layer above will trump the file and the layer below. So, for example, here our magenta layer, the second layer, has the same file as our top layer, the black container layer. So in that case when we flatten these down, the topmost file wins. So that's how you're able to make changes to existing files, and even delete files. And the neat thing is, a layer only has the files that were changed when that layer was created. Changed or created I should say, or even deleted. So a layer doesn't contain all the files in the layers below, it only contains what was added for that particular layer, which makes this very efficient, layers can be very small. So when we added in that app folder for our solitaire website, the only thing that the topmost layer contains is that app folder. So if we hop back over and take a look at that solitaire image again, and I'll run docker images here, and I'll pipe this to select string to just grab the images with the word nginx in them, you can see over on the right-hand side, the size isn't much different here, we only have about 2 MB more in the image that we created, and that's the 2 MB from the files for our website. And if I pull open the app folder and just take a look at its size, you can see it's about 1. 5 MB here, which adds up. If we take the 1. 53 from here and we add it to 181. 5, we get about 183 here. So now join me in the next video where we talk about how we go from this top read/write layer into a new image to create our own solitaire image.

How Running Containers Turn into Images
So we started that nginx container with docker run, which created a file system from the nginx image, and then of course, we get that container layer on top, and that's where we copied our own files into. And then when we got to docker commit, that's where we took what we have on the right here for our container and just took a snapshot of it to make a new image. So the process to make a new image is really easy, we just have to take a snapshot of that container layer on top and make a new image that has that one as well. So that's how we can take a container and make a new image from it. So really what we have here now is the bottom two layers, and then the next layer in our new solitaire image is from the application that we copied in, our solitaire application. And then when we ran this particular image and created a new container, we pulled all three of these new base layers from our image, our solitaire image. And of course we have a new container read/write layer on top if we wanted to make further changes. And we can go on and on and on with this cycle, adding more and more layers to an image and creating new images in the process. So let's recap this entire picture of what we more or less did here, except let's just say this time we were basing our new image on IIS instead of nginx. So, we could have started out by taking the Docker Client on our computer and then sending a docker run command to our docker engine to run the Microsoft IIS image. So we create a container then, and to do that we would need to first pull down the Microsoft IIS image, we pull down each layer separately, unless, of course, we had them already, and then we would spin up a container, and in the process we would just add a new container layer on top, that read/write container layer, and then we could also spin up a second container if we wanted to, reusing the layers that we already have. In fact, we don't even have to copy those layers. Because of how the union file system works, we can just have one copy of a layer downloaded, and we can use it in multiple containers. And then in our second container here with its own read/write layer on top that's unique to it, we could then use our Docker Client with the docker cp command to copy in a file. And the file in this case would be the contents of our website, there would be multiple files that we copy in. And then when we're done, we can commit to create an image, and then we could actually push that image out to Docker Hub, an image perhaps called wes/website, or site, whatever we would like. And then other people could pull that image as well. And they could add new things and push out new images with another layer on top.

docker history and docker diff
Before we move on, I want to show you a few more commands you can use to understand images, specifically the image that we created, and also to understand a little bit about what's inside of a container. So right now we have these two images here, and we ran the docker history command on the nginx image a moment ago. We could also run this on our solitaire image. Now this isn't super helpful, largely because of how we built this manually by hand with docker commit, but if we to use a docker file, you'd see something more meaningful here for the command. We'll get to that in a moment. But there you can see on top is the new layer that we added. And if you wanted to, as a learning exercise, you could pause the video and export with docker save, just like we did with the Windows container image in the last module, and you could extract out those layer tars and see if you could find the layer that has the files that we added in. That'd be a great little learning exercise. And another thing you can do, because we still have these containers running, there's a command called diff, this will perform a diff of a running container to see what changes a container layer has in it. It's not an exhaustive diff like a git diff, but it's somewhat like that. So I could come in here and specify the nginx container, and pay attention that this is a diff of a container, not an image, so we need to specify a container name here, and nginx, that's the container that we created and manually copied the files into, so let's look at a diff of that. But before I run this, what do you think will show up in the output here? Let's fire that off, and you can see all the various different files that were added. And most importantly, you can see the files that we added here into the website root, all the various different files from that app folder. And then there also are some other files as well. These were modified by the running nginx instance, so when we took a picture of this snapshot, we took a picture with these changes as well. And it's possible that we wouldn't want to do that if we were building a production image. So that's something you'll want to watch out for when you're building your images.

From Commands to Dockerfile
Let's recap the commands that we executed to create our new image. So first off, we created a container based on the nginx image. This set up a web server so that we could test out the files that we were copying in, which then, that's what we did next, we copied in some files, copied from that app folder. Actually, we also renamed that index. html to index2. html, the starter files that came with the nginx image. So there's another command in here missing, but we'll just gloss over that. And then the last thing that we did was committed the running container into a new image called solitaire:nginx, so the nginx tag for a new solitaire image. And you could imagine we could have run other commands in here, we could have copied other files in, we could have gone in and wiped out some files. Maybe we want to delete those starter files that came with the nginx image. Maybe we want to clean out those cache files that we saw a moment ago. So lots of things we could do. Maybe we want to install some extension or plug-in into nginx for our application. Or maybe we want to reconfigure nginx, there are lots of things we could do here. And they would all come between running to create a container and then committing to create the image. And we can type these out by hand, but that's going to be cumbersome and error prone if we're typing them out every time. And of course we could put these in a script file, but we don't need to, because Docker comes with a special script file just for this purpose, and it's called a Dockerfile. So let's transform the commands that we ran here into the equivalent of a Dockerfile. So first off, instead of a docker run statement to create a container, we just specify the image that we want to build on top of. And we've already seen this a few times when I've shown you Dockerfiles, and I was saying, hey, this image is based on this image. Well, that's what I mean here. So we're saying, hey, we're starting from the nginx image, and then we're going to add the rest of the stuff in this file, and next thing we do then is take our COPY command and turn it into a COPY command for our Dockerfile. We copy in the app folder into the website root that nginx is hosting out of. And then that's it. These very first two lines are everything we need to form a Dockerfile. We save that then, right next to the app folder, so that the pathing is correct here for our copy statement. And then instead of docker commit, we don't have a container running right now, so we can't do that, we instead use docker build and more or less pass the same parameters here. So, docker build, we specify the location of this Dockerfile, though actually the default is called Dockerfile so we don't have to do that, and then we tag the image with the given tag here of solitaire:nginx. And then the very last parameter, it might be hard to see there, is a dot. That's the path to what is known as the build context, which is the current directory, assuming we are sitting right next to that app folder. So let's take this Dockerfile and this docker build statement, and let's use this to build another image.

Using docker build to Create an Image
Okay, so I've created a new file here, and I'll save this, and I'll put it right next to the app folder. So inside my solitaire folder, right next to the app folder, and I'll call this Dockerfile. If I list the contents here, it's got the. txt ending, that's okay, we can move this, and now we've got the appropriate Dockerfile. If I cat out the contents of that, you can see the two statements that we discussed. Now if I clear the screen here, and if I look at docker images first, and if I look at my nginx images locally with docker images and then piping to select string here, you can see what we have before, and now I'll do a docker build, and I could specify the Dockerfile if I wanted to, that's not necessary, so I'll leave that off. And then I need to specify the tag for this new image, and I'll call this one solitaire, and then we'll do nginx- Dockerfile, so df. And then I need to specify the build context as the current folder, so I'll use dot for that, and I just hit Return, and we get a bit of output here. First off, we have step 1 hre, FROM nginx, so we're building our image on top of the nginx image. And when we do this, it's important to note that a container is created for us behind the scenes, and then the various different commands are executed for us, so then step 2 kicks off to copy in files. You can see then we are removing the intermediate container that was created to run the COPP command inside of, and then we have a successful build of a new image, and here's the ID for that image. I also have a warning here telling me it's probably not a good idea to be building an image from a Windows host against a Linux Docker host, but I'm going to ignore this for right now. In a development environment it's probably fine to be building things like this. In a production environment it would be a good idea to build your Linux images on a Linux Docker host, just to avoid line feed issues and permission issues and whatnot. Now if I take a look at my nginx images again, on the very top you can see a new image was added, and we have the tag nginx-df. And size-wise, it's exactly the same size as what we had before. So we automated the process with our docker file to create a container, copy our files in, and then commit that container to create an image. And the nice thing is, you now have this declarative file where you specify your instructions, and your image is built automatically for you. Now as you explore Docker after this course, I'd encourage you to refer back to the Dockerfile reference. It's a wonderful way to learn the various instructions that are available, for example, here is the COPY instruction, and then here are the various different options you have for this COPY instruction.

Creating an Image to Host a Web Site in IIS
I've got a challenge for you, if you would like it. I want you to take that microsoft/iis image that we worked with earlier in the course, and I want you to set up this Dockerfile to use that instead of nginx. So a Windows container instead of a Linux container, and IIS instead of nginx, so figure out where you need to put the files at and copy them in, and see if you can get an image created and then use it to create a container and see if everything's working. So give that a try if you'd like, and then come back and let's walk through things together. All right, let me walk you through my solution to this challenge. So, first thing I did was I copied that app folder from the materials for the course into a solitaire folder on my Windows Server machine, because I have Windows containers over there right now, and I don't have to then switch my Windows 10 machine to Windows containers, I can keep it on Linux containers. So I switched over to this server, copied that folder, and then I set up a Dockerfile, and that Dockerfile looks like this, FROM, and then microsoft/iis:nanoserver. And I'm using the nanoserver tag because it's a lot smaller image. And then I have a COPY command just like before with the app folder, just a new destination here, C:/inetpub/wwwroot. From all of that then, all I have to do is a docker build, and let me just pull up the one that I ran, you can see there with a tag of solitaire:iis, so an IIS tag on this one this time. You could imagine we could have multiple tags to be able to host our application in multiple different web servers if we wanted to. And of course the dot on the end of that build command to specify the current directory as the context and I'm in that solitaire folder right now. Okay, I ran that again, but nothing's changed, so that was actually rather fast here. One thing I would like to point out, when you run docker build again, on subsequent runs, you'll be using a cache if you haven't changed something in that Dockerfile, and that's really convenient, you don't have to rebuild layers if nothing has changed in your Dockerfile. And as you add more and more complex instructions in multiple layers to an image, it's very efficient not to need to rebuild layers unless they change. And then, of course, right now I have a container running based on that solitaire IIS image I created. I mapped port 8090 to port 80 in that container, IIS is listening on 80 inside the container by default. And then, of course, this is a Windows container, so I can't grab it from localhost, so I went ahead and opened it on another computer on my network here on my Mac and pulled up port 8090 here, and you can see that the game of Solitaire is working based on the Solitaire image that I built with the Windows container and IIS.

Pushing Images to Docker Hub
Before we wrap up this module, I want to show you one last thing. I want to show you how easy it is to share your images with other people. So if we look at our nginx images locally, we have the two that we created on top. And if I want to share these with somebody else, I need to retag them in a way that conforms with how Docker Hub recognizes the slug for an image, so basically the image name. And I can do that with the help of a command called tag. So I can take the image that I already have created here, solitaire:nginx, and I'll do the Dockerfile version, and I can add a new tag on the end here then. And my username is weshigbee, so I need to put that, and then / and then whatever I'd like the repository to be called, so I can make this Solitaire, and keeping with the image repository and name that I've been using locally, so this is going out to the weshigbee user on Docker Hub, and into the solitaire repository. And, of course, I could tag this if I want to, I could put nginx on the end here. Now all that has done is added another tag to refer to this particular image. I can use this locally, but then I can also push this remotely, so I can run docker push weshigbee/solitaire and then nginx. I can also push multiple images if I'd like, but I just have this one right now. Now you can see here at the end that I was unauthorized, so I couldn't actually push that out. I just need to come here and use docker login, specify my username as weshigbee here, put in my password, which by the way, I get my username and password by creating an account on Docker Hub, which is free, and before I push this, I'll type in solitaire here and show you that I don't have any repositories right now under my user account. Now if I come back, put in my password and log in, login succeeded, you can see I get the message login succeeded, so now I should be able to up arrow and push again. And each of the layers will be pushed out. Of course, the better way to go about this process would be to set up an automated build on Docker Hub, but that's a topic beyond this getting started course. If you would like to look into that, more or less you can set up a get repository and then link to your get repository's Dockerfile via Docker Hub, and whenever your check in changes, you can have a new build of your Docker image created for you out on Docker Hub. Now if I hop over to Docker Hub and just refresh here, I'll type in solitaire, and you can see the new repository that I created. And if you want, you could go ahead and pull this down and run this on your computer instead of building this on your own. Sadly, however, I am going to delete this, I don't want this sample sitting out here, so I will be deleting this. But I did want to show you how you can go about the process of pushing an image out to Docker Hub. And you could also have your own private registries that would allow you to share images internal to your organization.

Key Takeaways
Here are the important take-aways for this module. Number one, containers come from images and images come from containers! It's a circle of life when it comes to Docker. We create containers from images, we use the image to create the file system for a container, and then we layer on top a thin read/write layer where we can make modifications to the file system, and then when we're ready, we can commit that layer to form a new image. And then we can take that image and spin it up again, and we have a new read/write layer on top of it, and that's how we create layered images. This layered approach means we only have to be responsible for the bits that we need to add. No longer do we need to worry about installing nginx or IIS, we can simply pull those container images exactly as they are and just layer our website files on top and we're done. We're not responsible for those base layers anymore. Think about what that means in terms of maintaining software going forward. Now when it comes to making an image, use a Dockerfile to do this. It's a script that helps you build an image automatically. It has many of the same commands you would use at the command-line, just in a nice text file called a Dockerfile. Next we're going to take a look at databases and containers.

Running Databases in Containers
Module Introduction
Thus far we've seen how to run applications inside of a container, but we haven't talked anything about data yet. And come on, applications are only half of the puzzle. The other piece we need to understand is what happens with databases when we move to containers? And in this module we're going to take a look at how we can run databases inside containers as well.

Running MSSQL 2016 Express in a Container
So let's start out by talking about SQL Server. If you got to the Microsoft organization or user account out on Docker Hub, if you look through the list of available images, you should eventually fine some SQL Server images. There's an mssql-server-2014-express edition for Windows, and then there's also 2016 image. The 2016 one is named similarly to the 2014, just change the year here. So you could access either of these and quickly spin up a SQL Server instance, which has to be one of the coolest things you can do with a container, because if you've ever installed SQL Server in the past, you know that this is not a simple task. And it's not a task that can be done in any short amount of time. Let's see how fast we can do this with a container. I'm over on my Windows Server instance here, so I can run a Windows container. You could run this on your Windows 10 machine, just switch to Windows container if you haven't already done that. Now I've already gone ahead and pulled down the two images for both versions of SQL Server Express edition 2014 and 2016, so that I've already got those on my computer here. And these are based on the Windows Server Core image right now, which is a bit bigger, so you can see the size is rather large for these images. But again, not a big deal once you have the file downloaded, and the nice thing is most of it is this base image that you already have, it's just a few gigabytes beyond that first SQL Server. So aside from the time to pull down these images, all I have to do is a simple docker run statement, and if you hop out back to the docs on Docker Hub, you'll find a sample here. I'll copy the rest of this and paste that in. Okay, so let's walk through this. Let's come back here. So, first off we'll run this container detached. So it'll run in the background, and then we'll also specify a port here of 1433 on the host and we'll map that to 1433 inside the container. That is the standard SQL Server port. Now we have a new flag this time, --env. This allows us to specify an environment variable. And in this case we're specifying an sa_password environment variable, which allows us specify our own password. So I can come over to this, and I could type in something like password here. Though this won't work, there are some complexity requirements, but I want to show you what happens if your password is not complex enough. I want to show you how you can troubleshoot that container failure. Okay, so, we're then specifying that we'll launch a SQL Server 2016 Express edition instance, so let's go ahead and run this command then. And let's start the timer. Took about 6 seconds there, that's pretty darn impressive. Now, things are still starting up, a few seconds later things will be up and running. If you want, you can use the docker log statement when you don't attach to a container, so when you detach with -d, you can use docker logs then to see what's going on inside of that container, and in this case, you can see that we have a password validation failure. The password is not complex enough. So first off, I'll go ahead and stop that container, but it's still running, so 466 was the ID, or enough of the ID to specify that particular running container. A docker ps now shows there's nothing running, and then if I up arrow a few times, this time I can specify a password that's more complex. I'll put a bang in here, w0rd and 1. We'll see if this is complex enough. Okay, so I've cleared out the screen. Now I've got a question for you. I want to check what's going on. Quick quiz, what do I use to see the output of the container that I've started that I'm not attached to? And I'll do a docker ps for you so you can see that container. Well, if I want to look at the output, I'll use docker logs, and then I specify the container ID of 4e5, that's enough of the ID there. And you can see that this time we don't get the password failure, so it seems like things are okay now. Join me in the next video where we connect to this with SQL Server Management Studio.

Connecting SSMS to MSSQL in a Container
So we have this running container here with SQL Server running inside of it, and this is on a Windows Server machine here, a Windows Server container host, so I'm using a Windows Server container here, not a Hyper-V container, which means I can take a look at the processes, because I should be able to see the SQL Server process right here on the container host. So I'll open up the Task Manager, and I'll go to Details here, and if I look around, take a look at that, we've got a SQL Server process. In fact, in sorting by Job Object ID here, you can see all the various processes that are running inside of this container where we have SQL Server running, and there are a few more than we've seen in the past. Notably several of these are SQL Server processes or services. So, we've got SQL Server on the host here, and we're listening on port 1433, both inside the container and on the container host. So what I'd like to do is grab my IP address here, and I'm on the container host right now, this prompt right now, so I'll be getting the IP of the container host, and you can see it's 192. 168. 0. 15. So I can hop over to another machine that has the SQL Server Management Studio Tools on it, and now I can type in the password and connect to 0. 15, and I shouldn't need the port there, the standard port is fine, and take a look at that. I'm hooked up to my SQL Server instance, and I can take a look inside here. I can right-click and create a new database if I want to, maybe I want to call this payroll for a payroll database, click OK, and it looks like that database was created for me. Now, just to verify that this is the right instance, the one that I have in the container, I'll hop back over to the server, clear out the screen here, docker ps, I have my container here with SQL Server running inside of it. Now I've got a question for you, how can I stop the SQL Server process right now that's running inside of this container? Well, in this case, a simple call to docker stop, specify the container of 4e5, that part of the ID should be unique enough, and then hit Return here. And if I do a docker ps now, you don't see it running anymore. It'll only come up if I do a ps -a. And it's right up there at the top. I'll clear out the screen now, and if I hop back over to SSMS, and I try to expand this database here, the operation will time out eventually, and that's because I stopped that container, so the process is now stopped. Looks like it's still running, so let's hop back over and start that container back up. So how do I get the container back up and running? Well, in this case, the reverse of stop, I just need to call start. Now if I flip back over, looks like the operation did time out. If I refresh here, I can now expand the payroll database, so it's back up and running.

Running MySQL in a Container
We've now seen how to run MSSQL in a container, and way back at the beginning of the course we took a look at MongoDB, well, let's take a look at one more database, MySQL, and then let's step back and talk about some of the concerns you'll have when running a database in a container. So I'm going to hop over to my Windows 10 machine where I have Linux containers up and running. And a simple docker ps over on this machine shows that I have my solitaire site still up and running. I can stop all those if I want to, and I could use stop with two separate sets of IDs to stop both of those running containers. Okay, so now we have a blank slate here. Now let's take a look at using MySQL. If you hop out to Docker Hub and just search for MySQL, you'll find that there is an official image. And by the way, I think after some time, you'll start to see official images for Windows containers. For example, for IIS, or for SQL Server as we've just seen, and the various different editions of it, so I think you'll start to see some official images for these, but give it some time, probably six months to a year post Windows containers being released. So we have this MySQL official image that we can use, and like always, we can scroll down in the documentation here to understand a bit more about how to run this, and we have this nice command here to help us out. So let's go ahead and copy that, let's paste that in over here, and let's just walk through the components of this. So, we are going to name this container, some-mysql is fine. Then we also have a -e here, and this is a shortcut for specifying --env, like we just did a moment ago with the SQL Server container. So -e is a quick way to specify an environment variable. And in this case, we have one called MYSQL_ROOT_PASSWORD that gives us the equivalent functionality to set a password here for our MySQL server instance. So we'll leave that alone as well. And then also we'll be running this detached and then mysql as the image name, and we'll be using the latest tag. So I can run that then. It'll take a moment to pull down. Okay, and then once that's done, we should come back here to a prompt here, because we run that container detached. What can I use here to see the output of that container starting up? Well, if we want to see the output docker logs, and then I need to specify the container ID, and the container ID is this ID that's printed out here once the container starts up, so 126 is probably enough there. Whoa, that's a lot of output. So if you suspect that there's any issues with this container starting up, you could parse through this, and you could probably filter the output here to look for the word error. By the way, if you want, if you take a look at the help for logs, there is an option to follow the log, so even though you don't attach to the container, you could hook up and tail the log just to watch output messages as you are interacting with this process inside of this container. For example, if you had an nginx server, you might want to see the logs coming out, you could watch those coming out with logs then without needing to attach to the container. Okay, so we have this container up and running, our mysql server instance, let's take a look at connecting to it. Now I'm a big fan of using the mysql client when it comes to a mysql server, so let's just execute another process inside of our mysql server container, another process that starts up the mysql client. Now, I've got a question for you. What do I pass to Docker to be able to start another process inside of a container? Well, in this case, it's docker exec, and I'll then use an interactive shell here, I need to specify the container, so 126 is enough there for the ID, and then what process do I want to run? And in this case I want to run the mysql client. Let's see if that's enough to get us in. Looks like we have access denied, and that's because we need to specify a username and password. And then I need to specify the username here, we'll use root, that's the password that we set when we were creating this container, and then we'll also specify a password, and we'll set this to my-secret-pw. We use the default from that documentation. If we hit Return here, this will start up the mysql client inside the container. And in this case, it'll connect to the server running in the container, so we could show the databases here. And we could use one of the databases, and we could show the tables in this database. For example, there is a user table here, so I could run a select all from user, and there you go, there is the root user account that was set up, and I guess another mysql_native_password account was set up as well. I think that's enough to demonstrate that mysql works in a container. Next let's talk about some concerns that you would have starting with how data is stored in the container for our database system.

Docker Managed Volumes
So now let's talk about data and containers and what things we need to think about when it comes to having a database inside of a container. So let me exit out of the mysql client here, and if I do a docker ps, we still have the server up and running. Let me go ahead and stop that as well, so I'll stop 126 to stop that mysql server, and then if I clear out the screen here, we could take a look at something called a volume with Docker. And if you run docker volume and ls, you'll see volumes that have been created with docker. This is a new concept that we haven't touched on yet, because we haven't run into it yet, but when it comes to databases, databases have special considerations for storing their data, and it's pretty common for volumes to be created to house the data that a database is writing to disk, because the layered file system that we've been talking about with that thin read/write layer on top for the container, is not a performant way for storing data in something like a database. Especially when a database uses memory mapped files, which are pretty common. So instead is recommended to use something called a volume. And we've seen volumes before. We saw them when we mapped part of the file system from the host into a running container, so we could access files from the host, but there's another way that we can create volumes, they're a different type of volume, and these volumes then can be used to performantly store data for a database. And you can see that there's a volume here, and we have that even though we have no running containers. So volumes that are created for us, like in the case of this mysql server, persist whether or not the container is running. And that's because the data has to be there, so that we can stop the database and start it back up, like we saw with the SQL Server example and not lose our data. And it just so happens that that read/write layer for a container persists as well as this thing called a volume here. And in the case of our mysql server, a volume was created for us, even though we didn't specify that when we called docker run. This was created because if take a look at the Docker Hub page for the MySQL image, if we scroll up to the top here, and we take a look at the Dockerfile for the latest tag, the one we're using, version 5. 7, and if we scroll down inside of this Dockerfile, eventually you'll see a volume instruction. And when a volume is specified for an image, it's marking a certain path inside of the image to be created as an external volume outside of the file system for the container. And then that volume is created on the host and then mounted, much like a host mount that we saw earlier in the course. This is all done transparently for us behind the scenes so that we don't have to do this. We could have created this by hand, but most of the time you'll find that Dockerfiles, for databases especially, have these volumes specified. And the path here, var/lib/mysql, is the location where this particular installation of mysql inside of this container is going to write data to, so it's the default data location, and hence all the data we write to this volume will live inside of this isolated volume. And not only is that for performance reasons, but it's also so that if we remove a container, so if I do a ps -a and select for the string mysql, you can see here's the container that we have stopped right now for our mysql server. If I remove this, 126, which I said is like uninstalling software, this is going to destroy the container. But if I take a look at the volume list, well, actually, you tell me, what do you think is going to show up here when I take a look at the volumes on this computer? Do you think that the remove command destroyed the volume as well? Well, no it didn't, and that's kind of what I was hinting at. This volume then has a lifespan of its own outside of that of the lifespan of the container. And that's a good thing if you accidentally remove one of the containers that has a database in it, or anything else, and you had some special files copied in there or created, you don't accidentally lose those. And at the same time, this can be annoying if you don't know what's happening, because you're eating up disk space with these volumes and you don't even know it. Now this volume we have here from our mysql container, because we didn't specify it when we were calling docker run, it's what I like to refer to as an anonymous volume. It doesn't really have a name here, it just has an identifier. Join me in the next video where we go about creating our own named volume, and you'll see how we can use it to persist data across container instances. So we see how we create a container and destroy it and then create a new container and use the same named volume to pull back the data that we had in the first container.

Using Managed Volumes to Persist Data After Container Destruction
Okay, so we had this mysql container that's now gone. If I do a select string from mysql, we have no containers with that name. But we still have this volume persisting. And that's the data from that previous mysql container. We'll leave that alone, and I'm going to clear out the screen here, and I'm going to pull back the run statement that we used for that mysql container. Okay, and then I'm going to delete part of this. I'm going to put in a back tick so I can wrap a line here and we can see what's going on. And I'm going to create a new volume this time when we spin up this container. And we'll just call volume db, and if I put db as the name on the left side of the colon instead of a host path, so instead of a host path on the left side, if I put just a name like db in, then on the right-hand side I can map this somewhere. And then if I come out to the Dockerfile and grab the path here, and bring that back and paste that in, now I'm creating a named volume named db, and we'll see that in a moment with the docker volume command, and I'll use that instead, and then I can specify the same mysql image this time. So I'll spin up this mysql instance here, and if I take a look at docker volume and ls now, well, you tell me, what do you think is going to come back this time? All right, let's run this. And now you can see we have another volume created called db. It doesn't have a random sequence of numbers and letters, so we have this name that we specified. By the way, just for comparison, if I up arrow again, and let's go ahead and just create another mysql server, this time let's get rid of the volume specification and just use the default like we did the first time, and I need to give it a new name, so I'll come back here and we'll call this some-mysql2. Okay, now if I clear out the screen here, docker volume ls again, what do you think will show this time? i Well, now we have three volumes. The 63 is from the first mysql instance that we already got rid of, and then we have 2 mysql containers running right now, and each of them have a volume, 1 has the 70, which was the volume that was generated behind the scenes automatically, and then 1 has that db volume that we created. Okay, now, let's go ahead and get inside of our some-mysql container, which is the one that has the db volume, and I'll run mysql. So we'll get inside of the very first mysql instance that has the db volume. We'll show databases, and quickly I will create a database, we'll call it pets. If I show databases again, you can see we now have our new pets database added. So that's the data that we're persisting. Now let's exit out of the mysql client, let's do a docker ps, you can see we have both of our instances running. Let's go ahead and stop both of them, and actually, let's just remove both of them. And to do that while a container is running, if I put in a8f and 0b2, if I try to remove containers while they're running, I'll have an error come back. I have to stop them first. Or I can use -f to force their removal, which will stop them and then remove them. Okay. So I have gotten rid of both of those. If I do a docker ps with an sls on mysql, and I need to do docker ps -a actually, you can see I have no mysql containers anymore, they're all uninstalled, they're all destroyed. The containers are gone. But if I take a look here at the volumes, you can see we still have all the volumes for all of these mysql server instances, and that's the beauty of volumes, they stay around even when you get rid of the containers. And here's the beauty of these volumes. If I go up here back to my db command, which has the mysql instance started up with this named database volume here, so I can use that db name again, and when I run this one, it'll reuse the existing volume instead of creating a new volume, because there's already a volume with that name. So if I run docker volume ls, you'll see there's no new db volume, it's using the existing volume. And now if I exec to get in, show databases, what do you think is going to show here when I run this command? Is that what you expected? We have our pets database still. So our data is actually persisted to that volume, and it stays there across destroying a container that originally created this database. One of the things I love to do with these volumes is to use these volumes to keep my data around while I update the container, which updates the version of software. So maybe I'm running mysql version 5. 7. 1, and I want to go to 5. 7. 2, or whatever the versions are, I can destroy the container with 5. 7. 1 in it and start up a new container with 5. 7. 2 in it, and point it back at the volume I had with 5. 7. 1, and the data will automatically be ported over the new version. Of course, that assumes that your database server knows how to migrate data, but assuming it does, this is a really nice upgrade story. And these containers start in a matter of seconds. So I can pull down the latest version and within a matter of seconds I can have my database system upgraded to the latest version. And that's just something that was never possible with database technology in the past. So start to think, what could I do with the ease of spinning up database instances here, whether it's mysql, SQL Server, MongoDB, or whatever else you're using.

Cleanup - Stopping All Running Containers
In the process of talking about volumes now, we've started to see that there's this other component outside of a container that has its own lifespan independent of containers on your computer, and so you might be wondering, well, how do I clean up all of these things, and I hope you haven't ran out of disk space yet. It is possible at some point the more you do with containers, the more disk space you'll use up, especially as you start pulling down gigabyte images for various different pieces of software. And so you might want to clean things up at some point, so let's talk about that now. Let me exit out of my mysql container here, so exit out of the mysql client running inside that container, and I'll clear the screen here, and we'll do a docker ps. And right now we have this running container. And if we want to stop this container, of course, we can use docker and stop here, and then specify 432, that's probably good enough to stop it. But we don't want to have to stop things individually. Instead what you can do, if you wanted to stop every running container on your computer, after the ps command, add a q, a -q flag. This is a quiet output mode where the container ID is the only thing that comes back. Now if you take that command, you can actually pipe it into the docker stop command to stop multiple containers and you can do that just like this. So the output, a list of container IDs here, will be then passed in to docker stop. And now when I run this, all the containers will be stopped, and I don't have to specify each individual ID, and you will see each ID come back as each one is stopped. So that's how you can quickly stop a whole bunch of containers. Because you'll probably want to refer back to these videos from time to time when you forget what these commands are, I'm going to stop right now and I'm going to make a new video next to talk about how you can clean up or remove all of the containers on your computer. So, I'll have each one of these videos indexed in the table of contents for you to easily find these commands.

Cleanup - Removing All Containers
Let me clear out the screen here. So if we look at our containers right now, ps -a, we have a whole bunch of stopped containers, and you can see the status is exited on all of these. We have a few for the docs that we ran, we have alpine and nginx and solitaire and mysql, and at some point we might decide we'd like to just clean all of this up. Can you take a guess at we can remove all of these stop containers? It's going to be very similar to the command we just used to stop all running containers. If you want, pause the video and take a stab at that, and then join back and I'll show you how. So in this case, if we just add a q here to the end of our ps -a, then we'll also just output the ids of all of the stopped containers, well, all of the containers actually if there are any running as well. And the new can take this here, copy this, we can run a docker rm, and pass in this command to get all of the container IDs that are stopped or running or whatever state on our computer. And then the one thing you might like to know, if you come over and type in -f, you could use this remove command to not only remove everything, but also stop anything that's not stopped. So I'll run all of this just for your reference. Just be careful, obviously, because if you had any of these containers that you wanted to keep around, well, now you just lost everything on your system. If you didn't intend to remove everything, that could be problematic. I would only recommend these commands in a development or testing environment. Next up, let's talk about cleaning up volumes.

Cleanup - Removing Volumes
So even though we've cleaned up all of our containers now, we still have the volumes up above, and that's because volumes persist long after containers are removed, and that's a good thing. But if you don't keep an eye out for these, you can build up quite a few of them, and it can be problematic in terms of disk space. So there are a couple of ways you can get rid of these. First off, if you take a look at docker volume, this is a whole set of sub sub-commands, and we have ls as we've seen, but also have rm to get rid of volumes. And then with ls, just like with docker ps, you can see there is a q flag for quiet, so we can use that with docker volume ls -q here, just to get the identifiers or the names of these volumes. And then we can pipe those docker volume rm, and I'll clear the screen here so we have some space, and then docker volume ls -q here will grab all those identifiers and pass them through to docker volume rm, thus getting rid of all those volumes, and you can see each ID is printed out as it's removed. So that's one route you can go. The other route you can go, when you run a container, so let me pull up a run statement for mysql, and I'll get rid of this part right here, we'll just use the default anonymous managed volume here, and I could actually run several of these if I rename one of them. So let's just get rid of the name, so we don't have conflicts, and let's run three of these. If I list out my volumes now, you can see we have three volumes. Now when I remove one of these containers, with the docker ps, let's take this first one, 1e9, so docker rm, and then 1e9 as the container ID, but then I'll also specify some flags here. First off, f for foreseen the container to be removed, because it's running right now, but also v to get rid of the associated volumes. Now if I run this, and now a docker volume ls, what do you think will come back with docker volume ls this time? Well, in this case, we should only have two left, and that's what we have here, because we remove the volume that was associated with this 1e9 container. So your cleanup command for removing all containers, if you add -fv instead of just -f, well, then you'll blow away the volumes as well, just be very careful with that. One last minor detail, you'll blow away volumes that are unnamed. If you name a volume, the -v flag will not remove it. It's these volumes with these garbled up sequences of characters and letters that will be removed, the ones that you didn't explicitly create.

Cleanup - Removing Dangling Volumes
Okay, so we still have two containers running with two volumes associated with them. There's a special type of volume that I want to talk about, and that's a dangling volume. You'll amass a lot of these. A dangling volume is a volume that's not associated with a container, so the container that it was created with has now been destroyed. For example, if I do a docker stop -f on the 57, the very first container here, then its associated volume will be dangling. But before we do this, I want to show you how you can figure out which volume here, since we have these long IDs, which of these is associated with that 57 container. So what you can do here, you can run a docker inspect, and then you can pass a container ID, so 572 here is enough of this container ID, and when you run that, you're going to get a whole bunch of JSON output, a bunch of information about this container. For example, you can see the IP address here. We used this docker inspect command earlier in the course to get an IP address for a container. You can also scroll up then, and eventually you should find a section called Mounts. And inside of the Mounts section you'll see the various different volume names, and so here you can see is the 0c9 name that's associated with the 57 container. You can even see the destination path in the container. And one really neat thing you can see, there's actually a source path for this volume. So this is where the volume is created at on the container host. I didn't mention this yet, but these managed volumes have to be created somewhere, and they are typically created right on the host file system. And you could path to this if you wanted to. Now one caveat here, this looks like a Linux path, so this path is actually inside of that Linux VM, so it might be a bit difficult to get to. Definitely beyond the scope of this course to find this path on the Linux VM, but it is there. Anyway, so 0c9, this entire ID, is the one that's associated with the container that we'd like to remove. So let me clear the screen here. I'll do a docker ps again. Let's do a docker stop on 572, and a docker rm on 572. Now if I do a docker ps, we just have the d7 container left, but if I look at docker volume ls, we'll still have both of the volumes. And we know that this 0c9 one is the one that's associated with the container that we got rid of. However, if we didn't know that, we could list out the volumes and specify the f flag to filter for dangling=true, and we'll get back all the volumes that are not associated with a container. So these would be a good list of volumes to clean up if you haven't done this in awhile on your machine. So then I could up arrow here and I could embed inside of a docker volume rm, I could pass the list here that we get back from filtering dangling. And then if I add a q here as well, I'll just get the identifier, so I can go ahead and remove the one dangling volume that we have. And you can see the id is printed out here now. And then if I do a docker volume ls, we only get the one back, and obviously it's not a dangling volume, because its container is still around. Just for completeness to clean all this up, if I were to remove that last container -f, and its d7f right here, and then I do a docker volume ls again, the volume's still there, specifically it's now dangling as well. So I could up arrow to my remove all dangling volumes command, and there we go, now we have nothing left. And all of our containers are gone.

Cleanup - Removing All Images
The last thing that you might want to clean up would be images. Now right now everything we have on here is probably not that problematic, except that some of these are rather large. So, if you wanted to get rid of something like perhaps my nmap image, I could just type docker rmi and then weshigbee/nmap, and that will remove that image from my computer. And now if I run docker images, you can see it's not in there anymore. One thing you'll want to watch out for, make sure the image isn't in use; otherwise, you won't be able to remove it, because you need the image to have a container running from it. If you wanted to, you could wipe everything out by again starting out with a docker images and then do -q here that will quietly print out just the IDs, and then you could pass that on to docker rmi, docker images -q, and off to the races; however, I don't want to do that right now, so I won't run that. There's one other type of image that you might want to get rid of and keep an eye out for it. It's a dangling image. Let's talk about that next.

Cleanup - Removing Dangling Images
In addition to removing images that you've pulled down, you might also want to remove called a dangling image. A dangling image is an image that is untagged. It does not have a tag associated with it. More or less it doesn't have a name associated with it. We'll have to do a little bit of work to create this situation, because we don't have any dangling images right now. Let's clear out the screen here, and let's do a docker help and do that on images. You can see there is a filter for images as well, and if I run docker images -f dangling=true, I don't get anything back; however, we can create one, and if I list the contents of this directory I'm in, you can see I still have my Dockerfile. You typically end up with dangling images when you rebuild your own images. The old image that you had ends up dangling because the new image has the tag that you're using. So if, for example, I look back in my history for build here, and here's my docker build -t for solitaire:nginx-df. I could run this, but before I do that, let's do a docker images and let's look for just our nginx images. You can see right now we have four. Watch these IDs here. Right now nginx-df uses the ab identifier. Let's go ahead and rebuild, so I'll rebuild my nginx image. And unfortunately, that uses the cache right now, which means I'm still stuck with this one called ab. Just to prove that to you, I can look at my images and I still have a b here. I need to modify something to end up building a new image. So how about we move one of the files in the app folder? How about we move the index. html to app\index2. html. Now when I've done that, something has changed, do when I build I can't use the cache, and I'll produce a new image this time. and you can see that on step two here a new image was created, a new layer called 898, and now if I take a look at my nginx images here, you can see that the ab one is only referenced by the weshigbee/solitaire tag. So how about we retag our weshigbee/solitaire, and point it at the new solitaire image, and let me clear out the screen before I run this. And actually, before I run this, I forgot I have some tags that I'm missing here. So first off, I also had an nginx tag here, and this one was coming from, so the source here is solitaire:nginx-df, and then I'm tagging that as weshigbee/solitaire:nginx. Whew, that's so much to keep track of! I'll show you what that did here in a minute. If I run my select again for nginx images, you can see now weshigbee/solitaire uses 898, and so does the solitaire nginx-df one, so 898. So the important thing here, a long story short, is if I look for dangling now, you'll see we have a dangling image that comes back, that ab2 image id that was my first build of my solitaire image. And as you're building images over and over and over again, you will accumulate massive amounts of these dangling images, and you'll want to get rid of them. Now first off, if you take a look at docker images here, you won't see this dangling image. If you want to see it, you have to specify the filter with dangling=true. So most of the time you'll want to get rid of these dangling images, you usually don't need these, so you want to get rid of them, docker rmi, and then you can embed this command I have above -qf and then dangling=true, and that will get rid of that dangling image. And now when I run the search for those, they're gone now. So watch out for dangling images, especially when you're building your own images, they can build up rather quickly and consume a lot of space on your computer, especially if you're building up rather large images.

Key Takeaways
We have now reached the end of this module. I want to touch on a few key takeaways, the first of which is that we've now seen two volume types. In previous modules we took a look at mapping in the host file system, the container host. And this is commonly referred to as bind-mount in Docker parlance. That's always confusing to me, though, that really has to do with the technology of how these mounts take place, so I like to think of the first type as just from the host file system. And then the second type, as we saw in this module, are called managed volumes. These are managed via the docker volume sub-command. We can list them, remove them, we can even create them independent of containers, though we didn't look at that in this module. These create new volumes a lot like creating a new hard drive that we can then mount into a container that we create. When it comes to volumes, both types of volumes bypass the Union File System. So keep in mind that we have that container file system that's built from the image, the image that we pulled down has the entire file system to begin with, and of course, that file system has the read/write container layer on top, that's the Union File System, and the aggregation of that is the union mount point that we see when we list out the contents of a drive inside of a container. But it's important to remember that all volumes, whether they come from the host file system directly or they're a Docker managed volume, they bypass this file system. So if we created a volume here, like we saw in this module, a volume named db, and we map it into the container where the mysql database data is stored, then this volume will live on, because it's independent of the file system in the container. And if we recreate a new container specifying the same named db volume, we will get back the data that's stored inside that volume, almost as if the container was never destroyed. So the second piece is that volumes are independent of the lifespan of containers. You can get rid of a container and volumes still persist. And a key takeaway with this then of actually the previous two points here, is that volumes, or files and data inside of a volume, are not going to be part of images that you create. So, if you mount in a volume and you do all of your work inside of there, for example, copy your website files into a volume, you can't take a snapshot of that container and have the files mapped in. They're going to be in that volume and not in the containers file system, so they won't make it into the image. If I were to sum up all of this, take away two key points. Put your read-only data or your read heavy data into image layers, put your write heavy data into volumes, for example, a database, especially when you're working with memory-mapped technologies.

Composing Applications with docker-compose
Module Introduction
We've seen how to start up applications, we've seen how to run command-line apps and web servers, we've even seen how we can launch databases in a container. In this module we're going to bring it all together and see how we can take an application, both the logic that resides in a service or a web app, and tie it to a back-end database. And we'll see how we can run this all inside of containers, and we'll see how this thing called docker-compose can make that really easy to do. Before we get started, I do want to point out that the examples in this module in particular will involve pulling down some rather large images. If you are not on a fast network connection right now, don't feel obligated to follow along. Instead I'd encourage you to wait until you have a fast internet connection. Maybe for now if you're on a slow connection, just watch some of the videos and think about the things that you're seeing.

Why docker-compose Exists
Let's assume that we have a two-tier app, there's a web tier and there's a database tier, so we'll have two separate containers if we are to deploy this application in two containers, one for the web app, and one for the database. We could combine the apps into one container. That usually becomes more work than it's worth, so instead we will separate them out so the database process for the database server lives in its own container and the web process for the web server lives in its own container. To set this up, we'd have to start out creating one of these containers, so let's assume that we start out with the database container first. So, we use docker run, we specify a name of db so we can easily refer to it, and then I'm wrapping a line here with the back tick in PowerShell so that I can specify the -d flag next, so this runs detached in the background. And then I specify a port mapping here to go from the containers port through 3306 onto the host port through 3306 also. And this is really just for convenience so that I can easily access the database server from the host. I don't need to do this for the container, for the web server to be able to access this, I'm doing this so that it's easy for me from the host to get into the database and tinker around and see what's going on behind the scenes. Next then I might need to specify some environment variables to set up perhaps a root password for the database server. Maybe I also need to specify the database name that I want to create inside of this database container. So there are some environment variables, and then next I might have some volumes to mount. I might want to create a database volume here with the db name. That way I can persist data independent of my container and not lose data if I destroy my container. And then I can start this container from the mysql image, the latest version of that. So that gets me my database up and running. After I do that, then I'll need to inspect that database container to extract the IP address for the database, so that I can then pass that along to the web server, so it knows where to talk to. And then let's assume that we have another container for the web server, we'll name it web for ease of access, and then it'll run detached in the background as well. We'll run it on port 8080 on the host, mapped into port 80 in the container, and then we also need to specify some environment variables here, saying, hey, the database is on port 3306, in case we are running that on a non-standard port, and we also need to specify the IP address of the database, so that's where we can pass in the value that we get when we inspect the database container. We might want a volume mapping to be able to map in our website files, or we would need to bake these into our own image. And then, of course, we'd start this from some image like nginx, the latest version of that again. So we have all of these commands here that we have to fire off to be able to create this environment. And we could script all of this to automate all of this, but the good news is we don't have to. Much like a Dockerfile, we have something called docker-compose to help us out when we're spinning up lots of containers. So you could think of a Dockerfile as automating the process of creating images, well, a docker-compose file automates the process of creating containers. Because in reality if we're typing this all out of the command-line, it probably looks more like this, and it's really hard to read and understand, and it's easy to fat finger and make a big mistake. Instead of typing out all of these commands, we can instead use something called a docker-compose file. We can take these commands and transform them into a. yml format. The exact same contents, it's just the format that's changed. And we save this. yml into a docker-compose. yml file, and then we feed this into another command-line tool called docker-compose. Now even though the contents are pretty much the same, let's step back and take a look at at least a few of the lines in this file. So, at the very top here, we have a version: '2', and this means that we're using the version 2 file format of docker-compose. It's different from version 1. And one of the differences is that we have this services block now, and services, well, they're basically another name for the containers that we would like to run. I wish personally that these have been called containers, but there seems to be a shift with Docker to calling containers services in many contexts, so that's probably in keeping with that change with other tools and command-line interfaces. So we're going to start up two containers here, and the very first one is a db container, and the nice thing is, whatever name you provide here is the name of the container. And then we use the mysql image, we set up our ports, our environment, and our volumes, and these are exactly the same contents we had before, just in a different structure, and I like this structure a lot, it's very readable here, these nice key value pairs in our. yml document. And then we specify another container here, and notice that whitespace and indention is important here. So this is indented underneath the services element in the. yml file, so this means this is another container that we'll start up, this one being based on nginx, having these ports, these environment variables, and then this volume. Let's take a look at this in action. We'll start off with an example of three separate containers running a TeamCity installation. Let's take a look at that next.

A TeamCity docker-compose.yml with 3 Containers
If you hop out to the Gist for this course, and you click this TeamCity link here docker-compose, you'll be able to access a file and grab the raw contents of the file here. Copy all of this and download it to your local computer. I'll go ahead and copy the link here, I'll hop over into my terminal, I'm going to go up a directory, make a new directory called teamcity, and change into that. Nothing inside of here. I'll clear the screen out, and I'll use curl here, a shortcut for invoke web request, I'm a Linux junkie, so I like the Linux equivalent commands in PowerShell, and I'll write out to a file called docker-compose. yml, and grab that compose file from my Gist and write it out to disk here. And if I look in the directory now and I cat out the contents of that file, you can see that we have the contents that we had over here on the website. So if we take a look at this docker-compose file, in this case we'll have three containers, one called teamcity, one called teamcity-agent, and then one called postgres. So, we'll spin all three of these up with docker-compose, and you can see the respective images that these are based on. Now, as always, I would encourage you to make a decision if you think these images are safe to use in your own environment. This postgres one is the official postgres image, so that's somewhat trustworthy, but then the teamcity images that I'm using happen to be built by an individual, so you'll have to decide if you want to run this on your computer. The nice thing about Docker though, you'll be running these highly isolated; it's not like downloading random willy-nilly software on the internet and running it unisolated on your computer. Anyway, let's take a look at the setup here. On the teamcity server, which is this very first container that we'll create, we're mapping ports for 8111, which is the port for the teamcity web server, and it listens on the container port 8111 as well. On the teamcity-agent, we're mapping an environment variable here to point at the teamcity server. Now one really neat thing here, when you spin up containers via a docker-compose file, Docker also goes to the trouble of creating an isolated network for you, and these containers are all placed on an isolated network. And there's an embedded DNS server that's available that will resolve the rest of the containers via their container name. So, teamcity is our server container name, so we can go to http://teamcity inside of one of these other containers, and that will resolve to whatever IP address this container up above is created with, which is really, really convenient. And then we just need to specify the port that teamcity is listening on. And remember, this is the container port, containers talk to each other over their container IP addresses and container ports. This port mapping up here is merely for me to be able to get to the teamcity server from the container host. And then the last container we're starting up is the postgres database, and we're saying, hey, we would like a database called teamcity to be created for us, an empty teamcity postgres database. And that's where teamcity will scaffold and set up the initial database for us. So in this architecture, teamcity has a server with the web UI in it that controls everything, it has an agent that does the actual builds. If you've ever used a build server, you know that there's always an agent that performs builds. So we might want multiple of these builds actually to scale out our capacity, in which case we might want to scale out the number of agents we have. And then it also has a database back-end, in this case, we're using a postgres database, which is yet another database technology that we can use inside of a docker container. Join me in the next video where we take a look at how we can use this docker-compose file to execute it if you will, and create our containers and start them up.

Spinning Up Complex Apps with a Single Command: docker-compose up
So I am over on my Windows 10 machine where I have Docker for Windows, and one of the other tools that was installed when I installed Docker for Windows was the docker-compose command. This is also a Docker Client in that it talks to the Docker engine and sends commands to manipulate containers, and to get the status of containers. So you could think of it as a lot like the Docker CLI that we're worked with thus far, just a new set of commands, and the neat thing is, it works with the docker-compose file instead of needing to pass arguments to docker run all the time. So you could just think of docker-compose as a way of putting your arguments for docker run into a file, instead of typing them out at the command-line all the time. So we're going to use the up commands provided by docker-compose. Let me grab some help for that. You can see here in the output that this command basically does what docker run does, except it's built for docker-compose. It starts up containers for us. And there's one flag in particular that should look familiar, and that's the -d flag for the detached mode. So this allows us to decide, do we want to be attached to all of these containers that we start up, or do we want them all to run in the background? So that might help you understand attachment a little bit more. We can attach to multiple containers with docker-compose, so let's see that here. So first off, I just want to cat out the docker-compose file one more time so you can see it, and now let's take docker-compose up and then by default it will use the file named docker-compose. yml, which is why I named the file that way, and then we just have to hit Return here to go ahead and spin up our containers. And so obviously the first thing that's going to need to happen, we'll need to pull the images that we're going to use. We're using three new images in this example, so I have to pull all three of these down. And these will take a little bit of time, so maybe go grab a cup of coffee if you're following along, and I will go ahead and pause the recording until we've got the images, and then I'll start it back up to help you see what happens after we pull images with docker-compose. Now while that's downloading in the background, I thought I would show you that right now I have no containers running nor stopped on my computer, just to give you an idea of what the environment looks like before this all kick off. And as far as images go, we just have the images that we've worked on in this course. Okay, so the images have been pulled down and things have spun up now, and I've just kind of let things go, we can step back in time and explain what you've seen. Before we get into this output, though, let's hop over and take a look at the TeamCity UI. So I went ahead and pulled up localhost port 8111. I can do that because this is a Linux container, and because we published that port in our docker-compose file, so we have that on the host now. And you can see we're presented with the TeamCity setup dialog. I can click Proceed here, and I'll go through a few steps here, just so we can see the interface of TeamCity. I'll just Postgres as the database. I can click refresh here on the drivers, it looks like we have a postgres driver available here for TeamCity. I need to specify the database host, and in this case, that's just postgres. As I mentioned, we have an embedded DNS server that's available when we used docker-compose to spin our containers, and all we need is the name of a container to access the services that are running inside that container. So, postgres is the database host, I'll leave the default port on there, the database name we set to teamcity, so that was this right here, the POSTGRES_DB that we created, and then for our username the default here is just postgres. And then we didn't set a password, so that'll be blank. And just in case you're interested in the output here from docker-compose up, if I scroll way back up to the top here, you can see that there's a section that says that there was no password for the database, so we didn't set a password, and it's giving us a little bit of help if we wanted to set that. And you can notice that this is coming out of the postgres container that we created. Notice that over on the left-hand side here, you can see the output is named according to the container. So here's postgres, here's teamcity, and I believe if we scroll down, you should see some messages coming from the agent, way toward the end here, teamcity-agent. So I can hop back to the UI here and just click Proceed. I just want to show you the UI once it gets up and running. So teamcity is creating its own database now inside of the postgres container, and in a moment it'll be setup and we can poke around the UI and talk about the agent component. All right, we're presented with a license, you can accept or not, and then continue here. We can create a username and password, create an account here to login with, and then we're presented with the teamcity UI. So this is pretty darn cool. Join me in the next video where we recap what just happened, and we dig in a bit to see what happened behind the scenes here, so that we can really start to appreciate how cool this is to have a single docker-compose file to spin up all of this infrastructure.

What You Created with docker-compose
So just to recap what happened here so that you have your bearings before we go digging in further. We have a container host, and on that container host is a Docker engine. And in this case, that Docker engine is inside of a Linux VM. We can gloss over that fact right now, but we are using Linux container, so keep that in mind in this example. We set our. yml file to docker-compose, which then sent commands to the Docker engine. The engine spun up the containers that we asked for, we have three of them, a teamcity, a teamcity-agent, and a postgres database. So each of these are separate containers. And inside of these we have a single process for each of these different purposes. So really, we just have three applications running on our computer right now, that's all we've done. But we specified more. We specified a lot about the environment of each of these containers. Each of these has a virtual NIC, because it's isolated, a virtual network adapter. Now remember that each of these containers basically looks like an isolated machine, it looks like a lot like a VM inside of that container. Postgres is listening on port 5432, bound to its virtual NIC, and teamcity, the server, is listening to port 8111 bound to its virtual NIC. Now one thing new with docker-compose that we haven't seen before, the Docker engine created a private network, and it hooked up all of these containers to that network so that they could talk to each other. And that allowed the teamcity server, for example, to talk to the postgres database to store teamcity data. And it also allows the teamcity-agent to communicate with the teamcity server. So we did all this on the container host, but then we also made a web request to the teamcity UI. And I happened to do that on the same computer, but you could image that we could have done that from a different computer, so let's say we have a client machine here that would now like to request the teamcity UI. Well, it has a physical NIC that is hooked up to a switch connected to the container host's physical NIC, so these are the real network adapters inside of each of these computers, connected via a physical switch. And then because we published that teamcity port 8111, that port then is available via this physical network do that our client machine could send a message here, a web request, over to the teamcity process, and then the teamcity process could respond and the client could render the webpage response. Still to this day using this one docker-compose file blows my mind to be able to set all of this up. I've done a lot of teamcity consulting over the years, and it usually takes three or four hours to get a basic teamcity installation up and running, configuring environments, downloading software, and getting everything just right. With this one file, it's never failed when I demo this, aside from the time to download the images, the environment spins up in a matter of seconds. So now let's dig into some more of these pieces to see what is all going on, and maybe appreciate this a little bit more.

docker-compose Creates Isolated Container Networks
First off, I'd like to take a look at that network that I talked about. So, docker has a network command, and you can call network ls, just like volume ls, and this will allow you to list out the networks on your computer. By the way, I'm running this in a separate window here, because my other PowerShell prompt is consumed with the output of the containers. On the bottom here, you can see that there's a network that's been added. I should have shown this before, but I think you can imagine that this teamcity default network was not added before we spun up our containers via docker-compose. So just like we have volumes, so docker volume ls that managed storage, and we have containers that manage compute capacity basically, we also have network management. So there are three primary components that you can manage with Docker, processes or containers, volumes, and then networks. And this is very similar in the VM world where you manage virtual machines, you manage storage for those virtual machines, and then you also manage virtual networks, so same ideas here. In fact, most of the virtual networking in Docker, especially on the Windows side, is using virtualization technology similar to or the same as what we have over with VMs. Let me make this window a bit bigger here and clear out the screen here. If you want to know more about the network that was created, you could run docker network inspect, and then you can specify the name, teamcity_default. When I run that, I get some information about this networking. You can see that the gateway for this network is 172. 18. 0. 1, the network prefix is 172. 18, so we could have a lot of devices connected up to this network, and you can see down below right now we have two containers connected to this network. One of them is the teamcity_postgres container, and one of them is the teamcity_teamcity container. So the second one is the teamcity server, and it has an IP address of 0. 3, and postgres has an IP address of 0. 2. Now the name is a bit more complex than what I said. The name is actually the project prefix, and we'll get to that in a minute, and then the actual name of the service inside of the docker-compose. yml file, and then a number that increments based on the number of that particular service you run, because you can scale the number of a given service. So you can scale the numbers of containers you have, and this number will increment then. I said that the names of these containers are prefixed with teamcity, and I said that's the project name. The project name just happens to be the folder that you're inside of by default, the folder that contains that Docker. yml file. So if I list the contents here, you can see that the folder is teamcity and that's where teamcity came from. You can control and override that if you want. Which, speaking of which, if we want to relate to what's going on here, how about we run that docker ps statement again. You can see we have teamcity_teamcity as one of our containers, and the other one is teamcity_postgres. So we're actually missing one of the three containers, and that's the agent. The agent actually gave up, because the server wasn't set up in time for it to connect in, so it gives up after about two minutes. I'll show you how to restart that in a minute. So docker-compose saved us the trouble of creating these two containers, and if I run a -a, you will also see the teamcity-agent that was started up and it's now exited, and its name is teamcity_teamcity-agent, because teamcity-agent was the name we used in the docker-compose file. Now, since we're talking about networks, if I run docker network ls again, by default when we don't specify a network, which is what we've done throughout this entire course, so when you call docker run, if you don't specify a network, this default bridge network is used instead. You can actually specify a network when you're creating a container, and that's what docker-compose did for us. You can even manage networks with a series of commands. We can create networks, we can connect and disconnect containers to networks, and we can inspect networks as we've seen, we can even remove networks then. If we want, we could do a network inspect on that bridge network, the default one here, and it looks like I have a typo there, and now you can see the IP address information for the default network of 172. 17. 0. 0, and that should look familiar. When we were running containers earlier, if you took a look at any of the IP addresses for those containers, that's the IP address range that would be used. So we have a separate network here, 172. 18 here, just for our teamcity containers. This isolated network gives us an added degree of security. If we're really not sure about a container, we might want to run it on its own network so that it can't talk to our other containers, because by default with this bridge network, all of our containers can talk to each other.

Service Discovery via an Embedded DNS Server
Okay, so we have these containers spun up, and I said that there's something called an embedded DNS server. Let's get into one of these containers and see that in action. So let's run a docker ps here, and then we could do something like docker exec, and then -it and teamcity_postgres_1 to get into that container and maybe a Bash shell, and that would get us in, but I want to exit out of that. The equivalent now with docker-compose, which by the way, be careful with the tab completion with docker-compose, it usually picks up the. yml file, and that's going to be wrong, that won't be a command you can run. With docker-compose, let me dump out the commands, you can see that there's an exec with this as well, and this is a shortcut here with docker-compose exec. A couple of things, first off, we don't need -it, it's interactive by default, and all we have to do is specify the name of the service, so if I want to go into postgres, I don't have to type in the project prefix, I can just type in postgres and then bash here, and boom, I have the shell opened up. So, a shortcut here with docker-compose when it comes to running another process to shell into, if you will, one of your containers. Now from here I said that there's an embedded DNS server, so I should be able to ping teamcity. And take a look at that. I'm pinging teamcity, and look at the IP address, it comes back as 0. 3 on the 172. 18 network. If I want to, I can hop out of here, and I can hop into my teamcity server, just change the name of the service to teamcity here. And assuming it has a Bash shell, you can get in. This won't always work. Remember, you have to have whatever process you want to run, in this case Bash, you have to have that inside of your container file system if you want to be able to run it. So, if a container doesn't come with Bash, well, you can't use Bash inside of that container then. And from here, if I want to ping my other container, what do I type in? Well, in this case, I want to ping postgres, so we'll put that in. And take a look at that, 0. 2. And those IP addresses correspond with the Docker network, and then inspect command, and then we can type in teamcity_default. And of course, I should exit out of my Bash shell before I run that, so docker network inspect teamcity_default. There we go, so there is 0. 2, and there's 0. 3. So that's the embedded DNS server, very, very convenient. For service discovery, you don't have to know anything but the name of your container, or well, we should say the name of the service that you use in your docker-compose file.

Connecting Another Container to Your User Defined Network
So this network that we created, the teamcity_default network is what's known as a user define network, and all user define networks have that embedded DNS server. So this is not something that you have to use docker-compose to take advantage of, it's something you can take advantage of on any of your user define networks, just not the bridge network, you don't have it on the bridge network. The bridge network is here for legacy purposes. There used to be an old linking mechanism that you used; do not use that anymore. This embedded DNS server with service discovery is the appropriate way to communicate between containers going forward. But since this is just a user define network that I could create on my own, I could also take one of my images, for example, the alpine linux distro, I could do a docker run, I'll give this a name of alpine, I'll remove it when I'm done, and I need to specify the image, so alpine there, and I also need to specify the shell I want to run, so sh. And then I can use --net to control networking. Now I want you to take a guess here, what do you think I have to type in here if I want to connect up to that network that was created for me, the teamcity_default network, what do I type in here? Well, I pretty much gave it away. We just specify the network name here, and so now we'll be running a container, and I also need -it for interactive, we'll be running a container connected up to that teamcity_default network. Ours is named alpine, I'm running a shell inside of it. I should be able to ping teamcity now. Take a look at that. And if I take a look at my IP address, ip as will show it, and somewhere in here, here we go, I'm 172. 18. 0. 4 inside of my alpine container. So, how about we open up another Command Prompt, opening up all the rabbit holes, many, many, many rabbit holes. And inside of this one, I'll change into that teamcity directory, I'll use docker-compose exec, I don't need -it because that's by default with exec, and then I need to specify the service, in this case, teamcity, we'll go into the teamcity container, and then a bash shell inside of there. So I'll shell in to my teamcity container, and inside of here I can ping my alpine container. And take a look at that, that works as well. So you can see that the embedded DNS server for service discovery works, even if you don't use docker-compose to bring up a container.

Restarting Containers with docker-compose start
Okay, I'm going to close up some of these windows here to avoid confusion. I exited out of the teamcity container, killed that shell off that I created, and I'll also exit out of my alpine container, it was having some trouble, I think from trying to detach from it, which didn't work for me. It froze up a little bit there. If I take a look at docker ps here, it looks like my alpine container is still running, so I will remove that. I'll force and remove volumes if there are any, 23. Okay, now docker ps just has the two containers for teamcity, and of course, we have that agent as well that didn't really start up. If we want, we can take a look at the logs here, and we can see that the teamcity-agent said that the server did not respond within two minutes, so it just bailed out. You could see it exited with code 42 here. If we want to start it back up, a couple of ways we can do that. First off, with docker-compose inside of the folder next to the docker-compose file, we can run docker-compose and ps. When I run this, I can get some status about the processes just inside of the container associated with this docker-compose file. So it's like a limited view of the world down to just this compose file, which is really helpful, because I'm probably just working on a set of related containers if I wanted to use docker-compose. And we can see the agent here has exited. So this ps looks a little bit different than docker ps. You could use docker ps if you wanted to. So if I run docker-compose here real quick, we want to start this back up, tell me what you think we can do to get that agent up and running? Well, we have a start command down here to start a service, so we could run docker-compose, start, and then I can specify teamcity-agent, just to start up that one service if I wanted to limit that. Looks like it's started. If I hop back over to the original window that I was attached to, so that blue PowerShell window, I'm not going to show you exactly what's in it yet, but if I hop back over to this window, what do you think is going to show over here? Was that what you expected? You can see here we've got output coming out of our teamcity-agent. And it actually looks like we have a failure. If we want we can come back over to our other window, run a docker-compose ps again just to get the status of what's going on. It actually looks like our agent is up and running, so maybe that log message was a bit misleading. At any point in time that I want, I can run a docker-compose and then help, and then the command I want to look at here is a logs command. This is specific to a given docker-compose file. With this if I wanted to I could look at the logs just for one particular service, and so I specified a name here of teamcity-agent. So if I wasn't connected up, I could use this to see what's going on, and you can see that an upgrade is going to happen here. This is pretty typical when a teamcity-agent starts up. It grabs the latest version from the build server just to make sure it's upgraded, so it looks like things are working as expected right now, the agent is connecting up. If I want, I can run the help again, and you can see that we can follow that log file in another window if we wanted to, though we have that over here as well. By the way, if the output ever freezes, sometimes you have to come over and click on the window that you're attached to a container in to get the output to show up. Sometimes it just seems to freeze up. I'm sure this is just a bug in how containers work on Windows right now. And then now that's started back up, if I come back over to the teamcity UI and refresh, come to this Agents tab, I see that I have an unauthorized agent. So the agent has successfully communicated with the teamcity server, and the agent knew where the server was simply because of this environment variable that we set here pointing at the teamcity container, again, using the embedded DNS then to resolve the IP address for that. It's unauthorized, so if I want, I can authorize this now, and I'd have a build agent up and running. Now I don't want to turn this into a course on teamcity, I just wanted you to see how these various different pieces can connect together, all via a very simple docker-compose file.

Using psql in the postgres Container to Look at Your Database
Now let's say that I want to get into that postgres database and take a look around. How can I do that? Well, I could use a docker exec, but it'd also be a little bit easier to use a docker-compose exec. And then I just need to specify postgres. And then whatever command I like to run, so maybe a bash shell. This container also has the psql client tool, I could use that to browse my database, and for that I need to specify a username, which the default was postgres for a username. And that should get me into the container running a psql process, so I can interact with the postgres database server. And inside of here, \l, I can list out my databases, you can see my teamcity database here, and then I can connect to the teamcity database, and list out the tables. And you can see here we actually have tables inside of our teamcity database. So it's fun sometimes to just get in there and play around with the data. And again, it only takes a matter of seconds here to spin up this postgres database server, and maybe you've never used postgres before, so once again, Docker is inverting the process of learning for you. You can spin up this container, get in and try some commands out, and then later on you could look at the Dockerfile for this particular image and start to learn a little bit more about postgres and how it's set up, and you're learning probably from the people that wrote postgres instead of some random blog post or Stack Overflow post that you could find to help you set things up. Now, you have the definitive source of the truth here from the people that made the actual software that you're trying to learn.

Tearing Down Infrastructure Created with docker-compose
Let's shut down our docker-compose example. And first off, I need to get out of psql, so \q for that. And then I can do a docker-compose and ps, and you can see everything's still running here. A docker ps as well will show me this. And if I look at docker volume ls, you'll see that there are some volumes created for this example. And also docker network ls, we can see the network that was created for this example. Now if I take a look at docker-compose here, and I do help on stop, just like with docker stop, this will help me stop containers that are running as a part of docker-compose. So, docker-compose and then stop will stop everything. So I can bring down the entire site of infrastructure just like that. And I could bring it back up if I wanted to, or I can take a look at docker-compose help and rm, which is the equivalent to remove infrastructure that was created with docker-compose. Now, it will warn you here that by default anonymous volumes attached to containers will not be removed, so if you want, use -v to get rid of those, just like with the docker remove command. That way you don't lose your data, and that's one of the neat things about compose as well. So let's do a docker-compose, and then rm, and we'll use the -v here to remove the volumes as well. Are you sure? Yes, go ahead, and in a moment, docker ps, everything's gone. In fact, -a just to make sure. Docker-compose ps, everything's off there as well. Of course, that's the same view of the world as docker ps, and then docker volume ls, all the volumes are gone. Docker network ls, but the network is still there. There's a docker-compose down command that's the equivalent of the up command that will tear everything down, stop everything, remove everything, so we could use this to get rid of the network as well. And you can see we're removing our network now. Just to be safe, we'll check, and that's gone as well. But at the flick of a wrist, if we want, we could be back up and running with a single command, and that's the beauty of docker-compose.

Running the ASP.NET Core MVC MusicStore with docker-compose
Before we wrap up this course, we had this nice example of docker-compose with Linux containers, but I'm sure you're chomping at the bit to try things out with Windows containers. I would strongly encourage you to take this for a spin, docker-compose, on a Windows container example, and I have one for you. I just don't have enough time to go over all of this in this course, so I'd encourage you to pick this up and try it out on your own. It's a sample of the MVC MusicStore, a. NET application. You can take a look at this fork of it, frism/MusicStore, and inside of here, if you scroll down far enough, there's a docker-compose. windows. yml file, and inside of here is a nice sample, let's take a look at this raw file. This is a nice simple sample of how you could spin up a database tier and also a web tier. The database tier is based on SQL Server, the one we just ran in the last module, so the configuration here should look very familiar. And then the website of things, that'll be a bit different. This one's the MVC music store, so this is a custom application that has to be built. So in this case you'll see how a docker-compose file can integrate with a Dockerfile to both build an image and then also run a service with the image that was built. So the two in concert. It's a really neat combination to see working together. So you can see when we build, then, well, we need to have a build context, the path, just like we were specifying with docker build, and then also the Dockerfile. So the arguments to Docker build can be specified inside of a docker-compose file as well, and that'll be done for you then as a part of spinning up this docker-compose file. There's also some configuration here, a database connection string. And once again, you can see here that embedded server at work, the server references db, which is the same name as the service. And the password here of Password1 corresponds to the password that was set up for the environment for that database container. So you can see the linkage here. There's also a depends on clause. This is at that the web container should start after the database container. So there's an order in here with the docker-compose file as well, though the problem you will have is if the web server starts up first, it just won't have a database to communicate with, but that shouldn't be a big deal in terms of an ASP. NET application. Usually the app will just error out until the database is up and running. There's also a port mapping here so that you can access the web server, much like teamcity's 8111. And then one actually shows how you can configure networks with a docker-compose file. So, a docker-compose file is not just for containers, which come under services, but this can also configure networks, so docker network create, and guess what, also docker volume create, both of those commands map to corresponding declarative elements inside of the compose file. So you can create containers, networks, and volumes with a docker-compose file. Now one thing I will mention, when it comes to networks on the Windows container side of things, networks are a bit different than what we've seen here. You'll have to investigate that on your own; I just don't have enough time to get into that in this course. So back to this compose file. The neat thing is that it contains all the pieces you need to build up the infrastructure you need to run some rather complex applications. So, I'd encourage you to try pulling this file down, there are some instructions that you can find in this repository if you take a look at the read-me that comes with the repository. If you scroll down a bit, you'll see that there's a section in here on running this particular docker-compose file. You can see the docker-compose build command and the up command. Though, the very first time if you haven't built things up will trigger the build as well.

Key Takeaways
There are just a few key takeaways from this module. The primary piece I want you to focus on is that if you think of images with Dockerfiles, so Dockerfile is to images what docker-compose is to containers, and actually much more. So, Dockerfiles helps us get away from running all those commands at the command-line to create images, and docker-compose helps us automatically create containers instead of pecking out individual docker run commands. And it also helps us create volumes and networks, all the infrastructure we need to run rather complex applications. And as you've seen, at the push of a button, we can spin up really complex apps, like the three node app here we saw with teamcity. And then the last bit is that Docker also helps you define your own networks for an added degree of isolation.

What to Learn Next
As we wrap up this course, I want to share some additional sources that you can use to continue your learning, and maybe point you in the direction of a few things you'll want to investigate beyond what we took a look at in this course. So first off, I strongly encourage you to take a look at the docs for Docker. They are absolutely wonderful. And I think there are at least three pages in particular that I love the most and that I find myself referencing most often. First off, this docker run reference. This is a great overview of all the different parameters that you can pass to docker run. And this is kind of on a high level, explaining things like detaching from a container or a running detached, explaining running containers in the foreground. And then in addition to this reference, which contains some of the high-level concepts, over on the left-hand side there's a command-line reference. And if you click this, this is a command-line reference for Docker. You'll see on the left-hand side many of the different commands that we used in this course, and if you scroll down here, you'll find run once again, so there's another run reference that you can refer to, and this is a detailed listing of all the different arguments that you might pass to docker run. We only touched on a few of these. You should take the time and come out here and learn what many of these other parameters and flags do. Just for example, here's -d we used to run a detached container, or here's -e what we used to pass an environment variable into a container. In addition to docker run, there's also a nice Dockerfile reference. And again, Dockerfiles are for building your own images. So this reference will get into many of the different instructions or commands that you can use inside of a Dockerfile. We only touched on a few of these, so I'd encourage you to come out here and learn more about the different instructions you can use. For example, over on the right-hand side here is a guide to these various instructions. You'll definitely want to take a look at things like adding and copying files in, more details about what you can do to copy files in, or maybe setting the command that you would like to run inside of a container when the container starts up. There's also a nice reference for docker-compose. There are many, many parameters you can pass to this. There's a high-level reference here, and then if you drill down on the left-hand side, this command-line reference for docker-compose, and you'll see that if I come up here, you'll see docker-compose has its own section, each product has its own section. And if you scroll down here, there's that command layer reference right here, and inside of here are the sub-commands that we used in this course, as well as many more. And you can click on each of these to learn more. For example, I have the up sub-command loaded right now. I could pull up something like the rm sub-command if I wanted to learn about how I can remove resources or containers that are created with docker-compose. And then don't forget that I have the Gist for this course, there are several different sections in here that you can come read, and there might even be some more examples in here that we didn't touch on. I'm going to hop back to the docs for a minute so we can talk about some of the different products that we haven't covered yet. We took a look at the Docker Engine in this course, and we took a look at Docker for Windows. There's an equivalent Docker for Mac, if you also use a Mac. We saw Docker Compose, though you should definitely drill in more there. There's a commercially supported Docker Engine, and by the way, that comes with Windows Server 2016, so you have support as well. So this is in addition to the open source Docker Engine. The Universal Control Plane, the Docker Trusted Registry, and Docker Cloud are all pieces of Docker Inc's enterprise offering for running containers in a production environment, so you want to take a look at those as well if you're interesting in how you would deploy this into your enterprise environment. There's more information about Docker Hub. Docker Machine is a product that helps you spin up virtual machines or machines perhaps out on AWS, and get Docker Bootstrapped on those machines for you. You might also want to take a look at that if you want to use something besides just the Moby Linux VM that we used in this course. Docker Store is a new offering. It looks like it's basically adding the ability to buy software that comes in an image format. I think this will evolve quite a bit over the next couple of years. Docker Toolbox, the older product that you would have used on a Windows or Mac environment. And it comes with many of these other products we talked about; for example, Docker Machine, Docker Compose, and the Docker Engine. You can still use that if you would like to instead of running a Hyper-V VM. And then if you click on Component Projects, there are a few more things inside of here. Notably you want to take a look at Docker Swarm, but specifically you're going to want to look for the new swarm mode that was added recently. Swarm mode gives you the ability to easily spin up a cluster of Docker resources. So once you get to running containers across multiple machines, you'll want to take a look at something like swarm mode or something equivalent. And no matter where you go next, I think the best thing you can do to figure out what's right for you is to try to use Docker in some of your daily tasks. Is there a program that you'd like to use that's a hassle to set up? Is there some software that you would like to learn that's a hassle to set up? Try and find some use cases for it, and I think you'll quickly learn the things you need to learn about Docker, instead of being overwhelmed with all of the documentation you can see here and all the different products that you could be learning about. Learn what you need to learn to do something productive with Docker, and then expand your knowledge as you go. I would appreciate it if you would take the time to leave some feedback about this course. Let me know what you like, what you don't like. Ask questions. And then if you'd like to learn more about me, I have a blog, and I also have a newsletter that you can find via my blog. You can come and see some of the other things that I write about here. Until next time!
