Docker containers are in widespread use as the distribution vehicle for cloud native application services. An important enabler in the process of building, packaging, and running those containers is the Docker platform, which is comprised of several parts. In this course, Securing the Docker Platform, you'll learn about the fundamental aspects of security that relate to the platform components. First, you'll gain a better understanding of the platform components involved and the means of measuring compliance against an industry benchmark standard. Next, you'll discover how to configure the Docker daemon for best practice security, as well as for more flexible access control and authentication. Finally, you'll explore how to apply security controls to other aspects of the platform including a self-hosted Docker registry and a Swarm cluster. By the end of this course, you'll have the necessary knowledge to configure, measure, and optimize effective Docker platform security.

Course Overview
Course Overview
(music) Hi everyone, my name is Nigel Brown and welcome to my course, Securing the Docker Platform. As the leading components of the cloud native paradigm Docker has transformed the way that many organizations, large and small, go about building, packaging, and delivering software applications. The Docker platform is the vehicle for doing this and as sensitive corporate assets are exposed within the platform it's pretty crucial that it's configured to be secure. This course is all about the security controls and mechanisms that can be applied in order to protect those valuable assets. During the course the main topics that we'll cover include using an industry benchmark to baseline the security configuration, enhancing access control using an authorization plugin, deploying a secure registry for application images, and managing the security of a swarm cluster. By the end of the course not only will you have a good understanding of the Docker platforms default security mechanisms, but also the knowledge needed to create more flexible security solutions for your own situation. Ideally before you get going with this course you should already have some practical experience of Linux and some familiarity with the Docker platform and its command line interface. If you want to know how to measure the effectiveness of your existing Docker platform security and how to provide more flexible access patterns to the platform then join me to discover how to secure your Docker platform.

Establishing a Baseline for Docker Platform Security
Module Outline
Hi, my name is Nigel Brown and a big welcome to this Pluralsight course, Securing the Docker Platform. Containers increasingly play a big part, not only in the tool chain for developing modern software applications but also in the distribution and execution of those applications in production environments. And whilst it's imperative that you secure the container application workloads themselves, it's equally important to secure the platform that they run on. This course is all about securing the most popular container environment there is, the Docker platform. When we talk about the Docker platform, what do we mean exactly? This schematic provides us with a view of the many components that make up the platform all of which need to be considered when we implement security controls for the platform. Don't worry about the detail for a moment we'll return to the diagram in a little while. What's important to take note of however, is that the platform components need to be secured in terms of communication, authentication and authorization. Let's also be clear here that we'll be discussing the Docker community edition platform and not the commercial Docker enterprise edition platform. Many of the features and requirements are the same for both, as the enterprise edition is based on the opensource community edition but the enterprise edition contains additional features not found in the community edition. So now we're clear on our scope, let's have a look at what we'll cover in this module which is called establishing a baseline for Docker platform security. It's all about knowing what to secure as a baseline and measuring compliance against that baseline. Firstly we'll need to define what the Docker platform is so that we can get to securing the various components. It's quite important for us to find information about or report on, specific Docker platform vulnerabilities we'll need to find out how to do this. We can't all be security experts we have day jobs after all so we'll explore the sources of information we can make use of so that we know what the best practices are for securing the Docker platform. How can we get that all important security baseline? Well we'll take a look at a couple of tools that are available for measuring the security compliance of a Docker platform. And finally we'll see how to execute a compliance test using the Docker bench for security. Are we ready to get started? Let's explore the Docker platform.

Defining the Docker Platform
At the heart of the Docker platform is the Docker daemon. While it used to be a very large statistically linked binary which encompassed all of the code necessary to create, run and manage containers, today it's paired back with a number of important components spun out into their own binaries. The daemon is a server which provides a HTTP-based restful API called the Engine API. It's through this API that clients instruct the daemon to create, run and delete containers. It also provides an interface for building, managing and distributing Docker images which are the templates from which containers are derived. Much of the heavy lifting associated with container management is handed off to other bespoke binaries as we'll see in a moment. The client is the part of the Docker platform that consumes the Engine API. This can be achieved either by a way of the Docker client command-line interface or programmatically through the Docker engine software development kit or STK. The client CLI is extensive in terms of function and if invoked on the same machine as the daemon, normally communicates with the daemon using a local UNIX domain socket. If it's remote to the daemon, communication occurs over a TCP socket instead as we shall see, securing the communication between client and daemon is key to preventing platform compromise. The STK provides client packages for the golang and Python programming languages and there are very community-supported client libraries for other languages, including Java, Ruby and Rust. Securing problematic communication between the client and daemon is equally as important as when using the client CLI. When a client makes an API call to the Docker daemon, instructing it to perform an action on a container, it hands off the request to another binary, container D. Container D is a lower-level container runtime tool and takes the responsibility for managing the lifecycle of containers on a Docker platform although it can be, and often is, used independently of Docker. It's used to create, run and delete containers amongst other things. The Docker daemon communicates with container D using the GRPC remote procedure call system. Whilst container D manages the lifecycle of the containers on a Docker platform, the invocation of each individual container is the responsibility of the container executor, runC. Confusingly, runC is often referred to as a container runtime also, runC is involved by container D and is responsible for establishing the container's process, bound by its configuration by replacing itself with a process image associated with the container's entry point. RunC conforms to the open container initiative runtime specification, a standard for running application containers. The execution chain may seem a little convoluted but the decision to break out these components from the daemon makes perfect sense and were inspired by consensus in the wider container community. The final element in our platform scope is the Docker registry, where Docker images are stored. Images are pushed to and pulled from Docker registries. A Docker registry implements a HTTP API called, funnily enough the registry API. As a client, the Docker daemon issues API calls to the registry on behalf of its own client requests. Registries can be public or private and therefore it's important to ensure that access to registries and the repositories that store the images is carefully controlled. It also goes without saying that communication between a Docker daemon and any registries it addresses should be secure. So that concludes our overview of the Docker platform itself and defines the various components we need to consider when we take action to secure the Docker platform. But we're not quite done yet. A Docker platform can operate independently but in a production environment, it's far more likely to see a number of Docker platforms coupled together as a cluster. When multiple Docker platforms are coupled in this way, the daemon is operating in swarm mode and instead of dealing with individual containers, we operate with a service abstraction instead. The Engine API for each Docker daemon provides a number of swarm-specific endpoints that can be consumed by the client for establishing and operating a swarm mode cluster and the service-based container workloads that get deployed across the cluster of nodes. Swarm mode introduces a number of new objects which need careful consideration when it comes to security. A cluster is comprised of manager nodes and worker nodes. The manager nodes take care of managing the cluster state and in order to do this, they share a distributed state store. This store contains sensitive information which can include secrets used by application services. Communication between manager and worker nodes is secured by default using neutral transport layer security or TLS for sure and control plane traffic for virtual networks that span multiple cluster nodes is disseminated between workers using a gossip-based protocol. Despite the inherent security, there are configurable security features which need addressing in order to make the cluster optimally secure. In this course, as well as considering the security for individual Docker platforms. We'll also be exploring how to manage the security of a cluster of Docker platforms configured as a swarm.

Acting on Docker Platform Security Vulnerabilities
Before we get into the detail of what we need to secure on the Docker platform, and how, let's take a brief look at how we can respond to security issues. The first thing to say is that Docker provides a database with previously uncovered common vulnerabilities and exploits or CVEs. Thankfully there are few and largely relate to older versions of the Docker engine. The URL shown here takes you to a page that details the CVEs that are specific to Docker software. It's highly unlikely that you'll find a vulnerability but if you do, then it's genuinely recommended to follow the responsible disclosure model by contacting Docker directly which will give the maintainers time to remediate the vulnerability before it's made public. So where exactly do CVEs need to be reported? A single email address is provided for this purpose which is security@docker. com. As well as potentially being an informer of security issues, you'll also need to be informed and there are a couple of channels for this. Firstly, we can make use of the Docker community forum, the forum has a whole bunch of categories for many of the different Docker tools and products, but the announcements category has been used in the past to announce security vulnerabilities. Additionally if you sign up to participate in the Docker community, you can join the Docker community Slack workspace. There are many different channels to subscribe to but the one of interest for learning about security vulnerabilities is the docker-security channel. These community mediums are also great for learning about Docker in general especially the Slack workspace where you can post questions about many Docker-related topics and have a community of experts respond with appropriate answers.

Determining What Needs to Be Secured
Now then, when it comes to determining exactly what needs to be secured in the Docker platform, where do we start? There is a lot of information available from a wide variety of different sources. Discerning the best source is a tricky business. An excellent place to start is with the security pages within Docker's own documentation site. It provides a good introduction to many of the security features of the Docker platform. Here's a shortened URL. The NCC group published a seminal paper on container security in 2016 called Understanding and Hardening Linux Containers. This paper covers security for Linux containers in general as well as specifically for Docker and some other container formats. It's comprehensive, in depth and is a valuable resource when considering security requirements of containers. Whilst these two resources are extremely valuable, they are not necessarily prescriptive, they don't give us a concise step by step means for determining the effectiveness of our implemented security controls. If we want to measure the effective security of a Docker platform, we need a yard stick or benchmark to measure by. The center for internet security, or CIS for short is a non-profit organization dedicated to the promotion of best practice for cyber security. As such, it produces benchmarks for securing systems and platforms, including specific operating system distributions as well as popular applications including the Docker platform. We'll be referring to the CIS Docker benchmark on and off throughout the course so let's take a closer look. The CIS Docker benchmark was introduced in 2016 for Docker version one dot 11. Since that point in time, new benchmarks have been introduced in order to account for new major releases of the Docker platform. The title for each benchmark originally coincided with the Docker version, fore example one dot 11. However Docker changed the way it named docker platform released in March 2017, it made a distinction between the open source community edition and the subscription-based enterprise edition. Additionally, rather than the point releases it had hither to use, it chose to use a release naming convention using the year and month of release. For example, Docker community edition 17. 06 was released in June 2017. Following this change, the CIS benchmark focused on the Docker community edition but as of March 2018, a new draft benchmark has been produced which accommodated the Docker enterprise edition also. The current version of the benchmark is CIS Docker CE Benchmark 1. 1. 0. Okay that's enough about the history of the benchmark, we know that new benchmarks get released roughly in line with new Docker platform releases, but what is contained within this benchmark and how can you use it to your advantage? Let's take a look. At the heart of the CIS Docker benchmark is a set of recommendations for providing a secure platform on which to run containerized applications. These recommendations are compiled by security professionals and subject matter experts using a consensual approach. They are considered to be best practice for securing a Docker platform. The benchmark recommendations are assigned a configuration profile which is effectively a category describing the impact of applying the security measures. The level one profile will contain a base-set of recommendations which are straight forward to implement and that provide a good level of security without impinging on performance. The level two profile will define a set of recommendations which provide defense in depth for environments where tight security is essential. Whilst adherence or non-adherence to some recommendations are not scored, a large number of the recommendations are scored with a binary outcome. In other words, does the platform under review conform to the recommendation or not? The score obtained from measuring a Docker platform against their complete benchmark will give a relative indication of how secure the platform is overall. Let's have a brief look at a CIS Docker benchmark recommendation. The narrative has been shortened here for brevity. The recommendation is to ensure the logging level for the Docker daemon is set to the value info. It's a scored recommendation. First off, the recommendation is a level one recommendation. Which means it's practical and prudent to implement, provides a clear security benefit and doesn't inhibit Docker beyond acceptable means. The profile is Docker-related rather than the underlying Linux host operating system. Next, it provides a description of the purpose which in this case suggests that the Docker daemon docking level should be set to the value info. This is followed by a rationale which for this recommendation is so that the events logged by the daemon can subsequently be reviewed for operational purposes. The daemon logs engine activity but only at the level set. The benchmark prescribes the info level is the optimum level and that debug level should only be used when it is required. The recommendation then shows how to audit the platform against the recommendation before providing details of how to remediate non-compliance. After describing the impact of non-compliance, which is non in this particular case, the recommendation then describes the platform's default configuration setting which is a log level of info. In other words, unless the log level has been explicitly altered, then a Docker platform's logging level will conform to the recommendation. Now, if I were to tell you that there's a total of 105 recommendations in the CIS Docker CE benchmark 1. 1. 0, then manually checking a platform's compliance would be a long and tedious task and we'll be breaking out of shell script trying to introduce some automation. Thankfully, people have been there before us and there are some tools that can help us. So let's see what's available.

Measuring How Secure a Docker Platform Is
The first tool that we might make use of is InSpec. InSpec is an open source framework for auditing and testing infrastructure. It was originated by Chef and because it's a framework it's infrastructure agnostic. It can be applied to many different platforms. InSpec uses profiles to define the tests or controls used for testing infrastructure. A Ruby-based domain specific language, or DSL is used to write the controls which are applied to infrastructure resources. Community-provided InSpec profiles are available via the Chef Supermarket. A community-provided InSpec profile is publicly available which implements the CIS Docker benchmark. This means it's possible to use InSpec in conjunction with the profile to test the configuration of a Docker platform for compliance against the benchmark. At the time of authoring this course, the InSpec profile implements the CIS Docker 1. 13. 0 benchmark. This is a little behind the curve in terms or Docker engine currency but may be okay where an older Docker engine version is required. The second tool that we might use for measuring compliance against the CIS Docker benchmark is the Docker bench for security. Like InSpec, it's a script-based audit system but a set of shell scripts instead of Ruby scripts. Unlike InSpec, which is a general purpose auditing framework, the Docker bench for security was created for the specific purpose of auditing a Docker platform against the CIS Docker benchmark. The Docker bench for security is an open source implementation and can be found in Git Hub where you can contribute to its upkeep. The Docker bench for security is also maintained by Docker rather than a third party and conforms to the latest edition of the benchmark. Finally, whilst it doesn't need to, it can be run in a container using a pre-authored Docker image. This requires bind mounting some components from the host into the container in order for the test to complete successfully. Whether you decide to use InSpec rather than the Docker bench for security or vice versa really depends on personal preference. If you already make use of the InSpec testing framework for example, it might be sensible to adopt it for testing your Docker platform also. You may on the other hand determine that the close proximity of the Docker bench for security with Docker itself is reason enough for it to be the preferred solution. Either way, our end goal is exactly the same, to check the compliance of our Docker platform against a benchmark standard. Let's see how to achieve this first hand.

Testing a Docker Platform for CIS Docker Benchmark Compliance
It's time for a demo and in this demo we're going to audit a Docker platform against the CIS Docker benchmark using the Docker bench for security. This is likely to throw up some issues which we'll attempt to remediate as we go through the course. We won't be able to fix every detail during the course but there will be detailed steps for fixing each issue uncovered in the exercise files. This means that you'll get the chance to try out any remediation we don't recover in your own environment. The host on which we're operating is Ubuntu-based. It doesn't matter too much which flavor of OS we're using, the benchmark is platform agnostic so will work equally well on most Linux distributions. Whilst we could run the Docker bench for security from within a container, it requires that we mount a number of host directories into the container which can often be different for each Linux distribution. Additionally, some of the test don't work so well when run in a container and give false positives. This is because they use commands which rely on information specific to the host that can't be replicated inside the container. One of the tests for example uses the mount point command which refers to the file slash proc, slash self, slash mount info which will be different inside and outside the container because the container exists in its own mount namespace. So, in order to run the Docker bench for security on the host itself, we must first obtain it by cloning it from its Git Hub repository, it's located in Docker's Git Hub account in a repo called Docker dash bench dash security. Once the repo's been cloned, our next step is to change directory into the route of the repository and execute the Docker dash bench dash security shell script using the help option which shows us how to run the tool. There are just three possible config options to use, the first allows for specifying the log file that the Docker bench for security writes to. By default, it writes its output to the file Docker dash bench dash security dot sh dot log in the current working directory. Next, if you don't want to run all of the checks at the same time, we can provide a comity limited subset of tests instead. As we'll see in a moment, we can filter the tests we want to invoke quite finely or relatively coarsely. Finally, for any container checks, we can exclude specific containers based on a defined pattern which is matched against the container name. Now we know how it works, let's run an audit against our Docker platform. The tests are executed in no time at all and we get a lot of output on the screen. Given that the output is logged, we can check it to see what the audit has uncovered. The first thing to note is that the tests are categorized and correspond to categories defined in the CIS Docker benchmark. The first category here is host configuration and within the category there are a number of tests which the audit has reported on. Each test has a label based on the test result. The node result is a CIS Docker benchmark recommendation which doesn't have a score associated with it. It's referred to in the audit output for completeness. The info result occurs where there is deviation from the recommendation but where there may be a legitimate reason for doing so. For example, the test to ensure Docker is up to date has determined that the version of Docker is not the most recent version. Some organizations have a strict policy on upgrades and it may not be appropriate to simply apply the very latest release, hence the result of the audit test is set to info which suggests the outcome needs to be investigated further. Audit tests that fail have the warn label whilst those test succeed have the pass label. Moving through the audit results, we can see the other categories of the CIS Docker benchmark, Docker daemon configuration, Docker daemon configuration files, container images and build file, container runtime, Docker security operations and Docker swarm configuration. Our consideration in this course is for the configuration of the platform itself rather than the content of the Docker images and Docker files or the way any containers were invoked on the command line. Of course, the content of Docker images is very important from a security perspective, as is the way that container workloads are invoked. I recommend Pluralsight securing Docker contained workloads course for learning how to secure the container workload itself. We'll be focusing on the configuration of the host and Docker daemon as well as a swarm mode cluster. At the end of the test results, we get a tally of the number of tests and the overall score which is the net of the number of passes and warnings. It gives us an indicative sense of how secure the platform is, whilst we won't be concentrating on all of the categories in the CIS Docker benchmark, we'll certainly want to improve on our score.

Module Summary
We've reached the end of this module so before we move on to the next module, let's recap on the saline points that we've covered. We've defined the components of the Docker platform which we must take into account when we're charged with securing a Docker platform. Knowing what to secure is not enough, we also need to know how to secure those components and we've discussed the various sources of information available particularly the CIS Docker benchmark. We've also seen how we can employ some open source community tools in order to help us with auditing the compliance of a Docker platform against the CIS Docker benchmark. You now have a good overview of what's involved when it comes to securing the Docker platform. So we're going to move on to discover how to secure some of the platform components themselves. First up is the configuration of the host platform itself.

Optimizing the Configuration of the Docker Host
Module Outline
Welcome back to this Pluralsight course, Securing the Docker Platform with me, Nigel Brown. In the previous module we took some time to define the Docker platform, identified a best practice benchmark for measuring how secure a Docker platform is and learnt about some tools to help us in performing checks against the benchmark. In this module, which is called Optimizing the Configuration of the Docker Host, we're going to get right to it and look at securing the host platform that Docker runs on. When we ran the Docker bench for security previously we tested every aspect of the Docker platform in one go. At this juncture, however, we're primarily interested in auditing the configuration of the host and don't really need to run all of the tests. In the last module we mentioned that it was possible to run a subset of the test rather than the complete set. Let's quickly see how to do this. The Docker bench security shellscript sources a file called functions_lib. sh which contains a number of shell functions which define the audit tests. There is a function for each test category. For example, host configuration and each function contains calls to individual tests which are themselves shell functions. The host configuration check for ensuring the Docker daemon binary is audited is embodied in the function check_1_5 for example. Now if we want to run a specific test or check, say check_1_4 which corresponds to recommendation 1. 4 in the benchmark, all we have to do is use the -c flag and specify the name of the check. Only that specific check is performed. We, however, want to perform all of the tests associated with the configuration of the host. Well, that's easy to do. All we need to do is use the -c flag, but specify the function name associated with the entire category, host_configuration in this instance. This time the script executes the whole suite of checks associated with the host configuration category, easy. In this module we're going to address some of the issues associated with the host configuration, uncovered by the Docker bench for security. Let's see what we're going to cover. The benchmark encourages us to harden our platform against potential attack, and one way of doing this is to reduce the potential attack surface. What's the easiest way of doing this? We can employ a minimal operating system with a smaller number of moving parts. We'll discuss this as well as demonstrate the use of one of these new breed of operating systems as a host for the Docker platform. We'll also discuss the need for taking steps to harden whichever operating system we elect to use and where we can get information to help us do this. An obvious source of vulnerability comes from out of date software. We'll learn about Docker's release strategy, so that you'll be in a position to make informed decisions about Docker platform updates, and finally, we'll discuss and demonstrate the importance of auditing the use of some of the key Docker file system objects so that we can mitigate any potential misuse. By the end of the module we will have remedied some of the configuration problems that have been unearthed and you'll know how to address similar problems in your own Docker platform environment.

Employing Minimal Operating Systems
When we're considering the security of the host platform itself, then the configuration of its operating system is of paramount importance. In turn, this implies that the choice of operating system becomes an important factor also. But Linux is Linux right? Well, it is and it isn't. Generally speaking, Linux distributions share a common kernel, but after that they each take on a whole different persona in terms of tooling and configuration. What gets included in a Linux operating system is based on the preferences of the maintainers of the distribution. Whether they be commercial entities or otherwise. So, there may be a whole bunch of factors that influence the choice of platform operating system. Not least of these will be commercial factors. Which provider will give the procurement team the most favorable options, but equally, operational factors will play a part. What skills and experience do the platform support team have. If the choice already wasn't difficult with the multitude of options available, then in recent times it's become a little harder. With the advent of the application container paradigm and cloud native computing in general, a new breed of operating system has emerged, which is pared back and optimally configured for the job at hand. So what advantage do these cloud native operating systems provide over their (mumbles) cousins? There's no right or wrong when it comes to deciding the operating system flavor for hosting a Docker platform. It is useful, however, to know how cloud native operating systems differ from their traditional counterparts and what advantages they might bring to the party. By its nature the traditional Linux operating system is general purpose. It has a use cases to cater for and requires the components and configurations necessary to cater for those use cases. The minimal cloud native variant is designed for a specific purpose. Most often, this is for running Linux containers, but sometimes for Internet of Things devices also. The key thing is that it has just enough operating system for the purpose at hand. The traditional Linux OS is large and cumbersome, consuming considerable system resources just to function as a host. In relation, the minimal alternative consumes much less, thereby allowing the containerized applications more share of the system resources. You're probably familiar with your favorite Linux distributions package manager, which you'll use to incrementally update the operating system. Pretty soon you'll lose track of what's been updated and when that happens, it will become problematic to reproduce an identical Docker host on demand. Generally speaking, the new breed of minimal distributions apply updates atomically. That is operating system changes are applied as wholesale releases, which means it's much easier to revert back to a previous configuration. This transactional approach usually requires a reboot or a kernel exec in order to bring the new release into play. Traditional Linux operating systems have a read-write operating system partition, which allows for incremental updates as well as the installation of new packages. As a rule of thumb, minimal cloud native Linux distributions have read-only operating system partitions, which means they cannot be updated other than via the atomic updates we mentioned just now. Each of these differences can be perceived as advantages in the favor of the minimal operating system when it comes to hosting a Docker platform for containers. Perhaps the biggest advantage though, especially for our purposes, is that the minimal operating system is inherently more secure. Besides the read-only atomic nature of the operating system which provides us with a number of security benefits, the fact that the operating system is stripped of everything that is superfluous to the needs of its intended purpose considerably reduces the attack surface available. As a consequence it goes a long way towards hardening the container platform. This could be reason enough to convince you to adopt a minimal operating system as the host for your Docker platform. Pragmatism will play a part in the choice, however, and it doesn't necessarily follow that a minimal operating system is the best choice. If you want to investigate the use of a minimal operating system for your Docker platform then the following distributions and projects are worthy of consideration. All are open source. Container Linux was probably the first of this new breed of operating system and is designed for clustered environments where applications run as container workloads. Project Atomic is a collection of projects that are based on a minimal operating system platform called an Atomic host. Atomic host versions exist for Red Hat Enterprise Linux, its upstream variants Fedora, and its downstream variant, CentOS. RancherOS is a small Linux distribution based entirely on containers. That is the small number of system services that there are are all containerized. The init process is a customized Docker daemon. One of the system services is another Docker daemon, which is used to run the application containers. Ubuntu Core is a slimmed-down version of Ubuntu which can be used to power Internet of Things devices as well as container environments. Photon OS is another minimal Linux operating system that can be used for running containers. Photon OS is designed to run in virtual environments, especially in VMware infrastructure. Finally, LinuxKit is a relatively new open source initiative that exists under the MOBI project umbrella, originated by Docker. LinuxKit is a tool for building minimal custom Linux operating systems. As such, it can be used to host a Docker platform. Let's see one of these minimal operating system in action.

Deploying Docker on RancherOS in the Cloud
In this demo we're going to get a taste of a minimal Linux operating system for hosting a Docker platform. We'll demonstrate how to create an Amazon Web Services instance using RancherOS as the host operating system. We'll get a feel for the minimal nature of the host, demonstrate that everything runs in a container as well as configure the host to run application container workloads. To get going we just need to execute a couple of commands in order to allow us to access the AWS instance we're about to create. We need to add some Ingress rules to the AWS security group. This first command provides us with the public IP address for this local environment. Because we're going to use this a few times, we'll assign it to an environment variable so that we can easily refer to it. There are going to be a number of complicated commands in this demo, but they will be provided in the exercise files so that you can easily make use of them. We'll also need to retrieve the ID of the AWS security group that we'll be using. We'll use the default security group, and again, for convenience we'll assign this to another environment variable. With that done, we can go ahead and create the Ingress rules. We'll need Ingress via port 22 for SSH access to the host. Port 2376 to allow our local Docker client to communicate with the hosted Docker daemon and port 3000 which we'll use to access containerized application that will run on the Docker host. Now that that's done, let's turn our attention to the configuration of the host operating system that we're going to deploy. We're going to use a RancherOS Amazon Machine Image, or AMI, to create our instance. But we need a means of configuring the instance for our purposes. RancherOS allows us to make use of Cloud-Init for configuring the operating system. To do this we need to prepare a cloud config file which we'll provide to the instance via the AWS CLI. So what can we configure? Well, RancherOS allows you to configure a whole lot of things, but we're going to keep this relatively simple. We'll give the host a specific host name, and because all system services run in containers, including the host's console, we can pick the OS flavor of the console. Rather than the default BusyBox console, we'll opt for an Ubuntu console instead. Finally we'll configure the Docker daemon by turning off the userland_proxy, which is turned on by default, and we'll give the daemon a label, OS with a value rancherOS. We're all set to start our instance, which we'll do using the AWS CLI. There are a few things we need to supply on the command line. The ID of the AMI, a key name that has been previous imported, a tag to help us filter instances later on, and the all important cloud-config which we'll provide courtesy of the user data flag. Okay, so the instance appears to have started without a problem. So our next step is to gain access, so that we can get a feel for RancherOS. We need the public IP address of the instance to do this, which we can retrieve using the AWS CLI, making use of the tag we supplied for the instance object. We'll need this again in a while, so we'll assign it to an environment variable for convenience. Now let's log in to the instance using the Rancher user and the public IP address. Okay, that's the difficult bit over with. Let's check to see whether the cloud-config was implemented for the instance. We can see straight away that our console has a Ubuntu flavor and that the host hostname is set to the value that we specified in the cloud-config. What about the configuration of the Docker daemon? If we list the process for the Docker daemon, we can see that amongst the various configuration options that have been applied to the daemon, the userland-proxy is set to false and that the daemon has the label that we specified in the config file. All's well. Earlier we mentioned that the Init process for RancherOS is another customer Docker daemon. We can see this by listing the process with ID1, which is the Init process on a Linux system. It's system-docker. This is how RancherOS is able to run the system services as containers. Let's list them. To make the output a little more readable, the command has a format string which customizes the fields that get displayed. There are just a small number of essential services, including Docker, which is sometimes called user-docker to differentiate it from system Docker. The user-docker daemon then runs inside a Docker container. We're actually issuing these commands in the console container and normally when isolated within a container, we wouldn't get to see or access any resources that belong to the host. But because the system container selectively shares name spaces and components of the host file system as bind amounts, it feels as if we're interacting with the host itself. So what about running application containers? The user-docker daemon for running user application containers is configured for local use, so we need to configure it so that we can invoke containers from a remote Docker client. To do this, it's necessary to configure the daemon to securely accept remote client connections using TLS. We could have performed this configuration at the head of booting the instance, if we knew the public IP address and host name beforehand. In our case, we didn't. So we must make use of a RancherOS utility to perform this configuration. It's called ROS and it's used to configure the host post launch. One of the actions we can perform with ROS is to generate the server and client certificates and keys which we'll subsequently use to configure TLS communication between our local client and the Docker daemon on our hosted RancherOS instance. We can't do this, however, until we know the public IP and host name of the host. We know the public IP address already, but for convenience we can assign this to an environment variable in our session on the remote host. Using the special URI for obtaining instance metadata. A corresponding URI is available for the public host name too. Now that we have these items safely stored in their respective environment variables, we can use the ROS utility to generate the server's private key and certificate using a self-signed certificate authority. The -h arguments specify the valid ways a client can address the daemon and the means by which the daemon can prove its identity to a client using its certificate. The certificates and keys are stored in /etc/docker/tls. All that needs to be done now in order to configure the docker daemon to use TLS is to use the ROS utility to alter the daemon's configuration. For the change to take effect, the user-docker container needs to be restarted and we use the system-docker command to do this. Just as we listed the configure argument for the daemon previously, we can do the same again and we can see the various TLS-related arguments appended to the process. That's the service side taken care of, now let's turn our attention to the client. Again, we can make use of the ROS utility to generate the certificates and keys for the clients. We omit the -s config option this time, so we know we're generating artifacts for a client. These are safely stored in the default location of the Docker directory of the Rancher user. Any client configured to use these TLS artifacts will have secure access to the daemon. Before we test our local Docker client against the Docker daemon running on the AWS RancherOS instance, let's test it locally. Everything is configured appropriately for the Rancher user to envoke the Docker client using TLS, except that we need to specify the --tlsverify flag. The client is successful in communicating with user-docker daemon and it returns version details of the server. The last step in this exercise is to demonstrate that a remote client can consume the daemon's API and create and run containers. For that to happen we must exit from the host and then remotely copy the client TLS artifacts from the RancherOS host to the local client. The files are copied to local users. docker directory which is the default location for storing TLS artifacts. Additionally, we need to manually set two environments variables DOCKER_HOST and DOCKER_TLS_VERIFY which will instruct our local client to address the remote Docker daemon using TLS. With that configured if we issue the Docker info command with a format string filtering on the host name OS and any labels applied to the daemon we get details that correspond with the user-docker daemon on our remote RancherOS host. Our local client is communicating with our newly created Docker platform host. Let's run a container and make sure we can consume in service. The container serves as static webpage using Engine X on port 3000 which we can successfully verify in a web browser. That concludes the demonstration. We've created a very lean and efficient Docker platform host using a minimal container based operating system which aids us in our quest to make our platform secure from attack.

Hardening the Host Operating System
Using a minimal operating system can be considered a proactive step toward hardening the host operating system which is one of the recommendations in the CIS Docker CE benchmark. But it's not always possible to use a minimal operating system for reasons we've already discussed. Even in the event of deploying a minimal operating system a plan of action is still required in order to harden a Docker platform host operating system. For example, effective configuration of the SSH team for remote access is required irrespective of whether you're using a cloud native minimal operating system or a more traditional Linux distribution. That's if, of course, you allow remote access with SSH. So if we want to harden our host operating system what do we have at our disposal? Again, there are a lot of sources of information vying for your attention. In truth, hardening a Linux host is a full topic in its own right and it's not possible to cover the topic to any great depth in this course. In fact, the CIS Docker CE benchmark recommendation for hardening the container host is unscored for this reason. What we can do in lieu of some specific recommendations is call out some useful resources that cover the topic in more depth. Earlier in the course we discussed the role of the Center for Internet Security, and its benchmark program which provides best practice guidelines for securing the configuration of different platforms. It will come as no surprise then to find that CIS benchmarks exist for various specific Linux distributions as well as for the Linux operating system in general. This table provides the relevant information you need for measuring how secure your specific operating system is when measured against its relevant CIS benchmark. Most of the mainstream Linux distributions are covered, but none of the lean cloud native varieties are. There is, however, a distribution independent benchmark which could be retrofitted to cater for any flavors of Linux not covered explicitly.

Keeping the Docker Engine Current
Earlier when we ran the Docker bench for security and ran the audit tests for the configuration of the host we received some output with an info label stating the version of the Docker platform in use. The output also (mumbles) us to check whether Docker was up to date and suggested that the OS supplier might provide support and security maintenance for Docker. Well what are we to read into this? How do we know whether the software is up to date or not? What is the most recent release and where do we get it from? Should we be using the very latest version or should we be using a version that is a little more mature? There are no right or wrong answers to these questions, and much will come down to preference or policy. So let's take a look at some of the factors that might influence your decision making. Starting in March 2017 Docker instigated a new release strategy for the community edition platform which still holds true today. It comprises two different channels; a stable channel and an edge channel. MOBI which is the open source project that develops Docker platform continually innovates creating new features and functions from one release to the next. This constant change is one of the attractive aspects of Docker. We fail when we stop innovating, but the constant change can also be perceived as a negative trait, especially when stability is required for production environments. The stable channel then provides some respite from the constant change with new stable releases occurring every three months. The first stable release based on this strategy appeared in March 2017 as Docker Community Edition 17. 03. Release 17. 06 appeared in June 2017 and so on. The edge channel provides more frequent updates allowing for the provision of new features as and when they're developed. These releases occur monthly in between the stable releases and continue the naming convention. Edge release 17. 04, for example, was released in April 2017. Choosing where to obtain Docker platform software and deciding which release strategy to follow is clearly important. One of a number of sources for obtaining Docker platform software is from the package repositories of the host Linux distribution. Fedora, for example, or Ubuntu and so on. If you use these packages as the source for your Docker platform software be aware that the packages can sometimes be way behind the current release, even the stable release. It is far better to use the Docker Community Edition repositories maintained by Docker which will ensure the current versions are available during package updates. So which channel is the most appropriate to subscribe to? Docker Community Edition doesn't come with formal support, but it does receive fixes in the form of patches and updates. If you decide that you need to subscribe to the edge channel in order to make use of the frequent new features, then each edge release receives patches and updates for a single month only. Once the next edge or stable release occurs no more updates are provided. If on the other hand, you subscribe to the stable channel the fixes and updates for a particular release are maintained for a period of one month after the release of a subsequent stable release. For example, staple release 18. 03, released in March 2018, receives patches and updates until the end of July 2018. A final point to remember, updates and patches for stable releases are delivered as point releases. So release 18. 03. 2 will be the second point release associated with stable release 18. 03. These point releases usually contain bug fixes, but more importantly security fixes that remediate security vulnerabilities. Give good consideration to the most appropriate release channel for your project circumstances and make sure the platform is up to date accordingly. It will help to ensure the ongoing security of your Docker platform.

Auditing Important Docker Artifacts
If we return to the output of the Docker bench for security for the configuration of the host platform once again, we can see a number of failed tests that relate to auditing. More specifically the tests that check that important Docker platform artifacts, in the form of files and directories, are audited using the Linux Audit framework. In a short while we'll take a closer look at the artifacts themselves and find out why they are important to audit, but first let's get a high-level view of the audit framework itself. The Linux audit framework provides a means for analyzing the activity that takes place on the host. It can be used to monitor access to and modification of files and directories on the host as well as the execution of files and commands by users. Unlike some of the other Linux kernel security mechanisms such as SELinux or (mumbles), the audit framework doesn't provide real time preventative security measures. Instead, the audit framework allows for the tracking of system activity which then provides the opportunity for implementing preventative security measures. Once the audit framework has been installed and configured correctly on the host, the actual auditing takes places in the Linux kernel. The kernel audit subsystem implements hooks on system calls so that when a user space process makes a call to one of the kernel's system calls, an audit event is generated according to the audit policy that's defined. Auditd is the user space system process that writes audit events to the audit log. The behavior of auditd is governed by the configuration specified in the auditd. conf file. So how do we get to view these events that are logged to disk? Well, the audit framework gives us two utilities for querying the audit log. The first utility is aureport which enables us to view a summary report of all the logged audit events held in the audit log. It can provide us with a generic summary of audit events for a given time period or provide a summary for a specific area of concern, authentication attempts for example. You might use the Aureport utility as a first step before drilling down into more specific data. The second utility is ausearch. This utility gives us the ability to inspect audit events in more detail and we'll see how we can do this in a little while. Finally, the auditctl utility is used as a user space method for controlling the audit framework. It can be used to alter the configuration of the system itself as well as for applying audit rules on the fly. The audit. rules file contains the rules that dictate what is monitored and under what conditions. The audit daemon reads this file on startup. Okay, we know what the Linux audit framework is and what it's for, let's return to the recommendations of the CIS Docker CE benchmark. What does it recommend that we audit? Firstly, it recommends auditing some binary files. In addition to the Docker daemon itself, the CIS Docker CE benchmark also recommends auditing containerd and runc, the two binaries that Docker uses to bootstrap and manage the life cycle of containers. Next, it recommends auditing Docker daemon configuration files. The Docker daemon can be configured in a variety of ways, but there are two files that are explicitly used for providing daemon configuration. The first /etc/default/docker can be used to supply daemon configuration options on systems that use either of the SysV Init or Upstart Init systems. Most modern Linux distributions however now use Systemd as their Init system. One method for providing daemon configuration on systems that use Systemd is through the /etc/docker/daemon. json file. As long as the content of the daemon. json file is not at odds with the configuration provided as part of the Systemd unit file, it can be used to provide additional config options. Both of these files need to be monitored via the audit system. Control of the Docker daemon is usually governed by systemd and the various parameters and configuration are defined in a unit file called docker. service. Tampering with this file can affect the way that Docker behaves and it should be monitored accordingly. We've already seen that when local client API calls are made to the Docker daemon they are made over a Unix domain socket. Again, this socket is often under the control of Systemd and defined in a unit file called docker. socket. Just as we have a need to audit the service unit file so we must audit the docker. socket unit file as well. During its operation Docker needs to hold state and configuration information for many of the objects it creates; containers, images, volumes and so on. This data can be sensitive and access to it should be carefully controlled. It's location is called the execution route and in a standard Docker installation can be found at /var/lib/docker. The CIS Docker CE benchmark recommends auditing this directory. Finally, when the Docker daemon communicates with remote entities, a client for example or a remote registry, this should be conducted using transport layer security or TLS. Implementing TLS results in the creation of certificates and keys which need to be held on the host system. Typically these artifacts are held within the /etc/docker directory and it goes without saying that these important artifacts should be closely monitored. So we know what needs auditing and what mechanism to use to perform the auditing. So let's get on and implement this on our example host.

Creating Audit Rules for the Docker Host
We're going to get into another demo now. We're going to address a problem associated with a lack of auditing for the key Docker artifacts that we highlighted earlier. At the end of the demo we will have, hopefully, turned all of the labels associated with the checks related to auditing from warning to pass. Now it's possible that we may have the audit framework already installed, but just not configured to monitor the Docker platform artifacts. So let's go ahead and check to see whether the auditd daemon is running, and if not, whether it's even installed. It looks as if the audit framework is not installed on this host, so we need to install it before we do anything else. The host platform here remember is running the Ubuntu operating system. So we need to use the apt utility in order to update the package index first, and then once that's complete we can install the auditd package and its dependencies. If you're following along and your host OS is something other than Ubuntu, the exercise files contain instructions for installing the audit framework on some other popular Linux distributions. With installation complete, let's check we have a running daemon. All looks well. The first thing we can do is use the aureport utility to get a summary of the audit events that the kernel has provided to the audit daemon, and which have been logged to disk. Of course there are very few as we've only just turned auditing on. We also haven't applied any rules yet, so the small number of events that have been recorded in the log relate to certain system events that are automatically logged. Let's add an audit rule and we'll start with the Docker daemon itself. It's called dockd and it's located in a directory /USR/bin. Here's an important point to make note of. At the moment, the CIS Docker CE benchmark incorrectly suggests the Docker daemon is called Docker rather than Dockerd. The binary called docker is, in fact, the Docker client and not the daemon. Our need is to audit the daemon so our rule must reflect this. We can make use of the audit's CTL utility to add the rule. The first argument that we supply is -w to specify a watch followed by the path name of the file system object, /usr/bin/dockerd. The second argument we will use is -k which sets a filter key for the rule. This will enable us to use the ausearch utility to find only those events that relate to rules associated with a key labeled Docker. Using auditctl again, we can list the rules that are currently applied. It shows us our rule which specifies a watch for any attempt to read, write, execute or change the attributes of the Docker daemon. Time to test the rule. Let's use the Docker daemon to find out its version. This involves executing the daemon binary which should trigger an audit event. Now if we run an audit report using aureport and specify that we want a summary of audit events for all audit rule keys we get a summary report which includes any events generated by the rule that we've applied. The event ID is 422. We can then use the event ID as input to the ausearch utility to get some detailed information about the event in question. That's a lot of information and getting into the detail of the content of events is beyond the scope of this course. We can, however, provide a succinct summary of the event by combining the ausearch and aureport utilities. If we pipe the output of ausearch into aureport and report on file activity we can obtain a more human readable report of the event. It gives us a date and time for the event and indicates that the dynamic linker performed an execve system call in order to execute the Docker daemon which was successful. It also gives us the audit user ID which is the log in UID that we can determine who was responsible for performing the action on the binary. It's the user rackham in this case. For this rule in situ we can now audit all activity related to the Docker daemon binary which will help to ensure we keep a secure system. Of course, manually reviewing copious logs is not entirely practically, and it will be necessary to consider your whole policy on auditing beyond just the Docker platform. This may involve centralize logging and the application of other tools for searching, analyzing and visually the data. Now that we've implemented an audit rule that satisfies the CIS Docker CE benchmark check 1. 5 let's go ahead and do the same for checks 1. 6 through 1. 13 which we can apply using the auditctl utility in a four loop. Firstly, let's prepare an array containing the file system objects that we need to audit. Each element in the array corresponds to one of the path names in the recommendations. The four loop references each of the array elements in turn and uses each element as the source pathway for a watch rule. Just as we did for the Docker daemon binary we're setting the key Docker for each of the rules. Once again, let's use auditctl to make sure that the rules have been applied to the audit framework. That all looks good. You can now be confident that any operations performed on any of these artifacts will be logged as events that we can subsequently analyze. But we're not quite done yet. The audit rules that we've created using auditctl are not permanent. If the audit daemon were to be restarted our rules would be lost. The rules need to be recorded in the /etc/audit/audit. rules file in order to become permanent. The easiest way to achieve this is to take the output provided by auditctl when we list the current rules using the -lconfig option and append that output to the audit. rules file. We can go ahead and list the contents of the audit. rules file, and there are our rules. When the audit daemon is restarted it reads this file for the rules it needs to apply for auditing purposes. So our rules are now permanent. The changes we've made should ensure that our host configuration of the Docker platform passes the audit related checks made by the Docker bench for security, but before we run the checks to test the configuration we need to make a small change to one of the test scripts. Remember the discrepancy over the name of the Docker daemon? Well, the Docker bench for security faithfully follows the recommendations provided in the CIS Docker CE benchmark, so checks for the wrong binary for the Docker daemon. We can make a simple change in the script that executes the tests for the host configuration. Let's find the test for recommendation 1. 5 and make sure that the file that's been checked is dockerd instead of docker. That's done. So let's re-run the host configuration checks. The script calls the auditctl utility to check that audit rules exist for each of the file objects defined in the benchmark. So we should get a better result this time. In fact, we get a string of passes for each of the recommendations that relate to auditing. Job done.

Module Summary
Right then, that's the conclusion of this module. So let's summarize where we've got to. Don't ever trust me. Whilst your back was turned I fixed the checks for recommendations 1. 1 and 1. 3. Recommendation 1. 1 suggests placing the execution route /far/lid/docker on a separate host partition in order to avoid a full partition rendering the host usable. Recommendation 1. 3 as you'll remember warned us to keep our software up to date and our host has been upgraded to the most recent release. Should you need it, details of how to perform these fixes are provided in the exercise files. We have one outstanding recommendation to remedy. To ensure only trusted users have access to the Docker daemon, we'll address this in the next module. We've been discussing the optimization of the Docker host configuration. So let's briefly recap. You can lock down the configuration of Docker as much as we like, but unless the host on which it runs is hardened against attack, then our efforts will all be in vain. Be sure to adopt a strategy for securing the host and for ongoing measurements and maintenance of the security controls that you implement. Evaluate the options available to you for the host operating system and if possible, make use of a minimally configured Linux distribution designed for the purpose of running containers. Deploy the Linux audit framework, determine how you want to use it and audit the important components of your host, including the Docker platform artifacts that we've discussed. In the next module we'll discuss what we can do to better secure the Docker environment itself, specifically we're going to look at some of the Docker daemon configuration options that will help us to improve security.

Configuring the Docker Daemon for Security
Module Outline
Hello, again, good to see you back as we move on to another module in this course, securing the Docker platform with me, Nigel Brown. Our last module dealt with securing the underlying host operating system. And in this next module, we're going to consider how we can take security to the next level. The module is called Configuring the Docker Daemon for Security and addresses the various configurations of the Docker daemon that will enable us to enhance the security of the platform as a whole. We'll be measuring the compliance of a Docker platform against the best practice benchmark, the CIS Docker community edition benchmark, using a tool called the Docker Bench for Security. The benchmark has a whole section dedicated to the configuration of the Docker daemon from the point of view of security. If we run the Docker bench for security tests for this section, we can see there are 18 tests in all, some of which pass and a lot of which fail. We can't cover every single benchmark recommendation in this section. There are too many. Instead, we'll consider two key aspects of security whilst we'll consider the remainder in the solutions provided in the exercise files. Okay, let's have a look at what we'll cover in this module. Do you remember we had an outstanding test to remediate from the host configuration in the previous module? It was related to controlling access to the Docker daemon on a local platform. And we've deferred discussing it until now, because we're going to discuss the general topic of basic access control to the Docker daemon. Firstly, then, we'll look at how we can impose strict control over the access a local client has to a Docker daemon. Closely coupled with local client access control, of course, is controlling access to the Docker daemon from remote clients. This is a whole different requirement, which not only necessitates the use of authentication, but also encrypted communication between client and daemon over a network. We'll see how to configure this for the Docker daemon and the remote client. Application services running in containers that are isolated from the host and other containers by virtue of Linux namespaces, but what would happen if a container were exploited? How can we configure the Docker daemon in such a way as to mitigate against a container breakout? We'll discuss and demonstrate how to configure the Docker daemon to make use of user namespaces to help combat some of the potential consequences of a compromised container. When we get to the end of the module, you'll be equipped to apply some comprehensive security measures to the Docker daemon to further enhance the security of your Docker platform. Let's make a start.

Controlling Access to the Docker Daemon Socket
Usually, when the Docker client and daemon are located on the same platform, they communicate using a UNIX domain socket. A user might issue a Docker CLI command perhaps to list the images available in the cache of the Docker host, as shown here. The client will interpret the command and issue an API call in the form of an HTTP request. The request addresses a specific engine API endpoint and is sent via the UNIX Domain Socket where the daemon's API server is listening. The daemon will return an HTTP response code via the socket based on the success or otherwise of the requested operation. In addition to performing the requested action, the daemon may also return some data. For example, in response to the get images API call, it will return an array of JSON object, each describing an image stored in the daemon's local cache of images. The client will interpret the JSON payload and present the information back to the user. So where is this UNIX domain socket defined? How is it configured? And, most importantly, how is access to it controlled? Let's have a look. Let's start by getting to grips with how to address the Docker daemon. The daemon is configured to listen on any given socket using the --host config option, or -H for short. The argument that follows the config option is the address of the socket, which can be a TCP socket, A UNIX domain socket under the controller system D, or a UNIX domain socket not under the controller system D, as shown in the example here. The daemon can actually be configured to listen to multiple sockets of different socket types simultaneously. And by default a local Docker client will attempt to communicate with a Docker daemon on a UNIX domain socket at /var/run/docker. sock. As we'll see in a moment, a client can be instructed to use an alternative socket using its version of the --host config option or via an environment variable. The Docker daemon's local UNIX domain socket is normally under the control of the system D in its system. Daemon's that make use of sockets in order to receive client requests hand off the activation of the socket to system D and system D defines the socket within a socket unit file. Here's the content of the socket unit file for the Docker daemon. The unit section provides a benign description, but more importantly, configures the socket as part of the daemon's service configured in the Docker. service unit file. It's the service definition for the Docker daemon, of course. The next section, the socket section, is what matters to us in our journey to secure the Docker platform. Firstly, we define the location of the socket within the file system, which is /var/run/docker. sock. And then it will use stream-based communication semantics. Permissions for access to the socket are then defined in the mode, which is 0660. This specifies that the user and group owners of the socket can read and write to it, but nobody else can. The user and group owners are then specified on the next two lines, route and Docker respectively. Translated then, this means that only the privileged roots user and any user that belongs to the group Docker is able to use the socket to communicate with the daemon. This should preclude regular users on a local Docker host platform. Finally, the install section ensures that the socket is activated at the correct point during the system boot process. Let's quickly return to our benchmark tests 1. 4, which recommended that only trusted users should be allow to control the Docker daemon. It shows us a line from the /etc/group file, detailing the users who are members of the group Docker. That's the user's rackham, bolt and baxter. As these users belong to the group Docker, they have read/write access to the daemon socket and can issue API calls to the Docker daemon via the Docker client CLI. Now it might be considered acceptable for these users to be able to do this, but it's important to realize that it's a trivial exercise to gain root access on the host if you're able to interact with a Docker daemon in this way. Perhaps it's appropriate in a development environment where the convenience aids productivity. However, you should really think twice about allowing regular users to access the daemon's socket in a production environment. In our example host, we're going to remove the three regular users from the Docker group. There are numerous ways that you can achieve this. But the one that I prefer is to use the getent command to retrieve the group database entry for the group Docker. Trim the output to contain a comma separated list of the users, and then use this as input to a simple for loop in conjunction with the gpassword command. The gpassword utility removes the users from the group. And the next time any of them log in, they will no longer be able to interact with a Docker daemon using a Docker client. This ensures only privileged users have access to the engine API.

Using TLS to Protect the Docker Daemon
In many circumstances, the Docker daemon will be remote to a client that needs to interact with it. In such situations, we can't use a UNIX domain socket to communicate with a daemon as the socket is bound to a specific host. How do we facilitate client-daemon communication in such circumstances? Simply, instead of a UNIX domain socket, we use a TCP socket. In this way, we can enable a client to communicate with a daemon over a network, even a wide area network. This doesn't just happen by itself, of course. We need to configure the daemon to listen on the TCP socket. And we need to instruct the client to address that socket. Let's say that the host on which a Docker daemon runs has an IP address of 18. 130. 37. 71. Then in order for the daemon to listen for remote requests, it needs to be configured using the --host or -H config option. And the argument we need to supply must contain the full details of the socket. The relevant protocol, TCP, in this case, the location of the host, as a result, or name or IP address, and one of the two reserved port numbers for the Docker engine API, 2375 or 2376. Engine API calls originating from a Docker client can then be directed to the daemon if we use the --host or -H config option as part of a CLI command, or if we set the DOCKER_HOST environment variable accordingly. These config options will enable us to communicate with a remote Docker daemon, but in an insecure fashion, which means by convention we're using port 2375. Communication will be unencrypted and we will vulnerable to man in the middle attacks as we will be unable to authenticate our communication. It's absolutely in our interest then to make sure that we secure access to a Docker daemon listening on a TCP socket using transport layer security, or TLS. We can achieve this using public key infrastructure. So let's see what's involved. The first thing to consider is how we can encrypt the communication our client and daemon conducts. To achieve this, the daemon makes use of an asymmetric key pair, a public key and a private key. During the TLS handshake process between the client and daemon, the daemon's public key can be used by the client to encrypt the data it sends to the daemon. Which the daemon can then decrypt using its private key. During the handshake, a symmetrical shared key is created, which is subsequently used to encrypt and decrypt data communicated between the entities. But how does the client know that the public key it means to use during the handshake is really from the daemon? Well, the daemon's public key is embodied in a digital certificate, which is signed by a trusted certificate authority, which binds the daemon's identity to its keys. The client authenticates the daemon's identity by checking that the certificate has been signed by a trusted certificate authority and that the DNS names and IP addresses in the certificate match those of the server, as well as the client authenticating the server is also possible and encouraged to configure the daemon to authenticate the client. The configuration of the daemon to use TLS involves the application of up to five different TLS specific config options. The first, --tls explicitly instructs the daemon to use TLS in its communication with Docker clients. The next two, --tlskey and --tlscert provide the daemon with the path name to its private key and certificate respectively. The --tlsverify config option instructs the daemon to verify the authenticity of any Docker clients during the TLS handshake, which it does by checking that the client certificate has been signed by the certificate authority whose own certificate is found at the path specified by the --tslcacert config option. Not all of these config options are required. And there is some limited choice as to how to configure TLS for the Docker daemon. If we configure the daemon with all of the options, say the --tls option, then the daemon is configured to be authenticated by any connecting Docker client and expects to receive the client certificate in order to authenticate the client, too. The alternative configuration means that the daemon will use TLS for its TCP socket and will expect clients to authenticate it during the TLS handshake. An identical set of config options are available for use with a Docker client CLI, with almost identical meaning. The main difference is that when specifying the --tls config option, whereas as the daemon is configured to use TLS, it's used with the client CLI, specifies the client authenticate the daemon using a publicly available certificate authority located with the general port of CA certificates on the client host. The client's use of TLS can be configured in a variety of ways. Using the --tls option on its own instructs the client to authenticate the daemon using one of the local port of CA certificates. By specifying the --tlsverify option, in conjunction with the --tlscert config option, the client will attempt to authenticate the daemon using the certificate authority whose identity embodied in the certificate found at the path specific by the --tlscacert config option. The next configuration is the recommended one, which employs all config options, bar the --tls option. This configuration for the client is analogous to that of the daemon where both parties seek to authenticate the other using a common certificate authority. And, finally, another possible configuration enables the client to be authenticated by a Docker daemon it communicates with, and at the same time, will attempt to authenticate the daemon using a publicly available CA certificate. In a moment, we're going to demonstrate how to generate the TLS components and apply them to secure the communication between a Docker daemon and client. But before we do that, let's have a quick look at a convenience mechanism for the client. Configuring the daemon for TLS is easy. We just apply the relevant config options and restart the daemon. If, however, we want to invoke the TLS each time we issue a Docker client command, we would soon get fed up with typing out a convoluted set of config options. The Docker client can make use of the environment variables to help us out. If we place the CA certificate along with the client's private key and certificate in a location specified by the DOCKER_CERT_PATH variable, the Docker client will automatically locate these artifacts when it's required to use TLS for communication. In the absence of the variable being defined, the client will look in a directory called. docker in the user's home directory. These artifacts are not going to get used, however, unless the client is required to use TLS, which we can ensure by setting the DOCKER_TLS_VERIFY variable to a value other than an empty string. Convention suggest setting it to the value one. The remaining convenience variable is DOCKER_HOST, which specifies where the client should send this engine API calls. We saw this earlier when we were setting up communication for the Docker host we deployed on RancherOS. With these variables set appropriately, a user can issue Docker client CLI commands destined for a remote Docker daemon just as easily as if they were interacting with a local Docker daemon on a UNIX domain socket. Okay, that's the theory. Let's go ahead and put this into practice for our example Docker platform.

Configuring TLS for the Docker Client and Daemon
Okay, in this next demo, we'll walk through the steps for establishing mutual TLS communication between our example Docker host and a remote Docker client. The CLS Docker CE benchmark recommends that we have our daemon listen for client requests using TCP and that it should also be secured using TLS. We'll use a set of open source tools called CFSSL for creating the various keys and certificates that we need to generate to make use of TLS. A directory has already been prepared for our TLS artifacts. So let's go ahead and change into the TLS directory. The first step in the exercise then is to create a certificate authority, or CA, which we can use for signing keys and certificates. It's the authority we need to establish in order to provide authentication for both the client and the daemon. To create the CA, we're going to produce a private key and a self-signed certificate. For the self-signed certificate, we must provide a certificate signing request or CSR, whose attributes have been prepared in advance in JSON format in a file called CA-CSR. json. It comprises of components of the distinguished name for the certificate, including the common name which is set to Docker Platform CA. It also specifies the attributes for creating the CA's private key. It will be a 4096 bit key generated using the RSA algorithm. The CSR attributes are used as input to the gencert subcommand of CFSSL, which also uses the -initca argument to initialize our certificate authority. This command creates the CA certificate and private key, but in JSON format, which it sends to the standard out stream. So we must pipe the command to another utility called cfssljson which creates the relevant files containing the artifacts. If we perform a directory listing, we can see the various files that get created. In addition to the pre-existing ca-csr. json file, we now have the raw CSR in ca. csr, the CA's private key in ca-key. pem and it's certificate in ca. pem. One other file that preexists, that we haven't mentioned yet is ca-config. json. It encapsulates some default settings for the CA And what it will be used for in the form of signing profiles. Just like the CSR attributes, it's in JSON format. The file contains two signing profiles, one called daemon and one called client. Each has a certificate, expiry duration of 60 days and a definition of the usage available to public keys generated by the CA with that profile. That's the CA taken care of. Let's move on to the daemon. Again, a pre-prepared file exists defining the CSR attributes for the daemon we're planning to protect with TLS. It's similar to the CA CSR attributes, but with some important differences that will help us to define the identity of the daemon's host. So we'll use the hosts fully qualified domain name as the common name, but we'll need to add all the means of addressing the daemon host in the subject alternative names property of the certificate, which means adding a host key to the JSON representation of the CSR. We'll add the short host name, the fully qualified host name, the loop back name and address and the host's primary IP address. Let make use of the cfssl gencert command again to create the daemon's private key and signed certificate containing its public key. The different this time around is that we're not initializing a CA. Instead, we're using the CA to create the daemon's TLS artifacts. Hence, we have to supply the CA certificate, private key and the config we defined for the CA's signing activities. We also need to specify the signing profile we wish to use, daemon, in this case, as well as the daemon's CSR attributes. Again, we pipe the output through cfssljson to generate the key and certificate files. With that completed, we now have a daemon certificate and private key. We've got all of the components we need for the daemon. So it's time to configure the daemon to make use of TLS. First, let's tighten access to these important artifacts by changing the mode so that only the user that owns the files can read them. Ensuring that we make the destination directory first, we'll copy the TLS artifacts somewhere useful and safe, /etc/docker/tls. There is no official location designated for these files, but the /etc/docker parent directory is a good candidates since other configuration items are also located here. The files are now owned by the root user and can only be read by that same user. We'll remove the daemon key and certificate from our directory, but leave the CA's key and certificate for the moment, as we'll need them to generate the client's key and certificate. Now we have to add the various flags to the daemon's configuration. It will be nice to do this all in the daemon's configuration file called daemon. json. But the system D unit file already uses the --H config option. So we must add the address of the TCP socket there, otherwise our setting will be in conflict with that managed by system D. We're instructing the daemon to listen for engine API requests on all interfaces using port 2376, the port designated for TLS communication. The remainder of the configuration can be added to the daemon's config file. We want the daemon to authenticate the client so we should set the tlsverify option to true. As the tlsverify config option is a Boolean, its argument doesn't need any quotes. Then, we need a line to specify the location of the daemon certificate, which we've just placed in the directory /etc/docker/tls. Similarly, by making copies of this line and then editing each appropriately, we can also add a line specifying the location of the daemon's private key as well as for the CA's certificate. With all four config options set, the daemon is configured for mutual TLS communication. That's the editing done, so let's write the file. To apply these TLS config changes to the daemon, we just need to restart it using the systemctl restart command. To make sure that TLS authentication works both ways, we now need to create some artifacts for the client. It's an almost identical process, save the fact that the client certificate will be generic in nature so that it's not tied to a specific client host. This may be convenient where we have multiple trusted people requiring access to the Docker daemon who can share the client's TLS artifacts. But you must be careful about managing access to the certificates and key. The common name has been set to client. And we are supplying an empty host array for the subject alternative names. I've already created the client artifacts using the exact same command sequence as for the daemon, but using the client CSR attributes instead. Let's exit from our Docker host to the remote client that we'll use to test mutual TLS communication. We're now on a client that's remote to the Docker daemon that we've configured for TLS. The Docker info command allows us to query the Docker daemon that our client is configured to use. And we're filtering the response to simply provide the name of the host and its operating system. So our client is currently configured to communicate with the daemon running on the host Calculus, which runs the Arch Linux operating system. Our first step now then is to copy the CA certificate and the client private key and certificate to the local client's. docker directory. Remember, this is where the Docker client will look for TLS artifacts, assuming that the DOCKER_CERT_PATH environment variable is not set to point to an alternative location. Here's an important point to remember, which will save you much grief. When the Docker client looks in the. docker directory for the client's key and certificate, it's expecting the files to have specific names. This requires us then to rename the files using the move utility. The client certificate file must be renamed to cert. pem, also the private key needs to be renamed to key. pem. Failure to do this will result in these artifacts not being recognized for what they are and a bad certificate error message from the daemon as it's unable to authenticate the client. There's just one final aspect of configuration to attend to before we test our access to the remote daemon running on the host wolff. Remember the convenience variables for the client? We need to set the DOCKER_HOST variable to point to the remote daemon running on wolff. And we also need to set the DOCKER_TLS_VERIFY variable to a non-empty string value so that the client knows to the authenticate the daemon using the CA certificate. With everything configured, let's run a Docker client command to see whether we're able to communicate with a remote daemon. Our response shows us that we're communicating securely with a Docker daemon running on the host wolff, with both sides having authenticated the other. Our client an now cause the daemon to build Docker images, run containers, or perform any other function provided by the engine API. One of the failed benchmark checks

Minimizing the Risk Associated with a Container Breakout
in the section for the Docker daemon's configuration recommends the enablement of user namespace support. So what exactly are user namespaces? Why are they important and how do we go about enabling them on the Docker platform? Let's address these questions and find out how the use of user namespaces can significantly aid the security of your Docker platform. Containers running on Linux host use a kernel construct called a namespace. A namespace isolates a specific system resource from the perspective of the process that runs inside the container. For example, the process ID namespace isolates the set of process ID numbers for a given process so that it only sees itself and any child processes it creates. The process ID namespace is just one of many that can be applied to a given process. Other namespace types include mount, network, IPC, UTS and user. A container is simply an abstraction of the application of these namespaces applied to a process. Together, they provide such a degree of isolation that the process exists in its own host-like world separated from other processes whether they be host processes or processes that run in other containers. You can find out more about how Linux namespaces provide this isolation in my Pluralsight course, Securing Docker Container Workloads. One of the namespaces available then is the user namespace. It allows for remapping the set of user and group IDs that exist in the host's default user namespace onto a completely different set of user and group IDs for a process that runs in a new alternative user namespace. So what exactly is the benefit gain from doing this? By cloning a new process into a new alternative user namespace, an unprivileged user can run a workload as a privileged user. From the perspective of the default user namespace, the process in unprivileged, but within the new namespace, the process is privileged. When we apply this to a container then, we can run the container workload as a privileged user. But in the event of the container being compromised and a subsequent breakout of the confines of the container, the damage that can be done is restricted to the access controls associated with the non-privileged user that created the container. This is a very important and powerful security control. The not-so-good news, however, is that Docker doesn't create container processes in new user namespaces. This is partly due to the need to retain backwards compatibility in the default behavior of the Docker platform, and partly due to some practical considerations, which we'll discuss shortly. Before we take a look at configuring user namespaces for use with Docker, let's see how we go about defining the mapping used between the default user namespace and a newly create user namespace. Let's take a non-privileged user, rackham, which has a user and group ID of 1000. The mappings we want to use can be defined in a subordinate file, one for each of user ID and group ID respectively. The files are called sub UID and sub GID and are located in the /etc directory on the host. Similar entries exist in both files. A tuple record consisting of the user name, a starting subordinate user ID, 165536, in this case, and a count of subordinate IDs to map, 65536, in this case. So if the user called rackham were to clone a new process into a new user namespace or place an existing process into a new namespace, then the mapping commences at user ID 165536. The user ID in the new user namespace would be zero, the privileged root user. The user ID 165537 in the default user namespace will map to user ID one, and so on, for a range of 65536 user IDs. The same mapping is provided for the group IDs. So when Docker creates a container, the default behavior is that the container's process is not created in a new user namespace. To configure Docker to make use of user namespaces, the userns-remap config option needs to be set appropriately. And there are two approaches to consider. We can either set the argument to this option as default or we can specify our own pre-configured non-privileged user, and optionally, group, or user ID and optionally group ID. If you opt to specify your own user, then the user must already exist and have a remapping entry in the subordinate files. If you allow Docker to handle user namespaces, using the default argument, it creates a user called. remap and populates the subordinate files as well. Having turned user namespaces on, what if we want to run a container without the mapping? Well, we simply use the --userns=host option and argument for a Docker container run command. And the daemon will revert to the user and group IDs in the default user namespace. Let's have a quick look at some gotchas. Some Linux distributions automatically create subordinate mappings for users on the host, but some do not. Check to make sure a subordinate mapping exists before attempting to use an existing user as the remapping user when you configure the daemon to use user namespaces. Content that already existed in the data root, /var/lib/docker cannot be reused by the daemon, because of permission incompatibilities. For example, Docker image content is usually owned by the root user, whereas as the daemon with user namespaces enabled will need the image content owned by the subordinate user ID. For this reason, after user namespaces have been enabled, Docker uses a dedicated directory named after the subordinate UID and GID of the configured user located as a subdirectory in the data root. Similarly, any volumes that are to be used by containers located on the host must be configured with the necessary permissions in order to allow remap containers to read and write to the volume. The user of user namespaces can significantly reduce the risk of host platform compromised. And if it's practical to do so in your situation, you should definitely configure your daemon to use this capability. Let's reconfigure our example Docker host to use user namespaces.

Implementing User Namespaces for Containers
You should have a good understanding now of how user namespaces work with Docker. But there's nothing like seeing in first hand. So let's go through the steps involved and configure our Docker daemon to use the user namespace mechanism. Let's start by examining the ownership of a container's process by running up a simple container based on the Alpine Linux distribution. Once it's started, we have a shell inside the container. If we perform a process listing, we can see that the shell process, which has a process ID of one is running as the root user. Nothing unusual there. This is Docker's default behavior. We can detach from the container leaving it running in the background with a Control P, Control Q key sequence. And then if we use the Docker container top command to list the container's process, the output shows us that the process ID is 2590 and that the user is root. It looks like a different process, but in fact, it's on in the same. Because the container is in its own process ID namespace, its process runs as process ID one inside the namespace but its representation is the default PID namespace is process ID 2590. Same process, different IDs. Even though the shell process is running in a container, its owner, root, is the same in and outside the container. The container and its process have their own file system, but if the process were able to break out of this contained environment, it would have access to file system objects owned by the root user, which would provide considerable privilege. We need to implement user namespaces. Before we get to doing this, however, we just need to remove the running container. In a moment, we're going to reconfigure the Docker daemon to use the user namespace mechanism. But, first, let's take a quick peek at the subordinate file for user IDs. It has the entries for each regular user on the system as well as entries for the root user and another container platform called lxd. Now we want to configure the Docker daemon to make use of user namespaces. So let's go ahead and make the necessary changes. We'll use the daemon's configuration file to do this. And at the key, userns-remap with the value default as its argument. Because we're using the argument default, this should result in a new user called dockremap being added to the system. As we know, changes don't get configured instantly. We need to recycle the daemon for the change to take effect, which we do using the systemctl restart command. If we list the subordinate file for user IDs again, we can see that Docker has added an entry for the user dockremap starting at ID 362144. Well, that should be the configuration of user namespaces complete. It's time to see what effect the configuration change makes when we run containers. If we repeat the previous exercise and run up an Alpine container, something weird happens. We already have the Alpine image in the local cache, but Docker downloads the image from the Docker hub as if it weren't there. This is because the images used by the dockremap user's subordinate user ID needs to be owned by that subordinate user ID. The original Alpine image in a cache was owned by the root user. Inside the container, our user is still the root user, no change there. But if we detach from the container again, and check the process from the host's perspective, instead of running as the root user, this type is running as the user with UID 362144. Of course, there is no user with that UID. In fact, it's the non-privileged user at the top of the dockremap user's subordinate range. This time around then the process has a privileged owner in the container's user namespace, but a non-privileged user in the host's default user namespace. And if we take a peek inside the data root, at /var/lib/docker, we can see the subdirectory that has been created for the mapping. This is where the content for the Alpine image we just pulled resides. If somehow the container's process were to escape the confines imposed on it, it has limited privileges with which to exploit the host. And as a consequence, our host is far less vulnerable to an exploitation of the container itself.

Module Summary
That it for this module, which has been all about configuring some of the Docker daemon options for improving the effective security of a Docker platform. If we take a look at the summary of test results of the Docker bench for security for this section of the CIS Docker CE benchmark, the majority of checks have a past status. Obviously, we haven't covered each individual recommendation and check in this section, but most of the remedies involve the simple addition of an appropriate option to the daemon's configuration. The exercise files contain a before and after version of the daemon's daemon. json configuration file. But, please, don't glibly reproduce this in your environment. Take some time to understand the benchmark's recommendations before implementing any new configuration for your Docker daemon. There are just a couple of outstanding checks that still show up as warnings. One suggests that making use of version one of the Docker registry API should be disabled. This version of the API is now legacy. And, in fact, since Docker CE version 17. 12 is no longer supported, consequently, the daemon config option to disable the use of the outdated version of the API is no longer available in version 17. 12 and above. Second, check 2. 11 recommends that we implement authorization for Docker client commands. In fact, this is the topic of our next module. So we'll address this recommendation at length then. But before we do that, let's just summarize what we've covered in this module. We discovered that giving a user access to the Docker daemon local UNIX domain socket effectively gives that user privileged access to the host itself. We saw how to limit access to a local Docker daemon, but also recognize that it might appropriate in some circumstances to allow a limited number of trusted users access in development environments. We also learned how to configure the Docker daemon to receive remove requests to its engine API and that it's crucial to the integrity of your Docker platform to implement transport layer security for encrypted communication, preferably with mutual authentication enabled. And we saw how to employ the Linux kernels user namespace mechanism to mitigate against privileged escalation in the event of a container breakout. The all or nothing approach that access to a Docker daemon socket provides is somewhat coarse in nature. Fortunately, there is some flexibility that we can apply to make this more practical for our purposes. And that's where we're headed next.

Enhancing Access Control to the Docker Platform
Module Outline
Hi and welcome back to the next module in this course, Securing the Docker Platform. I'm Nigel Brown and in this module, we're going to try and improve our options for controlling access to the docker daemon. An access control policy of all or nothing maybe to course for some docker platform environments. So we're going to explore how to create a more flexible environment. The module is called Enhancing Access Control to the Docker Daemon. Before we get stuck in, let's see what we'll discuss during the course of the module. Authentication and authorization are two very different aspects of security. So to start with, we'll differentiate between the two and make a case for employing authorization when users attempt to access the docker daemon. We'll move on to see how we can extend the capabilities of the docker engine by unraveling the docker engine's plugin API and more specifically its authorization plugin mechanism. Then having painted a fictitious authorization scenario, we'll take a look at their open policy agent and its implementation as a docker authorization plugin. We'll create some policy to represent our scenario before implementing authorization based access control to our example docker platform. When we're finished, you'll have the requisite knowledge you need in order to enhance the access control capabilities for your own docker platform environments.

Enhancing the Default Access Control Mechanism
During the course of the previous module, we discovered the need for and the means to secure communication between the docker daemon and its clients. An important component of this secure communication is the process of authentication which we can configure the daemon to provide to any would-be client consumers. And similarly, we can configure our clients to enable the daemon to authenticate them in return. This is all well and good and provided we take care to only distribute the client TLS artifacts to those users we trust and who have a need to perform work-related tasks on the docker platform, then we can be confident that this will occur in a secure manner. But the docker platform is a powerful tool, enabling us to create and deploy container based application workloads, data volumes for use with those workloads, virtual networks to allow containerized applications to serve and consume each other's services and to build and maintain images which provide the template for each container. These are the main objects you'll come across on our docker host but there are more besides. And of course, if we can create, query or read and in some circumstances update these objects, then we also have the means to delete them too but is all this power appropriate for every user? Does every client user have the requisite skills and knowledge to wield the power at will? Do all of the available operations provided by the engine API fit the role of every user? The short answer is probably not. So in addition to the authentication of the client, we also need to implement some authorization for the activities that each client user is able to legitimately perform. So how is authorization different from authentication which is to do with ensuring the identity of the client? Well authorization is the act of determining what access privileges an identity has to an object or resource. In our docker world, an identity might be associated with a specific client user and an object might be a container or a docker image. In an organization typically we might want to base authorization on roles performed within the organization. So let's paint a role based scenario for a fictitious organization. This scenario might be a little crude but it gives us a reasonable representation of real life and serves our purposes here. Baxter is a service desk analyst who takes calls from customers and seeks to resolve their issues. To do this, Baxter needs access to the docker engine API in order to query the various objects that contribute to the services consumed by customers. However Baxter is not a technician and escalates issues to a relevant technical team. We don't want Baxter to have the ability to create, update or delete objects. And consequently, Baxter's access to the engine API needs to be limited to read-only access. Bolt is a network administrator and is responsible for defining the network capabilities used by the containerized application workloads. Bolt requires the means to create and configure docker network objects and to query their configuration but not to remove networks that are already in use. Bolt's access the engine API then needs to be limited to network objects and without the ability to delete those network objects. Last but not least, Rackham is a systems administrator and has an overarching administration role. As a consequence, Rackham requires full access to all of the objects present on the docker platform including the creation, inspection, deletion and where possible, the updating of any object. With these roles in mind, we need to find a mechanism for providing the appropriate level of access required by the different roles in order to perform their assigned duties. Now we know that Docker Community Edition doesn't provide this out of the box. And as role based access control is not an unreasonable expectation, how are we going to resolve this requirement? Docker doesn't and perhaps shouldn't provide authorization as a feature of the open source community edition. The project has chosen to leave this to third parties by providing the means to implement authorization using code developed elsewhere which makes use of a published plugin API. Let's have a look at this API.

The Docker Plugin API
Before we get up close to plugins that enable us to implement authorization, let's have a look at the basics of the plugin API which is RPC like in nature using JSON over HTTP. Docker's plugin API is a general-purpose mechanism that enables non docker code to be hooked into the docker engine at the point of need and by general-purpose, I don't mean that any arbitrary coded functions can be hooked in. There are a variety of plugin types. For example, for volumes, networks, logging, authorization and so on. They all however are registered with the docker engine in the same way but are required to implement different methods depending on the type of plugin. plugins can be installed and managed as docker objects using the docker client CLI or there can be manually deployed as legacy plugins. The former approach is the newer preferred means of installation but requires the plugin to be configured appropriately. Manage plugins run as docker containers whilst legacy plugins can be containerized but can also be implemented as system daemons. Anybody can write a docker plugin provided it's written to the constraints of the plugin API and that it implements the methods associated with the plugin type. Our interest is in authorization and so that we know how the authorization plugin mechanism works, let's review the steps involved in the workflow. First let's review what takes place during a successful authorization of a docker engine API call. A client perhaps a user who has issued a docker client CLI command invoked an engine API call which forms an HTTP request which is sent to the daemon. If the client and daemon are configured to communicate using neutral TLS, the daemon will authenticate the client. And assuming the client gets authenticated, the daemon sends an HTTP request to the authorization plugin. The request provides the necessary information required by the plugin's AuthZReq method and provides the plugin with some salient information which it will use to make the decision as to authorize the API call or not. It provides details of the engine API call itself, split into several different JSON key value pairs, the HTTP method, the path of the API endpoint and the request body and header. In addition to these details about the specific API call, the request also provides the authentication mechanism. for example TLS and the details of the user. So how exactly does the daemon know who the user is? Well the daemon extracts the users details from the subject name of the clients certificate. More explicitly, the user field is set to the certificates common name. If we provide each user with a client certificate that is personal to that user, then authorization for each engine API call will be evaluated against any policy defined for that specific user. Armed with the pressing details, the authorization plugin can then evaluate the request against the criteria it owns using its proprietary evaluation mechanism. This is the workflow for a successful authorization and so, the authorization plugin returns a response allowing the execution of the API call. The response body returned by the plugin will be the same whether the request is authorized or not. The message key then contains the message that is returned to the client in the event of a non-successful authorization whilst the Err key contains a string which is returned to the client in the event that the plugin encounters a problem. The allow key of course indicates whether the request is allowed or denied. With the API request authorized, the daemon execute the relevant client request. And normally, without an authorization mechanism in place, the daemon simply returns a response to the clients after it has executed the API call. With authorization enabled however, the daemons response is also subject to authorization against the client user using the plugins AuthZRes method. Most often an authorization plugin will simply authorize the response but it's entirely possible to apply a further level of authorization based on the response generated by the daemon after it has executed the API call. Assuming the responses authorized, response allowed his return to the daemon which in turn provides the client with the results of the API call. So much for a successful authorization, what happens when authorization is unsuccessful? Well we have the same components in this workflow too which commences with the client sending an API request to the daemon. Authentication takes place just as before and an authorization request is sent to the authorization plugin. If once the plugin has evaluated the request against its policy, the request is unauthorized. A request denied response is returned to the daemon. In turn, the daemon returns a response to the client and if the API call was invoked using the docker CLI, the string contained within the message key of the plugins AuthZRes response is displayed in the client's terminal. Now we already know that we could write our own authorization plugin. Docker even provides some open-source Golang helper packages for the purpose. It may save you some considerable effort however if you make use of some work already conducted in this area. We're going to make use of an open-source docket authorization plugin developed by the open policy asian project. But first, let's find out how to get the docker daemon to use an authorization plugin. Assuming for a moment that we have installed an authorization plugin either as a legacy plugin or by using the docker plugin install command, what must we do to make the docker daemon aware of the plugin? It won't be a surprise to know that a daemon config option authorization-plugin needs to be set for the daemon to make use of a plugin. The argument will be the name of the UNIX domain socket that the plugin listens on for legacy plugins or the name provided as part of the plugins JSON configuration for manage plugins. As usual, this can be defined in numerous ways. In a systemd unit file or override, the daemon configuration file and so on. What may be of surprise however is that it's possible to apply more than one authorization plugin chained together in a defined order. Should you elect to use more than one plugin, then the authorization request will be passed to each plugin in turn and each plugin must authorize the request before allowing execution of the API call.

Introducing the Open Policy Agent Docker Authorization Plugin
We discussed how we could write her own docker authorization plugin but we can also make use of existing solutions where they fit our purpose. There are several authorization plugin implementations available but one of the more advanced offerings is a product of the Open Policy Agent project. The Open Policy Agent or OPA as it's colloquially called is an open source general-purpose policy engine that enables unified context-aware policy enforcement. Let's find out some more about OPA. Just before we learn how to apply authorization access control to the docker platform using OPA, we'll take a moment to get some background and answer the question, what is OPA? The first thing to say is that OPA became a sandbox project of the cloud native computing foundation or CNCF in March 2018. The CNCF is a vendor neutral home for many open source projects that are driving the adoption of application or instant microservices deployed as containers. The CNCF is home to projects such as Kubernetes and the sandbox designation indicates that OPA is an early stage inception project. OPA enables the decoupling of policy decisions and their enforcement. It means that application developers don't have to concern themselves with building policy definition and decision-making directly into the application code. It can be handed off to OPA instead which means that policy can be amended without directly affecting the application itself. OPA is truly general-purpose. It's not designed specifically for the docker authorization use case and can be applied to many different use cases. Integration is achieved with OPA restful APIs which used JSON over HTTP. Whilst OPA has a much wider application than serving as a policy engine for a docker authorization plugin, it is an ideal use case for OPA. It's no surprise then to learn that the project has developed such a plugin which is called opa-docker-authz. It's tempting to dive into OPA's internals to describe how it works but we need to stay focused on the docker authorization plugin. We do need a high-level view of its concepts however to inform our discussion concerning the plugin. An application service that needs to authorize a client request submits a query to OPA for evaluation against policy. In the case of the OPA docker authorization plugin, the query is effectively the content the docker daemon provides to the AuthzReq method. So it's the API call header, body, method and so on and most importantly, the user. The query is then evaluated against the policy which is comprised of rules defined using a purpose-built policy language called Rego. We'll take a look at Rego shortly when we define some policy rules for authorization access to our example docker platform. Whilst it's not mandatory, it's possible to supplement the data provided by the query by adding more contextual data which can be used in conjunction with the query data in order to evaluate policy. Once the query has been examined in relation to the policy, a decision which in the general case does not need to be Boolean in nature is returned to the service that issued the query. In fact, we've already seen that authorization plugins return the JSON objects containing three key value pairs including the allow key which holds the Boolean authorization decision. OPA is a very powerful policy engine with a general purpose utility which is well suited to the authorization problem. It's an ideal fit for our requirement to authorize user client API calls. With the high-level overview you now have of how OPA works and how it integrates with docker as a plugin, let's address the outstanding question. How do we define authorization policy and how can we represent the fictitious scenario we outlined earlier in this module?

Defining Authorization Policy with Rego
The language for defining OPA policy is declarative and comprehensive in nature. For example, it contains upwards of 50 built-in functions. We can only scratch the surface here but let's have a look at some of the syntax involved in defining policy rules. Policy is defined within modules and packages grouped the rules found within modules into a namespace. Here then we have a module which is named spaced to httpapi. authz. The rules inside this module and possibly other modules with a similar package definition belong to this particular namespace. Every module must have one and only one package definition. It's not a requirement but it's possible to import rules that are defined outside of the package. The reserved keyword input contains data provided by the query presented to OPA. In the case of the docker authorization plugin, this will be the JSON data sent to the plugin by the daemon. The input data is implicitly included in the module but in the example module shown here, we're assigning the input data to a variable called http_api which can subsequently be used in the body of the rules. Next we have the use of another keyword, default. This enables us to specify the value of a rule set when none of the rules with a similar variable name provide an outcome. We're defining the outcome to be false in such circumstances for the set of rules associated with the variable allow. Then we have another rule associated with a set called allow. We can read this as allow is true if the body contained within the braces evaluates to true also. Technically speaking, we don't need the equals true part in what's called the head of the rule as the true outcome is inferred. The body of the rule has three statements and they are evaluated as an AND operation. Think of the statements as assertions made which need to be evaluated against the query data. So in this example, the method associated with the query must be a GET. The API path must be finance/salary/username where username is a variable assign the value at that point in the path and username must equate to the user provided in the query. If each of these statements are true, then the rule also evaluates to true. In the case that this rule evaluates to true, it takes precedence over the default value of false. If however any of the statements in the body had evaluated to false, then the default outcome would have been applied instead. We don't have time to delve into Rego in more detail but now that we've got a perfunctory understanding of its syntax, let's go ahead and create some policy to represent our fictitious scenario. To create the authorization policy for our fictitious organization, I've created a module in a file called authz. rego. At the very top of the file, the namespace has been defined for our rules with a package keyword. It's called docker. authz. Remember that we're able to supply OPA with some additional contextual data to enrich the policy definition and evaluation? We've done that here by assigning our users and their roles to a variable called users. We will be able to reference this variable in our rules. Ordinarily this source of information will be supplied to OPA from another definitive source and imported into OPA using its API but for the sake of simplicity, we're defining the data explicitly within the policy module. Next we have a number of comments which describe the various rules we want to implement. These are placeholders for our Rego syntax. So let's get on with defining each of the rules. The first rule seeks to define the default outcome should each of our rules evaluates to undefined. That is one or more of the assertions fails. For our simple scenario, we'll use the variable name allow and as our goal is to limit access control, the default outcome is false. If a query evaluates to false, then OPA will return a false value for the Boolean variable allow in the response body. The next rule is for our full admin role. We simply want to evaluate the user associated with the query and if the users role is full admin, the assertion is true and the rule has the value true also. There are no other limitations involved so a user with the role full admin gets access to each and every engine API call. The first step is to assign the user provided in the query and accessed using the input keyword to a variable called user_id. We can then use this as an index to look up the role of the user in the user's variable and if the role matches the string full admin, the rule evaluates to true. The third rule is needed to limit users with the service desk role to read-only commands. We can repeat the check for the user's role specifying service desk this time but we must also add a statement that checks the requested method is a get and not anything else. This should ensure that users with the service desk role only gets a query docker objects. And the final rule that we need to implement will allow any users with a net admin role to perform any action on network objects but not delete them. The first two lines in the role should be self-explanatory by now but how do we limit operations to network objects alone. Well, the easiest way to achieve this is to make a check on the path of the query and to ensure that the path contains the string/networks which is part of the endpoint path for all operations on network objects provided by the engine API. To finish off, we need to restrict users with the net admin role from deleting network objects, so we must also test to make sure that the queries method is not delete. That's it. With the policy defined, we can move on to testing it out.

Implementing Fine-grained Access Control to the Docker Platform
During the course of this next demo, we're going to configure the docker daemon to make use of the OPA docker authorization plugin. And we'll apply the policy that we've defined for our fictitious scenario. All being well, we'll be able to demonstrate how the use of the plugin can restrict our users to those API calls required for their role. Some prep work first. When we were configuring the docker daemon to use TLS, we created a client certificate and key using a common certificate authority which could be shared by all clients attempting to access the daemon. To implement role based access control however, we have to implement an authorization plugin with the user being identified by the common name embodied in the clients certificate. A shared certificate is not going to work for us. We need a certificate for each user. Now to create the TLS artifacts by hand for each of our three users, Baxter, Bolt and Rackham will take some considerable time. So to save us some time, I have a simple convenient script that would do this on our behalf. For each user, the script makes a copy of the clients CSR attributes, changes the common name from clients to the name of the user, generates the CSR private key and certificate before copying the key and certificate to the users docker directory in their home directory. Let's kick this script off as it's going to take a few moments to complete, and we'll fast forward in time to the point where it's completed. With it complete, we can check the common name embodied in the certificate for the user Rackham and we can see that it now contains the username Rackham instead of client. We've made a change but not materially altered the behavior of the clients. Each file users can still invoke any engine API call. Rackham, our system administrator can as expected still create and run containers. And what about our service desk analyst, Baxter? Well Baxter who is supposed to have read-only access can not only remove running containers but can also create networks. And when we turn to Bolt, our network administrator, who we want to devolve network administration to say from removing any existing networks, well, Bolt can happily remove networks at will. To fix all of these issues with our users roles, we must deploy the OPA docker authorization plugin and configure the docker daemon for its use. Our first task then is to deploy the OPA-docker-authC plugin as a container. As we construct the command to create the container, let's talk through what's being configured for the container. The first part here is the normal command we use to create a container and we're running the plugin in the background using detached mode. We're configuring the plugin to always restart in the event that it fails or is stopped by some event such as a restart of the docker daemon itself. And for convenience, we're supplying a representative name for the container. Importantly, we're bypassing the configured use of user name spaces and we're reverting to the host default user name space. We have to do this because the plugin needs privileged file system access on the host as we'll see next. Here we go, remember that the daemon and the plugin communicate using a UNIX domain socket. Well the daemon expects the socket to be available in the directory/run/docker/plugins and as the plugin needs to read from and write to the socket, it needs to be mounted as a volume inside the container. The directory on the host is owned by the root user which means the plugin also needs to run as the unmapped root user. The authz. rego file where we defined our policy is located in the policies subdirectory in Rackham's home directory, so this needs to be mounted inside the container at /policies. That's all of the docker configuration options that we need for invoking the container. We just have to supply the image that we want to use for OPA which is located on the docker hub in the repository openpolicyagents/opa-docker-authz. It's image tag is 0. 2. 2. One final element to provide. The OPA plugin executable expects us to supply the location of the policy which we do with a -policy-file config option. There, all done. Let's start the container and check that it's running. There's just one final step before we can make use of the authorization plugin and that's to configure the daemon itself. We have to add the authorization-plugin key to the daemons config file. Because the config option can take multiple arguments, it has to be made a plural in the config file. The value of course is opa-docker-authz. With that change made, we can get the daemon to pick up the change by sending a hang-up signal to the daemon instead of having to restart it wholesale. A number of daemon config options can be applied in this way and the authorization-plugin option is one of them. Now let's test the user roles once again to see if there is any change in behavior. As the user Rackham, we can still start a container which is exactly what we want. So so far, so good. Last time around the user Baxter was able to delete the container which is definitely behavior we want to curtail. And thankfully OPA has applied our policy and rejected Baxter's attempt to remove the container. Baxter was also able to create a network last time around but not this time. Is there anything Baxter can do? Well yes, Baxter can perform the read-only task of listing the last created container just as we defined in our policy. For good measure, let's see what Bolt can do. Bolt is unable to delete the container either just as we'd expect from our policy definition. Bolt is able to create networks however. And what about removing them? Not this time. Our OPA authorization plugin is working perfectly, applying the policy that we defined in our Rego file to simulate the roles of each of our organizations users.

Module Summary
With the OPA authorization plugin now in place, vetting API requests received by the docker daemon, let's just make sure that the docker bench for security check for CLI docker C benchmark recommendation 2. 11 passes rather than fails. We've got a pass this time so it looks like our daemon conforms to the recommended benchmark configuration for the docker daemon. Before we move on to the next module, let's just briefly recap on the salient points covered during this module. Without the aid of an external helper application, the docker daemon provides all-or-nothing access to its API which may be suitable in some environments but can functionally be extremely restrictive in others. In situations where access to the daemon needs to be provided but with a limited set of API calls, then it would be necessary to deploy an authorization plugin and to configure the daemon for its use. And whilst we made use of the OPA authorization plugin, it's possible to use any plugin that's been authored for the purpose and even to write a custom plugin to suit your own specific needs. If you have the need to control access to your docker daemons, which ever route you choose in applying authorization, some serious thought needs to go into defining the various roles and their associated access. We've reached the end of this module and you should have a good understanding of how to apply fine-grained access control to the docker platform. Our next module explores how to deploy a secure self hosted docker registry. So let's move on.

Deploying a Secure Docker Registry
Module Outline
Hello again. I'm Nigel Brown, and welcome back to the next module in this course. Securing the Docker Platform. In this next module we're going to turn our attention to another of the Docker platform components. The Docker registry. The Docker registry is where the images that define the containerized application services we run using Docker, are stored. As they're the very basis of a container workload the integrity of Docker images is of paramount importance. And it serves us well to make sure that our Docker registries are protected adequately. Let's find out how to do this. Before we dive in, here's what we're going to cover during the course of the module. If you host your own Docker registry and you're confident that the network you use is secure, you may not want to apply any additional security controls. We'll cover the basic concepts associated with the Docker registry before we move on to see what we must do to interact with one when it's deemed insecure. Assuming, at the very least, that we want to secure the communication between Docker and a self-hosted registry, we'll see how to configure the registry so that it securely serves the content that it hosts. Controlling who can access a Docker registry and, in fact, who can access specific resources hosted by a registry will be a key consideration when hosting a registry for most people and organizations. We'll be exploring the different methods for controlling access to self-hosted Docker registries. And finally, we'll be looking in more detail at applying access control to a Docker registry using the different authentication mechanisms. When we're done with the module you'll have a good understanding of the security measures available for Docker registries and the means for creating your own secured, self-hosted registry for storing Docker images.

Enabling the Use of Insecure Registries
Before we get stuck into learning how to secure a Docker registry we better take a few moments to make sure we understand what a registry is. A basic description of a Docker registry might define it as a mechanism or system for storing and delivering Docker images. Docker images, which are the templates for containers, consist of blobs of fast system content and metadata. They need a home from which they can be shared and that's a registry. To allow clients to interact with the stored images a Docker registry serves an API called the Docker registry HTTP API. The API allows the clients to perform a number of operations on the images, such as pushing or pulling images. Like the majority of components in the portfolio of Docker tools, the Docker registry is an open-source project which is called Distribution. And whilst the API has become a de facto standard over time, the formalization of that standard only commenced in April 2018 when the independent open container initiative launched a project for the purpose. There are numerous commercial implementations of the registry API including the Docker Hub, Red Hat Key, the Docker trusted registry, which is part of the Docker enterprise addition, and solutions from the major public cloud providers. If you have a need to host images of your application services a commercial solution might be a safe bet. But what if you prefer to self-host a Docker registry, either in the cloud or on premises. If the distribution project is open source then self-hosting shouldn't be such a problem. However, protecting a self-hosted registry from abuse is very important and requires some consideration so let's see what we can do to protect it and the sensitive corporate content it hosts. Exploring how to set up a self-hosted Docker registry is a whole course by itself. For our purposes, we'll assume that someone has taken care of this already. To enable us to make use of the images stored on a self-hosted registry, we must be able to address it correctly. Once we know how to do this we can then proceed to ensuring that the interactions are as secure as possible. When a user wants to pull an image from a Docker registry using the Docker CLI, the daemon that receives the request via an API call will assume the image resides on the public Docker Hub registry. That is, unless the name of the image contains the address of a valid or alternative registry. The implication is then that when using a self-hosted Docker registry, that registry needs to be configured to listen on a TCP socket and that the name of the addressed image contains the registry location. If we deployed a self-hosted registry listening on a TCP socket and attempted to communicate with it, we'd quickly run into a problem. We'd get an error message from the daemon looking something like this. Stating that, as an HTTPS client, it received an HTTP response from the registry server. This is because the daemon expects to communicate with the registry using TLS but the registry is not configured for TLS and responds to API calls using HTTP instead. This is another example of Docker's application of out of the box security. But what happens if you're confident about your network security configuration and you're comfortable with using unsecured HTTP instead of HTTPS. Whilst the Docker daemon expects communication with a registry to be encrypted and authenticated, there may be situations where it's desirable to remove this restriction. It involves setting the daemon config flag insecure-registry, with the address of the registry as an argument. If you choose to set this option in the daemon's config file then the JSON key to use is insecure-registries, plural, as more than one insecure registry can be configured. To get the daemon to pick up the config change when it's defined in the config file, you can just send a hang-up signal to the daemon. One other thing to note. If you expose your registry on the loop-back network it will be deemed an insecure registry by the daemon, available for use without any additional daemon configuration. It goes without saying that the use of insecure registries comes at a risk. You may be operating in an air gap environment in which case you may be content with the open nature of the configuration. In the majority of circumstances, however, the open configuration will not suffice. In fact the CIS Docker C benchmark recommends against the use of insecure registries use and the Docker bench for security checks for configured insecure registries. Let's see how we can take steps to tighten things up from a security perspective.

Securing Communication with a Self-hosted Docker Registry
We've previously learned how to encrypt and authenticate communication between a Docker client and a daemon using Transport Layer Security or TLS. It will come as no surprise, then, to learn that securing the communication between a Docker daemon and a Docker registry involves much the same process. What's different this time around is that the registry is the server and the daemon is the client and whilst the process is largely the same there are some differences in the configuration. The configuration can be limited to the daemon client, authenticating the registry server, or we can establish full mutual TLS between the two entities. A super-cool feature that the registry used to have was the ability to be configured to use the Let's Encrypt open-certificate authority, to automate the generation of the certificate and private key and its subsequent application for configuring TLS on the registry. Sadly, at the time of authoring this course, due to the withdrawal of one of the challenge mechanisms by Let's Encrypt, a newly created Docker registry is unable to use this capability. If it's of interest to you, however, do watch out for updates on the resolution of this issue. Now in a moment we'll get into the detail of configuring mutual TLS for the daemon and registry but first a word concerning how the registry is configured. A Docker registry is configured using a YAML-based configuration file. Here's a pared back example, with the majority of content snipped out. It has a large number of available primary keys but the one we're interested in, for configuring TLS, is the HTTP key. In fact TLS configuration is defined under a sub-key of the HTTP key called TLS. There are just four available keys that are used for TLS configuration and we'll cover all but the Let's Encrypt key in detail in just a moment. As TLS configuration is entirely optional then the whole TLS structure in the config file is also optional. But if it is specified then various sub-keys are required and again we'll see which ones in a moment. A final point to remember for registry configuration in general. Any configuration defined in the YAML file with a key and value, can be overridden with environment variables which take the form of the indented path. For example, the YAML key called certificate can be defined or overridden with the variable REGISTRY_HTTP_TLS_CERTIFICATE. Got that? Good. Let's see how to configure mutual TLS. Let's start with what's required to configure TLS on the registry itself. As we would expect, the registry needs an X. 509 certificate and an associated private key. This can be self-signed by a certificate of authority initiated for the purpose or they can be obtained from an external trusted certificate authority. These artifacts need to be hosted somewhere in the file system on the platform hosting the registry server and the registry's configuration file must specify their location as an absolute path. The certificate and key YAML keys are used for this purpose. The common name in the certificate must contain the registry's domain name and if your certificate was signed by an intermediate certificate then the issued certificate must be combined with this intermediate certificate. The combined certificates reside in the file referenced by the certificate YAML key. That's it, that's all there is to it. With the configuration in place the registry server can be started or restarted for the changes to take effect. The only other thing that might concern us is whether the daemon client has access to the CA certificate for authenticating the registry. If it's a self-signed certificate we need to make this available to the daemon client. To do this, all we do is place the CA certificate in a location where the daemon can find it. A sub-directory needs to be created, named after the registry server in /etc/docker/certs. d. The file must also be called ca. crt. Otherwise the daemon won't recognize the file for what it is. That's the registry side taken care of so now let's look at how we enhance the configuration to enable the registry to authenticate the daemon in turn. Again, the daemon must also have a certificate and private key. Perhaps the artifacts that have already been used to establish TLS communication with Docker clients. Bear in mind, however, that the certificate needs to have been created with client authentication in its extended key usage in order for the public key to be used for authentication. These files are also placed in the sub-directory named after the registry server. They should be given a meaningful name and must end in. cert and. key respectively. When these files are in situ the registry is able to establish the identity of the daemon client, sending requests to its API. Once again, if the daemon certificate has been signed by a CA whose own certificate is not readily available to the registry server, then it needs to be made available. The file can be placed anywhere on the registry host's file system and the client CA's YAML key in the registry config file needs to specify the path. Clearly, there may be multiple daemon clients accessing the registry and therefore the client CA's key value is a list. That's all the configuration needed to establish mutual TLS between the daemon client and the registry. So let's see this in practice using a registry and our example Docker daemon platform we've been securing along the way.

Configuring TLS for the Docker Daemon and Registry
In this next demo we're going to implement a containerized Docker registry hosted on a Docker host platform called Calculus. Through stages we'll secure this registry by implementing TLS between it and the Docker host we've been working with throughout this course which runs on the host Wolff. Let's get started and first of all let's run a minimally configured registry container with its service exposed on port 5000 of our host Calculus. If you're wondering where the registry's config file is, well a basic configuration is provided in the image which is the official Docker image for the registry. Okay, that's the container started successfully in detached mode. Now a simple way of checking that we can communicate with the registry is to use the curl utility to query one of its endpoints. The catalog endpoint returns a list of the available repositories in the registry. We've only just created our registry so, of course, there are none. There may be no images available but we have ascertained that the registry service is running and accepting API requests. Let's pull a popular image from the Docker Hub Registry which we'll use as an example image to push to our self-hosted registry running in its container. Once the image has been successfully pulled it resides in the local cache of images and is available for running containers, for use as a base image for images that you're authoring or for tagging with a new name amongst other things. In a moment we're going to push this image to our newly created Docker registry. But first, remember that we need to tag the image with a name that includes the name of the registry in order that the daemon knows where to locate the target registry. So in our situation here the image name will be calculus. lan:5000/redis:4. Now that it has the correct name let's try and push it to our registry using the Docker image push command. Unfortunately we get a server gave HTTP response to HTTPS client message which shouldn't be a surprise to us. We've created a Docker registry, accessible using plain HTTP, and as we've already learned the Docker daemon will return such an error unless it's been configured with the insecure-registry flag. As it's not going to be of much value to us let's remove the container before we attempt to provide a more palatable configuration for the daemon client. To save some time in the demo, the registry certificate and private key have already been created and been signed with a new certificate authority. The files are called registry. pem and registry-key. pem. Now let's take a look at the registry's config file and add the necessary additions to configure it to use TLS. Being very careful about the indentation used, we can add a new TLS key under the existing HTTP key. We're also adding two new keys called certificate and key, the names providing a clue about their relevance. Each with the full path name of the associated artifact. These files will be bind mounted into the container from the host's file system so the path that we provide will be with reference to the mount point that we'll supply when we start the container, which is /certs. For the moment that's all the configuration we need so let's go ahead and start our registry service container. Let's break the command up across a few lines and explain what's going on. The first line should be quite familiar to you already. We're running a new container in detach mode which we're giving the name registry, for convenience when it comes to stopping, starting or removing it. The next line defines a bind mount, the directory that contains the registry certificate and key which is mounted at /certs in the container. This is the path we specified in the registry's YAML config file. And, talking of the config file, we're also bind mounting this into the container at a location where the registry application expects to find it. Which is /etc/docker/registry/config. yaml. We don't want either of these files being written to from within the container, so they've been mounted read-only as a precaution. Next, we're mapping the container's port 5000 which is where the container is serving the registry, to port 443 on the host. Any clients can then access the registry by sending requests to the host Calculus on port 443. The final line is already familiar to us so with all the necessary config options supplied let's start the container. Let's just make sure that it started okay by listing the last created container. And everything looks good. Hopefully we should have better luck pushing the image this time. A few small things we need to do first though. As we've signed the registry certificate which the CA is unaware of, the daemon client needs sight of the CA certificate in order to authenticate the registry. So, first off, we must create a directory named after the registry which, in our case, is called calculus. lan:443. In a sub-directory called certs. d in /etc/docker. With the directory created we can then copy the CA certificate over to the directory but we must rename the file from ca. pem to ca. crt. Remember, the daemon is looking for CA certificates in files that end with the suffix. crt. Because we're now using TLS we've changed the port number the registry service is exposed on on the host so once again we must re-tag the image appropriately. There we are. The daemon should now be in a position to authenticate the registry so we can have another go at pushing the redis image to the registry service. And this time we're successful. The daemon is no longer complaining and is happily communicating with the registry using TLS. So that's the daemon client authenticating the registry but what about the other way round. Well in a moment we're going to hop over to the host Wolff which we previously configured as a Docker host platform to configure it to be authenticated by our containerized registry running on Calculus. Before we leave Calculus, where our registry is running, we have to give the registry access to the certificate of the CA that signed the daemon certificate which runs on Wolff. I've already established some trust between the user Rackham on Wolff and the local user on Calculus so we can simply copy the certificate from Wolff to Calculus, using the secure copy utility. Additionally, we must also add a new YAML key to the registry's configuration, client CA's, defining the path to the CA certificate we just copied over from Wolff. Unlike the other two keys used in the TLS configuration, the values associated with the client CA's key must be entered as a list. And because we've made a configuration change and the service is already running we need to restart the container in order that the changes get applied. With that done, let's head over to the host Wolff to configure it to be authenticated by the registry running on Calculus. When the daemon, running on Wolff, was originally configured its certificate was created with a profile called daemon which included the server authentication extended key usage. It's now been reconfigured with a profile called client-server, which contains both. The server authentication and client authentication extended key usage. Its key requires both capabilities so it can be used for authentication as a server to Docker clients and as a client of the Docker registry. If you're following along recreating this demo in your own environment, you'll need to perform this reconfiguration too. We can inspect the daemon certificate and if we scroll to the X. 509 v3 extended key usage component there are the two key usage extensions defined for use by the certificate. Time to do another secure copy. Just as the registry on Calculus needed sight of the CA certificate which signed the daemon certificate on Wolff, so must the daemon have sight of the CA certificate that signed the registry certificate. So, again, we can copy this but this time from Calculus to Wolff. We use the daemon running on Calculus as a client of the containerized registry and just as we needed to create a directory named after the registry's name on Calculus, so we need to do the same on Wolff. The directory is where the daemon will look for any CA certificates it needs for authenticating, so let's also copy the CA certificate to the directory we've just created, remembering to give the file a. crt file extension. Remember the redis image that we pushed to the containerized registry when we were interacting with the Docker daemon and hosted on Calculus? Well, now that the daemon on Wolff is configured to authenticate the registry, let's try and pull that image to the local cache on Wolff. Hmm. We get an error of response provided by the daemon which has been passed on from the registry. It's complaining about a bad certificate. In fact, we configured the registry to expect to authenticate clients and the daemon on Wolff is not configured to be authenticated by the registry. To configure the daemon on Wolff, to be authenticated by the remote registry, we must put the daemon certificate and key in the same directory alongside the CA certificate we just copied. We have to take some care here though. We can't leave the file name extensions as they are. The certificate file name must end with the. cert extension and the key file name must end with the. key extension. We're done. Hopefully that's the configuration complete. Let's see if we can pull the redis image that we earlier pushed to the containerized registry when we were interacting with the daemon on the host Calculus. This time, with mutual TLS properly configured, we can successfully pull our image.

Controlling Access with Basic Authentication
We've covered quite a lot about authentication and authorization so let's just stop for a moment and see where we've got to. We've discovered how to secure the communication between a Docker client and the Docker daemon and then between a Docker daemon and a Docker registry. All of this has been achieved using mutual TLS which provides us with authentication and encryption. And whilst we've discussed how to control access to the Docker daemon, what we haven't discussed yet is how we can control access to the Docker registry. Of course, we could try and control this using an authorization plug-in, configured on the Docker daemon. This is an entirely reasonable method of access control. However, if you're responsible for controlling access to a Docker registry, it doesn't necessarily follow that you also have control over access to every Docker daemon that intends to interact with your Docker registry. You may be running a public registry or it may simply be that the administration of remote Docker daemons falls to another team or entity. What can we do to implement some access control in such circumstances? We could move or augment authorization with a platform that sits in front of the registry. A reverse proxy. It could be configured with a straightforward access control mechanism such as basic access authentication or something more sophisticated involving an authorization server. This approach may be the best option, especially if you already have such a configuration controlling access to other resources in your enterprise. But, the Docker registry also has some means for authenticating users and authorizing their access to the registry's resources. We're going to take a look at how these can be implemented by configuring the registry itself. The first of these mechanisms is basic access authentication or basic auth as it's more commonly known. Basic auth is a challenge response system which uses HTTP headers in the dialog between client and server. The process commences with a client HTTP request for access to a resource. In our situation the client is the Docker daemon and the request will be a registry API call to either push or pull an image. The server, the registry, responds with a 401 unauthorized status code, and provides the client with information on how to authenticate a challenge using the www-Authenticate Response header. The header will contain the authentication scheme which will be basic and the realm which is the scope of the resources being protected. If the user has logged in to the registry using the Docker login command, the user's credentials are returned by the daemon in an authorization header, as part of a repeated HTTP request. The registry doesn't hold any states with regard to user access so the credentials are checked against its known set of credentials for approved access. These credentials reside and are maintained on the registry itself. If there is a credentials match the registry provides access to the requested resource and returns the outcome in the response body. There are a few important things to consider before you decide that basic auth is the right access control for your Docker registry. Credentials used for basic auth comprise of a user name and password encoded using Base64, which is trivial to decode. For that reason, the daemon and registry must be configured to use TLS when using basic auth. Whilst the registry API exposes several different endpoints, the only functions that are available via the Docker Engine API are those to push images to and pull images from a self-hosted registry. The access that is provided using basic auth is all or nothing. That is, if a user is authorized to access the registry then the user can push or pull from any repository that resides in the registry. So we now know about the process of basic authentication and some of the characteristics when applied to a Docker registry. But how do we go about configuring the registry for basic auth? It'll be of little surprise to you that, just like all the other configuration options for the registry, user authentication is defined under a primary key in its config file. The key is auth. There is a limited choice of meaningful user authentication mechanisms, just two in fact, and the relevant sub-key for basic auth is htpasswd. And there are just two further sub-keys that define its configuration. The first sub-key is realm which, as we mentioned earlier, describes the scope of the resources being protected. You can set this to be something meaningful to you such as the address of the registry. The second sub-key is a path to a file which contains the usernames and their hashed passwords which the registry uses to check credentials against those presented by the daemon client. Registry configuration for basic auth is pretty straightforward so now we have to address how the Docker daemon knows which user has issued any given Engine API call so that it can respond to the registry's basic auth challenge. The Docker daemon serves an Engine API endpoint which allows a Docker client user to log in to a specified directory. The client CLI command that invokes the API call is Docker login, which requires the input of a username and password. If the host name or URL, for the registry server, is omitted, the daemon defaults to addressing the Docker Hub Registry. So it's important to make sure that a server name or URL is specified when you're hosting your own registry. A quick word of caution related to Docker registry credentials. By default, when logged in, these are stored in a file called config. json, which resides in the. docker sub-directory of the user in question. The credentials are Base64 encoded, which is trivial to decode. So be sure to make use of a credential helper for your platform which will provide safer storage. Basic auth is just one of two access control mechanisms so let's move on and take a look at the other.

Controlling Access with Token-based Authentication
We've learned that basic auth, on its own, provides an all or nothing access control mechanism for a self-hosted Docker registry. The chances are that you may want to implement a more flexible access control mechanism. Well the Docker Distribution Project provides the means to do just that. Alongside the basic auth mechanism, the registry also supports a more sophisticated token-based authentication mechanism which allows for protecting the resources managed by the registry using access control lists. But whilst the Docker Distribution Project provides a specification of how its token-based authentication access control works, and how it should be implemented, it doesn't provide an implementation. It's left to the wider community to implement solutions that conform to the spec. Thankfully for us, some open-source implementations of the spec exist, which we can readily make use of. One such solution is provided as part of the Keycloak identity and access management solution and another is Cesanta's Docker Auth server. So how does the token-based authentication work? Well, it starts with the usual client request to push or pull an image, using the Engine API. The user needs to have provided the daemon with some credentials for the registry which we found earlier it stores locally. The daemon will issue a registry API call for access to the resource and just as it does, when configured with basic auth, the registry returns a challenge in the form of a 401 unauthorized code. This time, however, the header contains slightly different information. The authentication scheme type is barer instead of basic, indicating that the registry uses barer tokens to authorize requests to its resources. It also provides the realm which contains the URL of the authentication server and the name of the service which hosts the requested resource. The Docker daemon will then send a request to the auth server for a token. Depending on how the auth server is implemented, the daemon's request will contain the client user's basic auth credentials, or the request may use the OAuth 2. 0 protocol if supported. The request will also contain the scope which will define the resource the client requires access to. The auth server will refer to its access control list to determine what access rights the user has to the requested resources. The auth server then generates its signed token based on the allowed access the user has to the requested resources, which it returns to the daemon. The daemon then retries the registry API call, this time providing the token in an authorization header. And based on the content of the token, the registry either authorizes or denies the request. Either way, the result of the request is passed back to the client user. Configuring the registry to make use of token-based authentication is somewhat similar to that we've already explored with basic auth. We use the auth primary key but this time we use the token sub-key to define the configuration of token-based authentication. Three further sub-keys define the realm, server and the token issuer. The realm provides the location of the auth server which generates the tokens. And finally, the rootcertbundle key provides the path to a root certificate bundle which contains the public key, component of the key pair used by the auth server to sign tokens. We've mentioned a couple of implementations of the Docker registry's token-based authentication spec and in a moment we'll use the Docker auth implementation to secure access to a registry as part of a demonstration. But first let's see what it provides. Firstly, it provides the means to authenticate individual users as part of its purpose to serve tokens on behalf of the registry. We can provide user credentials as part of a static list in a file, sign in with a Google or GitHub account, bind to an LDAP server for authentication or make use of a MongoDB User Collection. Secondly, Docker auth provides the means of authorizing authenticated users. Authorization is carried out by comparing the characteristics of the request with a pre-defined access control list. For example, the request is likely to require access to a specific repository. The access control list can be held in a static file in a MongoDB database, or can be provided by an external program. We've got a lot of moving parts for token-based authentication, so to help cement the configuration and implementation details of token-based authentication, let's move on to a demo showing how to protect a Docker registry.

Implementing Authentication for a Self-hosted Registry
It's time for another demo and in this demo we're going to build on protecting a self-hosted Docker registry by implementing access control, first using basic auth and then using token-based authentication. Before we do anything at all, let's have a look at the configuration file that we want to use for our self-hosted registry. In the last demo we configured our registry for secure communication with TLS. So now we need to turn our attention to access control. More specifically, we want to configure basic access authentication for the registry. The first addition to the configuration is to supply the auth primary key, under which we'll be specifying the authentication details. For basic auth, we then need to provide the htpasswd sub-key which is used to differentiate between the different authentication methods. Then under this key we need to declare two further keys. The first is the realm key and the value we provide here should be something meaningful like the address of the registry. The second is the absolute path to the file holding the credentials of the users to be authenticated. And if we've specified a file for the credentials to be used by the registry then we'd better create one. We'll use a container to do this, using the registry image which has the htpasswd utility embedded within the image. We have to override the image's entry point to get the container to execute the htpasswd utility instead of the registry application. And we're asking the utility to encrypt the password and to return the result to the standard outstream which we're redirecting to a file called htpasswd in the auth directory. We're creating a single user, Rackham, whose password is, ssh, don't tell everybody, cheese. We have everything we need so it's time to start our registry and we can use exactly the same Docker container run command that we used when we implemented mutual TLS but with one important addition. We want the htpasswd credentials file to be available to the registry running inside the container so we must bind mount the directory on the host where the file resides, into the container using the /auth mount point. That's the registry up and running so how about we test where the basic auth access control is working or not. Let's hop onto the host Wolff and log in as the user Rackham. To test access to the registry, Rackham will be our client user and the Docker daemon running on Wolff will be the registry client. And from the previous demo, when we were securing registry communication with TLS, we have our redis image available ready to push to the registry that we've just started on Calculus. So what do we think is going to happen when we attempt to push the redis image to the registry. As the registry is now configured to use basic access authentication, the registry denies the request and the daemon passes the error back to the client. We need to fix this so what must we do. Well, you remember the Docker login command? We just need to supply our credentials to the daemon using this CLI command, specifying which registry we are logging into. The -u option specifies our user is Rackham and when prompted for the password we just provide that which we set when we created the htpasswd file. Whilst the login succeeded take note of the warning about the storage of the unencrypted password. Now if we repeat the push operation we successfully push the image to our registry. The registry authenticates the user Rackham by referencing the supplied credentials with those it has stored in its htpasswd file and serves the user's request. Okay. We're done with basic access authentication so let's return to Calculus and remove our registry container before we have a look at token-based authentication. Once again, let's return to the registry's configuration where we've replace the basic auth configuration with a token-based authentication configuration. The htpasswd key has been replaced with the token key. The realm key specifies the URL of the auth server which will be a deployment of the Docker auth server which will run on a third Docker host called auth. We're also supplying a service key which we've set to a name which describes our registry, an issuer key which is set to a name that describes the auth server and a rootcertbundle key which describes the path to the certificate bundle which is used to sign the tokens. The auth server signing certificate, auth. pem, has been prepared in advance, an activity we should be familiar with by now. The certificate has already been copied from the auth server to the TLS directory on Calculus, so that it can be available to the registry container at the path that we've specified. There, that's all we have to do to configure the registry for token-based authentication. So let's start the registry, which we can do with exactly the same command as last time around. Now let's turn our attention to the auth server. First, let's log on to the auth server which has a Docker daemon running on it, which we will make use of to run the Docker auth server as a containerized workload. The Docker auth server requires a config file so let's take a look at its content. There's a lot that can be configured. We don't have time to go into the Docker auth server in detail so our configuration is relatively basic. We'll need to configure the server to use TLS for communication and the public key contained within the certificate that gets created will also serve to sign the generated token. These have been prepared in advance using the same techniques that we've used throughout this module. We then provide some parameters associated with the token generation, the issuer, which must match that specified in the registry's configuration, and the exploration time for each created token. Next to last, we provide basic auth credentials for any users that need authenticating. We just have our user Rackham. And finally we're specifying a static access control list. This section has a list of rules and each rule contains the items that are matched against the resource request, the actions that are allowed on a correct match, and an optional comment. Rules are evaluated top down and once a rule matches, further rule evaluation ceases. Access to resources are denied by default. To make things simple, we have a single rule which states that if the user account requesting access has the ID Rackham and the name of the repository is redis, then any requested action is allowed. This effectively means that the user Rackham is limited to pushing and pulling redis images only. Time to check whether the rule that we've specified will do the trick, and impose the access restrictions we desire. Let's spin up a container with the Docker auth server running inside. A lot of the configuration for the container is similar to that we've previously discussed when starting a registry container, but with some obvious differences. The port mapping is different. We've configured the Docker auth server to listen on port 5001. And of course the image is different and the container's entry point requires us to specify the location of the config file as an argument. We're set to go. With the Docker auth server running let's exit from the auth server and head back to Wolff to see if we can push the redis image to the registry. Let me just provide you with a valuable piece of information which might save you some head scratching if you're recreating this demo in your own lab environment. The Docker auth server is configured for TLS so the Docker daemon client on Wolff needs sight of the CA certificate that sign the server certificate. So this has been copied to /etc/docker/certs. d/calculus. lan:443 with a. crt extension, where the daemon will find it. This will enable communication using TLS. Okay. We're ready to push the redis image to the registry running on Calculus and just as we would expect the push operation works seamlessly. Behind the scenes, the registry request was bounced back to the daemon which then requested a token from the Docker auth server using the credentials associated with the user Rackham. And having consulted its access control list the Docker auth server returned an appropriate token for the resource. The daemon then produced the token as part of a repeated request and the registry honored the request. Now I have another image in the local cache of images, an nginx image. What's going to happen if we attempt to push this image? Let's not wonder, let's see. It fails with a request to the resource is denied error message. This is exactly what we should expect, given the rule that we defined in the Docker auth server's access control list. We haven't really scratched the surface of what access control rules we can apply for our registry use in Docker auth but our simple example serves to illustrate how it can be used to control access to a Docker registry's resources. If you have the need to implement access control to your own Docker registry I'd encourage you to study the example configurations in the Docker auth GitHub repo. Alternatively, you can investigate another solution such as the Keycloak Docker registry authentication server.

Module Summary
Well that brings us to the end of this module and our look at deploying a secure Docker registry. The Docker images that organizations develop for their containerized services have the potential or likelihood to be commercially sensitive, and it's for that reason that private self-hosted registries are so attractive. Let's see what we've covered in our quest to protect those resources. We've learned that Docker daemon clients expect to communicate securely with Docker registries using Transport Layer Security. But if you've mitigated the risk it's possible to configure a daemon to communicate using plain HTTP. We've also seen how to configure a self-hosted Docker registry for secure, encrypted TLS based communication and that it's possible to lock things down further by requiring the registry to authenticate any Docker daemon clients in return. And finally, we've discussed the importance of controlling access to the precious resources hosted by a Docker registry and how we can implement simple, basic authentication to control access to those resources. And, if we're particular keen to apply finer grained access control, we've got a token-based authentication mechanism to help us achieve that goal. You've now got a good understanding of the security mechanisms available for a self-hosted Docker registry, knowledge of the configuration required and an appreciation of some third-party tools for aiding in the implementation of security controls. In our next module we'll be looking at the security aspects of another Docker platform entity, the swarm cluster.

Managing Security in a Docker Swarm Cluster
Module Outline
Hi, and good to see you back for this next module in this course entitled, securing the docker platform. My name is Nigel Brown and this module is called, Managing Security in a Docker Swarm Cluster. We've covered quite a bit about securing the heart of the docker platform which is of course, the Docker daemon. But in the cloud native era, it's not always enough to just deploy our containerized applications to a single docker host. When we have the need to deploy small, ephemeral, loosely connected, containerized services which require dynamic scaling and some protection from failure, then we might elect to make use of an orchestration platform such as docker swarm. Now, we've been referring to the CIS Docker CE benchmark on and off, throughout this course, and been using the docker bench for security to measure a docker platform against the recommendations in the benchmark. One set of recommendations in the benchmark relate to docker swarm configuration. And the associated bench for security tests all seem to pass. But that's because the docker host on which they run doesn't have swarm configured. So, in this module, we're going to cover various aspects of security related to a swarm cluster, before we move on, to demonstrate the application of those security techniques to a swarm cluster that we'll build. Let's see what we're going to cover in the module. A swarm consists of multiple docker host nodes tightly coupled together to form a cluster making sure that the host nodes communicate securely is really important, and we'll be exploring how docker helps to achieve this. Containerized application services often need access to data items that are confidential. In swarm, these items of data are called secrets and we're going to learn how to use and manage secrets in a swarm cluster. We'll also be looking at how we can protect the encryption key that the cluster uses to encrypt the data store it maintains for holding cluster state and other sensitive artifacts, like secrets. And lastly, we'll see what we can do to maintain the availability of a swarm cluster and how we can recover a cluster in the event of node failures. This module is going to give you a good understanding of what's required to securely operate a docker swarm cluster. A knowledge, that you'll then be able to apply to secure your own swarm cluster.

Securing Communication Between Cluster Nodes
Aspects of security for swarm clusters start as soon as a cluster is initiated, and a cluster is initiated using a docker CLI command against an existing docker host, docker swarm init. A number of things happen behind the scenes when we issue this command, which we'll look at in just a moment. But effectively, a cluster is created with just a single manager node. Swarm clusters are meant to comprise of multiple nodes cooperating together as a tightly coupled unit, so it's easy to add additional nodes to the cluster. In fact, the docker swarm init command writes a message to the standard out stream, detailing the command required to add worker nodes to the cluster. The command that needs to be executed on another docker host platform to enable it to join the cluster is, docker swarm join. It requires specifying a secret called a token, and the socket on which the manager is listening for API requests. The token is an important artifact that needs careful management, but we'll come back to that in a moment. First, let's have a look at what happens when the cluster is initiated. In order that all of the nodes that participate in the cluster can communicate securely with one another, there needs to be a commonly observed source of trust. For that reason, there must be a root certificate authority responsible for signing each node certificate for communication using mutual TLS. The default behavior is for the manager to create a certificate authority, self-signing the certificate, with the public key being used to sign the certificates of other nodes in the cluster. The CA certificate is located in the daemon's data root and the file is called swarm-root-ca. crt. The manager also creates its own key pair for TLS communication with the public key contained within the certificate. These artifacts are also located in the daemon's data root. The private key file is called swarm-node. key and the certificate, swarm-node. crt. As well as containing the node's public key which is used for TLS authentication, the node certificate has some important information which governs its membership of the cluster. This information is contained within the subject distinguished name components of the certificate. The organization attribute contains a randomly generated ID, which is an ID that was generated at the point of cluster creation. This uniquely identifies the swarm cluster. This ensures that only nodes from the same cluster are able to communicate using the swarm API. The organizational unit attribute defines the role that the node performs in the cluster. If it's defined as a swarm manager, it takes part in the management of the cluster but is also a worker capable of having tasks scheduled as a workload. If it's a swarm worker, then the node's sole function is to host scheduled container workloads. The final piece of information is held in the common name attribute. It's another randomly generated ID unique to the node, which serves to uniquely identify the node in the cluster. This node ID is used by swarm as the means for identifying the individual nodes that participate in the cluster. Let's return to the join token we discussed earlier to see what it contains and how it helps with securing the swarm cluster. In fact, when the cluster is created, the manager creates two tokens. One for the managers who want to join the cluster and one for workers. The token consists of four components separated by dashes. Let's see what these four components contain. The first component is a known prefix which simply serves to identify what the string is to make it easily identifiable. The next component is the token version number, that is, the version of the token format as opposed to how many token rotations the cluster has performed. Whilst version two is the most current version, it's reserved for use in federal government scenarios, so you should ordinarily see version one only. The third component is a hash of the root CA certificate generated by the manager. The hash is padded to a 50-character string which has been truncated on this slide. As the token is used by would-be nodes joining the cluster, the digest enables the new node to verify the authenticity of the manager node it's addressing when it requests to join the cluster. The final component is a 25-character, randomly generated secret, again, truncated here held in a distributed cluster data store shared by manager nodes which corresponds to either a worker or manager context. This is why there are two tokens. The secret contained in the manager token is different to that contained in the worker token. The token determines what role a joining node is allocated. Let's see how these mechanisms work together to enable a new node to join an existing cluster. When a node wants to join a cluster that already exists, we've already seen that it needs to communicate with a manager node in the cluster by specifying the socket that the manager is listening on, as part of the docker swarm join command. The node, say a worker node, initiates one-way TLS communication with the manager using the hash of the root CA certificate to authenticate the manager. It sends the token so that the manager knows the node has been approved for joining the cluster with the role that's appropriate, from the secret contained within the token. The joining node also sends a TLS certificate signing request. The manager then creates and signs the certificate using the root CA certificate and sends it to the joining node. Henceforth, the two nodes are able to communicate with mutually authenticated TLS. The joining node is then registered as a participant in the cluster with its unique node ID, and then becomes eligible as a recipient of scheduled tasks. The joining process then, enables each node participating in the process to verify the status and authenticity of the other before the joining node becomes a bonafide member of the cluster. Now, if the certificate authority is the root of trust and a manager node is compromised in some way, then there is no longer any trust between the nodes in the cluster, and the root CA certificate should be regenerated or rotated. We've got three options available to achieve this. If we're relying on the inbuilt, self signed certificate authority, then the docker swarm CA command with a --rotate config option achieves what we want. But if we want to use a CA key and certificate generated outside the cluster, then we can append two more config options to the command. - -ca-cert and --ca-key. Both require the path name to a file containing the relevant artifact. The final option is to use a remote certificate signing endpoint using the online certificate status protocol reachable via a URL. It's still necessary to provide the path for the location of the CA certificate, and the --external-CA config option, defines the signing endpoint. The swarm API only supports, CloudFlare CFSSL solution at present. Whichever means you employ to rotate the CA certificate, swarm seamlessly handles the transfer of trust from the old CA to the new, without impacting on the cluster's operation. This includes the regeneration of each node's TLS artifacts. If the root CA certificate is a sensitive artifact that needs careful protection, then so are the two join tokens. In the wrong hands, these tokens can be used to ensure that unsolicited join requests from rogue nodes to join the cluster, are honored by the addressed manager node. Consequently, the --rotate config option, applied to the docker swarm join-token command, along with a manager or worker argument, enables an administrator to regenerate the relevant token. The new token will then be required for presentation by any new node attempting to join the cluster.

Using Secrets to Manage Sensitive Artifacts
One of the great things that swarm provides is the ability to centrally manage secrets across a cluster of nodes. When I say secret, I don't mean the join tokens and TLS certificates and keys we've just been discussing. So, what do I mean? Well, a secret is a sensitive item of data that can be used to authenticate and/or authorize access to a particular service. In the context of a swarm cluster, when we talk about a service, we mean the service abstraction that describes one or more identical containers that provide a published function to be consumed by a user or another service. A database might serve as an example. Secrets can be anything that is deemed sensitive but might include passwords, TLS certificates and keys, SSH credentials, tokens, and so on. It can literally, be anything. Secrets are secrets for a reason, and therefore, they should be carefully managed. And as you'd expect, swarm provides some capabilities for doing this within a cluster. Let's take a closer look. The first thing we can say is, that a secret is a first-class object in docker, but they are only relevant in the context of a swarm. If you try and create a secret on a regular docker host using the docker CLI, you'll get an error message. The management of secrets must be conducted against a manager node in a swarm cluster. So, where does the swarm manager store the secret, if it's to be kept from prying eyes? Swarm clusters use a highly available, distributed data store called a Raft log for storing the cluster's state. And this is where secrets are also stored. The Raft log is replicated across the cluster's manager nodes and is encrypted whilst the cluster is in operation and also at rest. Secrets are associated with specific services and therefore, only the containers that belong to a service that has a secret associated with it, will have access to the secret. This ensures that the secret is available on a need-to-know basis. When a container is started for a service that has an associated secret, the decrypted secret is mounted into the container's file system using an in-memory tmpfs file system. It's mounted for the duration that the container runs and once it stops, the secret is removed from the node's memory. This ensures that the secret is never inadvertently laid open to inspection, outside the scope of its use. How do we go about creating a secret? Well, a secret is created using the docker CLI and can be provided in a file or from the standard input stream. Here, we're simply echoing a string which is piped to the docker secret create command, which then creates a secret object, called password. The secret can contain any data, provided it doesn't exceed the maximum secret size which is 500 kilobytes. Once created and safely stored in a cluster's Raft log, it's available for use by services running on the cluster. A secret can be associated with a service, when the service is created by specifying the secret in conjunction with the --secret config option. Secrets can also be added or removed from the services during a service update. Swarm's default behavior renders the secret in a file that has the secret's name, located in /run/secrets, on the container's file system. It's possible to circumvent the default behavior and specify an alternative tmpfs file system mount point that is more in keeping with the expected location of the secret. Your containerized application may expect a secret to be located at a particular location in the file system, and if so, it's straightforward to specify the desired location. Creating and making secrets available to containers that make up a swarm service is a nice and easy configuration exercise, and it can be done safe in the knowledge that the secret is never exposed during transit or inside any container that doesn't have a need to know it. Just as we might deem it important to rotate TLS certificates and keys, there may be a need from time to time, to rotate secrets. This may be dictated by policy or regulatory requirements but either way, it's good practice to rotate secrets and certainly a requirement under circumstances of compromise. The bad news is, that secrets are immutable objects in a swarm, they can't be changed or altered in any way. This makes it a little tricky for us to rotate a compromised secret. We can of course, remove a secret but not when a service is currently making use of it. We would need to execute a service update in order to remove the secret. We could also add a new secret during the update to replace the obsolete one. We can even specify that the secret be mounted to the same location, which effectively implements a secret rotation. The original secret is replaced with a new one during the update, which involves a restart of the container.

Autolocking a Cluster to Protect the Encryption Key
We've mentioned the cluster's Raft log a few times in this module. It's a highly available, replicated data store that holds the state of the cluster, the CA private key, cluster secrets and so on. It's integrity is clearly critical to the safe and secure operation of a swarm cluster and as such, needs to be protected. The Raft log resides in memory on each manager node, whilst the node is running. It's also resident on disk, encrypted. When a manager node starts, it needs access to the encryption key in order to decrypt the logs and load them into memory. For this to happen without operator intervention, the encryption key needs to be made available to the docker daemon running on each manager node. For this reason, the key is stored as a component of the manager node's TLS key, on disk. Whilst the operational simplicity of starting the node without intervention is appealing, it renders the node vulnerable to compromise. If someone were able to gain access to the manager node, the encrypted Raft log data on disk could be decrypted by the encryption key. All of the sensitive data held in the Raft log, including secrets, would then be available to the attacker. For this reason, swarm manager nodes can and should, be autolocked, which means the encryption key is itself, encrypted by an encryption key. This key, which is not stored on disk and must be looked after carefully, is then used to unlock the manager node when it starts. Let's find out how to enable swarm cluster autolocking. Enabling the autolock feature for manager nodes is quite straightforward. It's specified with a --autolock config option, either when the cluster is initiated or by updating an existing running cluster. The key required to unlock the cluster's manager nodes on startup is written to the terminal output. The key should be stored somewhere safe and secure and accessible to those responsible for administering the cluster. The key can also be retrieved from the Raft log using the docker swarm unlock-key command. But of course, this relies on there being a manager node running. When a manager node in an autolocked cluster is restarted, the host docker daemon will start up and commence serving the engine API, but all swarm operations will fail because the Raft log is still encrypted. Therefore, it's necessary to execute a docker swarm unlock command and to provide the unlock key when prompted, in order to return a functioning node to the cluster. Whilst it's not the default behavior for swarm clusters, it's strongly advised that autolocking is enabled on your cluster, in order to protect it from compromise. It's also a sensible precaution to rotate the unlock key periodically, which can also be achieved with the docker swarm unlocked-key command.

Managing the Availability of a Swarm Cluster
One of the key reasons for deploying containerized workloads on a cluster technology such as swarm, is to provide a degree of resilience in the event of failure. To make sure that a swarm cluster is resilient, it should have more than one manager node. With multiple manager nodes, provided they have a common view of the state of the cluster, in the event of a manager node failure, another manager node can pick up where the failed node left off. The big problem with sharing cluster state amongst manager nodes is that of distributed consensus. How do the manager nodes agree about the state at any point in time? Swarm and other container workload orchestration platforms use the Raft Consensus algorithm for maintaining cluster State across the management layer. To get an overview of how Raft Consensus works, take a look at The Secret Lives of Data Animation located at the URL in this slide. Without going into too much detail on the internal workings of Raft Consensus, for consensus to be achieved regarding state, the cluster must have a quorum of manager nodes. Without the quorum, whilst the worker nodes can continue to function, running containers, no new reliable management activity can take place as the nodes will be unable to achieve consensus. So, if we need multiple manager nodes, how many should there be? Deciding how many manager nodes to deploy is a trade-off between resilience and performance. Let's take a look at resilience first. As you'd expect, a single manager node provides no resilience at all. But in adding a second manager node, no benefit has been gained. The quorum required is two and therefore, no failures can be tolerated. This pattern repeats itself as more manager nodes are added. Increasing the node count from an odd number by one doesn't improve the number of tolerated failures. For this reason, it only makes sense to deploy an odd number of manager nodes in order to realize a given failure tolerance. You might be tempted to think that the best strategy would be to deploy a very large odd number of manager nodes, as many as you can afford. This is counterproductive however, as the more nodes there are, the more time they spend agreeing a consensus when cluster state changes. Five or seven is a good number, but the choice may depend on a whole raft of factors, no pun intended. Care should also be taken to ensure that the managers are split across availability zones such that if one availability zone goes offline, then the remaining managers can still form a quorum. What happens if we have a compromised manager node or one that has another problem that's not easily remedied? It has the potential to affect the smooth running of the cluster so, it should be removed from the cluster. If achievable, it should be removed from the cluster as elegantly as possible. The node status should be changed from manager to worker by issuing the docker node demote command against another manager node. Once it becomes a worker, the node can then be removed from the cluster. First, the docker swarm leave command can be issued on the node that's leaving to return its daemon to a simple docker host before a manager completes the removal with a docker node RM command. It may be that time is of the essence, especially in a compromise, and the node can be forcefully removed with the --force config option. If you do remove a manager node, be sure to replace the node with another manager node as soon as possible, in order to minimize the risk of losing the quorum. You might for example, elect to promote another worker as a stopgap measure. Hopefully, you'll never be in a position where a number of manager nodes have become unavailable such that the cluster quorum is lost. In such a scenario, whilst the worker nodes continue to function with deployed services, no new management tasks can be performed for the cluster. Obviously, this is a serious situation. A swarm cluster can deal with transient node loss such as restarting daemons or hosts, but when the quorum is lost, steps need to be taken to recover the cluster. The best remedy is to return the lost manager node to an available state, to re-establish the quorum. But this may not always be possible. If it's not possible, then recovery involves forcing the creation of a new cluster on a working manager node using the --force-new-cluster config option, applied to the docker swarm init command. This effectively redefines the cluster as a one manager node cluster. Apart from this redefinition, the cluster state is maintained with worker nodes running the various services that have already been defined and deployed. From this point, new manager nodes can be added to the cluster to enhance resilience. Clearly, backing up the data for a swarm cluster is important, especially for the purposes of recovering a cluster. And when we talk of using backups for recovery purposes, we're talking about remedying a catastrophic situation. Ordinarily, cluster state is replicated across all managers and provided we have a quorum, we can recover from node failures. But it's always good to be prepared for the worst. Let's see what's involved in creating a backup. First of all, docker keeps swarm data in its data root, normally, /var/lib/docker, in a subdirectory, called swarm. If the cluster has autolock enabled, be sure to have the unlock key available to enter, which will be required on startup, and also during a recovery operation. Ideally, the docker daemon on the manager on which the backup is being taken should be stopped in order to get a coherent backup. The cluster could continue to operate whilst the manager is offline. So, the backup is essentially a point in time backup. At this juncture, take a backup of the swarm directory using your preferred backup method and on completion, the manager can be restarted. Restoring a backup of a swarm cluster should only be required in the event of a total catastrophe, where a new cluster is required to be built from scratch. In all other situations, it's possible to replace manager nodes in an elegant manner whilst maintaining the operation of the cluster. If it's required however, restore a previous backup of the swarm directory from the cluster that needs to be recreated onto a host with docker installed, but not running. Start the docker daemon and unlock the swarm if it requires unlocking. The state of the lost cluster contained within the Raft logs will reflect the point when the backup was taken, which will include all of the services and their tasks, and probably a number of manager and worker nodes which will be missing. At this point, it would be necessary to reinitialize the swarm using the --force-new-cluster config option with the docker swarm init command. This will give you a one manager cluster but with all other state from the backup intact. Manager and worker nodes can then be added back into the cluster to bring it back to the required number of nodes. Provided you've given due consideration to the cluster requirements, in terms of the number of manager nodes and their location in terms of availability, complete loss of a cluster is very unlikely. But it's always best to be prepared for the worst-case scenario, so be sure to take regular backups and be familiar with the process of recovery.

Recovering from a Lost Quorum
In this demo, we're going to go through the steps to recover a swarm cluster that has suffered the loss of a quorum of manager nodes due to transient and permanent node failure. To manage our environment, we're using a docker tool, called docker machine, which enables us to spin up virtual machines on a local hypervisor or in the cloud, which have a docker daemon running on them. We can interact with the virtual machines using the docker machines CLI. For convenience, an alias has been created for the docker machine command which is simply, dm. Six virtual docker hosts have been created for the demo, three will be swarm managers, and three will be swarm workers. As we'll need to run docker CLI commands on a few of these different hosts, let's quickly explain a shortcut we'll make frequent use of. We can use the output generated by the docker machine config command to populate a variable which is substituted on the command line and enables us to target the relevant docker daemon on each machine. This saves us from perpetually setting and unsetting the docker_host and docker_TLS variables to address the daemon on each virtual machine. For example, if we wanted to run the docker info command to find out the name of the host that our address daemon is running on, we just place the variable between the docker CLI command and the arguments to the command. This command executes against the daemon running on the host node one. We're good to go. First, let's have a look at the cluster as it currently stands. We have three manager nodes, with node one as the leader, node two and node three being the other managers, and the remaining nodes, are all worker nodes. An application has already been deployed to the cluster using the stack abstraction. It's called, atsea, and consists of five discreet services, including a Postgres database. If we fire up a web browser and navigate to any one of the six nodes running in the cluster, we can see the application running live. We won't dwell on the application, as we're here to discuss the availability of our cluster. We currently have three manager nodes which means, we require a quorum of two manager nodes. And if you remember from our discussion about manager nodes, this cluster can tolerate the loss of a single manager node without losing the cluster quorum. If we were to stop one of the manager node virtual machines, let's say the cluster leader, node one, what do we think will happen to the cluster on its loss? In theory, the cluster should continue to operate seamlessly as the quorum is maintained. We can find out by attempting to list the manager nodes in the cluster, which is part of the cluster state that is held in the Raft logs. Of course, we can't issue a command against node one, as it's no longer running. So, we have to target one of the other two manager nodes, node two, in this case. The output of the command tells us that node one is unreachable as a manager and has a status of down. It also tells us that node three is now the leader of the cluster and that node two is still reachable as a manager node. The simple fact that swarm has returned this information, indicates that the cluster continues to function. Now, what if we were to stop node two in addition to node one? That will be the majority of the manager nodes in a stop state, unavailable to participate in the consensus of state for the cluster. And if we attempt to list the cluster nodes once again, this time using the remaining manager node, node three, we're not so lucky. We get an error message that indicates that the swarm has no leader, and that this may be due to the loss of the quorum. At this point, no cluster management commands will succeed, but the services running on the worker nodes will continue uninterrupted. The easiest and best way to return the cluster to a healthy state, will be to restart the stopped virtual machines in order to regain the manager node quorum. This will take a few moments, so let's jump forward in time to the point where node two and node three are back online. Now if we attempt to list the manager nodes in the cluster, we get the response that we'd hope for, with the two missing manager nodes backup and running, contributing to the management of the cluster. We've just demonstrated the temporary loss of the cluster quorum, but what would happen if we lost the quorum permanently? Here, we're removing the virtual machines, node one and node two, never to be seen again. Predictably, just as before, the cluster is unable to perform any of the management functions because of the loss of the quorum. This time, we can't return the lost nodes to the cluster, they no longer exist. So, what must we do to recover? Well, the first step toward recovery will be to reinitialize the cluster using the only surviving manager, node three. To do this, the --force-new-cluster config option needs to be supplied as a command-line argument to the docker swarm init command, and the new cluster will be created using the state available from the old cluster. We can see this by first, listing the manager nodes in the new cluster. The cluster leader is node three, and there are still references to the lost manager nodes, node one and node two. As they're not going to be returned to the cluster, the nodes should be removed from the cluster using the docker node RM command. All of the worker nodes are still available in the cluster, as is the multi-service application that was originally running on the cluster before it lost the quorum. Inevitably, some of the services have been restarted on different nodes, as and when various nodes were lost to the cluster. Hopefully, provided you plan the architecture of your cluster adequately, you'll never lose the quorum of your swarm cluster. But the steps we followed in the demo, show that it's a relatively straightforward task to recover a cluster from a lost quorum. One last word on the subject, be sure to either add in new manager nodes to enhance the resilience of the recovered cluster or another option, would be to promote existing worker nodes.

Module Summary
We've reached the end of our discussion regarding the security and availability aspects of docker swarm clusters. The decision to deploy containerized application services on clusters using orchestration tools should not be taken lightly. It ushers in another level of complexity. Whilst it can bring great rewards, it should be understood that it requires careful planning and operation. This includes, security and availability. Let's recap on what we've covered. We've seen how swarm automatically secures the communication between the cluster's nodes, and how mutual authentication can be achieved. The TLS artifacts and join tokens play a crucial part in this and should be securely managed. Application services often make use of secrets to control access to sensitive data and their integrity is vital to protect against compromise. Swarm has this covered and implements secrets as a first-class object in its API, and protects against compromise by never exposing secrets in transit or at rest. Swarm holds cluster state in its internal distributed data store along with other sensitive data. Autolocking a swarm cluster helps to prevent this sensitive data from being exposed to anyone interested in obtaining it for malicious purposes. And we've also taken a long look at managing the availability of the cluster itself, including how to recover from node failures which may result in the loss of the cluster quorum. We're almost at the end of the course but before we finish up, we're going to very briefly, recount the important topics we've covered, and also see what next steps can be taken to enhance the security associated with your use of docker and container technology.

Wrapping Up
Recapping the Journey
Well done, you've reached the end of the course. It's been a journey so before I recommend some additional material that you can use to help you in your ongoing quest to keep your container based applications and platforms secure, let's take stock of where we are. Security is a ubiquitous topic, it doesn't matter which level of the software stack or infrastructure component you're dealing with, it's first ought to consider how best to deploy IT assets in order to keep our data and systems safe and secure. The problem often is knowing where to start and that's why industry led initiatives such as the CIS benchmarks a great way to get it started. The Docker CE benchmark may not take all of your requirements, but it's a great foundation to work with, especially if you're new to Docker. And whilst we're talking about benchmarks automation has become a key ingredient in the evolving cloud native landscape we find ourselves in. So making sure of open source tools for testing makes perfect sense. Tools like the Docker Bench for Security and the Inspect Testing Framework are ideal for this purpose, I'd encourage you to use them. We've learned that establishing a meaningful benchmark for the security of a Docker platform is the basis for the ongoing measurements of compliance. Don't forget though, time doesn't stand still, especially in the open source world. So be sure to keep up to date with changes to the Docker platform. Changes will inevitably affect the security controls you impose on your own platform.

Where to Go Next
Okay, so I might've said that security is important once or twice during the course. We've addressed the big chunk of the security topics that relate to Docker, but by no means have we covered everything. This course is focused on securing the Docker platform itself, including access to and authentication of the platform's various components. If we studied section five of the CIS Docker CE benchmark, however, and ran the Docker Bench for security tests for its recommendations we quickly find that there is a whole other area of security we haven't discussed. Securing the platform is one thing, but what about securing the containerized application workloads themselves, how do we minimize the possibility of privilege escalation inside containers? How can we prevent containers from excessive consumption of physical resources? How can we control the access containers have to operating system objects? If you're interested in learning about the security aspects of containers then why not take the Pluralsight course, Securing Docker Container Workloads. It'll help you to understand what's important when it comes to securing Docker containers and give you the practical insight into getting started with securing your own containerized applications.

Final Words
Okay, that's us done. I really hope you found this course useful and I would genuinely appreciate your feedback on the course. If you can rate it I would be very grateful. If you have any questions at all feel free to post them on the course discussion board and I'll reply to them as soon as I'm able. And if you'd like to get in touch directly you can visit my blog site at windsock. io where you can find my contact details. Come and find me on Twitter and LinkedIn. Thanks for watching and I'll see you again soon.
