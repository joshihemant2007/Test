Docker is the new platform for developing and hosting modern applications. In this course, Docker Networking, you will learn everything you need to know about deploying and managing Docker networks. First, you'll learn about the Container Network Model (CNM) and Libnetwork, which are the foundation of all Docker networks. Next, you will delve into building and managing single-host and multi-host networks. Finally, you will learn how to build container networks that integrate with existing VLANs and application networks. When you are finished with this course, you'll have the skills and knowledge needed to start deploying and managing Docker networks within your organization.

Course Overview
Course Overview
Docker and containers, man I mean I can't think of anything else out there that is hotter than these at the moment. So whether you're an IT pro or a developer with a pressing need to get up to speed with Docker networking or maybe you're just curious and want to get ahead of the game. Either way you have come to the right place. My name's Nigel Poulton and I've been living it large in the container world since the early days. And before that, I was a hands on traditional infrastructure guy, so a bit of Linux, a bit of networking, and way too much storage. And along the way I managed infrastructure teams and some of the most dynamic and high pressure environments out there. And I cannot wait to deliver this course to you. Now it's going to be high paced with a lot squashed into the timeframe, we'll be covering the fundamentals of what makes a container network, including pivotal technologies like the container network model and the LIB network. But we'll also get into a bunch of real world requirements and use cases, things like scalable overlay networks, and how to add containers to existing networks and VLANs. And along the way we'll cover some of the network servicy stuff that makes deploying applications on Docker simple and easy. By the end of the course look you're not going to be an expert after a two-hour course, but I am telling you this, you will be skilled up and in possession of the right tools to take things further. And I dare say you'll be plenty confident enough to start building and managing your own container networks. And on that note, let's get started.

The Basics
Course Introduction
It's a standing joke right that whenever anything goes wrong in IT, it's always the network or we always blame the network, it's just the dumb thing, yeah, even when it's not the network, though quite often it is. So why is it so often the network and I mean why do we always default to assuming it is, even when it's not? Do you reckon it's like because networks are just flaky by nature or maybe like all network engineers are fat fingers and drink on the job? I don't know, I don't think it is and this is just the truth according to me, right, I don't profess anything even remotely scientific to this. But the way I see it, right, networks, I mean they are mighty complex beasts and they're quite often humongous and sprawling. And then they're right slap bang in the middle of everything, so they're huge and complex, and they're like freaking omnipresent, just everywhere. Well in the world of containers, it is pretty much the same picture, though you know what, if you deploy in containers that scale with micro service designs and the likes, then let me tell you, you are going to want to get your container network in right. And that's why we're here, yeah, Docker networking. So my name's Nigel, containerholic and longtime IT nut. And I'm going to be your wingman for the next hour or two as we face the good, the bad, and the ugly of Docker networking. But the good news, there's more good than anything else. Now before we start, right, I want to make you aware of some other courses out there that might be of use to you. We've got Docker and Containers the Big Picture, this is for those of you who are totally new to containers and I don't know, maybe need that 40, 000 foot view of what they are and maybe the impacts that they'll have on your career and your business. Then there's Getting Started with Docker, this is all about how do I install it and get it up and running? You'll build and trash your first container and all kinds of stuff like that. And that's it, I reckon those two are all the ground work you need before taking this course. So here is the big picture view of what we're going to cover. Module 1 is all about the basics, we'll start off with a bit of background. Why networking's so important in the container world and you know what, we'll probably hint at some of the challenges that have got us to where we are today. In that module we'll cover the container networking model and how it's implemented in Docker. We'll have a bit of a foray into LIB network and then we'll take a quick tour of the networks that are come with the default installation of Docker. You know what, we'll use that as a bit of a chance to familiarize ourselves with the major Docker networking commands. Alright after that we'll hit Module 2, that's where we start drilling a bit deeper by looking at different network types. We'll cover bridge, overlay, Mac VLAN, and IP VLAN, along with their major use cases. Then module 3 is all about network services, really good stuff by the way, things like service discovery load balancing, and routine. And that's it really, by the end of all that, I am telling you you'll be like Kings and Queens of Docker networking. So first up let's go set the scene.

Background
So we've already established that networks in general are big and hairy and that you can find them in every corner, crevice, and crack of your infrastructure. We know that, but what about Docker? I mean Docker's all about making our lives simple, right? Well yeah, but I tell you what, let's wind the clock back for a second. You see, when Docker first came along, it was a bit like VMware, in that it cracked the problem of compute or it tackled that problem at least first, and rightly so. I mean VMware came along and it made it way easier for us to provision servers and it made things about a lot more efficient as well. But the point is, it was only after they'd well and truly nailed the compute part, that they really got to work on the networking and storage bits with the likes of NSX and VSAN. Well you know what, no surprises right, it's kind of a similar story with Docker. They started out by tackling, I mean I guess what we can all effectively the compute side of things. They made provisioning application runtimes way simpler and way more efficient. I'm guessing we all know the story, right? Instead of provisioning and working with full fat bloated VMs, we got nice lightweight containers that can be provisioned faster than you can say, I don't know, Pluralsight rock, right? But the thing is, now that containers have pretty much nailed, the folks at Docker and the community of course, are a fair ways down the road of doing the same things for container networks. So making them simple, fast to provision, and lightweight, and you know what, scalable and secure and all that jazz as well. But the point is, it is only now, in my personal opinion right, since the days of Docker 1. 12 that Docker networking has really started to come of age. So back in the early days of Docker, networking really was like this ugly stepchild of the family, it didn't scale, and it was complex. Ah honestly, it was nasty, we had like this clunky port based netting and these diabolical constructs called links, ah and it just, ah it wasn't worthy of a platform for over scalable architecture. Well I guess right, the powers that be inside of Docker Inc big D Docker if you will, I guess they realized this. Now I don't know right, I'm speculating here, but I reckon they also realized that maybe they didn't have the time or the talent internally to fix it and make it the way they wanted it to be. So instead of baking it themselves, they went shopping and in the spring of 2015 the acquired a small startup called Socket Plane, a bona fide slam dunk in my option, proper like Hallelujah stuff here. You see socket plane was this small container networking startup full of genuine networking talent that grasp the challenges of container networking. So socket plane got brought into big D Docker, that's Docker Inc, the company, yeah, and they were brought in with the mandate to make Docker networking or out-of-the-box Docker networking amazing. Now do me a favor here, okay, and just allow me to get a little bit excited here, because I've lived through the crappy days and I feel like I deserve the chance to go a little bit OTT. So the socket plane guys came in and honestly they gutted it, they ripped things apart, they rewrote stuff, re-architected things, put a boat load of new shiny stuff inside, basically right, they did a full nut and bolt rebuild of the Docker networking stack. But pretty much improved everything and changed parts out as they did it. To the point, right, where today in the post 1. 12 world, Docker has honestly a proper worthy networking stack. Something genuinely architected to compliment the rest of the stack. Now look, that probably did sound a bit fanboyish, and I totally excuse you if you're thinking that I had stock in the company or something, which for the record I don't. It's just honest to goodness the fact that I've went through the bad crappy days before and the contrast between then and now, it's just, I mean, I don't know, how about this way. You know how absolutely hideous you feel in those moments right before you throw up, like throw your guts into the toilet yeah, I think we all know that that moment right before is just the worst thing in the world. But then when it's all over and done with you feel like, oh my goodness, life is great again. Well Docker networking, before the socket plane stuff, was pretty gut wrenchingly poor and now it is so much better, don't get me wrong, right, it's not perfect, but man the difference is. Well look I'm a happy chap now, and you know what, I think that's enough for the background. Docker networking, it used to be pants, they bought socket plane and made it a shedload better, it's pretty simple, it's pretty scalable, and it's getting more comprehensive. So stop, that is it, definitely enough sickly praise from me, in fact I'm just going to scribble a note down here, no more sickly praise. I don't want any of you dropping off the course just because you can't stand listening me kiss Docker's butt. Anyway, let's go and look at the two fundamental things that make Docker networking what it is today, the container networking model and the LIB network.

The Three Pillars of Docker Networking
Right, let's talk about the three major things behind all Docker networks, the container network model, CNM for short, LIB network and Drivers. The most fundamental way and I guess probably the most important, is the container network model. Now I'm saying it's maybe the most important, because it's like the grand design behind everything else. The DNA, right, lurking beneath the surface of everything that is Docker networking. I suppose we could say LIB network and drivers in a sense right, draw from the CNM DNA, how's that for an acronym. So the CNM, right, is a specification document hosted on GitHub, this is the link right, and I'll leave it up there for a while because I would say it's recommended reading for the course. Now look, we are going to run through its major components here, but reading through an actual spec doc itself, I don't think is ever a waste of your time. And it's definitely going to reinforce what we're learning here. Now the CNM is a spec purposed by Docker Inc, so everything about it is aimed at creating a networking model perfect for Docker. So networking for Docker containers and for Docker services, no surprises there right. Docker writes a spec, it's tailored made for Docker environments. Now there is a rival spec out there, and all of rivalries, you can't beat a bit of survival of the fittest and competitive tension. Anyway, that rival spec is called the container network interface, CNI, yeah, funnily enough, it's also about container networking, but this one's a bit more suited to Kubernetes environments. Now word is, that the Kubernetes people at Google and the likes, did have a play around with the CNM, the Docker one yeah. But they found that it wasn't quite what they wanted, well maybe it didn't fit quite as well as they wanted. Long story short, over here on the left we've got the CNM from Docker Inc, handcrafted for Docker environments then over here on the right we've got this CNI from CoreOS, custom built for Kubernetes, sort of. So similar, but different but we're all about the CNM today, so we can get rid of that over here and let's get back to business. Now the CNM defines three major constructs, the sandbox, the endpoint, and the network. Now if you know your Linux kernel stuff, right, the sandbox just means a namespace, but Docker's all multiplatform these days, Windows and all yeah, so they're opting for more platform agnostic terms. But if you do know your Linux, it's a namespace, and I'll be honest with you, right, I don't really know what it's called in Windows, but either way a sandbox is just a ring fenced area of the OS where you can do stuff in isolation. So tweak and tune stuff without impacting anything else on the system. So we get this sandbox right, and then inside of it we create network stacks, like IP configs, DNS, rooting tables, all the networking that we'll ever need right, and it's all isolated and ring fenced from the wider system. A lot like a container actually, just without an app running inside. Anyway, then we've got endpoints, these are network interfaces, like your F0 in the Linux world and maybe local area connection in Windows. Then there's the network itself, now as far as the CNM concerns, a network is just a bunch of these endpoints here that can talk to each other. And that's the CNM really from a design perspective, sandboxes, endpoints, and networks. But it's important right, it is the core DNA that everything else is going to be derived from. Now this is a bit of theory at the moment, okay, I hope you like what I did there, but seriously, if it does seem a bit vague right now, that's absolutely fine, we're going to be building on all of this as we go. At the end, right, if you still don't get it then, we'll shout me out on Twitter and we'll see if we can fix that. But it really is normal for this to feel a bit fuzzy and bit abstract right now, well anyway right. The next thing was the LIB network, LIB network is Docker's own real implementation of the CNM. So the CNM's the design doc, and LIB network is the actual go line code. In true Docker fashion these days it's cross-platform, if you want to dig a bit deeper, and especially if you're hands on with the code and the likes this is where it lives on GitHub. Now look, we talked about the socket plane acquisition a minute or two ago, well before socket plane came on board at Docker Inc., the main networking code was inside of the Docker engine. So one of the big things that the socket plane guys helped with was ripping all of that out of the engine and the likes and bundling it all together inside of LIB network. This obviously went a long way to tidying up things inside of the engine code, but it also gave us this nice one stop shop for Docker networking. So all the code, the logic, the API, and the UX stuff, the whole shebang living happily inside of LIB network. Now LIB network has to be portable as well, okay, it's written in GO, so you can run it on Linux and Windows, you can even run it on ARM, right. I mean it runs on bare metal on premises, it works in the cloud, it works on your lappy, pretty much anywhere that you'd want it to run. And then it's got this pluggable architecture as well. So while LIB network itself implements and defines the fundamentals, those sandboxes, endpoints, and networks, and the management APIs and UX stuff, right, the actual specifics of different network types, like I don't know maybe local bridge networks or multi-host overlays that are leveraging VX LAN and the likes. All of that magic gets implemented in drivers. So there's our third component, at the bottom is the CNM that's the design doc, then there's LIB network as the canonical implementation of the CNM, then on top of LIP network, drivers, and this is where the detail lives. Now each network type has its own driver, so the overlay driver, no prizes for guessing, but that implements overlay networking. The MACVLAN driver, MACVLAN networks, bridge, bridge networks, you get the picture. Well stepping back a bit and using the language that all the cool kids are using these days, LIB network implements the control plane stuff, see I feel clever already, and the drivers they implement the data plane stuff. And sticking with drivers, Docker has this concept of local and remote drivers, local drivers and native drivers the ones that ship out-of-the-box with Docker, and remote drivers are third-party drivers. So one's provided by ecosystem partners in the wider community, but both of them, local and remote, implement their specific network architectures on top of the stack provided by LIB network. So the idea is not for the drivers to re-implement any of the stuff already in LIB network, the idea is to take that foundation provided by LIB network and then build specific network topologies on top of it. And that helps keeps things clean and certainly more portable. Okay, right, yeah, the native or the local drivers, as of Docker 1. 13, are bridge, overlay, MACVLAN, IPVLAN, host, and none and here are some from the ecosystem under the banner there of remote drivers. Now look, Docker network does a bunch more than what we've said so far, but I don't want to bog you down here with more theory. Though don't worry right, we will cover this as we progress, but as a teaser right, I'm talking about things like the distributed high performance key value store that's used for storing network config and state. Then there's the network scope swim based infectious gossip protocol stuck out in your pipe. Native encryption of the control and data planes, service discovery using embedded DNS, IP address management, there is seriously some good stuff going on in Docker networking. And look I know there's a lot of buzz words been thrown around there, but we are going to cover the lot and I cannot freak'n wait. So let me just very quickly recap the main take away points here before we roll out sleeves up and get hands on. The CNM, the container network model, that's kind of the master plan or design behind everything in Docker networking. LIB network is the de facto implementation of it and look, I say de facto, because it's written and maintained by Docker Inc. and it ships with Docker as default, so it's everywhere. Then specific network types and the likes, these are implemented by pluggable drivers. Alright, theory done, time to wake up over there in the corner, because this is where the fun starts.

Hands on with the Basic Docker Networking Commands
I hope you've got your sleeves rolled up, because never mind turtles all the way down, we are hands on all the way down from here. Now first things first, for these demos I am rocking and rolling with the Linux and a clean install of version whatever of Docker. Though I shouldn't be plassay about the Docker version, it's actually really important. You're going to need to be rocking it with Docker 1. 12 or later for a lot of this stuff to work. Okay, obviously I'm fine with this version here. Now although I'm going with Linux, that's no slight on Docker for Windows, don't get me wrong, Docker on Windows is a beautiful thing, it's just at the time that I'm recording this course, I think it's fair to say that the networking stuff on Docker for Windows is still playing a bit of catchup. And that's normal, remember, compared to Linux, Windows is still relatively new to the Docker game. Now then, networking is so tightly integrated with Docker these days, it's got its very own subcommand, Docker network. And let me tell you, we are going to be properly intimate with this by the time we're done, in fact, if your significant other isn't jealous of how intimate you are with the Docker network command by the end of this course, then I'm sorry you have not been working hard enough. Anyway, hitting return gives us this list of sub subcommands that are currently available, no doubt they'll be more in the future. So to get things rolling, let's take a look at what networks were given for free, so that'll be LS. Alright for me here three networks, now remember, we're on a shiny new installation on Linux, so each of these networks was created automatically as part of the Docker install. Now the names and the numbers of default networks might change in the future, in fact it probably will, and you know what, I know that they're different on Windows, but none of that matters right now. What matters is that we've just used the Docker network LS command to list our container networks. Alright, but what are we looking at? Well we see that every network gets a unique ID, it also gets a name. We can see which driver owns it, and then we can see something here called scope. Now IDs are IDs, yeah, they're unique and they identify the network. Then the name here, that's arbitrary and it's really only there to help us puny generation 1. 0 homo sapiens that find network IDs too hard to work with. Anyway, by now this driver column here should be looking at least a bit familiar to us, it better be. So we can see that the network called bridge here, is associated with the bridge driver, okay. Now look the fact that the network name and the driver name are the same is pure coincidence here, the two fields are in no way related. I mean look this network could be called, well overlay if we wanted to, and it could still be owned by the bridge driver, but that'd be stupid, right? Oh look, this one here's the same as well but don't be swayed by what you see in here, names honestly are 100% arbitrary. So with the bridge one here the networks called bridge, and it was created by the bridge driver. Same for host, the network's called host and it's owned by the host driver, but the names really could be anything. But what about this scope column here? Well these three are all local and the other option that we'll see here in a minute is swarm. Now swarm scope is for when networks are multi-host and can span an entire swarm cluster. Well these ones here are all single host networks, so only available on this host, so we say that they're scoped locally. Now all of this is nice, right, but it's pretty high-level, network ID, name, driver, and scope, well if we want to drill deeper into a network, we want to go Docker network inspect and then here you can give either the ID or the name. Now I'm obviously fine working with 64-bit hex IDs, but some of you guys might not be. So I think we'll keep this simple and we'll use names and go on we'll have the bridge network here. Ahh, now that is what I'm talking about, proper good detail and it gets even better when we've got containers attached. But what have we got right now, there's name, ID, and scope, and the driver, so those are the things that we've already seen with the network LS. This network's got no IPV6 config, this here is the IPAM section that's all about IP address management, but just for this particular network okay. Well we're using the default IPAM driver, an IPAM driver's a pluggable by the way, so you can use others if you want. Well we're not specifying any options here and we've just got a basic sider block and default gateway. Then the rest tell us that this is not an internal network, there are no containers connected to it yet, and then a bunch of options. In fact, we've got yes this is the default bridge, yes it allows inter container chatter, yes for IP masquerading, oh now this own down here's of interest. Which bridge on the host is it connected to? The one called Docker 0, now we'll come back to this in a second, but let's quickly step back right and remind ourselves that, look up here, this is all on a Linux X64 machine. So some of these options might be a bit different on your machine, especially if you're rocking it with Docker for Windows or Docker for Mac. I think certainly in those cases you're probably going to have the nat network and the nat driver and stuff like that. So some slight differences in implementation, but whatever your OS and Docker version, we're learning stuff here, right? Okay so what else, we've got MTU, that's a standard 15, 000 bytes, but this Docker 0 thing, what's that? Actually no, yeah, sorry about that, I think we'll get back to that a bit later, we'll try and stick with more basics here. So as we've seen, Docker network inspect is a great way to drill down into the config of the network. What are the commands are there? Okay so we can connect to networks, that's connect containers yeah. We can create new networks, and I can assure you right, we are going to be creating plenty of networks. We can disconnect from them, and we can even remove them down at the bottom here. Cool, oh tut tut tut, yeah I did know there was something else. If we go Docker info here, and then right, all the way up at the top here under plugins, see how we've got a list of our network plugins? Okay, now a couple of things actually about this, these will be different if you're on a different OS, obviously, I'm touching it with Linux remember. But you know what, I've seen some inconsistencies with behavior of this, sometimes it only seems to show the plugins for networks that you've got in use. Other times, like now, it shows them all, or near enough. But the point I'm trying to make is that although Docker info isn't a Docker network subcommand, it is a way that we can see network plugins. And you know what, I reckon that's the basics, good old Docker network is going to be our friend, so Docker network LS is great for listing networks and seeing basic info. If we need more, we can use Docker network inspect to drill into the detail. Alright, that's the basics, now it is time to buckle up because we're shifting gears and opening up the throttle in module 2, where we'll look at the different container networking in use cases and the drivers that Docker provides to address them.

Use Cases and Drivers
Module Intro
Okay, module 2, this is where we look at some of the major container networking use cases and how drivers help us with them. First up, we'll look at single host networking with the built in bridge driver, it's quick it's simple, and you know what, it's been around since pretty much the beginning, so it should be super stable. Then we'll move into the good stuff. Multi-host networking with a built in overlay driver, now I don't want to spoil it by saying too much here, but halle-freaking-lujah, the endless nights when I dreamt of this, well it's here and it doesn't disappoint. Then we'll look at how to integrate containers with existing apps and workloads, so those scenarios where you're integrating containers with existing workloads. We'll look at the MACVLAN driver and the IPVLAN driver and how they help with that. So enough idling and revving, let's stick this into gear and put the power down.

Single-host Networking
Hmm, yeah, okay, I know I said we'd do demos all the way down, but you know what, I think each different network type that we look at, probably warrants a really quick bit of theory, just so that we know exactly what we're doing when we start bashing out commands. Because there is zero value in knowing a bunch of commands if we haven't got a clue what they're actually doing. So we're about to create a user defined bridge network on a clean Docker host and we're on Linux remember. If you're on Windows, it's totally fine the commands are going to be the same, but the closest type of network to bridge is probably going to be nat, N A T. So keep that in mind. Now first up bridge networks are single host. That means the network that we're going to create, I think we'll call it ps-bridge for Pluralsight bridge, we'll it's only going to be available on the Docker host that we created it on. Scoped locally if you remember the Docker network LS output from a minute or two ago. But what this means is that even if we create another bridge network with the same name and config on a different Docker host, guess what, we'd have two distinct networks. Meaning, containers on either of them could not talk to containers on the other, so single host. Now what it's going to do is create an 802. 1d bridge device on your Docker host. Now bridge is a bit of a Linuxy term, well I mean it's not, it's totally a legit networking term, but in the VMware world and the likes, we tend not to use the term. You see, for whatever reason, virtual switch or VSwitch sounds way cooler, apparently. No matter though, creating a bridge network is going to create a virtual bridge or a virtual switch on your Docker host. In the Linux world, that's going to be a Linux bridge right inside of the kernel, which is kind of important to be honest. So the Docker bridge driver on a Linux system actually leverages the tried and tested, battle hardened, production scarred, Linux bridge. And because it's in a kernel and it's been there since like the 2. 2 kernel if you overlook the odd rewrite or two, but because it's in the kernel, it is fast, back to business though. Docker network create with the bridge driver creates a local bridge or VSwitch entirely in software on your Docker host. Then once we've got that, we can plum containers into it. And wahlah, we've got a container network and the party can start, as long as it's a single host party, yeah? Well back to our cleanly installed Linux Docker host here. Now first up right, I was a bit naught in the last lesson, I was working as rude. I'm always being told off by my kids for doing that, anyway I'm a regular user now, so back on Santa's good list. Well let's call up our new friend Docker network we'll give it create, because well we're creating a new network. Then we'll tell it to use the bridge driver. Now as well as that, I'm going to tell what address space to use, and as I said before, we'll call it ps-bridge. Okay, quick check for typos, looks okay to me and we're in business. Now a string like that is a return code is good news, that's the ID of the network. But I think always good to check it with the Docker network LS, yeah we're cooking. In fact, a quick inspect, love it, now I'm going to get a bit Linuxy for a second, but stick with me Windows people it's just going to be a small detour. Well as I'm on an Ubuntu host here, I'm going to install the Linux bridge utils package, this will let me inspect the actual Linux bridge, hmm yeah. Nig you're not rude anymore, so let's try that the proper way, better. Now that means we can use the BRCTL command to look at kernel bridges on this host. So show, hmm okay the outputs a little bit skewed, but it's not too hard to see that we've got two bridges. So this BR long number one here, then this Docker 0 as well. Okay, so Docker 0, it's a bit special, if you remember back a minute or two, this is the default bridge that gets created with every Docker on Linux install. And it's the one for the default bridge network that we saw. So you know what, if we throw these two bridges up here, these remember are fully fledged 802. 1d bridges or at least a major subset of the standard. But they're fully functioning bridges or switches running inside of the Linux kernel. Now it might look a bit different under the hood on Windows, but the net effect is the same. A fully working bridge or VSwitch that containers can connect to. Well we've got two of them, so if I go Docker network LS, right okay, we've got two bridge networks. The top on here called bridges, the default network that's created with the install, remember that's built on the Docker 0 bridge up here, Docker 0 is always associated with the default built in bridge network. Then this one here ps-bridge is on this bridge up here. So that's our two bridge networks. And remember, they're isolated from each other, so the container's on one can't talk directly to containers on another. And well yeah I don't know what else to say about it really, oh again on Linux, if you go ip link show, okay, and if we look closely we can see Docker 0 and then our other bridge right here below it. Cool and all, but what about attaching containers to them? Well let's create a couple. So Docker run, we'll park it and give it terminal, call it c1, add it to the ps-bridge network we'll base it on alpine and let's run this command to keep it alive and kicking for a day. Lovely, okay well we'll do the same again, but let's come back here and call this one c2, fabulous, that's two containers and both, remember, on the same ps-bridge network. So if we go and inspect the network again yeah, see how we've got two containers attached right now, and check it out, they've both got Mac and IP addresses. And obviously we can see they're both on the subnet that we gave the network up here. So two containers on the same bridge with IPs and everything, surely that means they can talk to each other. Well actually you know what before that, oh in fact actually let me get rid of those bridge IDs up here and we'll park the container IPs in their place. Okay, but before we test connectivity, let's run that BRCTL command again. Okay, right, totally skewed again, but see how we've got two interfaces on that BR long number switch here. That'll obviously be the two containers that we created. Each one, okay, gets an eth0 virtual Ethernet adaptor inside of it that gets plumed into this ps-bridge switch. Hopefully we're getting a good picture here. In fact speaking of pictures, computer draw me a quick picture of the current Docker networking topology focusing on the ps-bridge network. Okay yeah, good visual representation there. Two containers, each with an eth0 adapter plumed into the ps-bridge network here, which comprises a single bridge or switch called br whatever yeah, well time to test it out. So if we docker exe onto, let's do c1 here, okay we're in, now remember this is our IP up here, but let's check it. Yep that matches, okay. Now if we ping the other one, we are cooking on gas. In fact you know what, because we created the containers with the name flag, we should be able to ping it by name, right on, love it. So just let me summarize right, we created a new network called ps-bridge, behind the scenes it's based on a single Linux bridge or VSwitch called BR whatever it was. But that is a fully working virtual switch running in the Linux kernel. We also told it what IP range to use, then we added a couple of containers to it. Each container got an IP from that subnet we gave it, and we tested they could ping each other and they could. You cannot argue with that. But you know what, we saw that they could ping each other by name as well, so how come? Well since I think about Docker 1. 10 or something, every Docker engine has had this embedded DNS service, meaning any time we create a new container with the name flag, an entry for that container gets added to the DNS server. Then any other container on the same network can ping it by that name. Now that's pretty high-level and there is magic to be understood behind the scenes, but we'll get to that later. For now, we are in business with a simple single host bridge network, but I want to cover one last thing before we move on. If we want containers on a bridge network to be accessible from outside of that network, so maybe from containers on other networks or another host or you know what, even accessible from clients on the other side of the planet. To do that we need to publish a container service on a host port. Now look, it's a bit old school and it's not recommended or used that much these days, but it will only take a second to show. So sticking on this same Docker host here I'm going to start a new container running a simple webserver. So we've named it, we've added it to the ps-bridge network, and this is the important bit, the webserver inside of the container, based on the image that we're giving it, listens on port 8080. So this container that we're going to start is going to run a webserver, listening on port 8080. But it's running on this local only bridge network, with no direct connectivity to the outside world. Well that's where this pflag comes into play. This is basically saying we'll take port 5000 on the host, now it could be any free port right, but we'll have 5000 on the host and we'll have that mapped to port 8080 inside of the container. Now that's effectively saying, anything coming into the host on 5000, send it over to 8080 on the container please. Then we're telling it what image or app to run and if we hit return, abracadabra, and that is running. So now then, this here is the public IP address of this Docker host that I'm on. And it'll be long gone by the time this course is outright, so don't expect this particular IP address to work when you're watching, the important thing is, right, if you're following along with the labs, this is the IP address of your Docker host that you're on. Well you know what, if I open up a browser here, let's whack it in there, we tell it port 5000, and there we go, we're accessing that container on its isolated bridge network from the outside world, pretty cool right? Only not really, I mean look it's okay, but let's not get carried away, it's pretty clunky and it's totally un-scalable. I mean on the scalability point right, no other containers on that host can use port 5000 now that we've laid claim to it. So yeah it works, but it's a bit mah, let's just say there's better to come. Now I think we're done with bridge networks, so they're single host networks, great for maybe testing and mucking about on your laptop, and if you need to use them and give access to services externally, then you map ports. But if you're underwhelmed, have no fear, things are about to get a lot better.

Multi-host Overlay Networking
Multi-host overlay networking. Let me set the stole out right now, this is where it is at, we are officially now entering interesting territory. So being as honest as I reckon I've ever been in my life, this here multi-host networking stuff is probably the single most important thing to happen to Docker networking ever. I mean, what we're about to get into was on every Docker users Christmas list or birthday list or whatever it is that you celebrate with gifts, right. Simple, scalable, and secure multi-host networking was item numero uno on that wish list. I mean we just saw in the previous lesson with bridge networking, which let's not forget was all that we originally had in the native Docker world, but let's be honest, it was a bit naff. Like if you containerized app infrastructures spanned more than a single bridge on a single Docker host, and let's face it, what apps don't? Well if that was you, you were staring into the barrel of a life of pain and misery, because making containers on different hosts talk was like toothache. Well now, it's like, and look I'm probably going to get a bit carried away for a second here, but it's probably up there with kissing the love of your life for the first time. Okay, no, probably not that good, but it's not far off. So you know what, the quick picture before we get into the action. We're looking at three Docker hosts here, on three different networks connected by a rooter. Now it's only three because I hate drawing in PowerPoint, in reality, this stuff scales, but at the highest level, right, with Docker network create a single new network. So a single layer2 network, layer2 broadcast in there, but one that spans multiple hosts. Then containers on any of these hosts, well obviously they get an IP address on this network and then as per networking 101, they can talk to each other, so ping, API, whatever they need, and all directly. So hallelujah right, no more need for port mapping, nating, any of that junk, you literally go Docker network create, give it a few options, then you spin up new containers, and put them on the network. And as if by magic, they can all party together. Heck I don't know, maybe it like kissing your girlfriend or boyfriend. Now obviously behind the scenes there's all this machinery and stuff that makes it happen, but from a user perspective, create a network, add containers, and hey presto, the world is a great place. But let's do a quick walkthrough of some of the behind the scenes machinery that makes it all tick. Alright, this time we're looking at just two Docker hosts here, that's just to keep the picture simple. Oh, okay, so I've called them node 1 and node 2, look I switch between host and node all the time, but it's the same thing, yeah. Two nodes, and both on separate networks connected by a rooter, router. Now this underlying network here or networks, plural, this is what we call the underlay network. We might here it called the IP transport network, but I'm just going to call it underlay for reasons which if they're not already apparent, they will be soon. Well on each host we'd create a sandbox, and then inside of that we'd build a network stack. Now the part of the network stack that we care about for this picture is the bridge called Br0. So on each node we've created a sandbox network, each with a single bridge inside. Next a VXLAN tunnel endpoint gets created and plumed into that bridge. Then from there a VXLAN tunnel gets brought up. Now it's this tunnel that is affectively the overlay network, so underlay down here and then overlay up here. And remember, this overlay is a single layer2 broadcast domain, which is just fancy networking jargon for a network where everyone that's connected to it gets an IP address that can talk directly to each of them without having to cross a router, fabulous. This means that we can have containers on any of these hosts and they call effectively be on the same network. So talking to each of the directly over the tunnel. Now the router down here, it knows nothing about it, I mean it still gets used, but the VXLAN encapsulation of packets in this stuff takes care of all of that transparently. And what we end up with is what the networking nerds out there, and look I love networking nerds by the way, the world would be a closed and lonely place without them, but this is what those boys and girls call layer2 adjacency. Well all that happens next is we create containers and plumb them into this overlay. This happens by Docker creating a virtual Ethernet adaptor inside of the container and virtually cabling it into the bridge here. Okay, if we do the same over on this side as well, what we've effectively got is two containers, on different hosts, plugged into the same network. I'm sure you get it by now, but they both get IPs on the same subnet or broadcast domain and they can talk to each other as if they were side-by-side on the same host or whatever. And it is all thanks to VXLAN. And it's all hidden away as well, so that we can do a handful of insanely simple commands and we're in business. Now look, yeah there's more magic behind the scenes like this mapping of VLANs to VXLAN network IDs and oh well we've got these TCP and UDP ports here that we need, but there's Mac address learning and all that kind of stuff, but we're not going to get into that, I'm kind of de-scoping it. If you need that detail, I've got a book that covers it. But I don't know, for this course, right, this feels like the right amount of theory and you know what, I'm chomping at the bit to get my hands on and show you. So here we are at a terminal again I'm doing the demos here on a Linux X64 instance in Amazon web services. And I'm running Ubuntu 1604, so some of this might be a tad different if you're running something else, but it's Docker, right, nothings going to be so different that you have to get sad about it. The commands are always the same. So first things first, I'm going with two nodes in this demo, on separate networks like this. The IPs and the likes in your config, they're going to be different, but the demo here is two nodes on separate networks that can talk to each other and I've made sure that these ports are open. Okay, first up, I'm going to enable swarm on both nodes, now I'm doing this because swarm modes the future. And you know what, we get a ton of networking goodies just by turning it on. Stuff like the built in KV store and I don't know, encryption of the control plane with out-of-the-box key management. There's more right, but just take it from me, swarm mode is where you want to be at. Anyway, I'm on node one here and it's a clean install. So I'll go docker swarm init and that is it for node one. It's now in swarm mode and it's running as a manager. And look, it even gives us the command we need to run on node two to make it a worker. Now we're not getting into the details of swarm here, I've got other courses for that. All we're doing though is creating a two node swarm with a single manager and a single worker. Okay, so this is node two here, funky blue, let's paste in that command, fabulous. That is our swarm. Now if we hop back to node one, let's just quickly make sure it worked with a quick docker node ls. Okay, two nodes, this one here a manager, and because there's nothing under the manager column here, we know this one's a worker. Well if we now list the networks, alright if we look closely a couple of newbies. This Docker gateway bridge here is a local bridge, right, for getting out to the wider world, a little bit like a default gateway. Then ingress, this is an overlay network used for swarm load balancing. Cool and all, but we want to create our own overlay. So still here on node one we'll go docker network create, we'll use the overlay driver and we'll call it ps-over. Now there's a boatload more options we can throw at the command, but that's enough for us now, barring any typos no I think we should be good. Alright, docker network ls, there it is at the bottom, ps-over, type overlay or driver overlay and it scoped to the swarm. Now remember, this scope it here means it should be available on both nodes, right, multi-host and all. Well let's see, docker network ls, huh, I'm not seeing it, so why not? Well Docker's got this lazy approach to creating networks, you see, with the exception of the node that you actually create the network on, it doesn't actually get created on any other nodes in the swarm, despite it being scoped to the swarm, until. And this is the key right, until we create a container on that node that needs to use it. So in our example as soon as we create a container here, on node two that needs to use that ps-over network, then as if by magic, it's going to appear. Alright that sounds great, but let's see if I'm telling the truth. So to start a container on our new network over there on node two, I'm actually going to create a service. Services are the future of Docker containers. So we'll create one with two replicas replicas just being a fancy swarm term for containers, but you know what, the way that swarm currently works out-of-the-box is it's going to load balance those two containers across each of the two nodes in the swarm. So one will end up here on node one and the other over on node two. And because we'll tell it use our shiny new overlay network, that network will get magic into existence on node two, or it should, and we'll test it. So we go docker service create, we'll call it ps-service, we'll put it on our new ps-over network, I think we'll just have two replicas thanks, remember that's just two containers yeah. And we'll go super simple and tell it just to sleep for a day. It looks okay, but let's check, ps ps, I should have picked a different name. No mind though, now we're not getting into swarm here, but this is just a command that runs a ps against the service we called ps-svc. Basically it's just showing us the tasks or replicas running in that service. And look, as we expected, we've got two, with one of them on each of our two nodes. So if we do a quick inspect of the network, okay, for starters we can see the subnet config here inside the notation with the default gateway. And somewhere on here, yep down here, we've got the VXLAN ID. Now in fact let's park those up here. Okay, now we can also see a container attached, but how come only one? I mean we created the service with two containers, right, well you know what, in this version of Docker, and this will probably change in the future, but in this version you only get to see containers on the host that you logged onto. We're on node one, so we only see the single container or replica on this node. But look, we see its ID and its IP config. Then if we scroll down, yep we can see two pairs down here that are the two nodes, so the manager and the worker that are participating in the swarm. So oops, node down here and then containers further up here. Now you know what, let me pop that IP up here again, I calling it ps-svc. 2, so that's just the way swarm names things, it's service and then task slot number. Okay, now if we scoot over to node two, should we see if that network's finally showing up. There it is, so remember, last time we checked this on this node it wasn't here and we said network's get shared across the swarm in a lazy manner. Like only when there's a container on that node that needs it. Well you know what, we created a new service that dispatched a container workload to this node. So the need for the network arose and the network appeared. And you know what, I'm pretty sure you are following along, but I'm just going to stress, this network here is exactly the same network as the one on node one, the very same layer2 network across both hosts. You know what, if we inspect it okay, well if we look up here, right, we can see the same subnet and VXLAN ID. But if we look at the container here, it's got a different IP. Again right, it is only showing the container that's running on this node. So, that is our overlay network created, it's available on both nodes and both nodes are on different underlay networks. And we've even seen evidence of some of the behind the scenes VXLAN stuff, but does it all work, that's the question. Well let's jump onto the container here on this node and let's see if we can ping near the container back on node one. Okay this it's IP up here, and just as we expected, Houston we have contact. Well things are getting along okay, why don't we recap what we've learned. Docker provides a native driver for multi-host overlay networks, basically it is easy peasy for us to create a single network that spans multiple Docker hosts. And it does not matter if those hosts are on different underlying networks. Well it doesn't matter so long as they're connected through a router and we've got a few specific ports open. We also talked about some of the under the hood stuff like sandboxes, bridges, VXLANs, VTAPs, loads of networking voodoo. But you know what? When we got down to actually building it, honestly it was like a walk in the park on a warm summer afternoon. All of the complexity that pulls it off in the background, is actually hidden in the background by super simple commands. So as we learned in a previous lesson, the networking voodoo is taken care of behind the scenes by the driver. Alright, so we built a network on a couple of hosts that run different networks connected by a router, and we did it with a super simple command. Then we spun up a container on each of the two nodes and we tested if they could talk and they could and we all lived happily ever after. But there's more, coming up next MACVLAN, it's great and no it's nothing to do with Apple Macs.

Joining Existing Networks with the MACVLAN Driver
MACVLAN, and like we just said, no it's nothing to do with Apple Mac, but it is a Linux specific driver. You see, at the time of recording, I'm not aware of Windows having an exact equivalent, now that said though, the Windows L2 bridge driver is similar. And I'm getting a bit ahead of myself here, but the way the Windows L2 bridge driver handles Mac addresses is different to MACVLAN, and I'll try and remember to highlight that when we get to it. Anyway, back to my notes, okay, yes, so far we've looked at bridge networking and overlay networking. And they're good, especially overlay, but they're both massively container eccentric. And what I mean by that is that both of them are great for connecting containers but you know what, neither of them are what you need if you need to connect containers to existing VLANs with existing VMs and existing physical servers. So yeah, while they're magic, especially overlay, they're really for greenfield container only networks. They're not really what you want for brownfield, you know, where you want to start trickle feeding containerization into existing workloads. And let's face it, that situation of starting to modernize and transition your existing apps with containerized parts, it's a pretty common scenario these days. So what we need is a way to plume containers into these existing apps and their networks. Well surprise, surprise, this is where MACVLAN comes in. The entire raise on detra of the MACVLAN driver is just that, to get your containers visible and accessible on your existing networks and VLANs. And right at the top here I'm going to say this, MACVLAN does this by giving containers their very own IP and Mac addresses on your existing networks. And this actually is where MACVLAN diverges from the Windows L2 bridge driver. You see, the Windows L2 bridge driver gives containers their own IP on the existing physical network, but every container on a Windows host connected to an L2 bridge network is going to share a single common Mac address. Not so with MACVLAN, this goes the whole hog and gives every container its own IP and its own Mac, proper first class treatment. Anyway, let's look at a quick example, right. We've got an existing VLAN with some VMs and physicals that are running, I guess, some important business app that you can't fully containerize right now, sound familiar? It should, because it's a pretty common scenario, but there are parts of the app that are just screaming out to be containerized, but you need a way of getting containers onto the existing applications networks. How can you do that? Answer, MACVLAN, end of lesson, next, no just kidding. What we've got here is an existing network with a VM and a physical, okay in the real world these would be part of some important business app and on tiered networks and the likes. But I just haven't got the PowerPoint skills to draw something like that, plus, you know what, I think keeping it simple should hopefully let us come straight to the chase. Now at the highest level, to get containers on the network and shoulder to shoulder with the existing app infrastructure here, we add Docker hosts. These boys and girls are attached to this network here. Then we spin up containers, but as part of that, we create a map VLAN network on each host and we plume the containers into those. But when building the MACVLAN networks, we do a bit of magic that makes the containers part of this network down here at the bottom. And then as far as the app and the network are concerned, all we've got is this. And just in case the picture isn't crystal clear, this is a bunch of containers sitting directly on the existing application network. The network and the app doesn't see any of the complexity that we took away. So you know what, let's bring it back. The network and app does not see or know about any of this Docker host or MACVLAN magic going on, all it sees is this, a bunch of containers that look, feel, and smell just like anything else on the network, magic. Well that's the big picture, let's drill into a bit more of the detail. So we've got a Docker host here attached to a single physical network with three VLANs. Now, let's say we need a container on VLAN 100 down here, okay, we create a new container network with the MACVLAN driver. Now as part of that, at the time of recording okay, the MACVLAN driver needs us to tell it things like subnet details, so like this inside a notation, it needs us to tell it about the gateway that we're going to use ether0 here as the parent interface. And obviously we need to give it a name. Once we're done with that, we create the container and stick it on that MACVLAN network, simple as that. That means when the container comes up, the MACVLAN network gives it an IP and job done, the container is now reachable from other nodes and devices on VLAN 100, cue fireworks and all of that kind of stuff. But hang on, there's two of the VLANs down here and this big patch of empty space inside of my Docker host. I guess that means I'm supposed to fill in the space with containers on those VLANS, alright I'm game for that. To put a container on VLAN 200 same old, same old. We create another MACVLAN network, tell it about the subnet and gateway for the VLAN, tell it to use eth0 again, and we give it a different name. Spin up a container on that MACVLAN network, more fireworks, same again for the last VLAN. Create another MACVLAN network with every required, spin up a container on it. Not really worthy of fireworks anymore, no. Now let just add some more detail here. The MACVLAN driver uses standard Linux sub interfaces and the naming convention that comes with them. So that's where you place a dot after the interface name and anything that comes after that is the VLAN tag from down here. So sitting between the MACVLAN networks in the picture here and eth0 at the bottom, we get three sub interfaces, eth0. 100 for VLAN 100, eth0. 200 for VLAN 200, and there's no trickery going on here, it'll be eth0. 300 for VLAN 300. Now then, it's these sub interfaces that process the 802. 1 q VLAN tags. And yes, we can trunk multiple VLANs over a single cable and a single host interface. And yes again, we can definitely hang more than one container off of each MACVLAN network. So you know what that means we can have this and probably will in the real world. Alright, great and all, but what about a demo right here you ask. Yeah, about that, I mean demos are boring right, they're massively overrated, so let's not bother. Okay, yeah I'm lying, demos are great, but and don't hate me for this, but there is actually no demo for this driver, but the reason is massively important. You see, I've not told you this yet, but MACVLAN requires network cards to be configured in promiscuous mode and if you know anything about the major cloud platforms, like AWS and Azure, you'll know that promiscuous mode just ain't happening on those. So my lab environment in AWS, it just won't let me do a demo on this, so actually don't blame me, blame Amazon. There's evidence right there that it's a poor workman who blames his or her tools. But it is a reality as good as MACVLAN is or isn't, if you're AWS or Azure or any of the other major cloud providers, then it's just not for you. Now of course, the same can also go for your onprem networks as well, it really depends whether your networking or ______ guys allow promiscuous mode. And yeah, I'll be honest, it is a huge downside before you get excited about MACVLAN, go check if your environment allows promiscuous mode nix. Now then, because there's no demo here, I'm tempted to get into a bit more detail and talk about some of the other limitations, but I don't know, that's the kind of thing that changes a lot and I don't think this course should really address things like that. Though I will say this right, when you tell a MACVLAN network about the address space of the VLAN it relates to, you need to give it a range of addresses that aren't already in use on the VLAN. Or that they're not in a pool that's about to be dished out by a DHCP server or the likes. And the reason for that, is the way that MACVLAN currently works, it will faithfully dish out addresses from the network that you tell it about. It is not going to bother checking to see if they're already in use, so if you're not careful, oh dear, you could end up with duplicate IPs and some very unhappy network and application people and we do not want that. Now what? I'm really sad actually, I don't like that it feels like we're leaving this topic on a bit of a downer. So if you're like me and you are feeling a bit sad, pick your chin up next up is IPVLAN and that might just address some of the issues we found here with MACVLAN.

Joining Existing Networks with the IPVLAN Driver
Okay, so joining existing networks with IPVLAN. And this one might be quick, because of in a lot of ways it is the same as MACVLAN. So for starters, it's all about connecting containers to existing VLANs and applications. Also like MACVLAN, there's no bridge involved, so you literally take a parent interface, something like eth0, and you slice it into sub interfaces. So it's a straight connection between the sub interface and the parent with no bridge in the path, making it pretty fast. Well so far we've identical to MACVLAN, now for the difference. The big area where it differs from MACVLAN is the handling of Mac addresses. Now we saw a minute or two ago, that MACVLAN assigns each sub interface its own IP and its own Mac. Well IPVLAN doesn't bother with the Mac, so a lot like what we said with the Windows L2 bridge driver IPVLAN gives each container or sub interface its own IP, but then they all share a common Mac. In fact it's the Mac of the parent interface. And while I get that this might sound a subtle difference, maybe not that significant, the network savvy among you are not doubt all over it, because it's got some pretty far reaching ramifications. For starters, you should have a better chance of IPVLAN working on public cloud platforms, your mileage may vary there though. But on the downside, you might hit some challenges if you're dishing our your IP via DHCP. You see, a lot of DHCP servers are configured to assign IPs based on Mac addresses. And that's just not going to work with IPVLAN. See, the reason being, you're going to have a single Mac address asking for multiple IPs. Well think of it this way, there's going to be a lot of DHCP clients walking up to the DHCP server saying yo give me an IP address and they're all going to be waving the same card with the same Mac address on it. Well guess what? The DHCP servers going to be like, yo friends, this is one IP per Mac address here. Now there are potential ways around this by configuring DHCP to use client IDs instead of Macs, but I'm being honest, right, I've not tried it, and I don't even know if it's possible in Docker at the moment. I mean, as I'm recording this, the IPVLAN driver is the newest kid on the block, I mean it's only just come out of experimental with Docker 1. 13, so it is early days for it, but it'll get better as times goes on. Now wrapping up the theory, IPVLAN, yeah it's a lot like MACVLAN, you create sub interfaces off of parent interfaces, well you know what, Docker takes care of that for you. For us, it's just the regular Docker network create command and then Docker run or Docker service create to spin up containers on it. But yeah, its sub interfaces and parents only this time the subs don't get their own Mac addresses, they all share the parents. Oh and you know what, it's worth noting again that the Linux kernel implementation filters requests from containers to the IP address of the parent interface, basically from inside of a container you can't ping the parent interface, the kernel drops them. You know what, I don't actually think I said that about MACVLAN but it is true about MACVLAN, I just maybe forgot to mention it in the MACVLAN module, let's hope the Pluralsight police aren't watching. Anyway, so it's demo time. Ah yeah, the sweet, sweet view of the terminal. Now, here's all I've setup. A couple of machines with these IPs. Notice that they're both on the same network and VLAN, and guess what, shock horror, my lab is not in AWS. That is pretty shocking for me, because I do pretty much everything in the cloud these days. But the thing is, AWS at the time of recording doesn't like the config I'm going to show you. Now I'm not actually a 100% certain why, my hunch is, that it doesn't much like flooding and learning and stuff, with IPs that it's not managing itself. And you know what, in this config we'll be given the container and IP that AWS is managing. So it doesn't work or at least that's why I think it doesn't work, I mean it could be for a totally different reason. Point being though, it's not working in AWS, so I stepped into my time machine, wound the clock back, searched Google, did I say that out loud? Anyway, I built a lab on HyperV on my laptop, oh and I broke my machine like proper blue screen of death stuff by trying to install a virtual box on it, not good. So I've gone through a bunch of pain to get you guys this demo, you better appreciate it. Anyway, too Linux nodes on a single network that I own and manage. Well we'll spin up a container on node one here and we'll build an IPVLAN network that'll let us put containers on this network down here and then we'll test. Now let's move all of this up top. So we're on node one in my retro on premises lab here, I've gone with a black and white color scheme to show you how retro it is. And I'm going to get straight to a Docker network create, no actually I'm not. Before I do that let me show you its IP config. I wouldn't want you thinking I was lying. Okay, there we go, see how it matches what we've got up top there. Okay, now I'm going to create a network. So the IPVLAN driver is just called IPVLAN and now we need to tell it a bit about the underlay network. So this here is the subnet and the gateway and we'll have it dish out addresses from the bottom of the range. Go on, we'll manually specify the layer 2 option here. Look this option's the default if you leave it out so you don't technically have to put it in, but I don't know it's good to show you and it's always good to be explicit. Well we'll attach it to eth0 as the parent and we'll call it ps-ip, typo check, okay, great. Now let's stick a simple container on it. Park that with a terminal, stick one on network, and we'll keep it running. Okay, let's take a quick look at the network. Alright, here's the config that we gave it, this bit here is our container and then these at the bottom or towards the bottom are the options that we gave it. Well we look like we're in good shape, so quick recap right. Although our container's connected to this locally scoped IPVLAN network, because of the way IPVLAN works, it's actually visible, reachable, the whole shebang okay, on the physical underlay beneath. So shoulder to shoulder as a true network peer with node one and node two. So let's jump onto the container here, quickly verify our IP, okay, and let's see if we can ping node two. Get in, blue screen and legacy lap build all worthwhile. Now let's try this. Now this here, if we check up at the top, is the IP address of node one here. So that's the node that our container's actually running on. What's the deal with this? Okay, so the same behavior is with MACVLAN, the Linux kernel filters out traffic between the container and its parent interface. Apparently it's all about network isolation and closing backdoors and the likes. So it is expected behavior and it's definitely something you'll want to be aware of. But you know what, how's about this? My laptop here's actually on the same network, so my Windows 10 machine that's running the HyperV, that's powering this lab, no probs talking to that. And if we jump over to node two here and try and ping the container back, magic. You know what, even a bit of Windows action here from our laptop. Fabulous. So let me throw this diagram up here, we've got our network down here with our two Linux nodes. Oh that's on my Windows machine, on this node we spin up a new container and we put it on an IPVLAN network that has linked it to the network here at the bottom and it's linked via the eth0 interface and the node. Okay, this affectively made the container a fully initiated member of the network, and everyone could talk or at least everyone except these two guys here. Remember, the kernel filters and drops packets between these two guys for stricter isolation. But yeah, that's IPVLAN, it's a lot like MACVLAN, only it doesn't require promiscuous mode and it doesn't give every container its own Mac. Though despite that, AWS still doesn't like it, though the cloud platforms may or may not be similar, and who knows, even that might change in the future with AWS. So your mileage may vary, but it sure as heck works on my legacy HyperV lab here where I own the network. Alright, I think it's fair as well to say like MACVLAN, it's a little bit harder to configure than let's say, overlay. I mean remember, we need to give it hints about the underlay that it's working with, so we need to manually get the subnet and the gateway stuff right and we really want to make sure that the range of IPs we let it dish out isn't already owned by some DHCP service somewhere, like a ticking time bomb ready to destroy us with duplicate IPs. And I think the last thing to say is, look it's still a bit new to the game and green around the edges. But nothing stands still in the Docker world. And that's us, hopefully that's given you enough to get started with IPVLAN. And you know what, we're about to switch gears a bit, we're done for now with the drivers and their major use cases, we're moving on now to network services. So up next we've got service discovery, which is really cool, but later on the card we've got a bit of routine, we've got a bit of load balancing, and in my opinion this stuff that we're getting into now, is the really interesting stuff.

Network Services
Module Intro
Okay, I've saved some of the best until last, network services. The stuff we're going to cover here at least the way I see it, kind of bridges the gap between the app and the infrastructure. Because sometimes I think we've got the application on one side and then on the other side the infrastructure, but they both need each other, which is sometimes lack simple and scalable ways of bringing them together. I mean what's the point of super shiny networks if it's a pain to run apps on them? Well I think the things we're going to cover here unleash the power of our networks and actually, you know what, the entire Docker platform to our apps. But I'm starting to waffle, get on with it Nig. First up, we're going to look at service discovery, unbelievably important if you're on the micro services train or if you're operating at scale. After that we'll look at port based routine with the routing mesh, routing with the routing. I am one confused block, the less said the better. It's pretty cool stuff though. And then we'll finish up with something that takes that to the next level, so builds on the routing mesh, ah the routine mesh, and makes it more application aware and the likes. And that is the agenda, then we'll wrap everything up at the end with a quick chat about what we've achieved and maybe what to do next.

Service Discovery
Okay, so we're kind of switching focus a bit here, in these last three modules we're going to look at service discovery and a bit of load balancing. Now there still totally networking topics, it's just that rather than being about building networks for particular use cases, we're now about network related stuff that makes building and managing containerized apps way easier. Anyway, service discovery is all about being able to find services. Oh yeah, I'm good. But stick with me here, because you see, modern apps these days are all being built by gluing together a bunch of small services. I don't think there's much doubt that the buzz word over the past few years has been micro services. Well when dealing with micro services, which by the way is just what I said earlier, composing an application from a collection of smaller services. Well when you're doing that, it is pretty darn uberfreaking important for services to be able to locate and find each other. Well thankfully, and I mean oh my goodness thankfully, in the modern Docker world it is automatic. In fact we saw part of it earlier, remember this video here. I love that retro replay sign, but anyway, we created a container called c2, and we pinged it from another container by name, and it just worked. I mean I don't know if you remember, but we hadn't done anything to make that happen, so we hadn't registered the name with DNS or anything, it was just all done for us. And it's amazing, honestly, do not be underwhelmed by that, it is game changing. Well every time we create a service in Docker, or a standalone container for that matter, so long as we create it with the name of the alias flag, then Docker's automatically going to configure name resolution. Now going a little bit behind the scenes it works like this, every container gets a DNS resolver, not a full blown DNS server, just a small resolver that can trap and forward name based queries. It listens on port 53 at 127. 0. 0. 11 on every container. And 53's just the standard DNS port yeah. Well this resolver intercepts all DNS requests from the container and it forwards them to a DNS server service running on the local Docker host. Then the DNS server on the Docker host either resolves the name or if I can't it sends it off to the big wide world where public DNS can have a crack. Now obviously, you can switch off that last bit about going to the wider world, but that's the big picture of how it works. So yeah, Docker's got this built in container and service name resolution _____ important for service discovery. But, service discovery is what we call network scoped, this means that containers can only resolve the names of other containers or services if they're on the same container network, keeps it scalable and secure. So the example on the screen here, this one's going to work because both of the containers are on the same network. But this one doesn't work, see how this service being resolved is on a different network to the container trying to resolve it? That isn't allowed, at least not at the time of recording. Now the way this network scoping works is that the container creating the request creates a socket back to the DNS server on the host. This embedded DNS server then traces that socket back to the network it came from. Once it knows that, it filters requests based on origin network. So yeah, that's how names get resolved to IPs, but there's a bit more still to go on the IP front. You see, every service gets a single virtual IP, a VIP, and this VIP stays with the service for its entire life, it is a hugely monogamous relationship of the very highest fidelity. Services mate with a VIP and they are together for life, no affairs, and no flings with other VIPs, it is like the stuff of fairy tales. Anyway, when you resolve a service name like we just talked about, it gets resolved to the service VIP, which in turn gets load balanced across all the IPs that the individual tasks in the service have, well actually just the healthy ones. Now look, let's just step back for a second here. Each service has one or more tasks, tasks are containers, right, and every task gets an IP. All those IPs get grouped together under a single service wide VIP, meaning, that if this VIP here is hit four times, it's going to round robin requests across all four of these IPs. So resolve the service name from here, get this VIP, from this VIP, load balance requests across all of these. Simple, automatic, effective, really makes things simple. But that's internal load balancing right, we're talking about resolving names and the likes from containers on the same network. What about requests coming from outside, I mean what about requests coming from the other side of the planet? Well there's technologies for that, one operates at the transport layer, layer4 and is called the routine mesh or the routing mesh, depending on whether or not you actually know how to speak English properly or not. And the other one operates higher up the stack at the application layer or layer 7, and at the moment it is called the HTTP routine mesh or HRM if you want to sound like you know what you're talking about. Well let's tackle them one at time and we'll go with the layer 4 routing mesh first.

The Routing Mesh
First introduced in Docker 1. 12, the routing mesh or routing mesh is all about routing and balancing requests coming in from external sources. Now, this can be clients from elsewhere on your internally routed and sanitized corporate network, or it can be web clients hacking from way across the filthy infected lands of the internet. But to have requests come in from the outside, be able to hit any node in the swarm, then be routed and load balanced across all of the tasks in the service, that is the daily workload of the routing mesh. Now maybe you let me draw a picture here, because I just love PowerPoint so much, can't freaking stand the product, but never mind. So service up here, running whatever it's running web _____ 10 maybe, then four nodes in the swarm, but see how there's only three tasks in the service, node four on the end here is not running a task for the service. But it's still part of the swarm, right, part of the cluster. Oh and look, the service is exposed over port 5000, okay fabulous. Now then, down here we've got a client issuing requests to the URL of the service on that port 5000. And we've got things rigged up so that everything comes in through a set of load balances down here that know all about all four nodes in the swarm. Well the load balances do what they do best and they balance incoming requests across all the nodes in the swarm. Only well what happens to the requests that end up hitting node four here? I mean it's not running a task from the service, so what, does the request just get dropped and lost or something? I hope not, because that wouldn't be good. Well obviously no it doesn't, you see the routing mesh that we're talking about here this is what it was built for, it lives and breathes for this problem. But we're still a bit high-level here and I want to get into a bit more detail before we show it in action. Though I do think that's essentially our problem statement, what happens to requests that hit nodes not running a task from the service? Well let's wipe this clean here and build it up from the ground. We create a service and apologies if it seems like I'm repeating myself here. I'm actually doing it on purpose in the hope that it might help this stuff sink in. So we create a service, oh obviously we'll need to have enabled swarm mode and built a swarm, so managers and workers, yeah. And I think we saw it earlier, initializing a swarm creates a couple of new networks. One of which is an overlay network called ingress and it's scoped to the swarm and instantly made available to every manager and worker. And take note of it, because it is vital to the routing mesh we're going to see. Anyway, then we create a service, three tasks again like we just saw, and we publish the service on a port. We said 5000, so let's just stick with that. Now then, this act of publishing a service, on a port, actually publishes the port on the ingress network. And of course the ingress network is on every node in the swarm, every node in the swarm is publishing it. So in our picture every node in the swarm is publishing our service on port 5000, even node four here that's not even running a task for the service. Anyway, when requests come into the swarm on that port, you know what, let's say to node four here that's not running a task for the service, it still gets the request, it does the name resolution that we've talked about earlier, so resolves the service to its VIP. And then it forwards the request to a healthy task using VIP based load balancing and the ingress network. Good stuff, well that's the theory, now let's see it. Okay, we're on node one and as we can see the lab here is a four node lab like we've just seen, but it's four squeaky clean Docker hosts. Now look I know we did this in an earlier lesson, but because I've cleaned the lab down, we've got to do it again. So we'll initialize a new swarm, we're on node one here, so that'll make this one a manager. And then let's grab this command here and we'll go and make the other three nodes workers. Now look, I don't care about swarm manager high availability and the likes here, it's not a swarm course, but here we go. Now I've gone with a classic green screen look here, I like that, okay well that's a worker now, node there here, okay a no pretty shocking colors, but this isn't owed to Sunderland football club, red and white, the greatest football team in the world, forget your Man United's and your Barcelona's. Okay, now for node four this one's my kids, they love purple, what can I say. Anyway look, that's our swarm build, easy peasy. Now while we're on node four here and I'm glad we're not in node three, those colors were a bit hard on the old eyes. But while we're here, okay see how the gateway and the ingress networks about both present? Now I wanted to point out the ingress network, because in a previous lesson we talked about the lazy approach to creating overlay networks. And look, ingress here is an overlay, but because it's a system network of sorts, rather than a user defined network, it is instantly available on all nodes, workers and managers. So I just thought I'd clarify that, it's only user defined overlays that are lazy created. In fact, let's inspect ingress. Okay, look I know we've seen this a million times by now, but look at this container, well sort of container. Again, this ingress sandbox or ingress endpoint, it is a system container. So a little bit like the ingress network, it's used by swarm for routing mesh operations. So it exists on every node. Well actually look, the container is a different container on each node, but the ingress network itself, that's a single overlay spanning all nodes in the swarm. Isn't it cool how we've got the skills and the tools to poke around like this now, you're getting pretty advanced. Well actually it's this ingress endpoint that sits on every node that lets us handle requests on nodes that aren't running a task for the service. But you know what, speaking of the service, let's go and deploy it. Well first up we'll create an overlay to put it on, call it overnet, great. I still smile how easy this stuff is, I love it, anyway, the service right. We're calling it web, whatever right, that's not important, but this bit is. We're publishing it on port 5000 across every node in the swarm. Then we're telling the swarm to map request that it gets on that port, on any of those nodes remember, map them through to port 8080 inside the service. Okay, well we're saying we'll have three replica tasks and we'll put it on our overnet network, then this bit here's the image. Okay, now let me compress space and time here so we don't wait around for the image to download in the background. And there we are, three of three tasks running. Now then, I don't know, let's check the ingress network again. Oh hello, that's interesting, we've got a task from our web service showing up, see its name here, that's from our web service, but we never attached it to the ingress network, just the overnet right, so what's at play here? Well every time you create a service with a published port, every task or container in that service also gets attached to the ingress network for routing and load balancing. So we've got our rig setup, four nodes in the swarm and three tasks in the service, let's ps the service here actually to find out which node doesn't have a task, oh four is not there. Hmm, I wonder what would happen if we hit node four on port 5000 where the web browser? Well I don't wonder, we already know right? We can hit any node in the swarm and the routing mesh, thanks to the ingress network and the likes, is going to make sure that the request reaches our service. So let's just quickly throw up the lab diagram here, I want the IP address of node four, alright we've have that and we'll put it up here. And port 5000, say the secret magic word, boom there's our service, pretty cool yeah. I mean just as a reminder, I know you this okay, but we hit node four, which does not have a task running for the service, but guess what, thank you routing mesh, we still reached our service anyway. And that ladies and gents is the routing mesh, port based routing, layer 4 or the transport layer if that's your thing, but a very useful network related tool for swarm services. But as cool as it is, it's limited. Well in our next and final lesson we're going to uncover that limitation and show you a technology that solves it.

The HTTP Routing Mesh
Last lesson. Quick raincheck, if you've been following from the start, do you feel it? Just close your eyes and think for a second. Can you feel it now? That Docker networking knowledge that wasn't there before, feels good right? Well bask in it, because knowledge is power and you are well on your way to being half decent at Docker networking. Anyway, the HTTP routing mesh, HRM, I think the name implies it, but this takes the foundations of the port based routing mesh and builds on it. Because you see, a major limitation of the port based routing mesh was that only a single service on any swarm cluster can listen on a particular network port. So put another way right, you can't have two services listening on the same port. As well though, the standard routing mesh operates at layer 4, the transport layer. So it has no awareness of things going on higher up in the stack like layer 7. Well the HRM fills both of these gaps, but before we go any further, at the time of recording, and I've got no control over whether or not this changes in the future, but the HRM requires Docker universal control plane or Docker datacenter. Now this is basically Docker Inc. 's native management GUI but it's a commercial offering that you obviously pay for. So if you want to fly with the HRM, you're going to need UCP or Datacenter. Now if you hunt around on docker. com, you'll find it and you'll probably even be able to grab a demo license. And you know what, there's even a simple installation for AWS and probably more, but with that right, your best bet is probably Google. Because as sure as bears do their business in the woods, the link would change the second I publish this course. So if you're looking for that, just Google it. Also though, at the time of publishing, HRM is uber new, so definitely expect it to morph and improve at a rapid clip. I'd expect things like certainly HTTPS at some point, but a bunch more. And definitely yeah, the UI and the features are going to change, but what we'll give you here are the fundamentals, so you should be able to roll with any changes that come in the future. Now I'll show you in a second, but let's cover off the theory first as usual. The first thing that we do is we enable the HRM feature in UCP. When you do this you've got to tell it which port to publish, we'll be going with port 80. Now this active enabling HRM creates a new overlay network called UCPHRM and a new global service also called UCPHRM. When you've done that, you create your service, but you do a couple of special things with it, special things, it sounds a bit creepy. Anyway, you need to attach your service to the UCPHRM network. And you need to give it a special label, oh there's that special again. Well we'll see it in a second, but the label contains a DNS host name that you want associating with your service. And that's effectively your config done. So on the picture here we've got a new service called Gt350 and it is associated with mustang. nigelpoulton. com and it's attached to the UCPHRM network. Now we'll see in a minute but thanks to this UCPHRM network and the UCPHRM service, the GT350 service is actually published on port 80. Now then, and here's the beauty of the HRM, we can add more services all on this swarm cluster on this same port 80, but responding on different URLs. Now that's exactly what the standard routing mesh couldn't do, multiple services, on the same port, and playing at layer 7 with HTTP headers. Great, so how about a second service called hellcat listening on challenger. nigelpoulton. com? Alright then, what we've also got here is this UCPHRM service listening on port 80. That's what happens, remember, when you enable the HRM on port 80. Then each time you create a new service with this label here, the UCPHRM service gets effectively a key value pair mapping the label to the service. In our example, we've got the GT350 service mapped to requests for mustang. nigelpoulton. com and the hellcat service mapped to requests for challenger. nigelpoulton. com. This means traffic coming into the swarm on port 80 to either of those URLs is going to get routed to the UCPHRM service. Remember, that's bound to port 80 across the swarm, so the UCPHRM service is going to get the traffic, and it's going to crack open the HTTP header and take a peek at the host field. If it sees mustang. nigelpoulton. com look at the mapping here, it's going to forward the request to the VIP of the GT350 service. If it's challenger. nigelpoulton. com, it'll route it the VIP of the hellcat service, and if it's neither it's not going to do anything. Should we see it in action? Okay, so this is UCP or Docker datacenter. Well if we remember, the first thing to do was enable the HRM, so that's up here, and then routing mesh. Then we just hit this checkbox, tell it a port okay it's already on port 80, so we'll keep that, and we go up date. And that is it, that is HRM configured. Now we said that enabling this created a couple of system bits and pieces for us, well if we come to Resources here, then if we click on Show system services, this is the UCPHRM service we talked about. And did I say it was global? I can't remember actually, but I definitely thought it was, hmm obviously not though, it's replicated. No worries though, that swarm detail and we don't really care about it, the point is it's there. Now if we click it here and somewhere on here, okay right at the bottom, we can see it's published on port 80 like we asked for. Cool and all, but time waits for no one and yours is precious. So if we scoot over here and hit Networks, okay there it is, UCPHRM. We can see its overlay and scope to the swarm. So with all of that in place we want to go to play our own services. So first up we'll go with GT350. We'll use this image, I'm happy only to have one replica running, I don't think I care about anything on here, nope. But here on Resources we've got a publisher port. So this one here, this is the port that the container or the image is configured to listen on, you definitely got to get that right. But this public one here, it's a bit annoying actually and I expect it to be fixed really soon, like probably before you even watch. But right now you need to specify a public port, even though it's redundant, because we're going to expose it over port 80 via the HRM. So I don't know, in future, I think this'll go away and the updated GUI won't ask you, but I've got to do it, so I'll go 5000 or whatever. Well let's attach it to the UCPHRM network, we've definitely got to do that and then jump onto the last page. Now this label bit here is mucho important, this is how we tell the service that it's associated with mustang. nigelpoulton. com or whatever it is in your environment. Well the label's got to be this, then in the value box here we'd go 8080, that's just because that's what our image or container's going to listen on, then HTTP mustang. nigelpoulton. com. It'll be different in yours and by the time you're watching this, those DNS entries and stuff that I'm using here, they'll be long gone. Okay, well that's good, now for the hellcat. Same again here, call it hellcat, give it a different image, we'll just have the one replica again. See we've got five nodes in the swarm, and this way we know that four of them definitely have no task for the service, but HRM builds on the port routing mesh, so any node in the swarm can handle requests for the service. That's straight over to Resources, publish the port again, container's still 8080, this doesn't matter right. Network will be UCPHRM, then come over here and let's lash that label in again, only this time we're obviously going with challenger. Alright then, there we go, we've got two services, both of them effectively okay, published on port 80 via the UCPHRM service and network. One of them is labeled the mustang. nigelpoulton. com and the other for challenger. nigelpoulton. com. Now I've got DNS and load balancing in the background, making sure that these names resolve to the cluster. So here's a browser and let's head over to mustang first. No need for a port okay the webs 80 by default, oh yeah that is a GT350. Now to be honest I added those sound effects in post-production, sorry I'm sad. Well you know what, let's crack open another tab and we'll head over this time for challenger. That's what I'm talking about, that is a hellcat, oh. So just a quick recap right. Two services on the same cluster and both of them, thanks to HRM, effectively published on port 80, but independently routed based on the host field of the HTTP header requests. Very powerful okay, and you know what pretty new at the time of recording, so expect change, the GUI will change, no doubt we'll be seeing HTTPS sooner rather than later, and who knows what else. But you know what, I reckon you've got enough to know what it is and to get started with it. And that, ladies and gents, I am sorry to say, is the end of the fun stuff. Now if you've got a couple of more minutes you can spare, we'll have a quick recap and maybe talk about where you can take your Docker journey next.

What Next?
That's it, we're done, you're officially better at Docker networking than you were when you started or at least you better be. Seriously though, thanks for watching the course, I genuinely hope it was everything that you hoped it would be, let me know either way though. Now I want to recap what I think of the real big ticket takeaways from the course before suggesting where you might want to take your Docker journey next. So I think we learned that networking is a fastly maturing part of the Docker ecosystem, gone are the days when Docker networking was rubbish, we are out of the dark ages and living in an age where it's pretty decent, the best it's ever been actually, but the future's bright as well. We should expect Docker networking to get better, more flexible, more features, more secure, more easy, or easier I guess, not that it's hard now though, but things are going to get better across the board. We said at the start, that the container network model the CNM is the design dock behind Docker networking. LIB network is the conical implementation of the CNM but there is a rival spec out there cutting its way through the Kubernetes world, the CNI. We also said though that LIB network gives us the common foundation for Docker networking, and that all the weird and wonderful different network types and scenarios and the detail is implemented by the drivers. So drivers plug into LIB network and expand Docker networking. The native drivers that ship out-of-the-box with Docker are called local drivers and the ones contributed by the ecosystem are called remote drivers. Don't ask me why, that's just the way it is. We also said that the future of Docker and most of the goodness of Docker networking is tied at the hip to Docker swarm mode. The swarm key value store gets leveraged by networking the key management is leveraged, so much of what is Docker networking today is built on and augmented by the stuff we get when we put Docker into swarm mode. So if you're going with Docker, then swarm mode is where it's at. We also mentioned service discovery and load balancing, name resolution, loads of things that hopefully you understand a fair bit about now. And I hope your appetite is wet for more. But that's where I think you should go next, sure check out the CNI and Kubernetes, but if you're going down the Docker route, then dig into and play about with the stuff that we've learned here, especially service discovery and load balancing. And as always, there really is no substitute for getting your hands dirty. And that's it, thanks again for watching the course, this here is me, I'd love to hear from you. And if you do ping me, I'll try and respond, but look I'm like you guys, right, I'm busy, I've got a family, all that kind of stuff. So if I don't respond don't be sad, it's probably not personal. It's more likely that I'm just shocking at keeping on top of my life and workload. Thanks again though and go change the world.
