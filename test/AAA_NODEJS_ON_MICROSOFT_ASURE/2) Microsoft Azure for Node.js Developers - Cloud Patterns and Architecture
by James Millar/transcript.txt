Designing applications in the cloud requires a different style of thinking and deep knowledge of cloud infrastructure and services. In this course, Microsoft Azure for Node.js Developers - Cloud Patterns and Architecture, you'll learn how various Azure services can help build scalable and resilient applications. First, you'll explore how to use specific Azure resources to add redundancy, fail-over, and load balancing to a system. Next, you'll support scaling with partitioning and caching. Finally, you'll discover Content Delivery Networks and API management. When you're finished with this course, you'll have a good understanding of the multiple resources that will help you as you build scalable Node applications in Microsoft Azure.

Course Overview
[Autogenerated] Hi, everyone. My name is James Miller. And welcome to my course Marks Dr Shirt for no Js developers cloud patterns in architecture. I'm a freelance software developer from Bristol. I helped companies built cloud systems that operate at a global scale. In this course, we're gonna look at techniques and patents for designing and building highly scalable, highly resilient cloud applications using marks off. Did you? Some of the major topics that we're gonna cover include adding redundancy and fail over to our systems patents for resilience, including re tries and seek your s data partitioning in cashing on using content delivery networks and a p I management to improve scalability. By the end of this course, you'll have covered multiple patterns of resources. They're gonna help you build highly resilient and scaleable cloud applications in a year before beginning this course. You should be familiar with no jazz on the general concept behind cloud computing. I hope you'll join me on this journey to Ellen had to build scalable and resilient cloud applications with the mark Soft azure for no jazz cloud patterns in architecture course that parasite

Architecture in the Cloud
Introduction
[Autogenerated] Hi, My name is James Miller. And welcome to Microsoft Azure for know Js developers, cloud patterns and architecture. In this course, we're gonna look at the techniques and patterns that you could use for designing and building scalable, resilient cloud applications on how the features and most often here, can help us. Designing applications in the cloud requires an in depth knowledge of cloud infrastructure, and service is so we'll start off by having a tour of azure, and looking at the range of service is available. You'll then explore specific is your resources for adding redundancy, fail over or load balancing to a system. You'll discover how to support scaling with partitioning and cashing on, how to implement content delivery networks and a P I management. When you're finished with this course, you'll have covered many different resources that will help you to build scalable note applications in Microsoft Azure. It's great to have you here, so let's get started in order to build a successful scalable application in the cloud, there are two things we need to take care of. Firstly, we need to have a good understanding of our unique business requirements. These are the rules and demands of the application itself. We also need to make the right development on architectural decisions now. In order to do that, we need a thorough understanding of what is available in the cloud already what resources and service has already exist that can help us to build our application. So in this course, we're not just looking at myself to just simply as a place to host a virtual machine or store some files. What we want to do is make the most of what's already available to us in the cloud, and this is where it gets interesting is there is a lot available on these, your platform. So we're gonna begin by exploring some of the features and service is in marks off Dasha so we can see what's available and how it fits into the architectural designs of the different types of systems that we'd like to build, such as Web applications, AP, eyes and Internet of things and big data applications. We're starting off with a high level overview off. What's inside is year on that allows to drill into the details on the patterns that we can use as we explore different scenarios

Azure Services
[Autogenerated] When you first start working with his, you're one of the first things that you're gonna notice is the sheer number of options made available to you. And to start with, this could be a little daunting on often, People don't quite know where to start. You go to create a new resort in these, your portal your present a bit of back 20 different categories. Everything from virtual machines and databases to more specialized service is dealing with mixed reality and artificial intelligence. Each category also has a dizzying range of sub options. This is just the tip of the iceberg. There are thousands of different service's and offerings, not just from myself but also from third parties. Now, with so many technologies service's and features available, it can be challenging to know where to start from an architectural perspective, all of the service's within Asia for broadly into one of three main categories. Firstly, we have tea service. Is it deal with infrastructure? These are the fundamental features of any cloud platform include things like access to virtual hardware stores, surfaces and virtual networking. But it sure was in its infancy. These infrastructure surfaces were pretty much all that was available, and they form the backbone of almost every other service. These infrastructures service is just like a virtual data center. Someone else is providing the physical servers and network switches, but we're still responsible for the configuration and management of the infrastructure. We install the operating system that database servers keep everything patched and up to date. This is what's known as infrastructure is a service, and it gives us almost unlimited control of flexibility but at the cost of increased administrative burden. Now, Bill, on top of the infrastructure service's are the platform service's or more commonly known as platform as a service platform was a service. Is one level of abstraction higher on is focused on function. First on, what do we want to build websites? AP Eyes Mobile app. Back End's large scale analytics doing a database storage or content delivery, perhaps some distributed cashing or artificial intelligence Service's under the hood. It's the same infrastructure, but the implementation is hidden from you. This allows you to focus on what your application does. One worry less about the maintenance and management of your infrastructure, and finally we have the assure Security and Management Service is now. These include things like the portal itself, along with support tooling workers, your CLI and active directory, or all the authentication in cryptographic service's. Now each of these categories can be broken down still further into more specific service is, for example, inside the infrastructure category. There's compute. This is where you can set up virtual machines. Running windows or Linux on virtual machines. Allow you to place almost anything into the cloud. Typically used these when you're doing a lift and shift, but you take systems you currently have running on your own infrastructure on move their system in their entirety. In Tunisia Virtual machine Now most applications were on some form of storage and then is your. We have a lot of options here, and the choices you make here will depend on the amount of data you need to store the type of data and have volatile. That data is how often, you know access the data. How often does it change? We need equipment afar system storage than that's available. Their storage options for blob data, storage for table storage and no SQL tables is even special storage for archive data that you know you need to store well needing frequent access to churches, logs and backups. Even if your application doesn't need to store application data, then you're still gonna want to sort things like logs and diagnostic information. Now there's some great course, some parasite that go into depth on the creation and use of virtual machines. On Different is your storage options. All you need to know for the time being is that there's a whole range of storage options available for different stories situations. Another key part via infrastructure is networking. This includes features like virtual networks to create private networks inside of Azure and to manage the flow of traffic and provide network isolation and security. We can connect, argue infrastructure without on premise infrastructure by using a VPN. Gateway in this course will explore some of the load balancing features of azure on Dive into using application Gateway in more detail. Let me start looking at the platform. Service's will also have resources. Within this compute category, we'll have service fabric, which will allow us to write in deploying Micro service is on function apse, where we can write and deploy individual functions in a lightweight in highly scalable way. There are batch service is if we need to invoke high performance backs. Processing observes is one of the most important of flexible features in Asia. It's a manage platform where all of the infrastructure is abstracted away of Angus have focused on the application we want to build. Everything with AP Service is all ready to go out off the box. We don't need to worry about the configuration and maintenance of any of the online infrastructure. It's incredibly flexible and works with a wide range of languages and technologies. Now I know what you're thinking. You can't provide a single development platform that works for just anyone. You may want to build a weapon using node on Lennox. Well, perhaps you want to build rest a P I using dot net on windows. What if you have a Ruby Web application you want to deploy using doc containers? Thankfully, all that and more could be done with AP Service, because, like most things in Azure, there isn't just one APP service. You can choose between a Windows or Linux operating system. You can choose to deploy a weapon, a container. You could have different development framework such as dot net or even PHP, and you can integrate with source control. I'm buildin released. All such as is your devils. You can pick and choose between. Different as your APP service is depending on if you're building a public facing Web application, we're building a Web service with an http based A p I natural. Building a back end of your mobile application because you can provide a complete manage platform targeted at that scenario on, as with all up service, is is your is managing the infrastructure so you can focus on the application development regardless of our data requirements. Is your has us covered? There's as your sequel database, and it's your cosmos __. They're also options for using my sequel and post Greece equal dates basis, but in this course will be looking at reckless cash mostly had to use it to catch a database query for a Web application. There's enterprise integration options like is your service bus, But will you delay to running the course too deep? Couple two systems and communicate using message kids we have media and content delivery service is to push content like videos and images out of the edges of the Internet so that we can deliver content facet to our customers. We'll be looking at content delivery networks later in the course for Developers Service's We've got things like is your dev ops, which not only officers say source code repositories but also a complete building delivery pipeline. You're so Vesti case and libraries to help us automate everything that we need to do. It is your, including adding deep diagnostics and locking abilities right into our laps with application insights, analytics and monitoring an important part of your platform. We can stop her. Duke Clusters with HD Insight Turn machine learning algorithms into a Web service is on move large volumes of data with is your data factory. Most of these technologies will be covered in data focus courses elsewhere on plural site. Finally, there are the security and management pieces of Asia. We're not really gonna delve too much into security on this course. We'll talk a little bit about authentication, but again, there are some excellent courses on floors like the focus, more on security authentication and using options like is your key vote to protect our application secrets. So the takeaway here is that as your office is, a collection of specific name service is to pick and choose from one building our applications. However, when developing our application and thinking about architecture, we have some decisions to make which service is do we need to use? It's probably worth learning a little more about these features so that we can either discount, and I think that's not needed or shortness the features that we need to explore a little deeper.

Key Deployment Scenarios
[Autogenerated] I should become familiar with service is offered. Bajor will soon realize that there are many ways to achieve the same thing. Let's imagine for a moment that you built a fabulous know Js application or a P I. You've developed it, tested it locally, and now you're ready to deploy to azure. Now there are multiple ways you could deploy this application. Let's go through three of the main options. Look at how you decide between them. Firstly, you could choose to configure a new virtual machine. This gives you complete control of the environment. You control the operating system configuration. You can install any custom software that you need and use any Web surfer, technology, package or module that you want. But with all of that, freedom comes responsibility. If you go the virtual machine room not only to consider what I will require to secure it, maintain it, patch it and update it. You'll need to configure virtual networking, configure the firewall and deal with security and access issues. Now you could choose to go with a more managed approach and choose one of the options from the APP service platform in the Now with that, you won't have as much control in the sense that you won't be able to just remote desktop into it, and it behaviors if you own the server. You also won't have to worry about patching the operating system, and I'll have built in features for managing them, deploying APs but having deployment slots as an easy way to switch between production and staging environments. Another option is is your service Fabric service fabric is optimized for micro service's, so it's a great option if your architecture is less for monolithic single application form or a collection of smaller, discreet independent service is now service fabric is actually the underlying technology for a number of visual resources. So if you want to build on top of the same platform as is your sequel database in Cosmos, __ and even other service is like Cortana, then you want to take a serious look at service fabric. Now, only you will be able to make the decision over which way to deploy your application, because it'll depend on the needs and requirements of your project. But if your system needs to install, for example, a custom third party component, then you might need to deploy at least part of your system on a virtual machine where you can install that custom component. But if you're building a straightforward Web application to host your company's website, then AP Service Platform will almost certainly be a better choice. Still gives you a great deal of power, but not the full control of a virtual machine, which will make managing and maintaining much simpler you're building the back end from multiple game in a collection of state for micro service's might work better, and that's a scenario the service fabric supports very well. Now for architect CEOs and other technical decision makers, you need at least a basic understanding of all the main resources in Azure so you can guide your system designed towards the platform or the service is best fit. Now what we're going to do in the rest of this module is walk through some sample system designs on discuss the various is your features that we could use to achieve them

Web Applications with App Services
[Autogenerated] So let's start off by talking briefly about service. Now it is your APP. Service isn't just a single product. In fact, there are several service is that are built around up service, such as mobile app, logic cap and Web app for containers. Now there are a lot of different types of application that can make use of an APP service, and each APP service type is configured slightly differently to give the most useful options and starting points for making that type of app. Regardless of the type of APP service you choose, you're gonna benefit from not having to directly manage a virtual machine. We won't need to do things like updating or patting the operating system, for instance. Now the default for AP Service is to run using the Windows operating system. However, you can choose a Lennox operating system instead, and you're not restricted to the marks of technology stack, either his full support for node ruby, PHP and Java applications. Now the choice of operating system that we make doesn't determine if we can run a note application or not. Thankfully, we can run a note application on either operating system. What will be affected by the choice of operating system. Those things like authentication options. Now you can see hopefully that there are several advantages to using APP. Service is over virtual machine, but we've already seen one thing we don't have to worry about is updating the operating system. But that's a minor benefit what we're really Maurin trusted in the additional features that we get with an APP service. So let's begin with that idea that we need to deploy a Web application or Web AP I service on we're looking at using as your as a platform is a service rather than just basic infrastructure. So we're gonna have our users using Web browsers on desktops, laptops and mobile devices. And they're all communicating over http and https with our application now. In this scenario, the most likely option to look at isn't is your abscess know what major benefit of the APP service is its scalability? Using an APP service plan, we can run our application of scale, and it's trivial to increase the amount of resources available, touches adding more ram or increasing the number of available cause I could scale horizontally, scaling out binding or removing instances and if I have two or more instances available, I get automatic load balancing. The great thing is, I can set up rules for sure to automatically do this toe automatically. Scale the APP service plan based on things like CPU percentage. Have it. Let's not pretend that there are limitations like virtual machines. If you have a nap service, you can't use remote desktop to connect to an instance directly and inspect the environment. But there's still plenty of diagnostic tools available. There's an interactive console. There are metrics, debugging tools, everything you typically need. One of the great benefits of APP service is well is deployment slots. We can define multiple versions of an application and then swap those versions when needed. For example, I can deploy the application to a staging slot. From there, I could do some testing to make sure the application and the new features are behaving is expected and then at the touch of a button, or even at the execution of a script to just swap staging into production. You're smart enough to make sure that nobody experiences any downtime to, as any requests that currently in progress will continue to the old environment until they're complete. Last new request will be sent to the new production version. Now, in addition to the APP service and the APP service Plan Resources, most Web applications are going to need some storage so we can set up a storage account for our application, if for no other reason than to have a place to store log and diagnostics files. The storage account will give us access to blob storage, which we could use to store users upload or static content for our app with an APP service. You do have access to local file system, but blob storage office. Some advantages because you could sit up to replicate to another is your region. We could create shared access signatures, which would allow us to grant access to a blob or blob container for specific period of time. On with specific permissions later will see a use case for putting your application static content into blob storage. This is typically media content like images or videos, but it could also be static files like JavaScript and CSS files that your site users. By placing it into a blob storage, you'll be able to push that content into an azure cdn, a content delivery network, and then deliver that to your users with a lot less Leighton See, regardless of where they are in the world now, typically, most Web applications are gonna need some kind of database. Perhaps this is an azure sequel. Databases. Perhaps it's Cosmos D B, maybe even both. One database optimized for relational data, another off twice for storing documents. But as without service, these are being provided. There's a platform is a service you don't need to worry about patches and updates. How the databases implemented under the hood, as is your manages all of these details for you. All you need to do is size the database appropriately for the applications expected use so it can have the performance characteristics that you need. So even with a straightforward Web application or a P I, we can see already how we could use several. Different is your resources map service up service plan the storage account, a database account, a seedy end with his year weaken group these resources together in what we call a resource group. This is a logical container for all of those related resources away to group them and manage the Imagine if we wanted to create a copy of our environment, perhaps for a dedicated customer to pull form some form of low testing. But a resource groups help you to simplify this kind of scenario. The resource group on all of the resources that it contains can all be scripted, defining code in what's called an armed template. And there's some great courses implore site that explores in more detail.

Authentication and Authorization
[Autogenerated] now, typically, when we host a Web application or a P I well, you're gonna want to make it entirely public. But well, often want to authenticate, authorize our users or authenticate the requests being sent to our AP eyes on. In many cases, we do both. We have some parts of it public in some parts of it protected. Now, when we're designing this element of our architecture, we don't start off by thinking, How do I do this in Azure? All we need to remember is that whatever authentications scenario you would implement if you physically own the server is also possible imager, and not just with a virtual machine, but when using an APP service as well. This isn't a decision that's driven by azure. It's driven by you. But as you can make it easier for any authentication situation, we first need to decide if we're planning to do everything ourselves on roller and solution, or do we want some kind of help? By that I mean doing on a fully custom built authentication system where were responsible for creating our own user. Registration pages are a log in page are in password recovery page, where we set up a database to store all the data and write our own routines for password hashing. This is a rather old school approach, and while you certainly can do it, it doesn't really have anything to do with this year. These days, many of us don't want to manage local accounts and passwords. Many users don't want that either. They want to sign in using the count they've already created, like their Facebook, Google or Twitter account. We'll use protocols o'War, Thor Open I D Connect. And in Asia we can support all of these options and even combinations of them. If you create Windows based APP service resource, it already has several authentication options built in. This is not an unusual thing for a Web app to need, so it's all part of the platform. By the fall, anonymous access is allowed for your website or a P I. You can switch the authentication mode that it supports. Is your active directory Facebook, Google Twitter. Microsoft. Now to use these will typically need to register our service with that particular provider like Twitter, for example. Unfortunately, we're not just limited to these providers. If we're using node there are middleware composed that we can use touches. The incredibly popular passport, which we could certainly use in a year. Passports and extremely flexible includes hundreds of individual modules that you can download for specific authentication situations, not using length in or steam. You also find azure specific passport strategies like Award and is your active directory. And if you're in a corporate situation way authenticating in an existing set of known business users, you might look at interacting with active directory if you're already using. Most off service is like office 365 you might already have on on premise active directory synchronized with his your active directory in the cloud. In that scenario, even with using knowed, your users can log into application using the same credentials that you would use to log in to office 365 or any other on premise service. In more public scenarios, you'll probably want to do some experiments on some prototypes to see what's going to work best of your environment. What your users are prepared to. D'oh! Rest assured, though whatever you decide is your concern port it

Scalable Web Services
[Autogenerated] Okay, so we've now got a good sense of how we can use APP service. And we've talked a little bit about security. Let's turn our attention now to a more complete design for a website or Web, a p I. That takes advantage of some of the additional as your service is available. Now, the two key things I want you to take away from this course. Ah, firstly, howto build an application that is resilient. And secondly, I had to build an application that can scale under load. So if we're designing a resilient and scalable solution, then we're gonna start off with our users. These could be actual people. Using our sight with a Web browser or perhaps other service is making a P. I. Course, we're going to expect a lot of users, though perhaps in different geographical regions. We're also going to assume that we're gonna have some form of authentication for our users. For a P, I course, So is your APP. Service is still the ideal solution for this. We can have a nap service plan with multiple instances running. Then we can set up auto scale rules toe handle heavy loads. Fortunately, notice something that already scales extremely well with a lot of users. But if Iraq requires CPU intensive tasks or long growing processes, take those images, video processing or encryption or has to sort large amounts of data. We could still be concerned about scaling and staying available and responsive, so we want to offload some of that heavier processing. Fortunately, there are a couple of different ways we can do this. One option is to set up in his your storage account and make use of its message cues. Now there are two times of Q in his year storage cues and service bus queues. We'll talk about the difference a little later. Cues offer a great way to decouple systems separating the main Web application, many computational intensive back in processing that we might need to do so that long running tasks won't block another webpage or service school waiting for that processing to happen. So instead of doing the processing in the Web app, we kick it off by dropping a message into the queue. Now the Q itself doesn't do the processing that's taken care by other components. Once the message is placed on a Q we need something to handle this background processing a client application that's going to retrieve the message from the Q and process it now. This could be another service that's deployed to service fabric or APP service, but it could also be in Is your function or a Web job? These technologies are great because they could be triggered automatically when a new message appears in a queue. They can also be triggered when new blobs have put into storage containers. Well, they could be configured to ask you on a schedule whether that shed was once a day or once every 30 seconds. One of the benefits of his your functions is you only get charged for when the code is executed. But both Web jobs and functions are capable of picking up a message from a Q and starting to orchestrate. Other service is that I needed to do the work taken updated database. They could do some processing, make it even drop a new message in a queue for another job to pick up and process. So cues give you quite a bit of flexibility. Of course they do, and some complexity into the initial architecture. But for many systems that complexity will pay for itself. In the end, if we have a storage account in place, we might also consider using is your table storage for some of our data Now, table stories might sound like a relational database, but it's no structure does that. Essentially, it's a simple key value storage system, which makes it very flexible on massively scalable. It's a great place to store things like high volume lock information, and it's very easy to query and retrieve the data with a storage account. We also have accessed a blob storage, which we can use to store our static site content are images JavaScript and CSS. We can also use that to push out content through these your content delivery network. Now we don't need to use blob storage in order to use an azure sea the end. But it does work very well indeed. And if we're focused on creating a farce and scalable Web application, then a CBN can certainly help to reduce the load times for the user because the static content served from it can be pushed out close to the user, so arrives faster. Now, the performance improvement that you see here will depend on where your users are. The type of application, your building and the type of content Lee you're serving. But as a general principle, a content delivery network is a great way to serve static content as fast as possible. Okay, so now let's look at data sources. If we're building a large scaleable application, then we're probably gonna need a large scale cashing solution. Now is your has a built in option for the open source distributed reddest cash. Now we're gonna be using Red is cash later on in the course. But the quick preview is that distributed. Cashing like red is cash can give you a very quick access to a data from all of the service is on all the instances running in your application. You can scale up new instances, and you don't have to worry that they'll have to warm up by populating Akash relatives distributed. It's out there on the network. It's already populated, and with his you're managing, this is a service. You can have tens of gigabytes of extremely fast cash in information available with guaranteed up times and even a disaster recovery plan. You might use readies cash to query results from sources like Is your sequel database or Cosmos Depay? Not only will those cash results typically be fast to retrieve, you'll also be reducing the load on your applications databases when the databases under stress. Now some applications you might need to provide the ability to search the data you stored in Europe. And for this you could use an azure search resource. This will provide indexes to crawl the data and cosmos debate and secret database on blob storage as well as other data sources. It'll give you full text search, faceted navigation, certain suggestions scoring on all of this in multiple languages. Now. One resource I think you should always include in any application is application insights. Application insights can monitor the performance of your application. They can help you track down errors. It can provide a lot of information about the behavior and well being of your system. One final resource to consider in this scenario is is your key vote. TiVo is a service is designed to protect secrets such as access keys and passwords. You don't want these hard coded in your source code, and there's a good argument to be made that you don't want to copy these secrets and have them sitting in plain text settings in a portal, either With cable, just your system. Administrators need to know the secrets they can import. Those keys and parts was into key vote. It stores them secretly, and then the administrator can provide your girls for the application to use to obtain those keys. And they'll have logs to see he's using them and when. Now, just to be clear, the point of us going through a scenario like this is not to find some exact blueprint that you can copy in your application. It's so that we can show you that with Azure, we need to start getting a feel for the different service is on where they might fit into our design. Sure, application grows and you want more features more likely to start bringing in additional is your service is particularly and a larger and more demanding situation. So let's take a look now at a situation where we may need to think on a more global scale

Multi-region Deployments
[Autogenerated] So if we think about a large scale deployment, we're still gonna have customers. And these customers may be actual users consuming our service through the browser or mobile device. Or perhaps there are other AP eyes, and service is on our users. They're going to expect and demand the highest performance and availability from our application. They're not going to care if the entire data center goes offline. They expect the application toe always be running on, always performing at its best. Now, taking the application scenario we just looked at using multiple is your service is we could deploy all of this into a single as your region a single data center. We could also deploy the entire application into a second region on that region could even be on a different continent. This application deployment will look just like the first deployment and in this scenario will use as your traffic manager to route the customer requests toe one of these regions we've deployed. Now we're gonna look at traffic Manager in more detail later in the course, but it works at the D. M s level. Traffic Manager has the public i p address bar application on all the incoming requests are rooted here by the N S. We can until traffic manager to route each request to the lowest Leighton Sea region for that specific customer. We can also do explicit geographic routing to make sure, for example, that a European customer use the data center in Europe. Well, we can set up an active region with a stand by region, settle traffic goes to just one of the deployments unless something goes wrong, and then traffic manager consent the traffic to the stand by region. Now, later on, we'll see how traffic Manager Kamala to the health of our departments to check if an entire region is healthy. And in this scenario, we can also set up geo replication for our data. Both is your sequel and Cosmos Deby have the ability to replicate data around the world. What is your sequel? For example, you can nominate a primary database in the first region and a secondary database in the second region, and you can also set up additional secondary databases on all the right operations, the updates and deletes and the inserts they all go to. The primary databases will replicate committed transactions to all the secondary databases, This all happens a synchronously to avoid slowing down the application. Then applications in other reasons, can read data from their local secondaries and in the event that the primary database fail for goes off line. For some reason, your sequel can automatically promote a secondly database to be the new primary. Now, I don't leave you here thinking that building a highly resilient on highly available application is an easy task because it's no, you need to design resilience into application from the start, and you can see in this example as you has multiple service is that can help.

API Gateways
[Autogenerated] now it's quite common to have users who only interact with our AP eyes. In this scenario, I'm gonna represent all of that functionality. Just a generic AP AP Service Icon here. But this service could, of course, include any and all of the resources we've seen so far. Now the requests coming into our A p I might be coming from all sorts of places. They could be coming from our customs directly from our business partners or maybe even our partners customers. Now, from a technical perspective where we're thinking about scalability and availability, there's a small part this picture where it almost doesn't matter who the call is coming from, after all, a requested to request. But what the situation does require is for us to be much more attentive and capable in managing and monitoring our E P eyes, because we need a way for those third party developers to sign in on arranged to use our I P eyes without giving them an administrative access to our portal, we might need to keep track of who's using our AP eyes when and how often they're making calls because we might need to limit those calls we might want to offer different service plans to different customers. We might offer a free plan, which will allow five calls a minute toe R a p I and a premium plan that allows 2000. Thankfully, we don't have to write all of that ourselves, because in this situation we can use Thea's. You're a P I manager. We'll be using a P I manage later on in this course, and we'll see how it provides an AP gateway where we can monitor and potentially throttle calls to our A p I am points. We can use a P I manager to require authorization tokens for these calls. They're using our own internal options or third party identity sources like Facebook. To manage all of this, we can use a self service developer portal where third parties consign up to use our A P I. They concede this service documentation. They can interact with the A P I end points get sample code managed access, tokens, everything they need In order to consume our AP eyes. We can even import and combine AP eyes from several different deployments, such as AP Service and as your functions into what appears to our customers is a single comprehensive A p I. So hopefully that gives you a brief overview of a P I manager, and there will be more about a PR manager later on in the course.

Mobile Applications
[Autogenerated] now another place where we can use an azure app. Service is when we're building a native mobile application like one installed on I also android device. Mobile applications typically have a different set of requirements. Talk browser based scenario. We're still gonna need an access to a p eyes and we could host those AP eyes in an APP service or any virtual machine or behind the AP. I Gateway Orin is your service fabric. Are there specific app service option for mobile homes? Because the expectation is that now it's know all about the service side. But with the native AB are uses might need to work on their devices even when they're off line. They might need to create a new expense ripple or enter their notes while they're disconnected. And that data must be persisted locally and then saved up to the server when their next connected. So is your provides a mobile app sdk to help provide offline data synchronization. There are different versions of this sdk that we can use with aisles and android, as well as with Apache Cordova and would know Zaman is even one for building Brown's airbase taps as well We can then use the SdK to help us synchronize data between a local data store on the device on the a p i n points that we create in the back end. Perhaps our users received a new order or a new chap message, or maybe even an image was uploaded for them. Using as your notification hubs, we can push millions of messages sufficiently directly to our users devices and notification Hubs know howto work with the different service is like the Apple Push Notification service on the Google messaging service, so we can push out to various devices, even kindle devices, without adding all the separate infrastructure code required by all of those Different service is to our system. Now, when developing a new native mobile application, we're gonna need a way of building and testing our reputation as well as a way of managing beater testing A way to push a new version of an application to our tester's on our customers. Lawns are testing the application. We also need to be able to monitor our reputations to find out what problems that uses their experiences while we're developing it. And this is where Microsoft App center comes in. This works with Android. It works with IRS from Windows for Court Over and Zama ruin. But it also was with Mac OS and React and Unity. Now it's debatable as to whether absent or is really part is. You're no. However, it doesn't write directly with Azure and Azure develops, although there's a little overlapping functionality. However, if you're looking for a way to manage the building, testing, distribution, reporting and feedback cycle, then it's definitely worth a look.

Microservices and Service Fabric
[Autogenerated] now we've mentioned is your service fabric a few times already, So let's look at it now. A little more detail Service fabric is a micro service's platform now, if you're not familiar with micro service is, they allow you to build your application. Logic from multiple small, discreet Web service is that are independent and self contained with as few dependence is is possible. Service fabric really is a platform for building a distributed system from these micro service's. Now we still have customers or users, but with service fabric, they're less likely to be just a normal visitor toe a normal website. They're more likely to be thousands of gamers in a multiplayer game or 1,000,002 devices connected to the Internet of things. Now each of these Independent Micro Service's has its own name, its own earl. Often they're developed and deployed by different teams, so it could be revised and version completely separate from all of the other. Service is typically micro Service's are stateless Service's. This is where the service is don't need to persist any data internally, all of the information the service needs to process a request is contained within the request itself. If it does need to maintain any kind of state, it'll push out to a Cosmos __ or a sequel database with service fabric. All of these micro service's air deployed into a cluster. Think that is a shared Paul of machines, and you can scale up to thousands of virtual machines with service fabric, you know, even limited to using virtual machines on his year if you need to, you can install service fabric on a windows or limit machine in your data center, or perhaps even on another cloud provider. Service fabric is really powerful, and it's really important. Part is, you're in fact, many of the court beaches of Azure itself, a bill on top of service fabric. So in a service fabric scenario, be traffic from the users will go towards your load. Batter on the load balance will deliver traffic to a particular service fabric node. We don't care which note, because again the service is being stateless, are independent and self contained. Under the hood service fabric takes care of the infrastructure. It detects failures within the cluster and least can heal themselves. We can have rolling updates and scale incredibly quickly. All of which give us and always on highly scaleable Micro Service's platform service fabric is used. A lot of those back end service is for chat in gaming applications, and any kind of real time system is a good candidate for service fabric.

The Internet of Things and Big Data
[Autogenerated] the Internet of Things describes a worldwide network of devices such as switches, refrigerators, heating units, batteries, light bulbs. So even cars, all of which communicate over the Internet. They're low powered devices like wristwatches on large devices like air conditioning systems on farm tractors. And if we're building a system to collect data from a large number of devices, we could use service fabric. Or we could use an APP service plan to host an M P I. But there are some offerings measure specifically designed for the Internet of things on. The first of these is as your event hub. Now Offender Hub is designed to scale out on ingest millions of events per second. These events could be temperature readings or GPS coordinates. Event hubs up, built on top of the Azure service bus platform on DDE, in addition to using http, is a protocol event have also understand other protocols that are common in the world of device communications like am Q P advanced messaging, curing particle. Once you receive an event in the event hub, you can transform the event and hand it off to various adapters for analysis and storage. Another, more specialized is your service is Theis U I o T Hub. Now this provides bidirectional communication with devices, so not only can you receive events on instrumentation from your devices, you can also send commands to the devices that you manage. I o. T Hub also support same Q P. And it's reliable, scaleable and secure because you can set up authentication for its device. Once you started receiving data more of these devices, you might then pass that event through Stream analytics. This is another part of a juror, and it's a platform for real time processing of the data so you can perform calculations and react to events that are happening in real time. You might want to know. For example, if a vehicle in your flea is out of the expected geographical area or if it's moving too fast or speeding or been involved in an accident. You can also transform data into an is your data. Like our data Laken take all forms of data device events, relational data videos, flat files, documents and much more. And then you could feed that data into other tools for analysis, like HD in sight. Hasty insight makes her do prevail Ebola's A service from within Azure. Her deep is open source software for big data processing. So if you want to run, say, a distributed map produce program over all of your data. What you wanted to use the aisle anguish to perform some statistical analysis then as you are, provides all of these capabilities that you would expect in a hood oop environment. There's also a CZ your machine learning where you could write algorithms based on past data to predict future events like device failure or fraud. With machine learning, you could expose these algorithms as Web service is. You can then call from another application. You might also want to use a sequel data warehouse. These are designed to handle large amounts of data and enable parallel processing of it on a massive scale. Now it's important to note that technologies like HD Insight machine learning, data warehousing offender hubs aren't limited to only using with devices. You might use something like stream analytics and machine learning to predict some customer behavior in a line of business application or to detect fraud in the commerce site. So many of these technologies are a pickle to a wide variety of application. So next let's take a look. A scenario where we're going to deploy a code into Azure using virtual machines.

N-tier Applications and Virtual Machines
[Autogenerated] so far in this module, we focused on scenarios that use platform is a service resources such a zap service. So let's finish off by exploring a design that uses virtual machines and some of the more infrastructure focus features of his year. Now, this is a common deployment pattern that you'll often see in on premise deployment. And it's likely to be the pattern that you use when you first move your applications into the cloud, because we can do what we call a lift and shift, lift your application from your data center and shift the application into a similar environment in Azure. So in this scenario, we still have our customers on the outside, and they'll reach our application by going through a load balancer. This'll oh, ballot will root the traffic to one of the virtual machines that we've set up a Web server. So we'll have multiple virtual machines running in this role, not only to match their load of our users, but also so that we can have some fail over resilience. To help with that, we can place all of the Web server virtual machines into what's called an availability set. This is a logical group of machines that is your will use to ensure that no, all of the PM's can be off line at the same time. Now I'm simplifying things a little bit here, but with an availability set you're essentially telling is your that if it needs to restart a machine because there are updates to the hyper visor that's managing that V M. Well, that kind of update and restart won't happen to all of the machines. At the same time, we can limit how many machines would be offline at any one time. Jules will distribute those machines across different Physical Server Act to avoid unplanned faults like failing network switches or faulty power supplies. Now all of these resources air also within the same virtual network. We can define a virtual network all V net to isolate and protect resources. We're gonna have multiple Venus in his you're on. We can connect them together. We can also set up It is your network security group for our peanut, which could filter inbound and outbound traffic. So if we're only expecting traffic to end to this veena over port for for three, we can allow that traffic with a network security group on deny all other traffic that'll help keep the PM's more secure. Inside our virtual network. We can also create sub nets. We could define a dedicated sub net for our Web servers because we can secure with its own network security group. That way we can control the flow of traffic and lock down the communication. Even inside this virtual network, when the Web front ends, need to connect to the database service. Is that traffic compulsory load balancer? In this case, it's an internal load balancer in his your load. Balancers could be public or private. Only public load dancers have a public i p address. So in this case, the internal low balance Ercan route traffic to another set of virtual machines that host our business service is, and these are defined an availability set of their own, also within their own dedicated sub net. So from here we might have connections to is your sequel or Cosmos __ or Virtual machine running sequel server Oracle. It's the same pattern. Just different service is something else we might want to consider. In this scenario is what's known as a jump box. We don't want to just allow anybody to access our servers to connect via remote desktop or on power shell scripts on a jump boxes, a virtual machine defined within its own sub net. But we can only access from certain locations or from certain eyepiece. We can configure our virtual network and sub nets toe only allow management traffic from our jump bots. This is gonna help secure our infrastructure by reducing the attack surface we can. Also set up in is your fee PN Gateway, which is going to create a VPN connection between our on premise infrastructure. Ondas your it would allow us to connect to resources inside this virtual network is if they were local to our company network. We could also connect resources inside of a sure to resources inside of our local network, like connecting a Web application in an APP service to a local database server in our own data center or our own environment. Now, a lot of the topics here, particularly those around network design and security, are covered in much more depth elsewhere on plural site. But I wanted to quickly give you a high level look, but what you can build in Asia on what resources are available. Certainly with the infrastructure features of his your things like virtual networks and virtual machines, you can install any software you like. You've got a long way towards replicating an existing private data center.

Summary
[Autogenerated] So in this module we looked at a variety of system designs for a year. The objective here was to give you an overview of most of the major platforms, and service is you could use inside of a year so that you can make better decisions about your system architecture. You have such a wide variety of features. It's challenging to make any kind of architectural decision without exploring. Some of the deeper scenarios are looking, or what available and when it should be used. Perhaps it doesn't need completely in depth knowledge, but enough knowledge to point teams in the right direction when choosing the features to evaluate and perhaps prototype for a system. I had this module, and the rest of the course will help you make those decisions.

Cloud Patterns for Resiliency
Introduction
[Autogenerated] welcome back, friends. In this model, we're going to be focusing on specific patterns and techniques for making our applications more resilient. I'm looking at how we can design a system to expect and handle failures when we put our applications in the cloud. We understand the added scale and the distributed nature of this larger system that we're now part off means that we're potentially introducing more points of failure, parts of this infrastructure and never gonna be under our direct control. And at some point, something's going to fail. But don't worry, because in the cloud failure is expected. We know it's going to happen. So we also have many built in options to manage and recover from these failures, both large and small.

What Exactly Is Resilience?
[Autogenerated] So first, let's clear up what we mean when we talk about things being resilient now Brazilian is a word we could use when talking about all sorts of things. The common dictionary definition is of something that bounces or springs back at the cushioning in a running shoe or a rubber ball. It's resilient. It bounces back. So if we talk about making a resilient application, then that word can seem a little vague and subjective. But it's not, because here we have two very specific things meant by the word resilient. First, a resilient application can recover from disaster. Small disaster, perhaps the failure of a hard drive. A larger disaster might be a catastrophe that takes an entire data center off line. But disaster recovery by itself is not the same thing as resilience. Because let's say your application can recover completely from a failed hard drive. But it takes you 36 to 48 hours before you're up and running again. You may have recovered, but not resiliently so as well as being able to recover. Truly resilient application would also have to be highly available. Meeting that application is responsive and healthy for a substantial amount of time. However well we can talk about an application to resilience is thes two components. Neither of them can be answered with a simple yes or no answer. How quick doesn't recovery need to bay how available is highly available, and what does it mean for you and your business? If your APP has 10 minutes down time a month, is that acceptable? What about 30 minutes of downtime, or perhaps 10 hours? For those of you with a deep programming background, it's easy to fall into the trap of thinking about resilience as primarily about the technical and architectural decisions that you need to make. But let's get clear. The amount of resilience your system needs is first and foremost a business decision. Now short. This business decision is something that is going to impact the architectural and technical choices that you make. But where we draw the line with resilience is about money on time. As developers. What we need to do is determine how much our business is willing to pay for these qualities. Any business critical system that requires extreme levels of high availability and fast recovery from disaster. This going to require more resources, more redundancy. That means the system is going to cost more to build cost more to operate and cost more to maintain. Now, as we go through this module, you'll see that thes two system qualities being highly available on being able to recover from the disaster do overlap. For example, having redundant copies of your data spread across multiple is your regions will help your application be highly available and also allow you to recover from disasters small and large. So let's take a look now at some specific is your resources and see what features they offer for resiliency.

Redundancy Features in Azure
[Autogenerated] Okay, so let's look at a common deployment scenario. You've done a lift and shift. You've moved your Web application out of your on premise data center into Azure, and you're using virtual machines. You're using a VM Egil public facing Web server. So in order to handle more visitors and have better server response times, we're going to duplicate this Web service virtual machine Three more times. Now you've added redundancy, but she's great. But if all of these virtual machines are running in the same physical server, or even in the same server rack, you have a potential problem in that a single power shoe or a network issue could bring everything down at the same time. So is your provides an option to define what it calls an availability set. This is a special type of resource for virtual machines on dhe. This doesn't change anything about what those virtual machines do, but there's a way you can tell a juror to separate them, to reduce the risk of simultaneous downtime when you create an availability set there, too important settings. The first is you say that you want your PM's to be put in this availability set to be distributed across two or three full Timmons. What this means is that the virtual machines they're distributed into different fault domains so will not share the same power supply or network. Switch on availability Set also includes a setting, but distributing your V EMS across a selectable number of update domains. Thes airways to guard against downtime. But where fault domains of concerned with possible physical hardware failures update. Domains are a little simpler. Unlike fault domains. Update. Domains don't move your virtual machines to different parts of the data center. They're just a way of telling his you're to group and separate those virtual machines for planned maintenance on update situations. So if I had, say, 10 virtual machines and I said I wanted them in five update domains in that availability set, then they'll have to v EMS in each update to make. Now, when is your is automatically rolling? An important patches and updates will only ever restart one Update two main at a time, so I should only ever have to virtual machines down any one time. So that's one of the options when using virtual machines. But we might be using as your APP service to provide a Web application. And once again, our first technique in making a resilient app is redundancy. As we know, we can scale out up service to run on multiple instances so we could begin with two instances running at the same time. And even if one of them failed, I still have one other server running and processing requests and tells your provisions and spins up a new instance to replace the failed one. So multiple instances are a great way of adding availability, but we're going to take things one step further. With AP Service, we could go beyond just having multiple instances in a single region. We can also deploy that APP service to multiple regions, more data centers, adding geographic redundancy. And then we can use traffic manager to know only route customers to the APP service with the least leighton see but also route traffic around any problems or failures that might be occurring in an entire region. Now that's one of the scenarios will cover in a little more detail in a few minutes time. But before that, let's look at one more databases in his year also offer their own replication options with Cosmos debate, for example, the idea of being globally distributed this bill into it from the start. It allows you to easily add global replication across multiple regions. Then you can have any number of regions and even add and remove them at any time Now. It's sometimes easy to fall into the trap of thinking that if we've enabled some of those years replication options for our storage or our databases, and we now have this redundancy, then we're providing some kind of basic backup and restore solution. That's a mistake, because replication and restoration the two very different things replication gives us redundant copies of our data, and it keeps those copies in sync. It offers you high availability and disaster recovery, but this isn't gonna protect you from bad data or corruption. So if we have a bug in our code that is writing bad data or causing some form of corruption, then that bad data is gonna be replicated across the region's. So we'll also want the ability to continually back up our data so that we can perform a restore to a specific point in time. Now, obviously, is your has a solution for this, but it's dealing with a different problem. Then we're focused on right now. So let's get back to resiliency on back to the idea that if we talk about high availability for storage accounts and databases or APP service or entire application, what exactly do we mean by high availability and more importantly, how can we prove it?

Service Level Agreements
[Autogenerated] in the cloud and in software development. In general, we always talk about high availability. We never talk about perfect availability, and that's because the world isn't a perfect place. Things break the behavior of our systems on the systems that we interact with such a Zaheer itself, never entirely predictable. But just because things aren't perfect, it doesn't mean they're random. You know, just cross our fingers and hope for the best and then shrug our shoulders when it doesn't work out. We can still demand high availability. We have high expectations of the service is that we consume within azure now. Fortunately for us, we have a way that we can measure those expectations Now. If you do a search for his, your service level agreements will eventually find this page. This page lists thesis er vis level agreements. The S L. A's between azure on Do yourself as the customer thes define most soft standard commitments for up time and connectivity. Yes, and I also defines what you're entitled to if Microsoft doesn't meet that commitment, and typically this means a credit against your monthly charges. For example, if you're paying Microsoft for a core service like D. N s, then Their guarantee here is an up time of 100% in a month. Many factors your service has less than that. You'll get a credit of 10% for that month. It is less than 99.99% up time. It's 25% credit and so on. Now it's important to realize that there isn't a single number for a sure at the time. Justus, there isn't a single number when being billed for is your service is because service level agreements are different for different resources. So the service level agreement for a nap service is different than that for a virtual machine, and you'll find a different S L a. For whatever service that you use within a Sure, but the S. L. A also says under what conditions much soft makes. These guarantees, For example, with an APP service is if your application is in the free or the shared tears, then you don't get any guarantees and the S and they can change based on war options you're using. So you can all get quite confusing with virtual machines. Biswas guarantee is different for PM's, where you're paying for an availability set versus a single instance VM Someone you're planning for a zillions. You have to understand these conditions and the guarantees so that you can calculate the up time numbers for your system. The way these are currently structured is that in the first sentence of the S. L. A, myself just gives the basic conditions on up time. Percentages for a given service for paid APP service the SL might save the at will be running an available 99.95% of the time. Now that sounds like a lot, but would equate to 21 minutes of down time in a month on. Microsoft would still technically be meeting that service level agreement. Now you need to be careful with the numbers because while and availability of 99% sounds like a lot, it would actually equate to between seven and eight hours of downtime in a month, and that could happen all at once. Also remember that the service level agreements only relate to outages that are Microsoft's fault. So if you make a change to your app for your configuration that takes it off line or if your application is rejecting requests because it's not scaled correctly. Then there's a very much down to you are not covered by the S L A. So it's important when we're designing a resilient application toe. Identify which resources that we're gonna use on which of those resources half dependence is on which other resources So you can figure out what your systems expected up Time percentage is going to bay. For example, I might have a nap service where the current S L. A is 99.95% up time. But that APP service depends on Cosmos d B now the cosmos D B s L. A. Says that it has a guaranteed 99.99% up time. Now that means that my application could potentially be down for 25 minutes in a month. A Microsoft would technically still be meeting the S L. A for both constituent parts of the system. 21 minutes of down time in a nap service on four minutes of downtime in a database Again, this now becomes a business decision. Do we need to do better in this, or is this sufficient? Is this a critical application where 25 minutes of downtime would create problems with the business. How much revenue would I lose? What my service level agreements with my own customers now improving this number could be expensive and not just in terms of resources, but also in terms of system and architectural complexity. I could, for example, at a cash to keep Reed operations alive if there's downtime with the database. But I pay for that cash, and I'll pay for writing and maintaining the code to use that cash. Now, these are the kinds of topics and trade off that will evaluate in the rest of this module.

Improving Connection Resiliency
[Autogenerated] Okay, so let's start by taking a look at some specific patterns that we can use to and resilience to our system. And let's kick off by looking at how we can make our connections more resilient. Now, when I talk about connections, I'm not just talking about database connections. I'm talking about any connection between any one part of our system on another. So this could be caused to Web service. Is a database or any other is your resource Now, when working with complex systems in the cloud, there are many opportunities for what we call transient failure. Our application might retrieve data from a database, place messages on a cue or trees, files from blob storage or, more likely, all of these things now, while processing request, it's possible for any part of this infrastructure to become temporary saturated, overloaded with traffic, more experienced, some form of outage. Maybe a resource we're working with might just be overloaded with requests and unable to respond, even if just for a moment. Typically, we don't just want our system to give up and fall over. We want the system to wait for a moment and then re try the operation. We might want the system to retry two or three times with a delay between each request and hopefully the operation will overcome that temporary problem and ultimately succeed. We call this the retry Patton. However, not all failures are worth retrying. So it's important that the system convicts torrential E eight between transient failures worthy of a retry animal critical errors that signify something more serious. So if your code calls and is your cue over http and receives a 503 error, which is service unavailable or one of the other 500 Siri's server errors, then that would represent a temporary problem with the service. So waiting for a bit and then re trying make sense. But if the response is a 404 not found or any of the other 400 serious errors that would represent a problem with the actual request itself, something doesn't exist, or at least not at that address on it makes no sense to retry. So let's take a look now know Js implementation of this connection resiliency that uses these your sdk. Let's imagine that we're writing a Web app or a Web, A P I that needs to store files of some kind. Let's say our processes image files for users to download. Or perhaps we allow users to upload documents or other content, and we've decided to use is your blob storage to store that data now under the hood. All of these, your storage operations happen over http. So we could write around code two, take those incoming requests from the user and then construct the http requests and post those directly against the issue of stores in point. We could also implement our own retry logic to deal with any momentary failures. Fortunately, though we don't need to do any of that, most of the individual esti case for sure, like the azure storage SdK for node, already have all of this functionality built in and ready to go. Let's take a look at an example. So here I am in visual studio code, and I've already installed these your storage sdk with NPM, so the first thing I need to do is have to require for these your storage module. Now you could add a require for the general jurist Ikea for note, which would also include these your storage module now with this, I can create an instance of the Blob service class. This class exposes lots of methods that I can use for interacting with my blob resource. I've got access to methods like create block, blob from text or create block blob from strength. But before I can create a blob, I need to create a blob container. So let's start with create container if know exists. This method takes the container. Name is a strength because this is knowed. We have to use a call back toe, handle any error or is open now because I'm using these your storage sdk here. The retrial logic is already built in. If I make this call and get a server failure such as a 500 response, that is your is gonna wait for roughly four seconds and then re try the operation again. It's going to retry three times with an exponential interval. That means it's first going to retry it four seconds than eight seconds and then 16 seconds. Now the exact tree triumphal as a level of random. This bill into it and this is to make sure that if there was a momentary failure in the is your network? No, everyone is re trying their operations at exactly the same time after three re tries. If we're still getting a failure, then we'll get an error return back to our callback. This retry behavior also pays attention to the status coat that we get back from making the request. For the most part, it's gonna retry if we get back a 500 Siri's era because that represents some issue with server like service unavailable or time out. But if the status code is in the four hundreds, like a four for not found or a 43 forbidden, they won't even retry because it suggests is an error with the request itself. Perhaps it's a badly formed request body. The only real exception to that is a four eyed status, which is a request time out that could be worth retrying. Now this behavior is the default retry behavior for azure storage. It's what happens automatically when using these your storage service, and it's known as the exponential retry policy filter. Filters like this, or what we used to effect the optional behavior of service is things like automatic retried or logging. Now we could change this behavior by changing the filter or even by defining our own filter. The SDK also provides the linear retry policy filter. Now this takes two pieces of information. How many times you want to retry on the time between each retry in milliseconds? Let's say we want to retry total of five times waiting five seconds between these retry To make use of this belter, we simply added, when we create the blob service class. Now, don't worry about the code here because all you really need to know this stage is that there is resilience features built right into the S T K and also that you can change this behavior if you need to. Now it's important to note that the retry options and the default retry behavior for is your storage not necessarily the same as with other is your service is service is like ready cash or cosmos __ or service bus. We're gonna have very different behaviors on defaults. Now there is a section in these, your documentation where you can find the current retried Details are all is your service is now. A lot of these just aren't relevant for no developers but using sequel databases with radio dot net. But this is where you can find a summary of the default retry behavior and how it can be customized Now. Typically, the specific esti case for all of visual Service's provide some form of bill in retry and resilience policy. So wherever possible, you should really try and make use of the SD case. That way you don't have to write your own retry strategies. However, there will be times when part of the system is in a failed state, and simply re trying an operation is not going to help. So what do you do then?

Graceful Degradation
[Autogenerated] graceful degradation is a term that most Web developers already familiar with It's a phrase that's used to describe the preferred experience for any user when their browser doesn't support all of the features we might like them tow have. For example, we might hope that I use his browser has support for local storage so that we can use it for cashing well that their browser supports Dragon Drop. But that may not be the case. Perhaps they're using an extension that blocks certain job script libraries across the main calls. Or maybe their security policy imposes restrictions that in some way restrict her features available within the browser. Graceful degradation. I asked us to cope with these kinds of scenarios. It's the principle that we can develop an application hoping that every feature is available but our application consult function with a limited subset of those features. This doesn't happen by chance. It requires a great deal of thought, a lot of testing to make sure that our application maintains the maximum out of functionality with the smallest available feature set. Now the principles of graceful degradation don't just apply to Web browsers. We can apply the same principle to cloud systems. How many features could we disable even if just for a moment, and still keep our system functioning? Now imagine we're building an application that requires Cosmos TB. What would happen if our application can't reach the database even after a few re tries? Now, perhaps the way that we've provisioned our cosmos TV is fine for our typical usage. But occasionally we have a spike in traffic and that causes it to get overloaded and start to reject requests. Now, in this scenario, we don't just want to raise an exception, then give our uses an error page. We need the system to be able to cope with this outage. There's still provide some minimal level of functionality now one solution that we can use, which will add resilience to database connections and help protect us against these kinds of database faults. There's no Agnes. You're ready cash? No, we could use this cash to start copies of recent queries or regularly access data. This would allow the application to continue to operate even with slightly stale data until the database connection could be restored. Now, if we using register cars as he read only data store. Now we might want to consider using a message que toe handle any updates that we need to make to the database. Messages in the CUC contain the data that we need creating, updating or deleting when the database comes back online. A Web job or is your function app can process all those messages in the background on the cash could be refreshed. Now, if the database is down, the application may not be considered functioning perfectly for the user. It's in a degraded state, but it's degraded gracefully. We're maintaining functionality with this few features possible. Now you have to be careful here because while it's certainly easy enough to add a cash and a Q into our solution, this is going to increase the cost and complexity of the system. We're gonna need more code, and you're gonna be spending more money in his year. You're gonna need to make sure that you have robust exception logging and tracing available to That's when things go wrong. It's gonna be harder to trace the issue now. As I've said before, all of this comes down to a business decision. If you decide that the service level agreements for AP Service and database combined. A good enough for your business, then you probably don't need the extra complexity and cost that this kind of solution is gonna provide. Having said that if you need a high degree of availability that Mi es ella guarantees, perhaps because your application is mission critical to your business, then you're going to have to take on the extra commitment. But a solution like this requires now an increasingly popular pattern for data access is the commanding query responsibility segregation Patton or seek you rs for short. Now, with a traditional data access pattern, we work on the idea that it doesn't really matter whether we're creating, updating, reading or deleting data. We're always going to use the same data model for it, the same definition of records or schemer in secure as we move away from this idea entirely. Security is a pattern where the parts of the application that read or query data can use different models on abstractions, even totally different techniques in the parts of the application that change that data. You could even build out these parts of the system completely independently now, while security is a Sim ply their. It is a lot of code to write and maintain if you don't actually need it, but it does provide you with some interesting technical options in high performance applications. Your requirements for querying data and your requirements for changing that data often scale completely differently. Think about product data for an e commerce website now the customer focused data model for querying. That data is probably very different from the actual way it's internally safe and stored. You're likely to have many hundreds of times more reads as rights on your performance requirements are likely to be different to, you might demand exceptionally far three times for product data. I'll be happy with delays when updating the database so the sea Curis approach we can separate them. We could use different techniques, like managing the queer in part with reddest cash and updating with database. With a service bus Q and a Web job. Separating it out like this makes it easier to scale we can allocate, considering more resources to querying our data. Can we do to updating it? Its patterns, like seek your arrests that can give you structure and make a resilient system much easier to implement. So we've seen how you can make your application more resilient at the data by using, cashing and massed in queues. Patents like Seek your S. But typically there's more to a system than just database. If you've got a system built with micro service's or even just a collection of Web AP I projects, then you're gonna want to explore the circuit breaker pan. Now the key aim of the circuit breaker pattern is to allow you to fail fast and prevent further requests to a service that you know has already failed, as we've already seen in distributed environments. Calls to remote resources such as Web service is sometimes fail because of slow network connections or time outs because the resource is temporarily unavailable. Normally, these faults are only temporary and often resolved themselves within a few minutes. It's common, therefore, for service is to implant some form of retrial. Timeout Patton. We saw this earlier when looking at storage accounts unusual sdk. Sometimes those faults are more serious than take longer to fix, perhaps a network outage or a hardware failure. When this happens, it is pointless to keep retrying a call to a network resource, but it's unlikely to be successful. If the system is very busy and you have a service that's failing, then you might be tempted to rely on time out. How about these can lead to resource exhaust you, as all of those waiting requests are consuming threads database connections on memory resources. This resource exhausted can then cause problems on other service's or elsewhere in the system. This is why we prefer, if bail far scenario. But we only attempt to invoke a service if it's likely to succeed. So let's look at the circuit breaker pattern in a little more death. Circuit breakers operate like a proxy that generally implemented as a state machine with three possible states. Let's walk through it so the circuit breaker starts in the closed state, and as it receives requests, it passes them on to the remote service. Now, as it does this, it keeps a track of the number of filed requests. Once a certain threshold is reached immediately transitions into the open state. Now, once in the open state, the first thing that happens is a timer is started on. This runs for a predetermined time and is designed to give the remote service time to recover. During this time, any received request is failed instantly and a suitable era is returned to the calling party. Once the time has expired, we transition into the half open state now in this half open state, we only let a limited number of requests through now. This is important as it prevents the underlying service from being swamped with requests. Justus. It's recovering now. If any operation fails whilst in the half open state, then we move back into the open state. Whilst in this state we count the number of successful requests. If a predefined threshold is reached, then we can transition back into the closed state, and so it continues.

Load Balancing and Request Distribution
[Autogenerated] when we deploy any kind of Web application over to the cloud or our own on premise infrastructure, it's common to use a load balancer to distribute the incoming requests Hello balance and not only helps our system be more resilient, they also makes it easier to scale our application by increasing the number of hosts that sit behind the load balancer. Now there are a number of options for low balloting in his year, not just from Mark Soft but also in these Your marketplace. Now, in terms of the director's, your service is offered by Microsoft. There not one but three main options. There is Thea's Your load balancer is your application gateway and is your traffic manager. There's your load. Balancer is what we call a layer four or transport level load balancer. Layer four of the networking stack is the fundamental TCP new __ level, So this load balance it doesn't look at the contents of the packets. It looks at the TCP or UDP port. Now it's relatively simple to distribute incoming network requests across multiple virtual machines, and typically we distribute packets on poor 80 import 443 as your application gateway is a a seven or application level load balancer. Now this is a lot smarter about how it Lobo's his network requests, like using cookie based sessions to make sure that a particular user is always directed to a particular machine warrant traffic based on the girl application. Gateway also has a Web application firewall that can scan your traffic for things like sequel injection and cross site scripting. But the service that we're gonna look at in more detail here will be, Is your traffic manager. Now? This one's a little different from a typical low balance. Because traffic manager uses the L S lookups to distribute requests across your system, and it supports issue APP service is.

Demo: Setting up Azure Traffic Manager
[Autogenerated] So in this demo, we're going to take a look at how we set up is your traffic manager So here we are in the is your portal. I'm assuming you're already and little familiar with some of these your resources, like app service. As you can see, I've created to Web app. Service is here in Azure. The 1st 1 is running in the northern Europe region on the second up, service is running in the central US region. Both of these surfaces are running an identical know Js app. This deployed avenues, your dev ops repositories. Now there are several reasons that we might want to deploy our application into multiple is your regions. Maybe we're in the enviable situation of having so many users that we need to run Maur instances than a single app service plan can provide. Perhaps you want her application to be physically closer to our users so they have a lower Leighton see connection. If redundancy is important and we want the application running in at least two different data centers at all times in case one data center becomes unavailable, there are many different reasons to run redundant Service's across multiple regions. So at the moment there's nothing connecting these APP service is together. They're two separate sites that just happen to contain the same application thanks just on their own unique Urals. And if you look at the home page for these websites, you can see that I'm displaying either wired brain coffee Europe or Wide Brain Coffee USA. Just so we know which APP service instance is serving the content. So what we need now is for I uses to be able to use a single U R L to access our site eventually, even accustomed domain name by wide bring coffee dot com and have the request intelligently rooted to the correct AP Service. Instance. And for this, we're going to use is your traffic manager. So in these your portal that's create a new traffic manager profile. Let's start by giving the profile and name. This name will be used to make a single U R L that can reach either of these APP. Service is if you want to use a custom domain name than you can. So if I did want wired brain coffee dot com, I can configure it so that traffic manager would resolve that two main aim tow either of these two instances. But for this demo, we'll just use this is your name because the principal is exactly the same. I'm going to have to tell traffic Manager about my two different app. Service is the one in central us and then one in Northern Europe. After that, traffic manager will resolve the D. N s query for this address and hand. Now, either one of the I p addresses the APP service is I've configured. But the question is, how will it decide which app service to send the traffic too? There are essentially four routing methods now the default routing Methodist performance. When I have end points in different geographic regions like I do here, traffic manager will determine which location currently has the least amount of network. Leighton. See, I'm we'll send the user's browser off in that direction. So generally speaking, customers in Europe in the UK another spots on this side of the world. They'll probably go to the APP service that's located in Northern Europe. Customers in North America, South America. They'll be directed to the central US region. Another option that we have is to use the weighted routing method. Now with this, we can configure traffic manager to send some of the traffic to one in point, then some to another. For example, we could send 80% of the traffic to Northern Europe and 20% to central US. Now, I can also use the priority reaching option. This is good for the active passive bail over scenario. So, for example, I could say the central US is the top priority for a request till all requests will be sent there. But if for some reason that primary endpoint is faulty or unhealthy, then traffic manager will look for the next end point according to priority, Let's start directing traffic there. Finally, there is geographic rooting now, while the performance routing method will often route customers to the closest region based on network Leighton, see, this geographic ruling is more for when you need to keep specific customers within a specific region. Now it's common these days for some countries or regions such as the U or China, to have specific data protection laws that restrict where users data can be processed. You can use traffic manager and geographic routing to make sure that you're staying in compliance with any laws around data collection by assuring that the request is processed within a specific region. Now you'll also see options for multi value and sub net rooting as well. Multi value routing is for profiles that can only have I P v four or I p B six addresses added as endpoints. In this scenario, when traffic manager receives a request, it's going to return a list of all of the healthy end points with sub net routing. You're not a map, a range of end user I P addresses to a particular endpoint. But for our discussions on resilience, we're gonna focus on performance rooting for this example. I'm going to select the default routing method of performance, but we can change this later if we need to. As with most things in his year, I need to nominate a resource group. I've gotten existing once. I'm going to use that here now. This is a bit misleading because traffic manager is a global resource exists across all regions, so it's not located in a specific region. So that's how you create a traffic manager profile in the next demo. We're gonna at the end points

Demo: Azure Traffic Manager Endpoints
[Autogenerated] So in the previous demo, we created a traffic manager profile. In this demo, we're gonna configure this new profile with some end points, so it knows exactly where to route the traffic too. So here we are in this your portal, and this is the traffic manager profile that we just created. You can see here many of the commoners, your settings for access, control and order ting. But what we need is in the configuration option, because from here, we can change the routing method, Michael, the same options as we had before when we create the profile. But beneath this we have a bunch of additional information, including the very important Dean s time to live. As I mentioned before, traffic manager works by resolving the NS ______. Now that means that when the user browses, toothy girl traffic manager is going to use its routing method to decide on return. The D. N s information for one of the M points were about to add, As with all the NS queries, the timeto live determines how long that's retained by the client before it becomes invalid. The default hair is 60 seconds or one minute now in the earlier days of the Internet, it was common for the time to live, to be set to about 24 hours. Now. That was because the I P addresses of Web service just didn't change very often. These days, things are much more dynamic, and the server that handled your request could change on a much more regular basis. Now you can set the TT l 20 But that would mean that every time a user made a request, it would have to go through traffic manager to get the delis information lower. Tt Our values here will mean your users will fail over to a different end point in the event of a fault in much less time. But lower values also mean there'll be more Dennis ______. We shall increase the Leighton. See, So there is a tradeoff. Let's leave this at 60 seconds. So if we switch over to a terminal, then we can see how this works. If we used the N s look up command to look up, are you r l? Then the response shows that this is a non existent domain. Now that's simply because while trumpet manager might know about this address. It has no idea about what end point it's supposed to provide. But before I add those endpoints, let's come back to the configuration settings and talk about monitoring. Now, once we configure our in points, traffic manager starts monitoring the health of those APP. Service is every 30 seconds. It sends a get request to check that the end point is still alive. If it receives a 200 okay response, then everything's fine. If not, then assume there's a problem. With that end point. After four such attempts, traffic manager is gonna assume that that end point is degraded and it'll stop forwarding requests to that end point. Now. Traffic manager will continue to monitor it. I want it's healthy again. It'll be available to start receiving requests. This is one of the great beaches about traffic manager. Not only can it help us manage the scale of our applications, it can also improve our resilience by ensuring requests are only routed toe healthy end points. Now, by default, this get requests that traffic manager sends will go to the root of the endpoint. But you can change this relative girl to go anywhere in complex systems and Marshal Service deployments. It's common to implant the health check pan. Here you'll define an end point that will be called by traffic Manager To perform a series of tests has to check database connectivity. Four connections to other service is these checks? Don't just check for your application is responding, they check. The key components are operational as well. But for any of this to work, we're gonna need to add our in points. So let's have a new end point now. But first I'm asked the endpoint type, and most of the time I'm gonna want my traffic going to an M point inside of a year. But I do have options for external endpoints. There's also an option for a nested end put that would allow me to combine traffic manager profiles and give a very flexible routing rules, combinations of geographic and performance and waited versions. But all I need to do in this scenario is just point to the two different locations that I've defined. We need now is your end point and it's an APP service. So we select that finally we can select our app service instance and give it a name. Let's do this once more for the central US end point. And now, after a moment or two, we have to enabled end points in the traffic manager profile. Here we can see the monitor status and we can see that both end points are online and responding Well, if we switch back to the terminal now, we can reissue the NS. Look up command. Now the girl we're using here is for the traffic manager profile, and it doesn't contain any regional information. Plus, you can see now we've defines our men points. We get a proper answer amusingly routing method of performance. But I'm currently in the UK so traffic manager has decided to send me to the Northern Europe. Instance, if we open a browser on Brown to the traffic manager, you Earl, you can see that we're hitting the European app service. Instance. Now what would happen if this instance was degraded? Somehow let's turn the app service off and then jump back over to traffic manager. Now remember, it's performing a health check every 30 seconds, so we soon see that these detective, this end point is being unhealthy. But our Deanna's timeto live is 60 seconds, so it's gonna take a few minutes for our browser to catch up. If I skip forward a little bit on, browse to the side again, you can see we're now being sent to the U. S. Website to effectively. What's happened here is that traffic manager has allowed us to bail over from the European region into the U. S region. Keep the application up and running. A typical user would have no idea that anything it just happened. Now you might be wondering what would happen behind the scenes if we were using a database. Well, we could certainly take the same approach and use geo replication for a database so that it also existed in multiple regions. Well, we could use a cash in later to help with availability and load balancing. Or we could use Meskhi on message cues to help. Let's talk about message cues next

Load Leveling
[Autogenerated] so we've explored some options for load balancing. But another technique that we can use to build resilience systems is one that we call load leveling. Now, many components of the cloud application can experience heavy on unpredictable spice in traffic. And we don't want these spike toe overload any part of our system. We also don't want to have to over provisional of our resources to expect extremely high traffic. 24 7 We can set up automatic scaling rules for some of our components, but these can't always respond quickly enough to deal with sudden spikes in traffic. What option we have is to introduce a cube will act as a buffer between two components or service is in the system, and there are several advantages to doing this with load leveling being just one of them. Load leveling is where the system on the left may drop a message in the queue, with all of the data needed to do the heavy processing inside of the system on the right, the system on the right doesn't need to be over provisioned to run at maximum capacity for spikes. Instead, it could take messages off the queue, but a consistent right, receive all the data it needs to do the updates and notifications that are required by the business logic and then just average out the workload. All of this processing happens a synchronously, meaning that the system on the left is free to process more requests. Now it's important to make sure that the system on the right that's processing these messages has enough capacity to keep up. If you're writing messages much faster than your processing them, then you'll never keep up. But the point is that this service can scale independently and that the spikes in demand had just averaged out over time. Now there are many situations where a queuing architecture could be useful. For example, if I place an order online, then the website will respond fairly quickly with an order confirmation. But behind the scenes, several tasks are likely being orchestrated. Some of these may happen very quickly, whereas others may take several days. It depends on what I'm ordering. I'm booking a flight. Then there may be seat allocation and financial confirmations that need to be done. The order may impact other part of the business, such as purchasing and staffing I'm trying to book a flight for tomorrow. Then these things may be processed with a higher priority than if I'm booking for next year. So messaging systems like this allows to decouple our system design instead of having our Web application itself directly process everything about transactions. It simply gives us a confirmation, and that's a message to a Q. It's then up to the other systems that subscribe to that cute to process those messages. This'll. Loose coupling can also allow us to use multiple technologies and multiple different skill sets. Perhaps we build our Web application part of this using node, which has things to the Q. But the actual que processing might use other technologies like C Sharp within his your function or a Web job. Now in a show, we have the option of two different message keys. Kathy storage cues that come with an issue, a storage account and these shoes are reliable. They're persistent, and they support http interface. But reading and writing to the Q. They can cover many of our messaging needs. That is also is your service Bus queues service bus itself provides a broad range of messaging infrastructure for the cloud, much more than just is your stories case. So let's talk about service bus next

Demo: Creating and Using Azure Service Bus
[Autogenerated] So in this demo, we're gonna look at how we create a news and is your service bus queue. So the first thing we need to do is create a service bus name space. Now, this is the resource in his year that groups together any and all related resources that we're gonna create within service bus like multiple cues and multiple topics. So let's give it a name and select a resource group. And in just a few minutes, we should have a new service bust name space available now on the overview blade. Here, you can see that I have some action _______ to create a new Q or topic. So let's go ahead and create a new cue again. Let's give this Q and name and let's call it orders. So we're gonna have some part of our system that's going to add orders to this message, you and then another back end service, Possibly a Web job or is your function will be responsible for processing these messages. Now I'm going to accept the defaults here. That's the maximum message size off one gigabytes for a Q, which is fine for us. I'll allow messages to live in the queue for 14 days on a service. Read a message. We'll give it 30 seconds with that message locked where it can process it. Otherwise, the message will be available to Deke You again later on. There's also some features here, like what to do with expired messages. Do you wanna move to the dead letter? Sub Q. Do enabled duplicate detection. We could also enable sessions if we wanted to group a collection of messages together. Now the Q has been created, and you can see appear here in the service bus list if we click on the orders. Q. We can inspect it in the same basic metrics here, a small dashboard that shows how much free space exists in the queue. What the active message count is on what the dead letter, Mrs Count is dead letters or messages that the receiver has failed to process. Now it contain the dashboard several minutes, sometimes to update, so you probably shouldn't rely on this for accurate counts of messages. But while I was still looking at the Q, let's talk about security. So the cue that we've just created is something that isn't part of any particular service. It's going to be a broker. It's gonna sit between two. Independent service is. So if we select shared access policies, we can define a shared access policy that says what individuals or service's or applications can access this Q and whether they'll be able to read from it right to it, we'll manage it. So let's define a new policy and call it order sender listener. When I create this policy, it's gonna give me a key that I can then use in my application code to buy send messages to the Q as well as to listen to the Q and pull messages out of it. Now I could create one policy to send and a different policy to lessen. But just to speed things up in this demo, I'm gonna place both of them into the same policy. Now, if I look at that policy that I've just created, you can see that I have primary and secondary keys so I can hand out to grant access to just this specific que in service bus. In fact, I'll copy the Primary Connection string right now, you have now created a service bust name, space which contains a cue called orders. But I have to find a shared access policy called Order. Send a listener. So I'm gonna quickly simulate a service or process that's going to create service bus messages and it's going toe anthem to our cue. And then I also have another process that will listen for and consume those messages. Now, in order to make things simple, I'm gonna write some simple code in node and just run this locally on my machine using usual sdk with the connection string that I've just copied. Now, in the real world, this code will be running in an APP service or perhaps of Web job or function app. Now I'm gonna car trunk. It is really simple. So we're gonna need a few lines of code. So my local machine, I've already got a very basic notes I configured. I've already installed the azure sdk and I have a blank Java script file called on two Key. Now the first thing I need to do is add a required for sure. Using this reference, I can create a service bus service object. Now, in order to use this, I need to tell the object have to connect. And for that, I'm gonna use the connection string that I copied earlier. Now, you wouldn't normally hard coded connections drink like this in a production site. Probably want to use a configuration file. Well, maybe the highly secure is your keyboard in order to protect the secrets. But to simplify the demo because I'm running this code outside of his, you're gonna hard coa disconnection string into the source code. Now, this connection string includes everything that we need. It includes the end point, the actual your l the service address the name of the policy which was order. Send a listener. Lane closed. He shared access key. Now, right in the end, here is the entity path, which is the name of the queue that I want to operate on. Now we're going to be specifying the name of the cube when we add the message so we can remove this part. The connection string. So now we have a reference that we can use to connect the service bus. So let's not build the body of a message. Now. Our message could be Jason. It could be XML. It could be binary data I could even just use a simple string and literally dump it into the queue. So this is my message and added ther Q. I just need to call send Q message. I specify the name of the Q and provide the message itself again because this is knowed. We need to supply a call back, and that's it. That's just add a simple output if there's an error. So we know we successfully added a message to the Q. So if we say that in Jamaica, the terminal, we can just run that directly using node at two q dot Js and then you can see it says that the message was added to the Q. Let's just do that a few times Maur on. We can see the callback happening on. Our confirmation is written to the console Now. We didn't get any errors, but let's switch over to these your portal and double check. It worked. We're looking the orders. Cute shows us that there are three messages here, which is great. So let's write something to pull these messages off the cue. So back in the terminal, and let's create a new file for this called re from Q on Before jumping into the code, I'm gonna copy the 1st 2 lines to connect us to service bus. So everything up that new read from Q file I'll paste those end on. We now have our connection to the Q. So I'll just use that to call received que message with the name of the Q orders. Now you can an optional premises to this call to say whether you want to just peek at the Q. Which means that when you read a message, that message will stay in the queue. But the default behavior and what I'm doing here is that as soon as we successfully read a message, that message will be deleted from the Q. But of course, another call back here where I'm gonna simply write out the information about that message as long as there's no error and that's it. So let's say that file and then switch back over to the terminal and run it. So, as you can see, we do get a message. It's wrapped in some additional method ater that's been added by service bus. But we do have our custom information, and if we really is going to get the next message slightly different times time here, but still the same data. Now. I don't have duplicate message detection on so service buses allowing retired multiple identical messages. You can turn this behavior off. So another read and we get 1/3 message. One more read and I get nothing, because I have now read all of the messages from the Q. They're being deleted as I read them, so there's nothing left to read now. I can, I wrote. Those two independent files want to add to the Q. I want to read from the key. I'm running them within the same local node application, but it is a simple, in fact, even simpler to read from the Q. Using Liz your function. So if we switch back to the is your portal, I've already created an issue of function app that's at a new function that will read messages from our cue. Now, depending on your requirements, you could run a function on a timer that would check the queue every hour or every 10 minutes. You could also have the option to automatically have a function triggered every time a new message is added to a Q. But as we're using service bus queues, we need to create custom function. Now there are a lot of templates to choose from. One of these here is thes service bus queue trigger. Let's give it a name and then select our service bus name space. We need to provide the name of the Q we want to listen to, which is ordered. Let's create it now. This could take a moment to create, but eventually I'll get the function back, which even includes a few lines of template code. This is the code that gets executed every time the function runs. Now you contest the function from right here within the portal. But as we've already written code to add to the Q, let's just use that. So in the bottom half of the screen here, you can see the output that comes from the function. So if I hover over my eternal window here, we should see the output as I add messages to the Q. Let's run the code now to add a message, and then immediately. What you'll see is the function being triggered in this year. You can see the message written to the locks. And if we had another, you see it triggered again. Hopefully, this simplistic example give you some idea of just how easy is to decouple two parts of the system from each other by exchanging messages through a cue instead of making direct cools.

Automation and Deployment
[Autogenerated] now it's generally considered best practice within this year, too. Automate everything and for resilient applications that are highly available, you need to take automation to the next level you see. No, only do you need your deployments to be automated, but also your role banks your disaster recovery. You'll fail overs you need to automate as much as possible and then use training and documentation when you require some manual steps or human judgment is part of that process. Now there are a couple of different ways within Azure talked about your set up. We've got automation accounts you can use power shell or the is your CLI. You can use tools such a chef for terra form. There is your Resource manager templates, which are Jason format that you can use to describe a set of resources. These would all allow you to set up an application with an is here the same way that we've done in the portal but without actually going into the portal. Now, using the portal is a great way to get familiar with. The different service is on the different options available, but with production systems, everything that you need to deploy that system should be codified and placed in source control. This is gonna allow you to easily replicate the entire system. We're just pieces off it. You'll be able to manage your infrastructure in the same way that you manage your source code, and this is known as infrastructure is code. Now you're also offers other features that help us manage our deployments like deployments Lot in APP service With deployments lots, You can easily deploy your code into stating environments and have that code switched into production. At the touch of a button, We can use traffic manager even of a single region to do what's known as a canary deployment. This is where you deploy a new version of an existing system and then use traffic manager to route just a small percentage of the load to that new version, say five or 10%. And a canary deployment can help you uncover potential problems before you roll out that deployment to a wider audience. So deployment on automation are crucial for resilience, and so is testing, which will talk about next

Testing
[Autogenerated] these days, every application should have the unit tests and integration tests to verify the correct behavior of the code on the system as a whole. Now, if you're not writing unit tests, then there are some great courses on plural, so that cover tester of in development in great detail and I urge you to watch them. But when resilience and high availability of your focus, then you're gonna need to take your testing one step further. You need to understand where your system can file and, more importantly, how it's going to behave when something doesn't file. Now, if you're using multiple APP, service is, for example, it's easy to simulate a nap service shutting down. You can just click stop when you do this. How is this system? Behave? What alerts get triggered? Do you get notified? What do you have in place to reroute the traffic? What happens if you regenerate one of the access keys in your storage? A camp well for a service bus queue or cosmos D B. And there's some other component of the system that was using the old one. It's much better to simulate these problems and find out what happens and how to diagnose and sold these issues when you do as part of an organized test. And while the focus of this course is not going to be on test simulation, you should also be placing the system into a mirror of a production environment, a staging environment where you can perform low testing. You'd be surprised what solve new problems you can uncover when your system is under load and you need to understand the behavior of that system and know the limits of it. Now, of course, all of this is going to require careful monitoring, so let's take a look at that.

Monitoring and Logging
[Autogenerated] distributed and loosely couple systems can make it much more difficult to diagnose problems when they occur. Now, fortunately, every is your service. Provide some basic level of diagnostics you can use to uncover the root cause of her failure. But you also want to add logging and instrumentation to the code in your system as well. For example, you'll need to know if your application suddenly needs to retry an unusual number of operations well, when the number of orders being placed has exceeded a certain threshold. At the very least, you should set up alert. Make sure you know who is responsible for dealing with them. She always be on the lookout for strange and unusual behavior from your system issued. Consider implementing a centralized locking platform that can correlate all of the raw data into a single location. Now the state can come from a variety of sources like Web server logs and database logs. Operating system performance counters and application insights is a great tool for this because it not only monitors a lot of information about an app to begin with from page views in response times and session and use accounts, but it could be extended so that you can hook it into your APPA monitor. Whatever is meaningful to you, I have all of this information stored in reliable storage but blob stores with table storage. You can then use the application insights, reporting tools for analysis and visualization you're using Micro Service's then application. Insights also supports correlation tokens, which allow you to track events and exceptions that span multiple different service calls. The key thing to remember is that as you start to add resilience and high availability features to your application, it's going to grow in complexity. It becomes harder to visualize your system and more difficult to track down problems, so it's important to invest time in monitoring and analytics early on in your system design.

Summary
[Autogenerated] in this model we focused on how we build resilience systems in the cloud. Now is your provides a lot of features to help us improve the quality of our applications from things like availability sets. Traffic manager We saw how we can decouple different parts of the app using cues, and we saw that when the default features air no enough, you can make use of others. Your service is to add architectural resilience, using traffic manager for everything from multiple geographic locations to canary deployments, about how you can use a cash to have more flexibility, more resiliency and different types of cues for decoupling component in a larger system. But ultimately every system is different. How you achieve resiliency in the cloud is gonna depend on the technical specifications for your application as well as your unique business requirements.

Cloud Patterns for Scalability
Introduction
[Autogenerated] Welcome back, friends. In this model, we're gonna be looking at the patterns and architectures that we can use to make our applications. And the service is that Dr, um, highly scalable. We're going to look at several approaches to improve scalability for both our front end Web applications on the back end date basis and everything in between. We're gonna look at topics like Partitioning and Scharping cashing using content delivery networks on managing AP eyes on as before, we're gonna be concentrating on those features, and service is within azure that make all of their so much easier to do.

It's All About Performance
[Autogenerated] Now it's impossible to talk about scalability without talking about performance. And while performance and scalability are related, they have very different implications. When we're talking about the performance of an application, then we're often talking about how responsive it ese. How long does it take to load a page, or how long is this database query take to execute? We're often measuring the performance of single operations or single transactions. So even if we have a single user using our system, then we can measure and meaningful observe the system's performance. Now, when we talk about scalability, we're interested in how the system performs as the amount of work grows. Perhaps a greater volume of transactions, more data, more users or more computation, the expensive operations. What would happen to our system with 1000 concurrent users? What about 200,000 concurrent users? Does this system perform the same with terabytes of data as it does with gigabytes of data? So by making our systems scalable, what we're trying to do is ensure that the performance of the system remains consistent regardless of the load. We don't want to lose performance as our load increases. So scalability two livers, performance, performance to the system, performance of the business and, more importantly, performance to the user or the customer. Now, when a customer hits our home page, they expect a responsive system. It's a vital element of that customer experience they won't know or indeed care. If our system is under heavy look on, a slow response for application will just cause them to go somewhere else. So let's take a look now at the type of architecture that we might consider for a skeletal system.

A Sample Architecture
[Autogenerated] now every application is unique, so the application architecture that we're looking at now probably won't be a perfect fit for your application. But it does show you a more generic system design that you might use in the cloud. So, firstly, let's talk quite users. Now, in the most part, we might expect these users to be our customers. Interacting with our application using Web browsers or native applications are mobile devices that could be other systems making. A P I caused to our Web service is in the most part, all of this traffic is gonna be arriving over http. So the first thing we're gonna need to do is have a way to distribute these requests on as we saw in the previous model. We can use your traffic manager for this, but there are many other options available to traffic Manager. Allows us tow handle the load by distributing request across multiple app service deployments. Now, our APP service is could be configured to automatically scale both horizontally and vertically in response to this increased load. So by combining traffic manager with the auto scaling features of APP service, we only have an application that's both resilient and highly scalable. But that's something else to consider here, too. And that's that. Not all requests coming into our system are the same to start off with, we're gonna have requests for things like Web pages for calls to an A P I. Now, some of these requests are gonna be simple operations, like asking for an image or C S s farm, while others are gonna be much more complex requests that kickoff complex operations elsewhere on the system. So there are a couple of is your resources that we might want to consider here to start with. If we're providing a programmatic a p I for other developers to access, then we can use this year's AP. I'm management options with an A p I management gateway, and this is gonna give us a way to manage the AP. I n points in our application. The house is to secure them, monitor the health of them, throttle a p I cause if we need to and scale that much more successfully, and we'll be looking at this in more detail later on in this module, another resource we can make use of here Is these your content delivery network, or CDN. We'll use the CD in in this module as well, to see how it takes the static content in our reputation, like images and style sheets and video and audio on, dhe pushes that content automatically out to multiple edge service around the world so that those resources will be closer to the users and therefore faster to download. Let's take a look now at the back end, but our persistence layer now here we might be using blob storage where might be using table storage or cues doesn't really matter when we're talking about scalability. Where's your storage? What we're really talking about is partitioning. Essentially, partitioning is a way of splitting your data into multiple chunks were rather than keeping it together in one massive chunk, we're gonna be exploring partitioning in much more detail later on in this module. Now, in addition to is your stories, you might be using the command and query responsibility, segregation or C. Q. R. S. Patton. We touched on that briefly in the previous module. It simply means the system could use one approach to read date or in a different post right data, perhaps operations that require a database update or database insert. We're gonna send that commanders a message to a service prosecute if you have lots of message service. But itself could be scaled using politician splitting. One message came into multiple message keys. And then perhaps the system uses Web jobs or even as your functions to process. The messages in those cues and functions provided very easy scalability model as their service was terrible just to sign the resources it needs to match the load. Ultimately, all of that back end processing is probably gonna update or insert data into a database like Cosmos Devi. Where's your sequel? And these technologies also support data partitioning to achieve large scale. So as you can see, partitioning is an important idea for large data on large systems. And if we manage the rights and the updates using a Q and A function, we might manage the reed operations. Using a red is cash to improve performance and scalability. We've got a demo of ready cash later on in this module two. Finally, in order to build a high performing and scaleable system, we're only good monitoring and instrumentation because you need to understand how the system is performing and be able to locate bottlenecks when the system stop scaling, what starts to slow down, And for this we could use the application insights. Application insights gives us a huge amount of information about how the application is behaving on performing. So hopefully that gives you a good overview of the various elements of a scaleable cloud design and the rest of this model. We're gonna explore these different topics in much more detail, so let's start with politician ing.

Partitioning
[Autogenerated] Now it's likely that if your application uses any kind of data store, then that data store is potentially a major bottleneck to scale ability. Imagine your application uses a single database server, and the load on your application starts to increase well. Eventually, as you make more and more demands of your storage, you'll reach this service. Physical limits Even if you pay for a bigger, more expensive machine with more CPU and more memory, the resource is available to that machine are finite. So this is where data partitioning can help. And if you look at any of your data storage resource, be that cause most eBay or is your table storage where's your sequel? You'll see options for partitioning, even if sometimes it might be using a different name. Now you will see this term partitioning pop up a lot in computing, there's hardest partitioning memory partitioning. There's network partitioning and logical partitioning with virtual machines. But what we're talking about here is data politician. Now, with data partitioning doesn't really make any difference what the data is. It could be orders, products, employee information. But instead of managing the data in one big monolithic chunk, we're gonna split it up or politician. Essentially, we're gonna split our data up into two or more pieces. That'll partitioning really is in its most fundamental sense. But there are different ways in which we can partition our data. So let's look at the three main techniques horizontal, vertical and functional partitioning. Okay, so first up, let's look a horizontal partitioning. We're gonna start off with some data. Now, again, it doesn't really matter what this data is. It could be orders or products or inventory details about media. The data itself just isn't important at the moment. So let's imagine. We have a table of customer data and then this table. We've got multiple attributes, different pieces of data for that customer. Now, to keep this simple, I'm just gonna call those a, B, C and D. Now, these attributes could be keys and values or columns if we're using a relational database, but it doesn't really matter. So now let's say that we're expecting to store about 50,000 customer records and we need to think about how we're gonna partition the data now. One simple ways to take that collection of customer records and split it across multiple data sources or multiple service. Now, if this were a relational database, said it ended up with one customer table with 50,000 rose in it. You might have five customer tables each containing 10,000 rose, and this is what we call horizontal partitioning. It's a popular, easily understood approach to partition in your data, and it provides. A lot of options were scaling. Now it's called horizontal partitioning because you split the data by drawing a horizontal division after a certain number of rows. It's also known as Charlene Picture, breaking the date or up into different charts. Now, one problem with horizontal partitioning is how do you determine where to draw the line where two sharp the data? Exactly which customer records are you gonna put into which politician? Now remember that scalability is all about performance. If we split our data in the wrong way, we could negatively impact that performance. Let's say we split our customer records based on when they were created. Let's say customers 1 to 10,000 goingto one partition on the next 10,000 goingto another and so on and so on. Now the problem here is that we're going to see some partitions being used more than others. The majority of reads and writes are going toe. All occur on the same partition rather than being spread across all of the politicians. Now there are some star is where this might be okay, but more typically, we would want to use partitions to more evenly distribute the workload. So a different way to do this is we might split these customers alphabetically like a phone book. Let's say customers with a surname starting a through G, going one partition and customers h through n into another partition and so on. This kind of approach will ensure that the return rights to this data and more evenly distributed across the partitions and this is known as range partitioning. Now this is still horizontal partitioning, but we're splitting the date on some kind of range, like a range of customer names or perhaps a range of ZIP codes to say what data belongs in what partition we could also partisan, based on a specific value or a key, we could perhaps put all customers from one country or one city into a single partition. We call this a partition key and we use it to help us decide how to distribute our data. Now, we might not always have a suitable piece of data to use as a partition key, so we might need to use an algorithm that will place new customers across different partitions in a round robin fashion. Nobody's done anything other than just distributing the load. Now, making a choice here really just depend on the application. It depends on your data on the resource that you're using and how that data is used. But generally speaking in horizontal partitioning, we use the same schemer for the same documents shape in all of the different partitions, which is spreading the date around. So that's easier to scale the hardware. And I had more processing power. As the data grows, horizontal partitioning is no our only option. We can also split our data vertically. Let's say our customer data Hearst five treats on our application frequently uses attributes A and C, but only very rarely uses B and D and a well. In this scenario, we can partition our data so that are frequently used. Attributes are in one partition on data that is used infrequently is stored in another partition. So in horizontal partitioning, we end up with multiple partitions, have the same number of columns, but fewer rose and then in vertical partitioning. Each politician has the same number of rows, but fewer columns, and there are a couple of advantages to vertical partitioning To begin with, there could be less database contention if the attributes are separated on the tables aren't is wide. We can also use faster high performance storage for are frequently access data and slower, less expensive storage for our infrequently used data. Also, because a N c contains less data, it would be easier and cheaper to cash, and it's less data that needs to be replicated. So there are several opportunities when using vertical partitioning to optimize your data. So the last 78 we're gonna look at is what we call functional partitioning. Now let's imagine that our application stores customer data along with order information that both our customer needs and even our own internal expectations for how this data is used might be very different. We might want really responsive riel time usage with customer later put our order. Data requirements might be simpler, slower, more batch orientated. Perhaps we're running over that reports or weekly print jobs. So instead of just assuming that all of our data must go in the same data store in functional partitioning thes two different sets of data could live in completely different data sources that are optimized just for that data on how the system needs toe interact with that data. So with functional partitioning, you might use a relational database such as is your sequel for the orders on dhe, perhaps Cosmos TV. For the customers, we could go further news horizontal or vertical partitioning for scaling specific set of data? No. Well, this is almost certainly adding a huge amount of complexity to our system. And you certainly wouldn't want to use functional programming for a simple application for large systems that contain huge amount of data. And that might need to be highly scaleable. Then functional partitioning is a worthwhile strategy that should be considered when trying to figure out the best data storage and partitioning strategy for your application. Now deciding on a partitioning strategy is not an easy thing, and it's gonna have complexity to your system design and impact how your system is implemented it can be challenging, for example, to join, date or enforce transactions across partitions. So partitioning your data like this will impact your architecture and how you write your code. But if you want to be scaleable on performance and you don't really have a choice, so let's now look a couple of specific is your surfaces and see how to use a partitioning strategy.

Horizontal Partitioning in Cosmos DB
[Autogenerated] Fortunately for us most is your data service is provide some level of support for partitioning right out of the box. Three years in cosmos __, for example, it has a horizontal partitioning built in on partition isn't just an optional setting, it's a requirement. Cosmos TV is a globally distributed databases designed for running a massive scale. When configured correctly, your date will automatically be distributed across multiple partitions on as it grows. It's gonna spit across even more partitions, but you won't have to manage those partitions yourself. You won't need to explicitly say how many partitions to create or which politicians to write it well, which partitions to read from. It's just gonna work and as you'll manage it all for you now, in order to make this happen to configure it correctly and describe how your data should be partitioned, you're going to need to think about it, and Cosmos TV is gonna make sure that you think about it up front at design time. When you're creating collections or tables or graphs, you're going to need to specify a partition key for your collection. Partition case should be some piece of data that you're already intending to store with each item on it can take a little while to figure out the best choice here. This is not like a primary key. We're not looking for a unique value to distinguish one document from another. We're after something that's going to allow us to distribute lots of data evenly across lots of partitions. So we need to think about how large it might be and then likely values it might contain. So, for example, if we were creating a collection to support orders, I'm were expecting 98% of photo orders to come from the U. S. And 2% from anywhere else in the world. Then country would not be a good partition, kay, because most of their values would be United States. And you're going to try and keep all of the same values together in the same partition, so the partitions would be hugely imbalanced. Now that doesn't mean a piece of data like country is always a bad idea. Because on the other hand, if I was creating an online registration site and expecting registrations from many different countries, then country might be a good choice to distribute this data We're creating a collection to store data from our Internet things devices such as status pings from thousands of different devices. Then device I D. Might be a good partition key, because if we expect the same device to connect multiple times, this would be repeated piece of data on we're wanting something that helps us distribute the data both the amounts of it on the return rights that we need to deal with it. Now it's important to realize that the partition key doesn't have to be something that only has a few possible values, like a country's name. In fact, it is best practice to choose a partition key that has as many values possible. I really want something with hundreds or indeed thousands possible values. It sure isn't going to create a politician for every distinct value it finds. But as your collection grows and when is your automatically decides that it wants to partition your data, it's gonna use this value to a group tater, and it'll keep the same values of this key in the same condition. So this is what I mean when I say that horizontal partitioning is built into Cosmos D B We don't get options for vertical partitioning here, but that's typically a much more manual process. But let's finish by taking a moment to talk about your sequel databases. Now you may have seen a reference to sharpen and is your sequel. Remember that Charlene is just the horizontal partitioning now, as your sequel takes, the data that would otherwise A will be in one single database and distributes it across multiple is your sequel databases for multiple shots. Each art has the same schemer, but with different amounts of data inside it. And you could describe how to partition data across these different charts, perhaps using a range like a range of dates or a range of customer names. However, we're not going to get into any more detail on this here because, well, there is good support for reading and writing is your secret databases using note the tooling that you need to manage the creation Skating of sharks is only available for dot net and Java run times, so let's move on and talk about some application patterns that we can use with data access to make our implications of more scalable

Application Patterns
[Autogenerated] so partitioning is a common and very effective architectural pattern we can use to scale up the data layers of our application. But there's a lot we can do inside our application to improve scalability as well. In the last model, we talked about an application that might use a single data source, such as Cosmos TB or is your sequel. But my also make use of an architectural pattern like the command and queer responsibility segregation pattern. Seek your s. This is where the abstractions and the code that you used to write the database the commands a separate and distinct from the code that's used to read from the database the queries now secure. It gives you a lot of flexibility for Reed operations. We might read data from a separate reporting database that's optimized just for the date of the system needs and pre aggregated data, we might go completely different. Way decided to implement the reed operations using cash like readies cash. Both of these approaches could increase the performance in the scalability of the application for right operations. The system might not even talk to the database directly. It might send commands to a message cues such as There's your service bus on. These might be processed by an issue. A function after a Web job. Using a hue like this helps us to build a more resilient system because a Q can capture message even when the other back end systems are being updated. Cues can also help with the scalability of an application. By leveling at the load and smoothing the traffic spikes in the system, let us keep the front end of our system responsive by not waiting for a long running operation to complete. We also never overwhelmed the capacity of our backend systems. Now, with all of this talk of queuing and cashing and partitioning, there might be wondering what the impact on the user is. What happens if he uses trying to publish data for other uses to say What happens if we Curie uses command and don't execute it until later? What does it mean if he used a saves data into the primary database? But another user is reading data from replicated database that isn't gonna be updated for a few seconds? Well, let's talk about that next

The CAP Theorem
[Autogenerated] now you don't have to work very long with large cloud systems before somebody mentions the cap. Fear, Um also known is bruised there after the computer scientist Eric Brewer. Now, a lot has been written about cap here. Um, we're not gonna go into too much depth, but it's important to have an understanding of this is it's gonna allow us to recognize some of the constraints and trade offs we're going to encounter in designing our system. So the C in camp stands for consistency. On here. We mean that consistency guarantees that every read operation will see the most recent data. Imagine an airline checking system where user checks in for their flight and there see, allocation has changed now in a consistent system. As soon as another user requests the flight manifest, even if just a few milliseconds later, they're going to see that the sea has changed. I'll see the most recent data. Now, if you're familiar with acid transactions in a relational database, then there is some crossover here with this idea of consistency, although they're not exactly the same thing. But we might use database transactions to support the sea in camp here next D A. Now the A in camp stands for availability, and we talked about this extensively in the previous module. Now, when we're talking about availability within cap fear, um, we say that every request must receive a response. Now we know we can never achieve perfect availability. But if we follow the guidance from the previous module and we can build availability into a system from the star, and then we might hope to achieve a system where, for example, 99.9% of the requests will receive a message. Finally, the p r. The peeing cap stands for partition tolerance. Now, this is not referring to a date A partition on I said, that partition crops up a lot in computer science in capturing the partition is a network partition. This is a partition between two parts of a system, something that splits the network and causes parts of the system to become unavailable. In some ways, this isn't this similar to the idea of graceful degradation expecting a system to remain responsive even when features we hope for aren't there. So let's say we have a system that's composed of four components A B C and D. Let's say that knows A and B can communicate. And let's say that notes seeing deacon communicate. But there's a problem which is stopping A and B from talking to see Andy. Well, if we're partition, Torrent then will still allow. And B and C indeed continue to operate and do. Their work is normal now, while Camped Theorem is specifically talking about network partitions. In reality, we're not just talking about network level areas here. We're talking about any kind of failure that takes a component off line, so it could be a hardware failure or software maintenance. Now, okay, that's what this C A M P River, too. But that's only really half of the story. You see Eric Brewer, the computer scientists who came up with this states that a distributed share data system can only have two of the three guarantees shown here. So what we're saying is a system cannot be available and consistent and partition tolerant all at the same time and in the design of distributed system, need to find an intersection of two of these qualities, the ones that you're most comfortable with now, this doesn't mean that we just give up on one of these features because if things were running successfully, we can continue to do this and have no issues. I'm more interested in what we do when there's a problem. Which of these things are we going to prioritize now? Technically, one intersection between the two would be between consistency and availability. But this is an unlikely spot for a cloud system because a lot of what we've been doing up to now has been introducing partition tolerance. We've been working with redundancy and queuing and cashing and other techniques. So let's make things a little easier because most typically with this camp here, um, I'm with cloud architectures. You're gonna begin by assuming that partition tolerant is something that you want on. If you need to pick two things, partition tolerance will be one of them. Then you just need to prioritize either consistency or availability. So if we decide that two of the three things that we're interested in the intersection between partition, tolerance and consistency is important to us than that says that we want to maintain consistency and have partition tolerance, even if this means sacrificing availability. So if there's an issue with the network, then I'm willing for the system, or at least parts of it, to stopping, available and stopped processing requests to make sure that this system stays in inconsistent state. There is another choice. The intersection of partition, tolerance and availability. We might want our system to be highly available and resilient network issues on. We'd be willing to sacrifice consistency to achieve it on. It's not as if we give up consistency. All together, we can assume that most systems are eventually consistent. As an example. Imagine I update my profile on a social media site. Now perhaps other users in other countries won't see my latest updates for seconds or minutes, or maybe even ours. Because these systems are no immediately consistent, the system will eventually be consistent on other users will eventually see my updates. This is the sore thing that might happen if you prioritize the system to be available on partition tolerant. We're not worried about up to the second consistency. As long as the different pieces of the system stay up on available now, obviously, the choice that you make here depend on your business. Does it need to prioritize consistency what does it prioritize? Availability. So while an airline check in system might finally consistency, my social media site might value availability. Now it's true that this isn't always a simple choice, but it's also true that it's no. One choice for an entire system. You might have pieces of the system that are AP on other pieces that are C P. If you have financial transactions in the system, you might need them to be consistent on partition tolerant. Where's the user profile of the system could be available on partition tolerant the camp there. Um, just says that what you won't be able to do is build the entire system in a way that makes all three guarantees for availability and consistency and partition tolerance all of the time. So with this in mind, let's start looking at different pieces of architecture for a system that falls more into the A P category.

Caching
[Autogenerated] So in the zone Pole Architectural diagram We looked at the start. This module, we placed it cash between the application on the database. Now cashing could be useful for a number of reasons. It can improve both performance on dhe scalability because it's usually faster to retrieve data from a cash than it is to go all the way to a database, execute a query on wait for that data to be put together. Cashing can also help with high availability on partition tolerance, because even if the database is unavailable, I'll have a copy of the data in a cash. How her second read only data source to work with. Of course, cashing does handsome, complex student system. You'll have to write and maintain more code, and we'll have a bit more work to do and debugging a date or a ship cashing can take away some consistency guarantees. If you allow the users to read stale data, you also need to figure out an effective cashing strategy and know how and when to invalidate CASS interests. And these are not trivial problems

Demo: Configuring and Using Redis Cache
[Autogenerated] So in this demo we're gonna look at how we configure and make use off on is your reddest cash. Now relative is an open source cash service that you can use for multiple application instances. You can think that isn't in memory distributed database, so read and write to extremely fast. It also supports replication and clustering and cannot pray at a massive scale. So let's take a look now in this demo we're going to add and is your reddest cash to a simple note application. Now I've pared this right back so that we can concentrate on the cashing aspect of this. So we're using a very simple load AP using express on pug Now the ant displays to pieces of text, a title and a message, and the message is retrieved from Cosmos D B now because this message doesn't change very often, we don't want every request to the home page, resulting in a round trip to the database. What we really want to do is cash this string. This will reduce the load in that database, make our application Maur responsive and help to make our system more scaleable. So let's take a look at the code. So the first thing to point out is the configuration for the cosmos client. Now there's a ll the usual stuff you'd expect here, such as connection keys and point the dresses. But we don't really need to worry about any of that here because we're concentrating on readies cash. So every time a user requests the home page at the moment we're running this code and it's firing off a request to Cosmos TV to retrieve the latest message. Once that data is retrieved, we render the page and you can see here in the pub template that the message is being written out here. Now I've deliberately tried to keep. This is basic and low tech as possible. But in the real world, I'm gonna want to extract this large account, probably into a separate component. But for this demo, it'll work. Fine. Now, note itself gives us a few cashing options all of its own. They're simple in memory, cashing in the pug engine on models like no cash. So you may be wondering why we need to use relished it all well read. It is a distributed cash that runs in its own process on separate infrastructure elsewhere on the network. It doesn't share the same memory. Space is our application Now. This gives us a few advantages. Firstly, we can have multiple instances of our application connected the same red it's cash. So if we deployed this website at different locations say the U. S. A. And Europe and each of those regions around multiple instances of our application, then each APP service instance can share the same cache data. Now, we also can't guarantee that every request to user makes will be processed by the same instance. So we need to make sure that every instance of our APP has the same consistent view of our cache data. Now, before we do any of this, we're gonna need to create a relish cast resource, Unusual portal. But before we do that, let me just jump across to the home page for readies itself, which is ready. Stop, bio. Now, this is what you need to go to learn about howto work with really set a low level. All of the documentation you need is going to be on this website. The other thing we're gonna need is the readies client for note. And it's worth pointing out here that this isn't a mark soft specific version. It's the regular Readies client for node on. There's great information here on that, too. There's a lot of usage examples and some great information to help you get started. Now read. This is a very versatile because not only can it function as a cash, but it can also be used as a database that has a message broker. We're gonna focus on the simple, most fundamental scenario of using reddest as a cash. Essentially, it's a key value store. So after creating our cash, we're gonna be able to tell readiness hiss and data. Can you store it with this kay? And then anybody that comes along with that key couldn't retrieve the data. Now read this keeps everything in memory, so it's extremely quick to both store and retrieve. Now, in this demo, we're just gonna be storing the simple string. But there are a number of different data structures that are supported hashes lists, sets, sorted data sets and so on. You can also get into replication and scripting with red lips. But again, we're just focusing here on how to cash our data. So here in the usual portal, click New Resource and then search the marketplace for readiness. Now, there are a few different options here, but we want the azure cash for Read it. This is the one from Microsoft, as ever. We need to give it a name and add it to a resource work. If we wanted to, we could set read it up in multiple regions. But at the very least, you should make sure it's in the same region. Is your most important service is this is gonna give you the very least. Leighton see? No, we then have the pricing tier. Now there are three separate is basic standard and premium on each offers several distinct options. Now, if I click this link here, we can see how the features vary by option, more money buys you more capacity clustering and a whole host of other features. Remember, though, that you can start small and scale up was your application grows for our demo. We're nearly two standard, so I'm gonna select that. So with that, we click create and after a few moments we'll have our new readies cash. Now if we look at the overview blade. Like most overview blades in this year, they're our metrics. We have hits and misses. How many times are applications going to the cash and not finding what they're looking for? Both the metrics on gets and sets, how much we reading and how much we writing to the cash. There's total commands, number of connections as well as memory usage in server load, which will want to pay attention to all of these things in a production environment. Now, if you run out of capacity, you can go over to the scale blade. I'm under scale. You concerted, different pricing tier. Perhaps a more powerful ready cash, right? So let's see if we can get our application working with reckless now, Like almost everything else it is You're breathless is has secured resource. So one of the things I need is the access keys. There's gonna allow me to get in step values from the cash. So let's just copy that now. So it's ready for later on. So back now on my coat and here I am in the index jazz file. The first thing we need to do is NPM. Install the node credits module, which I've already done, and then we need to add a required for readiness. Now, in order to interact with readiness, we need a reddest client that connects us to already server. There are a few overloads for this, but in this scenario, we want the one that allows us to specify a port for u R l connection. Key information. The default poor forever. This is 63 80. This is the U. R l for ready cash, which you can get from the overview blade, Unusual portal and in options. I just have a couple of things I need to provide first. It's the key that I just copied from eyes your access key section. And then I need the server name, which is the same name as the girl here. I can now use this reddest client to interact with the cash. Now, at the moment, we're using this retrieve message function to always connect the database to grab our message. I'm gonna still made this code, but let's wrap it up with some additional logic. So the first thing we're gonna need to do is try and retrieve our message from the cash now, if it exists in the cash, then we're gonna use that value. Otherwise, we need to go and get the value from the database. Once we've retrieved the value from the database, we can then update the cash so it's ready for the next request. Now, we don't leave this to live in the cash forever, so we need to set an exploration time for the key. So using the reddest client, we can start to build out our functionality. So first, let's call get Tau, Ask the cash for a value using our key. Now, if we get a value out of the cash, we can just render the page. But if no, we'll have to go to the database instead. So if we got a response to be called Cashed so we can just rented out here now in the else block, we now need to call the database from retrieve our message as before. However, this time, if I get a value back from the database, I also want to save it into the cash so it's available for the next request. So before I render the page, I'll save the message to the cash now instead of just using the set function, I'm going to use attacks. Set exploration. Now this takes an additional argument of the expiration time in seconds. This is how long it's going to exist in the cash before it's automatically deleted. Now, this could be an hour day, whatever you like, But for this demo, let's just set it to 10 seconds, and that'll make it easier for us to see what's going on. Now. This is just a demo, and in reality this code will be much more robust. However, one of the things about successful cashing is that is, by its very nature, it's kind of invisible. When it works. It looks exactly the same as it would if we were just going straight to the database. And now I'm gonna test this by just running this code locally. Even running locally, the Coke can still reach into both these your cosmos d be an angel, reddest cash resources just using the connection strings that I've provided. So here we can see the message, and it's clear that it's been retrieved from the database. Now that makes sense because at this point nothing has been added to the cash. However, if I hit refresh, we are going to be bringing back from the cash, which means that that didn't result in a round trip to the database. Now, if we wait a few seconds, what's going to happen is the key is going to expire, be deleted. So if I make another refresh, we're gonna get it from the database again. Let's hit refresh a few more times, then switch over to the portal. We can see things like gets and sets and hits and misses of the cash. Now the benefit here is that in a production application, you'd also expect to see so improved response time unless we load on the database.

Introducing Content Delivery Networks
[Autogenerated] So there is another way that we can improve the scalability and performance of our application, one that takes advantage of the general idea behind cashing and focuses on the front end. Vier Implication. I'm not excusing content delivery network or CD in a content delivery network is a specialized network of servers all around the world that you used to distribute static, publicly available content such as images and CSS files and JavaScript. In fact, anything that you might ordinarily serve publicly from the far system in your Web server. Now the CD in takes your content and makes it available from multiple points of presence locations around the world. So not only gonna cdn benefit your users by making content arrive faster. It could also help the performance in scalability of the application itself, because the main application no has fewer requests. Toe handle picks. Instead, the CDN handles some of those requests. There's your content. Delivery network is easy to set up. It includes support for SSL and custom domains, and you can cash public content from your cab service is or blob storage or, in fact, any Web location. So that's how the Moken sea how we use it

Demo: Configure and Use an Azure CDN
[Autogenerated] Okay, so in this demo, we're gonna take a look as your cdn. Now, a cdn allows you to take the static file content from your website, such as images, jobs through files on media files and this treatment globally. This means that they'll always be downloaded from an edge server. Closest your user. Now this reduces the load on your Web application host be that up service instance, or a virtual machine, and it makes it easier for you to run your application that scale. So the first thing we need to do is create a new lease you a CD and resource. So here in these your portal, or click the new button on just time to see the end. Now the first option here should be the CD in by Microsoft, and it's these your content delivery network. As ever, we need to give it a name, and so that's a subscription and a resource group. Now the resource group Location. Justice with traffic manager. It's a little bit misleading because Thea's your content delivery network is a global resource. There are notes everywhere in the world. We could have pops or points of presence. You also hear the term edge service to represent the actual physical service themselves. So the Resource group location is really just a place for all of the meta data about this resource to be stored. So now we get to select the pricing tier that there are standard and premium plans, and you can sell it either of them. From the top down. You can click the link here to open up more information. Now you'll notice that the issue of content delivery network is not built purely on Microsoft infrastructure. He's either horizon or recognize infrastructure, and the features will change depending on the plan that you choose. But remember, you can scale this up. Your application grows now for this example. I'm just gonna choose the standard plan, and they create. Now all we've done so far is create a content in every network profile. There's nothing actually in the content that every network at the moment, So let's do that now. So before our cdn is going to be able to serve any content, we have to describe some hasty tbe end points. But the Cdn confess the original content from now. It's important to understand here how many content of every network works. Now the first thing we're gonna do is at an end point. Biking is your Web app. This end point is thes source of our content that in point will have its own u R L. Now, when the request comes into the CD, and perhaps for an image or a Java script file, if the CDN doesn't already contain that content, then it's gonna go and retrieve it from the M point and then add it to the cash. This is essentially the exact same pattern that we saw with the reddest cash. If the item you want is in the cash, then return it. Otherwise, go and get it and then add it to the cash. The upshot of all of this is that the first request could be a little bit slow. It retreats the original content. Little subsequent requests will be very fast indeed. So let's see it in action. It's now in the content that every network profile I'm gonna have an end point. I've got to give it a name Now. This name will form part of the new you, Earl. It's for may. Well, be pierce wide brain doctors. You're edged up there, and that's your I'm gonna use to make requests for this content. So below this awesome settings for the origin. Where is the original content? Where do I want this content in every network? To find the assets from this drop down box, I can select things like a storage account or globe storage confession cash assets. From there, we also have cloud service kind of the Web APP service. I can even specify custom origin. So if I had a Web application running in azure of Virtual Machine or even in my own internal data center, I can still use the content delivery network to cash that content for this demo. I'm gonna select Web. This gives me another drop down where I can select. One of the APP service is that I've already got deployed. So let's select that. And then let's talk about origin path. So let's say I want the CD end to fetch assets from my APP service, but I only wanted to fetch assets for a specific location in my AB service. Now I can specify this in the origin path. So, for example, I could specify public If I knew that the assets I wanted to cash, we're in a folder called Public. Well, perhaps the images. Then every time the content delivery network needs to fetch content, it's gonna take the earl that arrives at that CD and address, and then it'll turn around. Go to the origin location into this path. Images. So here we're telling these CD, and it can only fetch assets that are in the images path of my Web application. Using an origin path is a great way to restrict what content gets cashed. It is useful if you have a dynamic content or protected content that you don't want to push out to the Cdn. Now the bottom here is a drop down box to say what I want this CD into optimal list for, and then we have options for large files videos on demand streaming. General Media's dreaming. I'm gonna accept the defaults of General Webb delivery. I just go ahead and at this end point now, remember, a cdn is a massive global network. So when you create in U. N. Point, it isn't always immediately available for use. It can take a little time for it to propagate through the Cdn, sometimes as long as a few hours. Okay, so let's switch over to our code now. This is the puck template for our home page Here. You can see where we add the image on, but this is the original location off the content. So what we need to do is serve this content through the Cdn. So let's update the U. R L so it uses the address of our CBN edge server now because we've configured in origin path, we don't need to specify the images folder here in this you are out now. Obviously, there's nothing there right now in the CD N, but we don't need to manually add anything to the CD and either because the first time this request happens to see the end or realize it doesn't have the file, go to the original APP service and retrieve it. So let's save this file now and deploy our up service so into the term like, Oh, let's do it, commit and then push it. That was Your devils will build and deploy that, and then we'll have a new APP service deployed in a few minutes. So in the browser now, and here is our home page, and here's our image. But the big question is, where is our image coming from? Well, if we said, Let me image and open it in a new town, you can see that it's being served from our CD and EJ server. So when it first received the request, it went to our up service grab the image in cash on that image is now being cashed on the CD en now you might be wondering what happens when you want to change this file. Come for a different image? Well, a general recommendation is that if you're expecting files to change and you should create new ones with a version number in the farming and that not only make sure that the CD and counts it is a new item, but it also takes care of any cashing that might be happening in the user's own browser. However, if you were in the CD in the portal, you can go ahead and purge the content that every network, by providing explicit parts to perch like logo PNG and that will get rid of it. And then the next time it's requested, it's gonna go out and grab it again. That's the basics of using a content delivery network

Exploring the API Management Service
[Autogenerated] If your application provides an A P I for other developers to use, then you might want to consider using these years AP I management options to bride in a PR date way. Now we're not talking about hosting or a P I. Here you can use an APP service for that, or you could use a virtual machine or a function. The AP Management Service allows you to take your AP eyes and organize and publish them for other developers are the partners or customers, and if you want to, you can charge money for the use of your A P I. The A P I management. Gatewaycan provided a lot of the infrastructure of monitoring to support that. I'm from a scale ability perspective on why we're covering it here. And I wanted to even think they are busier times. You can do things like rate limit throttling so a specific customer can't _____ your A. P. I. You really should think of a P. I. Gateway is a proxy is a layer of indirection between your clients and the actual operations across one of your AP eyes. It also automatically generates its developer portal, where uses your a p I from the log in tests a p I course create an access keys on view the documentation. There is also a publisher portal, but this is being phased out in all of the functionality there is being incorporated into the main is your portal. Let's take a look.

Demo: Preparing and Importing an API
[Autogenerated] So in this demo we're going too important. A p I interviews. You're a P I management service, but before we can do that, we need in a p I that we can work with. So let's look at that first. Now, in preparation for this demo, I've already created a new year, a p I am service. I've deployed a simple A p I that I've written in note. Now the code for this is fairly straightforward. The A P I exposes to operations a get operation that retrieves a list of all products. This has optional parameters that support searching and paving and that sort of thing. Then there's a post operation that support adding new products so I can see this working by browsing to the service. The home page of this site will just give me a cannot get message. But if I follow the euro with products, I get the J song with all of products in it. So this simple a p I that we have is gonna be enough for us to get started. But before we do, if we want to allow the developers to invoke our a p I. If we want them to be able to call these methods these functions that were defining them. We're going to need a way to be able to provide some kind of description. Some way to be able to say, What's this called? What operations are available here, What parameters dough I need. Why's DTB? Ferb is used to call this and so on. Now there are a few different metadata for months that we can use to describe it, a p i in a way that other developers or even other tools can understand. Firstly, this whistle, the Web serves his description language. Now this is an exile former and is often used to describe. Soap service is like W C F Service's. There's also Waddle short for Web application description language, But a really popular way to describe rest play P Eyes is a little swagger, and swagger is an implementation of the Open A P I specifications, which was designed specifically to describe modern rest ful ap eyes. Now our a p I is described in a swagger file, and we can see that here. It describes the operations that parameters and contains everything that you would need to have to consume the A P. I are no doubt also uses the swagger tools to produce a swagger document, and it even produces a simple website that you can use to help test it. I go back to the _____ of our AP eyesight, and instead of looking up the products, I follow this with dogs. I'll get a simple site where I can see the operations that are available. I can even see examples of how to call them now. In a way, this is very similar to what we're gonna be asking is your to provide for us, but in a much more managed way. Not just do. We want a way to view and test. But we also want to be out to restrict access throttle calls and combine multiple AP eyes altogether, which is something that's when all his own won't do now. Well, swagger is not required. It does make life easier because I'll be able to give these swagger meta data to the is. You're a P I management service and uses that to define the AP I and documentation instead of having to create all of this information by hand. So with this a P I in place and I'll swagger. Document defined. Let's look next that creating the AP. I'm management service itself. So in the portal, let's add a new resource has searched for a P I and then select a P I management. Now, as usual, it asked us for a name, and this name is gonna form part of your L for the A P I manager. One of the benefits servers years FBI management is that will allow us to have a single location through which we can expose multiple AP eyes. So I could use this not just to manage the AP I app service I made. I can also bring other AP eyes and service is elsewhere. Is your like a pea eye's hosted in service fabric all running on a virtual machine or perhaps a functionary? And this is the single Ural that customers will use to reach these AP eyes and any operations inside of them. Now, customers won't need to know how these AP eyes have been implemented. The FBI Management service will just root the request to the proper place within this year, and we'll see how to do that in just a bit. Next we select a subscription in a resource group, and I also need to provide an organization name here. I'm providing administrative email address. After this, I can select a pricing tier. I'm just going to go with the developer tear, which is ideal. Fantastic, although there's no service level agreement, so you should never use this for production. As with most resources in Asia, you can scale us up. Is your application crows now? Creating this could take quite a while, so I'm gonna skip forward a bit. Great. So the AP Management Service has been created, and here we are now on the overview blade. Here we see details about the management service as well as top level metrics for requests that have come in through the service, such as request oration on errors over on the left. We have some of the usual options that you would expect to see in the initial resource, but security configuration scale pricing, options, monitoring. That's foreign troubleshooting. What we want here, though, is the developer portal, which you can access from the link at the top of the overview blade. Now the developer portal is a generated website where our customers, business partners or anyone else that wants to use our A P I's consign it and get the information that they require now because I'm locked into the regular is your portal. It recognizes me as an administrator, which opens up extra options and extra menus that I only to design, brand and configure this site in anywhere I like. It's very easy to use and to customize, and I can publish this and make it available to developers anywhere in the world. And they can use it to get information about and start using my AP eyes. So just so you can see the difference. If I switch browsers on Brown's to the site, you can see what the typical develop would see if they came to this page. They can sign in, sign up, browse the AP eyes and much more. Now under the AP eyes tab. Here, I can see that we already have available for this website. You see, when you first creating a P I management service, you'll have a simple default ap. I called Echo a P I, but you can see here we're going to be adding our own a PR in just a minute. But first I want to show you, in general what developed can do through the documentation available in the developer portal. Now, remember that this documentation is either created manually or it's created from your swagger. Documentation. So if we select the echo a P I, then we'll get the AP on documentation. On the left hand side, we see a list of all of the operations and it shows you what, http, For years here, we have post put to lead head and get operations. Selecting all of these operations will give you a description at the function along with your l that you need to invoke it. It provides details of any request parameters you need and details of the response that you can expect. So if I wanted to interact with this a p I just to see what sort of data it provided for different parameters that I can click the try it button here, this gives me a simple form that I can use to create an issue requests at the bottom here. There is even coach snippets in various languages that I can use to get me started. Now if I come back to the top of the website. There's also a list of products on by products here. I mean, a way to use this AP myself as a product as a service now by the full. The AP on the management option in his year comes with two products. One is the starter. Proctor More is the unlimited product. Come on, you important you a p I. You can assign that a p I to one of these products, or even to find a new one. We can then apply policies to these products. So, for example, with the starter product any AP eyes that fall into this category will only allow subscribers to make upto five calls a minute. There's also unlimited product on any AP eyes. I place here give subscribers unlimited access, but it's set up to require my approval. Before subscriber can have access to this much. You subscribed your products. You'll be able to manage your access keys here so you can see that the developer portal officer lost features on its highly customizable. What we're really interested in is how we use this with our own a p I. So let's go and do that. Now, Now there is a special publisher portal, but this is being removed, which is why it's Martha's being legacy. All of the features of being incorporated into the main is your portal. So in order to add our product a p I we need to select the AP Ice blade. Now, here we have quite a few options to choose from. Depending on what sort of a p I we're working with. We could use whistle or water'll also lets a function apple cab service. Okay, so you want to choose open a P I now here. All we need to do is provide the u R l for our a p i documentation. Now, as your is gonna read that document to retrieve all of the metadata it needs to construct the AP, I in the management service. Now we can see the operations have been created, but we're not ready teas. R a p I just yet First we need to configure it. So let's do that next

Demo: Configuring an API
[Autogenerated] So in the previous demo, we imported R A P I into the A P I management service using this swagger document when we had a brief tour. If the developer portal in this demo, we got to configure the AP I and then issue a request through the portal. If we select the AP, I from the A P I's Blade, then we can see that we have a few more options here. The ones were really interested in here. Our design on settings. The design tab is driven by our swagger document. It describes the AP I operations and parameters that our service is made up off on the Settings town. We can configure the AP on itself. First, we need to configure the products that this AP eyes available in. Let's have this, a P I to the starter and unlimited product so that it will be available for everybody. We can also specify whether a subscription is required or no, but we can also configure some security settings. Next, we need to configure a cause policy cause or cross origin. Resource sharing is a mechanism that allows restricted resources on a Web page twice as resources on another domain in order for the test functionality in the developer portal to have access to our a p I, we need to configure a policy. Now there are lots of policies, and they allow me to configure all sorts of things, not just cause I can limit cause perky or per subscription Aiken blacklist or even whiteness specific. I p addresses on much, much more. And I could do this either at the A p I level or four specific operations. So if we select all operations on, then add a new inbound processing policy, we can then select its cause policy. Here I can enable methods and configure the headers as well. And this one is sure that requests from the developer portal past correctly thrilled to our underlying a p I. So if we switch back to the developer portal now, we can see this in action. So if we select the get operation and then click, try it. I think click send. You'll see. We get a 200 response back from the service along with the Jason Response, consigning our products. Now there's a lot more that you could do with a P. I manager, but I hope you can see how it could be a very effective resource if you want to share, combine and control access to your AP eyes.

Summary
[Autogenerated] So in this model, we looked at some of the issue of features and patterns that we can use to build more scaleable applications. There aren't many options available from partitioning data, and our data sources, such as using partition keys in Cosmos D B or sharking, is your sequel. We can also use features such as AP Service Auto Scale. Well, it's a cap fear, um, and how we need to typically prioritize either data consistency or data availability. And then we looked at how we can cash data. Using readies cash. We explored how we can use your CD end to distribute our static front and content across the globe. Reduce the load on our APP service is making our applications more scaleable on more resilient. And finally we explored a P I manager saw how we can use it to manage all right, guys. So with that, our journey into sure has come to an end. Hopefully, you have the knowledge and understanding toe. Apply these lessons to your own applications. Thank you for watching. And if you have any questions or comments, please feel free to reach out to me in the course comments or contact me on Twitter. I'd love to hear from you
