If you are a Node.js developer, and have ever considered deploying an application into the cloud, this course is for you. In this course, Developing with Node.js on Microsoft Azure - Getting Started, you will learn how to work with Azure to deploy Node.js applications into the cloud. First, you will learn how to scale, monitor, and troubleshoot a Node.js application on Azure. Next, you will implement databases in Azure including the Azure SQL database and Cosmos DB. Then, you will discover Azure storage and take advantage of serverless computing concepts with Azure Functions. Finally, you will learn how to set up a continuous delivery pipeline in the cloud. By the end of this course, you'll have the knowledge that you need to get started with your Node.js applications in the cloud.

Course Overview
Course Overview
[Autogenerated] Hello, everyone. My name is David Tucker and welcome to the course Developing with no Js on Microsoft Azure getting started I am a cloud consultant and I helped organizations plan, build and implement custom software solutions in the cloud I have over 15 years of experience in software, architecture and development. And during that time, few things have fundamentally changed computing like the cloud. If you're in no Js developer and have ever considered deploying an application into the cloud, this course is for you in this course developing with no Js on Microsoft Azure getting started, you will learn how to work with azure to deploy no Js applications into the cloud. First, you will learn how to scale, monitor and troubleshoot and no Js application on Azure. Next, you will implement databases in Azure, including the Azure Sequel database and Cosmos DB. Then you will discover azure storage and take advantage of serverless computing concepts with azure functions. Finally, you will learn how to set up a continuous delivery pipeline in the cloud. By the end of this course, you will have the knowledge that you need to get started with your no Js applications in the cloud

Foundational Concepts
Introduction
[Autogenerated] Hello and welcome to the first module of the course Developing with no Js on Microsoft Azure getting started. My name is David Tucker, and I work every day with organizations to help them bring their visions to life in the cloud. And I'm excited that you're here joining me and looking at how you can take your no Js applications and launch them into the cloud. Now, in this initial module, fundamental concepts, we're gonna be covering the information that you need to know about Microsoft Azure to be able to progress through the remainder off this course, we're gonna be looking at several things. For example, we will be exploring how Azure organizes their global infrastructure. In addition, we will be launching our first virtual server and we'll be exploring the command line tools that are provided with azure. And you'll be able to do all of this with your own account as you follow along. So I'm excited that you're here with me now Let's dive in to the course

Cloud Computing on Microsoft Azure
[Autogenerated] So next we're gonna talk about how we leverage the power of the Azure platform. But before we get too deep into that, let's take a step back and look at what we're going to be covering here within this initial module. First of all, we're gonna be reviewing the capabilities of the Azure platform and is a part of that. We're going to be exploring a concept called azure Regions and understanding how Microsoft organizes the global infrastructure for the platform. Then we'll be examining some different cloud computing models, and then we'll get into creating an azure subscription so you can follow along with everything that we have in this course then will be accessing. The azure portal will be launching a virtual machine, and then we'll be utilizing the azure CLI, or command line interface. So let's take a minute and begin to talk about what Azure is. So as a platform, Azure has many, many different services were on Lee going to be using a very small percentage of those overall services. Let's talk about some of the different categories of services that are provided by the platform as a whole. So first of all, we have storage, and there are multiple services that are provided for you on the platform that can enable you to store your data in the cloud. In addition, there are multiple services that provide compute capabilities. So if you want to do something like launch a Web server that then conserve users or if you want a batch process large amounts of data, those are things that are all provided in terms of compute services, and there are many different services, and we'll be exploring a few of these within this course. Then we have databases for situations where we have structured data that we need to be able to analyze and access. And there are multiple databases that are supported in the platform. Now, maybe at times we're looking at how we automate aspects of this and how we build a process around how we interact with the cloud. And in that case, we have azure Dev ops and an entire suite of services that gives us the ability to automate what we're doing from beginning to end. And maybe you're mawr interested in some advanced insights out of your data, and you want to look at the AI and machine learning capabilities that are provided. Maybe you want to deploy devices that have sensors that can gather data for you all around the world, and you're interested in the Internet of things capabilities. With I O. T Central and I O T Hub. Chances are whatever you're doing, you can do it better and more efficiently with the services that are provided by Azure. Now, one of the myths that I often hear from developers is misunderstanding and thinking that the platform is really only targeted at Windows developers. But in this case, if we look just at the different languages that are provided for the azure sdk is we see support for Java as well as typescript and JavaScript and python .net go and PHP. And if we want to look at one of even the newer services what we have with azure functions, we can see that out of the box. You have support for C sharp JavaScript F sharp Java power shell python typescript. So we can see here first that this is not just a platform for .net developers, not just a platform for Microsoft developers. Chances are there is a way to run a solution in whatever language you prefer on the platform. There's just some support for some languages out of the box, so let's begin to dive in and understand how Azure organizes its infrastructure globally.

Azure Regions
[Autogenerated] now a majority of the work that goes on behind the scenes to make Azure a viable platform around the world you don't have to worry with. You will never have to deal with understanding how different data centers are connected. Understanding the underlying configuration layer, understanding how data flows. All of that gets managed for you without you having to even think about it. But there is a concept you will need to be familiar with, and that's the concept of azure regions. So, according to Microsoft, Azure Region is a set of data centers deployed within a Layton see defined perimeter and connected through a dedicated regional low latency network. In other words, this is a set of data centers that work together within a geographic area, and when you're going to deploy a solution onto the platform, you're going to need to select the region where you want it to be deployed. Let's quickly take a global look at the infrastructure. So first of all, we have several regions that are currently launched for azure. Here, you can see these regions around the world. You can see that we have many, many different regions within the United States. Europe has several. We have several in Asia. There's a lot of support for azure all around the globe. Chances are, wherever you're at, there is a region near you. Now, in addition to these regions, we also have several that have been announced and depending on when you're watching this, and these might have already launched by this point. But we see we have new regions here in Mexico and Spain and a few in the Middle East. Now, this gives us overall 58 regions worldwide. And of those that means you have access to Azure really for 100 and 40 different countries. Now, an important thing to note here. Not all of these regions are available to you. Some of these regions are not available to the public because they are for specific government uses. Now let's talk a little bit more about what Azure manages for these regions. So out of the box, you don't have to worry about concepts like power. The platform takes care of making sure that each region is going to have the power that it needs, as well as networking and hardware. They take care of things like physical security who can actually access these data centers and who can't and even the underlying layers that handle things like virtual ization. So when you're using a platform like Azure and these are things you don't even have to worry about now there are two additional infrastructure concepts that I want to make you familiar with. However, you won't need to know these in depth for this course. So first, we have geography ease, and these are groupings of regions that enable organizations to make sure that they have compliance with regulations. So you might have a law that says that your data needs to reside within a certain country, and you need to have an assurance that if you put it in one of a few different regions, that that will remain true. So this is the concept of geography Ease now. Another concept that has to do with high availability applications is the concept of availability zones. So some regions support these separate physical locations that utilize separate resource is meaning separate power separate networking so that you can ensure high availability within that region.

Azure Resources
[Autogenerated] So next we're going to discuss the azure services that we're going to be leveraging within this course now. First of all, let's talk about some different cloud computing models that exist because this will factor into how we leverage the platform. So first of all, let's imagine that we have a spectrum that goes from left to right, and on the far left end of the spectrum, we have maximum control, meaning that we can do whatever we want in the cloud exactly as we want to do it. But with that, we're going toe own, having to maintain that long term and making sure that we keep it up and running. But on the far right, we have minimum maintenance where we might have to build our application in a slightly different way than we would have originally. But we don't have to do nearly as much to make sure that it stays running. Let's talk about this in terms of actual terms on the spectrum. So on the far left we have what we call infrastructure as a service, and we're actually going to be launching a virtual machine within this specific module, and that is following an infrastructure as a service model. This means that we get access to those underlying servers. But we have to configure them, maintain the OS, install patches. But we can configure that server to run. However we would like now, on the far right, we get the other end of the spectrum and that software as a service or s A s. That means we get to use a service like maybe Gmail, for example, where we don't have to worry with any of the underlying services at all. We simply get to use a piece of software that's given to us. And there are some services on azure that are implemented in a software as a service model. Now, in between those two, we get in some ways the best of both worlds, and that's with platform as a service. And that means that while we will have to maintain a little bit more than we will with software as a service, we also will gain a little bit more control in the process. So here, within this particular course, here's what we're going to be implementing. So in terms of your no Js applications, we're going to cover how you deploy how you then monitor how you manage, troubleshoot and then ultimately scale your applications. Now, to do this, we're gonna be looking at four different categories of services. So first we have databases and here will be looking at how we leverage Azure sequel as well as Cosmos T B, which is a document based database. Then, from a compute perspective, will be looking at how we leverage APP services and azure functions will then be looking at automation and how we leverage the azure sea ally as well as Azure Dev ops, and finally will be looking at just raw virtual machines or infrastructure as a service. And we'll be implementing that here within this module.

Creating an Azure Subscription
[Autogenerated] So the first step that we need to take before weakened I've into Azure is to look at how we can create an azure subscription. So here's what we're gonna be doing within this clip. First of all, we're gonna be creating a new azure subscription and then we're gonna be reviewing. The resource is that Microsoft provides four new subscriptions, So let's dive in. So I'm here in the browser and I'm at azure dot Microsoft dot com and you'll see here that there is a big green start free button. So I'm gonna click on that button, and here you can see I'm at a page that is highlighting the resource is that Microsoft makes available to new subscriptions. So if I scroll down, they're going to show me three things that I get First. There are 12 months of popular free services, so many of their most popular services on the platform. They've given you a set of things that you can do within this initial 12 months and use them totally for free. In addition to that, there is a $200 credit. So for services beyond those free services or beyond the free capacity they've given you. You can use this credit now. Once your credit is done, you can transition your subscription to a pay as you go subscription. And this will give you access to use anything you want on azure, but you will pay for those. Resource is as you use them. But here, while you're in this trial period, you can use that $200 credit and that $200 credit will be more than enough to complete everything that we have included within this course. Now, another important note here. If you or your company are on MST and subscriber, you might have additional azure credits that are available to you. So be sure to check with your subscription or check with your employer to see what resource is you can leverage. Now. Once you've seen this, you can scroll down this page and begin to look at the different products that are free within that initial 12 month period. And this means potentially five gigabytes of file storage or sequel database up to 250 gigabytes. You can see all of the things that are provided here. It also will give you a list of the services that are always free, things like a five gigabyte azure cosmos DB and a 1,000,000 requests per month for a function. So those are the different types of things that it will show you on this page. Once you are ready to actually create your subscription, you can scroll back to the top and simply click on the start free button. Now, an important note here. You will have to have a credit card to create your subscription, but you won't be charged anything during your trial period. You won't be charged anything until you transition over toe a pay as you go subscription.

Using the Azure Portal
[Autogenerated] So in this clip, we're gonna walk through the process of how you leverage the azure portal now that you have a subscription that you can leverage. So here we will first be walking through how you access the azure portal will then be reviewing your subscription settings and showing where you can go view those. Then I'll be customizing the portal experience and finally, viewing the resource is that you have launched in azure on your account. So I'm here within the browser, and I'm simply going to go to portal dot azure dot com. Once I'm here, I'll need to log in with my account and then I can hit Sign in, and this will launch me into the default view for Azure. Now I want to point out several things. First of all, I want to point out that we have the Azure page header and this is going to be present all the time when we're navigating through. So if it any point, there's something that you need to do that maybe is normally here on this initial home page, you can search for it here from within the search box in the header. In addition, I also want to call out that over. At the far left, we have the azure portal menu, and you can access that at any point by simply clicking on this button at the top left of the page header. So when you do, you'll see that there are several items that are provided. Some of them are here by default, including home and dashboard, as well as creating a resource. But then you have the opportunity underneath toe. Add in your favorite services. Now there are several things that are provided here by default, but this is customizable Now. Azure provides so many different services you're not going to see every single thing listed here. And this is where the search box at the top of the page header can be beneficial in helping you find what you need. But if it is something that you use on a regular basis, you can choose to go in and add this in to your favorites. Now, in addition, I want to point out over at the far right, you have the ability to go in and see your user account. So if I click on my name in the top right. I'm gonna be given an option to see my account as well as toe log out here of the portal. Now I'm going to navigate over to the gears icon here within the page header. So from here, within this menu, I'm able to go in and customize my experience with the portal. It gives you the opportunity to change things like how often it signed you out when you're inactive as well as changing your default view. For now, we have home selected. That means that we're going to be able to see this view any time we log in to the portal. However, you might later have a custom dashboard that you've configured with specific information that you're interested in. And you could choose to change that to your default view. In addition, you could change the mode for the portal menu to instead of being a fly out like it currently is, it could be docked, in which case you would always be able to see it. We'll choose to leave it as a fly out for now. In addition, you can go in and change your theme if you prefer to have more of a dark mode. You can choose to set that here as well. So these air valuable settings that will help customize the experience for you as you work with Azure. Now, another thing I want to point out there will be times, especially as you're launching. New resource is where you will be getting notifications from the platform. Those notifications will be coming in here within this item and you'll be able to see a log of those notifications. And it also will show you an icon if there are current notifications that you haven't yet viewed. So this gives you a high level overview of initially how we would actually navigate within the azure portal. Now, one of the things I want to show you how to do next is I want to show you where to go Look for your subscription settings. Now, you can see here that I have an option here for subscriptions on the home page. But as I mentioned before, if you don't see this option, you can always go here under the search bar and simply type in subscriptions. From here we can click on subscriptions and we can go in and see our information. Now I have a single subscription applied to this account. You could certainly have more than one subscription. If, for example, you might want to divide your different usage in Azure based on different clients that you have, you could certainly do that and have different subscriptions for them. But in this case, I have a pay as you go account that is specific for the work that I do here for plural site courses. So this is where I go in to see information for my subscription. And if I click on this, this will allow me to go in and see information about the amount that I've been billed as well as looking at historical cost that have been associate ID with this subscription. So now I'm going to navigate back to home. Now, the next thing I want to show you is something that will be valuable for you as you navigate this course. There may be times that we have you set up different pieces of infrastructure for this course, and we want to make sure that you actually spend those things down. We don't want to use more of your $200 credit than what we need to use. So if you wanna have one place that you can go to where you can see everything that has been launched on your azure account, you can click on the option here. For all resource is when you click on this, it will give you a list of resource is that are currently on your account. And if you have multiple subscriptions, you can choose to go in and actually filter this based on each specific subscription that you have. No, in this case, because this is a very simple account that I use on Lee for the work that I do with plural site. I only have a single subscription, and right now I only have a single resource that has spun up. But here, this is where you can go in and find the resource is that you've launched. So if you wanna have one place to go where you make sure you can see all of your resource is and that you can delete them from this spot so that you are no longer charge from them. This is the location you can get there from the home screen you also concert for all resource is from within the search bar within the page header

Creating a Virtual Machine
[Autogenerated] now, the next step that we're going to take is we're going to create a virtual machine on Azure. Now, if you remember, we talked about infrastructure as a service as a different cloud model. And this is what we're going to be leveraging here within this clip is setting up just a plain virtual server that we can run at. No Js on. So here's what we're gonna be doing. First, we will be launching the virtual machine. Not to do that, we're gonna have to review several concepts, for example, looking at virtual machine sizes and Siris on Azure as well as looking at topics like resource groups. And we're gonna be configuring public key authentication, toe access, our server. So let's dive in. So I'm here in the portal and I see that I have virtual machines here on my home page. But as mentioned before, I could simply go in and search for this as well. I'm going to select the option here for virtual machines. Now, I don't have any virtual machines currently under this account, so I'm going to go in and click the add option. Now when I do, it's going to walk me through the process of creating a virtual machine. Now, I'll go ahead and tell you we're not going to look at every single option that is available when creating a virtual machine. But we are gonna hit the most critical ones that you will need to actually be able to get a virtual machine up and running. Now, the first thing that you'll need to do is you will need to select your subscription. And this is important because if you have multiple subscriptions, this is going to dictate under which subscription you're actually going to be charged for this resource. Now, the next thing I'm going to do is I'm going to create a new resource group. We're gonna call this test VM now a resource group is just a logical grouping of the resource is that you have in azure and this is important. Let's talk about how you might use this. If you are working with multiple clients, you might choose to group each individual project under a resource group. Doing this makes it very easy for you to go in and manage these as a group. For example, if you're doing a test like we're doing right now. We can choose to create a resource group for this. And when we're done with the test, we can go in and delete every resource that is associated with this resource group. It just makes it that much easier to know what resources are associated with which project you're working on. So we'll click. OK, now, the next thing I'll need to do it. We'll need to give the virtual machine and name also call this test VM Now. The next thing here is that you're going to need to select the region. Now I'm going to leave it at East us. But you can go through here and see all of the different regions that are available to you, the regions where you can choose to launch this virtual machine into. In most cases, it makes sense to have this be near where you're at or where your client is located at, or you expect the users that are going to be using the server to be located at because the closer they are physically to the server. Generally, the less Layton see you're going to see in that communication with the server. No, I'm not going to select anything for availability options because we're not looking at creating a high availability environment at this point. But the next thing we need to look at is the image. Now there are several different images that are provided, and we're actually going to stick with the one that is selected here by default, which is in a bun to 18.4 LTs version. But you could go in and browse all of the different images that are available, and there are different types of images you can choose to leverage some that are just base OS versions. But you could also choose to leverage some that have been pre configured for specific tasks like here. You can go in and see that we have the wows of streaming engine that's available, for example, and you can see all of the different categories here within the market place that you can go in and launch. But in this case, we will choose to leave it just at the default value that we had previously, which is the oven to server 18.4 Now the next thing we need to look at is the size because the size will dictate a couple of things that you probably care about first, the size will dictate. The resource is that are available to your virtual machine, including your CPU capacity and the memory that the server has. But maybe even more important for that is this will also dictate the cost. So what your server will actually cost to run on azure so we can click on the option here to change size. Now, by default, it has selected some different types of servers for us that we can choose between. Now you'll see on the last we have VM size, and this is a string that just indicates the name of a specific VM size. For example, we currently have selected a DE to S V three. Now the first letter indicates the Siri's, and there are different series that are available. For example, the D series is just a general purpose Siris for V. EMS, where the F series is one that is compute optimized. So if you're doing something that's going to be really heavy on that compute like maybe you're trans coding some videos, the G series is going to be something that is memory optimized. So if you're doing something that's really in depth from a memory perspective and the in Siris has GPU capabilities that are included with it, and that might be really valuable if you're doing some machine learning activities now, there are many more Siri's beyond the ones I've just listed here. In this case, we will choose to keep it at the default value that was previously selected, which is a DE to S V three. Now the next thing we need to do is we need to decide how we're going to actually connect to our virtual server. This is important because if you don't get this part right, once you launch your server, you won't be able to do anything with it. Now I'm going to choose toe have ssh public key authentication. In most cases, that's just a better choice than using a password. So I'm quickly going to switch over to my terminal, and I'm going to choose to go in and get the public key of a key that I've already created for the specific account. And when I do, I'm going to copy the key value and then I'm going to go back over to the browser from here. I'm going to paste this in and then I'm going to enter in a user name. Now you'll see here that it is validating both my user name and the public key. But both of those have green check marks, so we're good to go. Now, At this point, we do have the option to go in and configure inbound ports. Now, we can go in and do a lot more with this later. But for now, we want to be sure that ssh and http are both open and available for our server. Now we could walk through many different options, including looking at storage and networking and many other things. But for now we have entered the base information that we need to be able to create our virtual machine. No, if you want to step through as I mentioned, you can see all of the different options that you can configure here from discs, networking management, some advanced capabilities, tags. All of those things are available. But since we've entered in the information that's needed, weaken simply click, review and create. And we can see here that there is an option missing in management. I'm going to click on this option, and it's letting us know here that we need to create a diagnostics storage account. And in this case, for now we will simply click off for boot diagnostics, and that will actually take this option away from us so we won't need to fill it in just yet. And then we'll hit, review and create. And now we can see that are validation has passed and we're ready to launch our virtual server and we can see here that it is deploying. This deployment is currently in progress, so here, within the next clip will be walking through the process of connecting to this virtual machine and beginning to work in it.

Connecting to a Virtual Machine
[Autogenerated] So in the last clip, we walked through the process of actually starting the deployment of a virtual machine on Azure. And here within this clip, we're going to connect to that virtual machine and begin to actually work in it. So here's what we're gonna do. First, we will log into that virtual machine. As I mentioned, we will also review how you find the resource is that you've launched within the Azure portal. And then we'll look at how, in a very quick way we simply host an express app on our virtual machine and make it available over the Web. So let's dive in so you can see here that our deployment of our virtual machine is complete. In addition, you can see that we do have a notification here within the portal, letting us know that this deployment process has completed. But we're going to choose to ignore both of these screens for the moment, and we're going to go back to home because here I want to point out how we get back to that resource is list that shows everything that we have included on our account. So, first of all, I'm going to bring this up. The next thing I'm going to do is I'm going to filter by Resource Group. If you remember, I mentioned that a resource group is a logical grouping of resource is within azure. And so from here, I'm going to go in and just select test VM and you can see here the different things that were launched as a part of this deployment. We do have our virtual machine. We also have a public I p address. We also have something called a network security group. And that's what configures the ports that are available. If you remember, we made both port 22 for ssh and Port 80 for http, available to our virtual server. We also have a virtual network, a network interface and a disc. So all of these things were launched by simply launching one virtual machine. Now, the next thing I'm going to do from here is I'm going to click on the virtual machine. And when I do, you can see here that I get the detail information about my virtual machine. I can go in here in the center and I can see all of the information such as seeing the status, so I know that it's running. I can see that it is in the East US region. I can see that it is associated with my plural size subscription. I can see the name I've given it, the operating system, the size that I chose. And I also can see things like the public I p address. Now if I wanted to, I could simply click on the stop option to just totally stop this VM. But in this case, actually want to connect to it, and I want to work with it. And so to do that, I'm going to click on the Connect option now. It's not going to connect me to the server right away. What it's going to do is it's going to give me some information on how I can connect into the server. So from here, it's given me the information about how I need to actually connect to this server, and in this case, we're going to use this command. Ssh dash. I put in our private key path and then log in with our user name and the public I P address. I'm going to copy this piece here at the end because I'll pace that directly into the terminal. So now let's navigate over to the terminal. So here I've already referenced the key that I'll be using to connect to the server and that will actually paste in the string that I got from the portal. Once I do, it's gonna let me know if I want to accept this fingerprint. I'll say yes. And now I have to enter in the past phrase for my key. Now that I've entered in the past phrase, I am now connected to the server. So now that I'm connected to the server, I actually can begin working with it. I want to quickly run a very basic node application, and one of the ways I can do that is by using express generator now, to make this happen, though, I need to install a few things on this server. First, I'm gonna update the package manager. Then I'm going to go in and install both know J s. And in PM once that is completed, I can now install the express generator and I'm going to install that globally here on this machine. Now that I have that I can simply type in express and test, and this is going to create a sample express application within the test directory. Now that that's created, I can change into that directory and we can install the dependencies. Now, I'm going to go ahead and launch this express application so that we contest it. But to do this, I'm going to need to pass in an environment variable that dictates the port that it's gonna use. Now, we just want to have this go on Port 80 so that we can access it just as a normal website. However, to do that and to be able to launch this on Port 80 we're gonna need to do this as root. So I'm gonna start off by saying Sue do and then we're gonna set port to be 80 and then we're going to say in PM, Run, start. And now we actually have this up and running on Port 80. So now I'm going to navigate back to the browser, and from here, I'm going to take this public I p address gonna open a new tab. I'm gonna paste it in, and now we can see our sample express application up and running. So just in this limited amount of time, we've been able to launch a virtual machine and we have a node application up and running on this virtual machine. Now this might seem great. This might seem like, Wow, we've solved all of our problems. But as we talked about earlier with infrastructure as a service, there's a lot of things that aren't managed for you by default. By taking this approach, you would be responsible for making sure your note applications stays up and running. You would be responsible for maintaining the underlying OS and making sure that it stays updated. You would be responsible for what happens when it's not up and running. There's so many things you would need to handle here. So we're gonna be looking over this course at several different ways that you can launch your no Js applications in Azure. Now, in the next clip, we're gonna be walking through the process of how we actually spin down this virtual machine and how we leverage the azure cli

Using the Azure CLI
[Autogenerated] Now, up to this point, we have been working within the azure portal, the Web interface that allows us to configure how we're leveraging azure, and you might be tempted to think that this is just the way you interact with azure. But that is not the case. In reality, Azure is simply a set of rest AP eyes that you can interact with tow. Launch new resource is, and in addition to that, there are other ways you can leverage those AP eyes. You could go in, for example, and utilize the sdk is that are included. They have provided S DK's across a number of different languages that could automate the way that you interact with azure. But there also is the azure command line interface, or CLI, that we can use to interact with a platform. And that's what we're gonna be covering within this clip. So here's what we're gonna be doing. First of all, we will be installing the azure cli onto our local machine and then we'll be logging into the sea. Ally will be executing commands on the CLI, and then we'll be looking at a different way that we can interact with the CLI and that is using the Azure Cloud show. So let's dive in. So I'm here in my browser and I'm looking at the instructions here for how to install the azure cli. And you can see here that there are options for Windows, Mac and Linux in this case because I'm on Mac, I'm gonna choose this option, but let me call out something 1st 1 of the great things about the CLI is that it works exactly the same on any of these different platforms, and the command structure is exactly the same. So if you're going to script different ways that you're interacting with azure, those can actually run across Windows or Mac or Lennox. So in this case, I'm gonna go to install on Mac OS because I'm currently on a Mac and you can see here that there is an option for installing with home brew. Aiken, simply run this command. If I have homebrew installed and that will install the azure cli now, I've actually already installed it on this machine, So I'm gonna navigate over to the terminal, and I know that I have it installed because I can run the command ese, and then we'll see the output of the command. Now, this gives us all of the different sub commands that are included. If we go in and look here, we can see that they are arranged in alphabetical order. And we have things here all the way from account of virtual machine and a lot of different sub commands in between. Now, the next thing we need to do before we can interact with any of these services is that we need to log in to our azure cli, and that's done using a combination of the command line in the browser. So I'm simply going to type ese log in. And this launches the browser in an interactive manner for me to select my account. And now it lets me know that I have logged into the azure cli. So now I'm gonna navigate back over to the terminal perfect. And you can see that it has returned information here about my account and it lets me know the difference subscriptions that I have available in this case. You can see that I have one subscription, that it returns here in Jason format, and it is the plural site subscription, and that's the one that we're going to be leveraging. So now that we've logged into the azure sea ally, we're now able to actually execute commands against our subscription. So the first thing I'm gonna do because I want to manage a VM is I want to learn how to use the V M sub command. I'm going to type in a Z and then VM. Now, this is not a valid command in and of itself, but it's going to return for me the different options of what I can do under this sub command, and it gives me a huge list here. However, some of these might be a bit confusing. We might not know what all of these do. So if we want Mawr information for any of the sub commands, weaken simply add in dash H. When we do that, I'll now give us the help text for each of the commands under that sub command. Now what we want to do here is we want to actually get rid of the virtual machine that we launched earlier within this module. Now there's several options here that you might look at, such as stop and d allocate, and you might wonder which one you need to use. Well, in this case, stop is just going to stop your virtual machine. It's just like turning it off. It doesn't mean that it's going to free up. Any of the resource is that it's using. And that should matter to you, because as long as those re sources are allocated to your virtual machine, you will be paying for them. So in this case, we want to use the D allocate command toe. Actually, d allocate this virtual machine. So we're gonna go in and type in a Z V M d allocate. Now this also is not a complete command, but we don't know what options we need to include. So we're simply going to run it like this, and it's going to let us know. And it's telling us here that we need to include one of two things. We can either include a combination of the resource group and the name or we have to include the I DS. Now, I don't remember the ideas off the top of my head. I actually could list those here from within the CLI and get them. But I know the resource group and the name. So I'm going to type in a Z V M. The Allocate Resource Group and the resource group here is tasked VM. In addition, the name is also test VM. So now that I've entered in the resource group and the name I'm in the V m sub command and that I'm doing the d allocate command under that, this should be a complete command that will de allocate the virtual machine that we launched previously within this module. And now it's letting us know that it is actually running this command. And so it will take just a bit for it to run through the process of actually de allocating this virtual machine. So now that our command has returned, we have d allocated are virtual machine by using the azure cli. Now, to see the results of this, I'm going to go back into the browser. So from here, I'm going to go into all resource is we can see here that we have the same list that we had previously. We can even filter it down by going under resource group and making sure that we have only selected the test VM option, and from here we can now see that we still have our virtual machine. But if we click on it, we can see here that its current status is de allocated. Now, we could choose to restart it if we wanted to, but in this case, I'm going to go back under. All resource is I'm going to select everything that we have associated with this particular resource group and will go into delete. We'll need to type yes, to confirm. But this will now delete. All of the resource is that we have created within our Microsoft Azure account over this initial module. Now there's one additional thing that I want to point out before we finish out. This clip is that you don't have to go to your local machine to use the azure sea ally. There is an option that is included within the CLI called the Cloud Shell and hear this option is found within the header. So I'm going to click on Cloud show to the Azure Cloud show, launches a version off the cli that you can interact with simply from within your browser. So the first time that you launched the azure CLI, you're going to have to first set up a storage account and then after that, you will need to select between either using bash or Power Shell. Now, I spend a lot of my time working on my Mac and Linux servers, so I'm more familiar with Bash. But you could certainly leverage power shell if you so desire and you can even switch later if you want to. With this option in the top left. However, in this case I'm going to stick with Bash and this provides again a version of the azure CLI we can use directly from within our browser, and we can see that this works by simply going in and typing ese account list. And it already has integrated the log in step for us because we're logged into the portal. And so in this case, it's going to return my subscriptions that I have for this particular account and you can see here every turns the plural side subscription, which is the one that I have configured, and I would recommend for most users to get the azure cli installed on your local machine and also take advantage of the cloud shell if there's a situation where you don't have access to your machine, but this is a valuable tool that you'll be leveraging as you're working with Microsoft Azure.

Summary
[Autogenerated] So we've covered quite a bit here within this initial module. So before we go any deeper, let's take a minute and look back at what we've covered. So first we reviewed the capabilities of the Azure platform. We began to look at some of the different categories of services that are available from storage to compute to databases to AI, A machine learning to Dev Ops into the Internet of things. And while we're only going to be using a small percentage of those overall services, it helped us understand the power that is provided by the platform as a whole. We also looked at how Microsoft organizes the global infrastructure. For azure, we understood the concept of regions and how we need to choose regions when we're deploying our code into the cloud. We then examined some of the different cloud computing models and we talked about everything from I A s, which is infrastructure as a service platform. As a service P A s and software as a service s A s. We also went through the process of creating an azure subscription, which is going to be necessary for you to follow along with the work in this course. We then access the azure portal and used it to actually launch a virtual machine. We went through the process to configure that virtual machine to our specifications, and then we were able to connect to it. And then ultimately, we utilize the azure sea ally to D allocate the virtual machine that we had launched in the cloud. And as we move forward in this course, we're gonna be looking at the platform as a service approach offered by azure APP service.

Building Web Applications and APIs
Introduction
[Autogenerated] So now that we've talked about how we launch a virtual server and run our note application on a virtual server, we're now going to look at some different models. And if you remember, we talked about three different models. We talked about infrastructure as a service, which means we are just running our virtual server. We talked about software as a service, meaning that there's very little configuration will just run whatever you send to it. But here in the middle, we have platform as a service, and in this case, that's what we're gonna be leveraging within this module. We're gonna be looking at something called Azure App service, and we're going to be seeing how we can use it as a platform as a service tool to deploy our node Web application. So over the course of this module, we will first be introducing this service and understanding its capabilities will review those capabilities in terms of how it applies to the application that we're going to create. And then we will go in and actually create an azure app service Web application and as a part of that will be creating a node express application that will be using throughout. The remainder of the course will then talk about how you select an APP service plan and then ultimately will deploy our node Web application on to the cloud, and we'll actually make sure that it's working as it was intended.

App Service on Azure
[Autogenerated] So here I want to dive in and explore at a deeper level the azure app service. Now, just as a reminder when we talked about cloud computing models, Azure APP service is a platform as a service offering that is provided on azure. And I want to dive in and talk a little bit more about the differences between these options, as well as looking at the option of even running your own data center to help you understand what a service like azure APP service actually provides. So when we look at this, we have some options that are self managed and some that can be managed by the platform, and those will differ depending on which option you select. So, first of all, I want to look at the option of having your own data center. And if you have your own data center, we have these elements of your application and your data, as well as the underlying os and the hardware. And when you're looking at your own data center, all of these elements are going to be managed by you. But then, if we transition over to infrastructure as a service like we did in the previous module. We still need to worry with our applications and our data and that underlying OS layer. However, when we look at the hardware layer, we're going to see that that is something that gets managed by the platform and not by us. When we begin to look at a platform as a service offering, like azure APP service. In this case, we're still going to need to build our application, and we're going to need to upload it into the platform in some manner were also still going to need to worry with our data. But when we look at the underlying OS layer as well as the hardware layer, these elements are managed entirely by the platform. So as your APP service is an http based service for hosting Web applications rest AP eyes and mobile back ends. So if your use case fits in one of those three buckets, either as just a Web application or as an A P I, either for a Web application of for a mobile application than azure app, service is one of the best fits for you. So let's look at the different platforms that are supported out of the box by azure app service. So first of all, we have dot net. We have dot net core. We then have Java, node, python, PHP and ruby. And if you're looking to deploy an application that doesn't fit into any of those buckets, there also is the option to create your own docker container and then either included in the Azure Container registry or in Docker Hub and utilize that on the APP service platform. Now, why would you do that? Well, let's talk about what it gives you out of the box. First, we have provisioning. We have scaling deployment custom domains and SSL load balancing and back up. These are all things you would have to build by yourself if you were choosing to do this directly on an infrastructure as a service or within your own data center. And these are things that are simply provided by the platform. In addition, there are a couple of advanced features that you can also leverage with the platform. First, we have high availability. This allows us to leverage multiple regions to make sure that our applications stays up and running. No matter what happens then we have compliance and APP service is an AYSO soccer and PC I compliance service. So if you were having to build out your own infrastructure, you would need to meet all of this compliance standards. But here, in this case, it's provided for you out of the box.

Creating a Web App on App Service
[Autogenerated] So now that we've talked about azure app service, we're gonna be diving in and actually implementing a Web app on the service. So over the course of this demo, here's what we'll be doing. First of all, we're going to be setting up a sample note express application that will be using throughout the remainder of the course. In addition, we're also going to be creating an APP service Web application. So let's dive in. So we're gonna be using the express generator on our local machine to create a Web application. So to make this work, I'm simply going to install the package using in PM and install it globally. Now that I have the package installed, I can now run express. And then I need to pass in the name of the directory where I want to create my application and in this case will call it Web app. Now that I have it installed, I can now change into that directory and I can run in PM install Now. The next step is I want to take a look at the package, not Jason file that we have for this application, and you can see here that there is a start script that actually starts the server running locally so we can then view it in our browser. So I'm going to run in PM start, and now it's actually running locally. So now I'm gonna navigate over to the browser. So here, now that I'm in the browser, I'm gonna navigato local host and I'm going to go to Port 3000 and you can see now that I'm running the express application locally on my machine. So now that we have this in place, let's go back to the terminal. So now that we've seen this running, I'm gonna go ahead and kill the local process. And now I want to begin editing this application. So to do this, I'm going to use visual studio code so I can simply type in code if I have it installed and in the current directory. So now I've opened up my web application here within visual studio code. Now, the next thing we're going to need to do within here is we're going to need to create a git repository because we'll actually be using our local git repository to deploy our application to azure app service and I can do a majority of this from within V s code. So I'm going to navigate to the source control option and then I'm going to click Initialize Repository. And now we can see that we have our web application within source control. However, there's a problem. We currently have knowed modules included within our get repository. And so we need to add this in to get ignore. So one of the things we can do is we can utilize some of the get ignore files that get hub has provided and they have standards for most platforms, including no Js. So next I'm gonna navigate to the browser. So here you can see the get ignore file that get hub includes for node. So I'm simply going to navigate to the raw version of this file and will actually copy this, and then we'll move back over into V s code. So from here, I'm gonna navigate back over to the files pain. We'll add a new file, which will be dot get ignore, and then we'll paste in what we got from get hub. Now that we've added this in, we should see a change to our repository. So I'm gonna navigate back over to the source control tab and we can see here that the right items are included. We have just the files related to our application. So one of the things that we need to do here is we need to add in our initial commit. So I'm gonna go ahead and select all of these changes and we'll go ahead and enter a message now will commit those perfect. So now we've created our sample application. We've initialized to get repository and now we have made our initial commit into that repository. Now that we have these things in place, it's now time to jump over into the azure portal so you can see that I'm signed in here to the portal and I can see that APP Services is currently listed under the shortcuts that I have. I also could choose to search for that from the top, or I could click on the fly out menu on the left. But in this case, I'll simply click on it here in the shortcuts and you can see that I don't currently have any app services created. So I'm going to click on the option to add. So once this blade loads, I now have the option to begin configuring my Web application. And just like most everything else that we encounter, we need to select both a subscription and a resource group. In this case, I'm gonna create a new resource group, and we're going to call this Web app. So now that I have my subscription and my resource group selected, I now get the opportunity to go in and enter in the remainder of the information. Now I'm gonna call this PS node Web app, and we can see here that that is available now a note here because this references a globally available you are l. This name has to be unique globally, and once this is deployed, this Web application will be available at PS Dash. No Dash Web app dot azure websites dot net Now, depending on how you configure your Web application, you will potentially have the ability to use a custom domain. But we'll talk more about that a little bit later, so we're going to use code here as opposed to a doctor container, which means that we need to select the runtime stack. Since we're building a node application, we want to use the most recent version of node that is included. Currently, the most recent version is No. 12 LTs or long term support. So I'm going to select that version in pre selects Lennix as the operating system, and then I'm going to choose to place this in the Eastern U. S. Because that's where I'm located. Now the next thing will note. Here is something called an app service plan, and this is a new concept for us. So before we continue with the creation of our Web application, let's dive in and understand what APP service plans are.

App Service Plans
[Autogenerated] So next we're gonna talk about the concept of APP service plans, and this is critical when you're setting up your APP service applications. Because in this case, an APP service plan it's going to define the resource is for your Web application as well as the specific features that are gonna be available for your application, depending on the plan that you select. Now, there's a lot of similarities that exist between an APP service plan and a virtual machine size. So if you remember when we set up our virtual machine, we had to go through in select a size that dictated the resource is that would be available to our virtual machine. It's the same thing here, but because we're looking at more than just a single virtual machine, there's more concepts that need to be considered. So let's look at the different characteristics of an APP service plan. So first of all, it is region specific, so as a part of setting up your service plan, you're going to be selecting a region, and so any applications that you deploy onto that service plan will exist within that region. It's also going to specify the number of VM instances and specifically the size of the resource is that are going to be available. And this includes both memory as well as compute power. And we'll talk in a minute about how we specify compute power for APP service plans now. This also is going to include a pricing tier, and depending on the tear that you select, there will be different features and capabilities that will be available to your application. So let's look at a high level of some of the different pricing tier categories. So at the low level, we have the shared compute, and what this means is your application is going to be running alongside other applications from other subscriptions on this shared compute. Then we have dedicated compute, and this is going to run on dedicated V ems that are specific to your application so you can make sure that you're the only one utilizing those virtual machines. And then if we go to the far end of the spectrum, we have isolated compute, and this runs on dedicated virtual machines and dedicated virtual networks on the far right here with isolated compute. This is what gives us the most options for how we actually scale our application. Now it's important to note here that the sheared compute here is really designed for development and test workloads. You would not want to leverage it for production workloads. However, if you're looking to try out the service, this could be a great way to try out some of the basic features. Now let's talk about how we scale our APP service applications. We can use vertical scaling or scale up. What this means is we have the ability where we can go in an update. Our plan to include more resource is, and in doing so we gain the ability to handle more traffic. But another approach here is that we can also do horizontal scaling or scale out where we're going to add additional virtual machines to handle the demand of our application. Now let's talk about how we measure compute power when we're looking at APP service plans. So an azure compute unit is currently standardized on a small, which is a standard a one instance being a 100 so what they could do is they can benchmark all of the other sizes of virtual machines, and so if you have something that is twice as fast as a standard a one than it would be a 200 for example, and so you can use that number to help determine the compute power that is provided within a service plan. So here's one example of a service plan. Now this falls under the shared compute tear. And so we're looking here at an F one and with an F one. This comes with one gigabyte of memory, and in this case it has 60 minutes per day of compute. And if you look down here in the bottom right, it lets us know that this really includes memory and it includes storage just up to one gigabyte of shared disk storage. But that's all the functionality that it includes. But if we move up to the dedicated tear, we can begin to see some different options. And first we have a P one V two, and so this is kind of the premium tier, and within this premium tier we can see that it has 210 total azure compute units or a see you, and 3.5 gigs of memory. And this is equivalent to us using a devi to Siri's virtual machine. But if we look here under features, we can see that it has a lot more than what we saw with the shared tear. We have custom domains, auto scaling, staging slots, daily backups, traffic manager as well as not just having a limited time for our compute power. But we actually are measuring it in azure compute units. So this is an example of what you will see when you go into select your APP service plans. Now, if you're looking to do just a standard application but you want to test out a majority of the functionality with an APP service, I would recommend looking at the standard tier. So in s one, for example, and in this case, you could run an S one for roughly $70 a month. Now again, that's estimated. But in this case, that would certainly fall well within the $200 free credits that you have when you're creating a new subscription

Deploying an Application on App Service
[Autogenerated] So now that we have an understanding of APP service plans, we can now move towards finalising the creation of our Web application and then deploying our code into that application. So over the course of this demo, we will first, based on what we talked about previously, select an APP service plan that works for our Web application will then be configuring deployment settings for a local git repository. And then we're gonna be viewing the application that we've deployed. We'll also talk about how we then update the application after it's deployed. So I'm here in the azure portal. And if you remember from a previous clip, we had almost completed the process of creating our APP service Web application. But we needed to select an APP service plan. So in this case, I'm going to select the option to create new and then I'm gonna click. OK, now, from here, I get a chance to select the type of APP service plan, and so I'm going to click Change size now. First, you'll note here that we have Dev test production and isolated tabs for APP service plans. If you simply want the cheapest option, you can go to the Dev Test option and we have the F one tier which enables us to utilize up to one gigabyte of memory and have 60 minutes a day of compute completely for free, which is a great way to try out the service. If you notice here, we have memory and storage included. But some of the more advanced features are not included. Now there are some additional options here within this tear that we could select the B two and B three options. But in this case, I want transition over to the production tab you hear within the production tab. We also concede that there are additional options here as well. We have both premium and standard options. Now, with these, there's going to be some additional capabilities, including things like custom domains and staging slots, daily backups, things that you don't get with the lower priced development tears. So in this case, I want to select an S one. Now this is going to cost roughly $69.35 per month and that's estimated and we can see here that this has 100 total azure compute units with 1.7 gigabytes of memory. Now this will be perfect for what we're testing out, and you should be able to utilize this and certainly fall well within the $200 credit that you have when creating a new subscription. So now that we have that in place, will select apply, and we now have our APP service plan in place. So next let's go to review and create. So here we can see that we do have our proper resource group and names selected and that this is a node 12 application Weaken CR App service plan. And now we're ready to create an azure. Lets us know here that the deployment is now in progress and so it's deploying. All of the resource is needed for this APP service Web application. So now we can see that our deployment has completed. I'm going to click on the option to go to the resource, and we can see here that we're at our web application and there are several things I want to note. Here first is you can see the girl that was included. Now, at this point, we haven't deployed anything to the girl, but we will be using that in a bid. So here you can see in the detail pain, all of the high level information that we need, including how to stop, how to restart and a lot of the different pieces of information about our Web application. But down the left, we have a lot of the specific settings that we can access, and one of the things that we want to take a look at here is we want to look at the deployment process. So I'm going to click on the Deployment Center, and the first thing we need to do is we're going to select a source control provider for our deployment. Now there are options here to use azure repose to use get hub to use bit bucket. But in this case, to keep things simple, we're just going to be using our local git repository, and then azure will provide a remote that we can push our code to so all select local get. So next we get to select what is going to build our application. We can use either azure pipelines, and we're not going to do that yet, or we can select the Kudo engine in this case will select who do well, then select. Continue. So now you can see that we have completed the needed items to set up deployment for our web application. Now, here you can see that we do have a git clone, you or I and will need to add this in as a remote for our local git repository. So I'm gonna go ahead and select that to copy it. And the other thing I want to point out here is there is an option here for deployment credentials. So here you can see that we do have credentials for deployment. In this case, we're going to choose to use APP credentials as opposed to user credentials that you can set up across. Multiple applications will be using the per app credentials. So I'm gonna now navigate back over to the terminal. So here within the terminal, I need to add in the remote that Azure has created for our web application. And so I'm going to do get remote ad, and I'm going to call it Azure and then paste in the u R I that was given. So now that I have this in place, I should be able to push toe azure. But in doing so, I'll need to enter in both the user name and password for the application that I saw in the portal. And so I'm going to push my master branch to Azure and we can see here that it's asking for the user name. So I'm going to paste in the user name and then I'll paste in the password. Now you'll notice here that we see some information we would expect to see when doing to get push. But now we're going to see some things that we don't expect. It's going to actually walk us through the build process here that it's doing through coup, and now we can see that our deployment has completed. So I'm now going to navigate back over to the browser. So here, within the browser, I'm now going to navigate back to the overview section. So here I'm gonna take the girl. I'm gonna open that in a new tab, and we can see here that the PS node web app dot azure websites dot net u R l I'm able to see the express application that I deployed from my local git repository. But let's quickly take a look at that. How we would go about the process of updating our deployment. So now I'm gonna navigate back over to V s code from here within V s code, I'm going to navigate under the routes directory to my index route. And this is the route that handles the default of you for the application. And currently we're passing in a title of express to that view template. I want to change this here to be azure app service, and then we'll save it. Now, I'm gonna go ahead and go to my source control tab here, and I'm going to choose to add in this change. And we're going to give it a commit message that we're updating the title and now will commit that locally. And from here, we're going to say that it's going to push toe azure, which is what we want. And now it's actually going through the process of doing the deployment from a get perspective and then running the build process with coup do. Once this completes, we should be able to go and view the changes in the browser, and now we can see that that push has completed. So now I'm gonna navigate back over to the browser. So I'm going to go here to my tab that currently has the application in it, and I'm just gonna hit reload. And here we can see that the title has been updated from express toe azure app service. So this shows us how we can take and deploy our application locally and make changes all using functionality that's built in with azure APP service.

Interacting with App Service in VS Code
[Autogenerated] So we've looked so far within this course on how we interact with azure, utilizing the portal and utilizing the CLI. But I want to showcase another way that we can interact specifically with APP service and that is using an extension that is provided for V s code. So here in this demo, we're gonna be installing the azure APP service extension for visual studio code and then configuring our application using that extension. So from here within V s code, I'm going to navigate to the extensions tab. And from here, I'm going to search for Azure. That will see here that the second item in the list is for the Azure app service extension. So I'm going to install that extension. We can now see that that extension has been installed globally. So now I'll navigate to the Azure tab and we can see here that our first step is to sign into Azure, so I'll select that option. And so it's going to have me select Mike out perfect. And now it's letting me know that I am signed in. So I'm gonna close this tab and then I'm going to navigate back over to V s code, and so from here were first going to see a list of our subscriptions. And we currently have plural sites selected from within here. We can now see that we have our node web app and we can see the information that we have everything from our application settings, connections, deployments, lots and our deployments. So if we expand deployments, you can see here that we can see the list of the different deployments that we have had. And so, for example, here we can see the entire build process for this version, which was our initial commit, and we actually have that entire log available to us here. And so as an example, we could choose to select this because it does back up our deployments and we could go to redeploy. And this will start the process of redeploying our web application to our initial commit as opposed to the committee where we updated the title, and we can see now that that process has completed and that commit has been redeployed. So if I now navigate back over in my browser, I'm going to now open this back in a new tab and we can see here that the title has been changed back to express. So now I'm going to navigate back over to V s code. Now, we're not just limited within V s code to working with our existing azure APP service web applications. We could choose to actually go in and create a new web app, in which case it would walk us through the process of configuring one, just as we did when we used it to the azure portal. So this just provides yet another way that we can interact with azure and specifically app service through the extension NVs code.

Summary
[Autogenerated] So over the course of this module, we have implemented quite a bit. We started with nothing, and we ended up with a deployed application on the platform. So let's quickly review what we've done. We first introduced this service for azure APP service. We then reviewed the capabilities that this provides for us within created a Web application. We selected an APP service plan, and then we deployed this onto the platform. Now, in the previous module, we looked at the concept of infrastructure as a service. We saw how we could pretty quickly get an application up and running just on a virtual machine. But let's take a step back and look at what we have done here within this module because there are things that the service provided that we would have had to implement ourselves if we were purely taking an infrastructure as a service approach. And I'm not going to review all of them here. But I do want to highlight a few key elements of it. First of all, we have provisioning, so our virtual servers were created for us and we didn't have to manage the underlying layer that our application is running on. So then we have deployment, and here the process of taking what we push in our local git repository to the platform, making sure that we're then able to take it and move it on to the servers that are actually running our application and then making sure that we have zero downtime in the deployment process is all something that is provided by the service. In addition, and we haven't talked about this fully yet, but we have backup that's integrated in. That's something else that we would have to build on our own as well as the concept of scaling. So we get all of these things out of the box because we're choosing to use a platform as a service option, as opposed to building it just directly on a virtual machine.

Monitoring and Scaling Web Applications and APIs
Overview
[Autogenerated] so, up to this point, we have been able to deploy our application into the cloud. But deploying an application is on Lee the first step. If you're having toe own and maintain an application, there's a lot of considerations that you need to be thinking about things like your application life cycle. So how are you going toe update your application? And how does that affect your end users? Things like monitoring, making sure that you know, if there are things that are happening within your application, that shouldn't be happening then looking at concepts like scaling. So knowing when we need to add more capacity to our cloud infrastructure and concepts like debunking because we all want to believe we can write a perfect application. But we know that that's not reality. And at times we're going to have to debug an application that's not working properly. So here, over the course of this module, we're gonna be addressing these considerations by hitting the following topics. First of all, we're gonna be examining application lifecycle by looking at deployment slots in azure app service. And then we'll be looking at how we can see metrics for our app. service application. Then we're gonna be able to create metric based alerts for our application. So if something isn't working right, if we're getting a lot of server errors Weaken be notified of that and then choose to take action, then we're gonna look at how we scale our application using a concept called Auto Scaling. And then we will be debugging our application using azure application insights.

Deployment Slots Overview
[Autogenerated] So next we're gonna be talking about the concept of deployment slots in azure APP service. But before we do, let's talk a little bit about a traditional application life cycle so that we can understand how we can approach things differently in the cloud. So for most traditional organizations, they're going to have separate environments that are created for their application. So maybe it's a dev and a test staging and production. I've even worked for clients in the past that had seven different environments to support a single application. But whatever you have, you're going to have an application where new versions air going to progress through different levels and eventually end up in production. So if we look at that according to this, we're going to now deploy version two onto Dev once that is in a spot where we want to move that to our test environment, we would then deploy it on to test. Once we do the needed work and testing, we're now going to move that over into staging. Now we have users currently interacting with our production environment and they're interacting with version one. However, with this kind of process, because we're dealing with the same servers that are supporting each of these environments, probably in your own data center. That move from staging into production is probably going to be something that's going to break a bit of the experience for any users that are currently using the application, and there are certainly ways around that. But for most organisations, those deployments can create a problem. So if we go in now and remove version one and then we look to deploy Version two now, we have the ability for users to use that. But there probably was a bit of downtime in that period in between. Now let's talk about the concept here of deployment slots. Now a deployment slot is a feature, and it is included in some but not all app service plans. And so if you're using any of the shared hardware plans in that case, you're not going to have deployments. Lots included. But if you're using standard premium and isolated, you will have access to a specific number of deployments lots and what it does is it enables you toe have these kind of environments that we just talked about. You can basically have different versions of your application running simultaneously to support this multi environment lifecycle. But let's talk a bit here about how this is different than what we have with the traditional model. First of all, with our traditional model, we were dealing with our application running on specific servers, and so those servers would never change. But that's not the case here with APP service. So let's say that we want to move our version two onto production well, in this case, because we're dealing with cloud infrastructure and we simply need to route our end users to the right location, we can just choose to actually swap our production and staging environments. And if our user currently had a Web service call that was running, they would actually stay connected to the version one environment here, which is staging at this point until that was completed. And then once it's completed, we would then route that user over for all new calls to the production environment. And this enables us toe have a zero downtime deployment to our production environment. Now, let's say, for example, that we made a mistake in coding and we didn't catch it in depth or in test. And so now, once it's in production, we have a problem and we need to roll back. Well, the great thing is here we still have a version one of our application running, and that's currently on our staging environment. We could choose to now switch this back. Any calls the user was currently making would stay with the environment that they're connected to, but that all future requests would then be routed back to the production environment, which is now on version one. So we gain a great degree of flexibility by using deployments lots, and this creates flexibility we don't have when we're running these environments on our own servers in our own data center.

Using Deployment Slots
[Autogenerated] So now that we've talked about deployment slots, we're going to put them into practice. So here, within this demo, were first gonna be setting up some configuration values for our application, and then through that will be creating a new deployment slot and deploying to a specific new deployments lot. So let's dive in. So before I create a new deployment slot, I need to make some changes to my application. And so, first of all, I'm going to look at my index route. I'm going to create a new object that I'm going to pass in to the rendering of my template. And in addition to the title, we're going to add in another value, which is going to be message. And this message is gonna help us know what environment were in. No. One of the easy ways that we have to pass in data to our application is through an environment variable. So I'm going to use the process dot envy dot message. So as long as I have an environment variable called message, it'll be able to be passed into the application. However, if I don't have an environment variable set, I want to have a default value. So in this case will make that default message. This is development. Perfect. Now, instead of passing in this object literal, I'm now going to pass in this data object. Now, in addition to this, I now need to navigate over to my index template. And here we will include the message in the template. Perfect. So this should now display right after the title. Now, the next thing I'm going to do here is to commit, but not push These changes perfect. These changes air now staged, and now we're going to commit them. Now I'm ready to navigate over to the portal. Now, here in the portal, I have my application pulled up, and the first thing we're going to do is we're going to navigate to some configuration settings. Now, hear this Lets us have application settings and application settings are going to be our environment variables. So we're going to go ahead and create a new one. Now, the 1st 1 I'm going to create here, we're going to create because of express. So express is going to look for an environment variable to determine if it's in a production environment and if it is, it will provide optimization is that it doesn't provide when you are in a development environment, and so that variable is going to be node envy and the value here is going to be production, and we're gonna go ahead and save that. Now, the next one that we're going to add is going to be the one that will be specific per environment, and this will be our message value. And so for this one, we're going to say this is production. Now we're going to make this a deployment slot setting now. This means that this is going to stick with this particular environment even when we swap. And this is important because if we have anything that's going to be specific to an environment like our production environment, we want to be sure that we check this box so that when we do swap different employment slots, we don't swap that setting as well. We'll go ahead and hit OK, and then I'll hit save. Now it's updating those settings. It'll take just a minute for those changes to be committed. Perfect. So now we can navigate over to deployment slots so you can see here that by default I have one deployment slot, which is our production version of the application. And it has the same name is our application, which is PS Dash no dash Web app. Now we're gonna create a new slot, So there really are only two pieces of data I need to create a new slot. The first is the name, and we'll call this one staging and the next is the clone settings option. Now, this allows us to either create a new deployment slot, which again is basically just a copy of our application. We can either create it completely from scratch, in which case we would select Don't clone settings or we can have it clone from our existing application PS node web app, which is what we would want to do now. I could click add here, and it would add this in. But there's another way we can do. This is well, so I want to navigate back over to V s code. So I'm here in V s code. If you remember, we installed the APP service extension for V s code, so I'm gonna click on the Azure tab to the left and I'm going to expand my application and you'll see that one of the options that has provided here is deployment slots. And so we currently don't have any besides our production slot, so none will show up here. But I can right click on this option and go to create new deployment slot. We're gonna go ahead and give it the name staging, and it will guide us through the process here. If we simply press enter, it will take us to the next option that we need to configure. And so, in this case, we're going to say that we want to clone the configuration from our production slot, and now it's creating our deployment slot from directly NVs code. So it'll take just a minute to finish that creation process. Perfect. And now we can see that it has completed the creation of our deployment slot. So one of the things we can do from within V s code is we can also right click on our deployment slot and we can say open and portal. And so now we can see that we're here in the portal and we're not in our normal application. We are in the staging deployment slot. But everything looks very similar to what we would see for our application. And this makes sense because this is basically a copy of our application, but you'll notice here that it has its own unique. You are l It has its own unique get clone URL, but it also has the same resource group and the same subscription that we have with our application. So the first thing we're gonna do is we're going to take a look at this version of the application. So I'm gonna go ahead and click on the browse button and so we can see here that we don't have anything deployed yet. This is just the default page four node applications. So we need to work now on deploying our application out to staging. But before we do, we're gonna go back to the portal and we're going to go under configuration settings. Now, the first thing we're going to do because it did clone from our production version of our application, we need toe update our message value. So here, instead of this being production, we're going to say this is staging, and now we're going to save this. Now, the next thing we're going to do here is we're going to go under the deployment center. I mentioned that we have a new git clone. You are l That is specific to this slot, and we can see that value here. So in this case, I'm going to copy this. Get clone you are I And then we can also pull up. Here are deployment credentials. So we will have an APP credential that will be using here. And this will also be a unique user name and password. So we'll need to use this when we're pushing our application. So now I'm going to navigate over to the terminal. So the first thing we need to do is we need to add in our new remote, and we're going to call this staging, and from here, we're going to paste in the remote U R l Now we've added this in. Now we need to push our application our master branch to this staging remote. And here we're gonna have to use our user name and password. And now we'll see that it has started the deployment process and after it completes the process of pushing it. It's now going to start the build process to then push it out to our staging environment. Now that the deployment process is complete, will now navigate back over to our browser. Now we should be able to go back to our running staging application and we should be able to reload this and see what we've committed perfect so we can see our application running. We can also see the message indicating that this is staging. So through this process, we've been able to create a new deployment slot and then push our application to that deployment slot In the next clip will be walking through the process of utilizing the swaps lots capability of azure APP service.

Swapping Deployment Slots
[Autogenerated] now, one of the capabilities that we have within azure app service is to swap deployment slots. So here, within this clip, we're gonna be walking through how we use that feature. So first we will be utilizing that capability, and then we're gonna be verifying our slot specific configuration. So let's dive in. So here I have my application as well as each of the sites. Both production and staging pulled up here in the browser so we can see our staging slot currently has the version that has the message included that says this is staging. But if we go to the production version of our site, we haven't pushed that change yet, so it doesn't have any message that's included with it. Now, if we go look at our configuration settings, I'm currently here in the production version of my app. I do have under configuration. I do have the message and this message states, this is production. So now I'm gonna navigate to my deployment slots. We'll go to our staging web application and then we will go under configuration and we can see here that this has the message that this is staging. Now this is important to note that these air set as deployment slot settings. So when we do a swap, what happens is a majority of the configuration for applications actually moves to a different slot. But if we have this set as a deployment slot setting, it will stick with where it's at. So we will move the production version of the application onto staging, and it will then pick up the message that says, This is staging will move the version in staging to production, and it will then pick up the new value in our production application. So let's walk through that process. We're gonna go here under deployments lots, and we're going to click the option that says Swap. We're now going to have our source be staging and our target be the production version of our app. So from here, we can now click swap now, as mentioned, if any users are currently using the application, that they'll be able to finish out the current version of their requests on whatever version they were connected to. But all new requests will then be routed to the new version of the application that's going into that specific slot So now we have completed the process of swapping are staging and production slots. It will hit close and now will navigate and look at our application. So first of all, here within staging when we hit Refresh. We'll see now that the message goes away because this is the production version of our application that doesn't yet have that message attributes at it in. So now, if we go over and look at the production version of our application, this should pick up the change that does include the message in our index view. In addition, it should also pick up on the application setting that says This is production, as opposed to this is staging. And so we can see here that we have been able to swap the staging and production version of our application, and we have been able to keep slot specific configuration where it needs to be. And again, this helps us facilitate our application life cycle so we can deploy something out to staging. Make sure that it works as it should, And once we're ready to deploy that to our users, we simply swap those slots so that we now have our staging in production

Monitoring an Application
[Autogenerated] So now we're going to talk about how we monitor the applications that we launch on azure app service. So here, over the course of this demo, we're gonna be looking at our application specific metrics as well as our APP service plan metrics. And then we'll be looking at how we review metrics for Azure, for any of the resource is that we have launched in the platform. So here in my browser, I'm in the portal and I'm looking at my web application and on this overview pain, if I scroll to the bottom, I'll see some of the charts that they've provided for metrics that are specific to my application. And this includes things like our server airs, which would be our http 500 status codes are data in and data out as well as our requests and average response time. So if I'm really interested in the number of requests and I want to be able to change how I'm viewing that data, first of all, I can click on this to get into the metrics view for this, and I could choose to go in and change some information like maybe I don't want to see the last hour. Maybe I actually want to see the last three days. I can click on that here and then hit, Apply, and well, didn't see that updated for that specific time frame. Now, in addition from here, I could choose to go in and pin to the dashboard. Now, we haven't talked a lot about the dashboard yet, But dashboards air Great, because if you're trying to keep an eye on a specific application or multiple applications, you wanna have one place where you can go to see all of the needed information and this is what you can do with a dashboard. So simply by clicking on this, you can either pin it to your current dashboard or select another dashboard to pin it to now. From here, you also have the ability to go in and change how this data is being displayed. You can even go in and look at log data around this. You can even add multiple metrics, which will be doing in just a bit. But next I want to talk about where we go inside of azure toe. Look at metrics for any resource that we've launched on the platform. So I'm going to go up here to the search bar and I'm just going to type in metrics and this will bring up the Azure Metrics service. So I'm going to click on this now. Once this launches its going toe, ask me to select a scope for a metric that I want to view now. In this case, let's say I'm very interested in looking at data around the CPU and memory performance. Now, in this case, that's going to be for the APP service plan and not for the application itself, because the APP service plan is what gives me data around the virtual machine or virtual machines that my application is running on. So here I'm going to select my Web app Resource group, and from here I can see both the APP service as well as the plan. So I'm going to click on the plan. Now let's say here that I'm very interested in the correlation between CPU and memory usage. So first I can go in here and select the metric CPU percentage, and we can see here that it charts out my CPU percentage and it's going to be showing us the average aggregation. Next, I'm gonna go in and add another metric, and this metric is going to be memory percentage. And now we can see these two values graph side by side in one view. Now, again, I might want to look at this and pin this to my dashboard so that I have this available. But one of the great things here about the metric services I could choose to go in and combine metrics anyway, that makes sense. I could choose to go in and get metrics from some virtual machines for some APP service plans for some applications. I can choose to put all of this here, and I can create views of that data that are specific to what I need. But in reality, we don't want to be logging into the portal to find out that our application is not working as we intended. So next we're going to be looking at how we take the metrics that we have in Azure, and we can configure alerts that let us know that there's a problem before our users have to tell us

Creating Alerts for an Application
[Autogenerated] So now that we've talked about how to view metrics for our application, we're now going to talk about how we create alerts for our application so we can be notified when something isn't working the way that it should be. So here, within this demo, we're going to be reviewing the capabilities of alerts in Azure. We'll look at how we configure alert signals how we create an alert action group. Then how we specify metric based alert conditions. So let's dive in. So I'm here in the portal and I can see that the alert services listed here under my shortcuts, I could just as easily go and search for it from the search box. But I'm going to click on it from here now. From here. The first step to creating UN alert for my Web application is to click on new alert rule. Now, from here, the first step is I need to select the resource that is going to be tied to this rule. So when I hit select, I'm gonna have an opportunity here to filter by resource type. Now you'll see here I have one subscription plural site, and it is currently selected so it's going to filter. All of the resource is by that subscription. No, I'm going to type in APP service, and I can see all of the different resource is that can have alerts configured on it. Now, in this case, I'm going to select APP services because I want to monitor my Web application. But if you are interested in creating an alert based on things like the CPU, our memory usage of the virtual servers that are application is running on, we could select APP service plans. But in this case, I'll select APP service, and then I need to click on my application PS node Web app. Then I can hit done now. The next step is that I need to specify an alert condition for this rule, so we'll hit ad now. There are two different types of signals that we can configure here within alerts. The first type is going to be a metric based alert. This means it's tied to a specific metric and a threshold for that metric like CPU utilization percentage. Or I can have an activity log signal and that's tied to an action. For example, if we swap deployment slots that is a type of activity logs signal. In this case, I'll leave this to all so that I can look at the entire list. And it's important to note here that there are 63 different signals that we can configure for our APP service application. Now, the first thing I want to search for us. What I'd mentioned earlier, which is swapping deployment slots so I can see here that swap Web app slots is one of the signals that I can configure an alert for. And let's say you had a development team that managed our web application and you wanted to know whenever someone pushed a new version to production, you could go in and configure this as an activity log signal to alert the rest of the team. But what I want to look at here is http air count. So here I have http server errors. So I'm going to select that option now. It's going to quickly provide just a graph to show us the occurrences of this within the last six hours. But in this case, I want to scroll past that, and I want to start to look here at the alert logic. Now we're going to keep this as a static threshold, meaning that no matter how much our applications scales, we want this threshold to remain the same. And I'm going to say that I want to be notified if we have greater than two of these http errors that happen within a five minute time period. And here Azure is going to evaluate this every single minute. So this is a good enough alert for what I need now. So now I'm going to hit, done. And that signal is now configured now. Azure also provides an estimated monthly cost for this condition directly here within the portal. Now, the next step is I need to configure an action group. So what happens when this threshold is met? So I don't currently have an action group configured for this Web application, so I'm going to create one, so we'll click on create and first I need to give this a name. So we're gonna call this Web APP development team group. And the way this would work is if we had 20 different alert rules that we had configured for our web application, we could tie them all into this action group. And again, this is our development team that's managing our Web application. We're gonna give this a short name, and then we need to assign it to a resource group. Now I'm going to choose to associate this with the current resource group I'm using for my web application. Now, the next thing here is we need to define an action type, and this is where we can really start to see the power of the alert service here within Azure. Whenever we have an action that needs toe happen, we can do an automation run book, an azure function. We can email the azure resource manager role Weaken Do email, SMS push voice, I t s m logic app on web hooks. So there are many different things we can do now For the purpose of this demo, we're gonna go to the option for email SMS push voice, and we're going to set this just to email, and I'm gonna put my email in. But we could just as easily configure an SMS, a push notification or even a voice call. I need to give this action and name and we'll call this email. Now that I have that in place, I can hit OK And we've created our action group and it lets us know here that this contains one email address that is going to be notified if this alert is triggered Now, the last thing we need to do here is we need to give this alert ruling name. We can choose to go in here and set a severity, but I'll leave it at the default value. And I'm also going to leave the setting at its default value for an able rule upon creation because we do want to start tracking this right away. It does let us know here that it can take up to 10 minutes for a metric alert to become active. So now I'm going to create the alert rule. Now it lets us know here that it's done creating this rule. So I'm going to go to the option for managing my alert rules and I'm going to refresh. And now we can see that our web app. Http Air rule is included here, and we can choose to manage it from here within the portal

Configuring Application Scaling
[Autogenerated] so we've examined how we can look at metrics within the azure portal. We've also learned how we can configure alert so we don't have to go in if our application is running correctly. But in some cases we don't need to be notified of changes. We simply want Azure to be able to make changes to our application for us. And one of those situations is scaling. If we have more users going to our application, we'd love it if Azure can just scale it for us. So let's talk about how we're going to do that. So here, within this demo were for first going to review the different scaling approaches on Azure. And then we'll look at how we can scale up an APP service plan, but then will be configuring scale out auto scaling for our APP service plan. So let's dive in. So I'm here in the portal and I'm going to go to APP Services now. From here, I'm going to click on my Web app and here on the left pain, I can actually search for scale, and I can see that there are two options. Scale up and scale out. Let's first go to scale up, which is vertical scaling. Now when we do, we see options here that we have already seen before. So here's where we get to select the tear for our APP service plan. So when we scale up, really, what we're saying is is that we need MAWR CPU, our memory for our application. Now there are absolutely scenarios where that is needed. However, this should be tied to situations where we need that CPU and memory. On a single instance, if there are situations where we just need to accommodate additional load in those cases, we want to look at the other option which is scaling out. So here for scaling out, we have a couple of options. First of all, we can do manual scale and this is a simple is just increasing our instance count. So if we want to have seven instances up and running, we can set that value here. Save it and our APP service plan will scale to that number of instances. However, in most cases we don't want to do that. We want to take another option, which is we want to configure custom auto scale so that we can set rules on how our application scales. So from here we can give our app auto scaling. Setting a name in this case will just choose to make this Web app plan auto scale. Then it needs a resource group which will keep it at the default value here, which is our Web app resource group. And then we need to go in and create the conditions for scaling. And in this case, we're going to choose to scale based on a metric. So we need to go in an ad rules. But before we do that, I'm going to set my instance limits. So I want to say that this particular app service plan needs toe have between one and six instances and we can set the default value here at one. Next, I'm going to start adding in rules. So the first rule I'm going to add here is I'm going to base this on CPU percentage because one of the things we can do is look at how much CPU is used for a server to indicate whether or not we need mawr. And we're going to say here if this is greater than 70% of the CPU capacity for a duration of 10 minutes. We want to look to increase our count by one. Now we have a cool down period here of five minutes, and here's what that means. We're going to let Azure scale for us. But after it adds another instance, it's going to wait five minutes before it considers adding another one. This gives our application time to stabilize, So this rule is the way that we want it to be. So we'll click, add, But usually we're going to add at least two rules one to increase the size of our APP service plan of the number of instances and then one to scale in or decrease when we don't need additional instances. So I'm going to add another rule, and here we're going to say if our CPU percentage is less than 30% for a period of 10 minutes here, we're going to now actually decrease the count by one and will give it the same cool down period of five minutes. Now, if you look here from an operation perspective, we have several options. We can increase the count, but we also can increase percent. We also could just increase the count or decrease the count to a specific number of instances. Now, For this to be variable, we generally want to use these options for decreasing and increasing the count by a specific number. So here I'll add this. And now we have added our rules for both scaling out and scaling in. There are other situations that we might want to configure scaling four. So I'm going to click the option to add another scale condition. Now, one of the ways that we can schedule are scaling is for specific times. So if we know, for example, that are, application is gonna have a ton of visitors on a very popular shopping day of the year. If we're running an e commerce site, we can choose to increase those instances to a specific number based on a time frame. But in many cases, you might want to do something like this. If you're building an application for your business that needs to run Monday to Friday from, let's say, 6 a.m. to 6 p.m. And that's when a majority of your users are going to be leveraging it. You can use this option for repeating specific days. This way you could maybe have several instances up and running during the work week and then have maybe just one instance up and running for the remainder of the time. Now, we're not going to add in this second scale condition, so I'm going to delete it. But we now have enough included here with these two rules to scale our application based on that CPU percentage, which is a good way for us seeing if the server has the capacity it needs to handle the users that are coming to our application.

Monitoring and Debugging an Application
[Autogenerated] So now we're gonna talk about what we do when things aren't going exactly right when we need to look at how we monitor and debug our application. So here, within this demo, were first going to see how we can inspect our log data from a running application within the portal, and then we'll see how we can do the same thing within visual studio code. Then we'll look at how we can implement azure application insights into our no Js application, as well as reviewing to limit tree that is provided by that service. So let's dive in. So I'm here in the portal and first I'm going to go toe APP services. I'm going to select my Web application and then here in the left pane, I'm going to scroll down until we get to the section that includes monitoring. And from here we have several options that relate toe logs, and one of the things that has been added to the portal that is very helpful is the ability for us to see a live log stream of our application. And here you can see we're now connected and we're getting a live log stream now I'm going to go just off screen and I'm going to load our application and so you can see that this pulls it in directly. So we're able to see this basically live as we're running our application, and this could be a great tool to debug any issues that you're seeing. But we also can get this exact same experience from within visual studio code. So I'm here in our application and we have the extension installed for APP service, and I see that there's an option here for logs. If I expand this option, I get several different things I can take a look at. And one of them is the ability here to connect to the law. Extreme, if I click on this option will get a very similar experience to what we saw within the portal is that we are getting this live law extreme where we can see our live application logs coming directly here within our I d eat. So this is a powerful feature if you simply want to debug something that's happening as you're working on your application. But there are other options that are provided for us, and so for that I'm going to go back to the portal. I have the ability in here to define a diagnostic setting for my application. Now, this is in preview, so this might look a little bit different by the time you're viewing this course. So here we need to enter in a diagnostic setting name, select logs and metrics. But then we can choose whether or not we send this into a log analytics. We can archive them out to a storage account or we can even stream them to an event hub. So here we have a great deal of flexibility and how we deal with the log and metric data for our Web application. So here we're not going to implement any of these options. But we are gonna look at another service provided by Azure called application Insights. So I'm going to close out diagnostic settings and then here within the left pane will search for insights, and I'm going to select application insights. Now from here we have the opportunity to turn on application insights. And here's one of the features that's in preview. That's really powerful. If you're developing no Js applications is that you can turn on application insights without even redeploying your code. So they've added some hooks into node so they can get the needed information to pull out the insights from our application. So we're gonna click on the option to turn on application insights, and we'll choose toe Have this enabled. Now we need to give this a name and we'll call this PS node. Web app. Insights will keep this in the same location, and now we're going to select no Js. And it lets us know here that the node in process agent is enabled so no additional configuration is needed. And this is quite a bit easier than it used to be to integrate application insights into your note application and then I'll hit apply. And it lets us know here that we're going to have changes to our app settings, and they're going to install their tooling on top of our application, and it's going to cause us to restart the site. And it says, Do we want to continue and we'll say yes. And now it lets us know that our APP is now connected to an application insights resource, and so directly from here I can click on the resource to jump into application insights. Now the data that comes in tow application insights is batch to some extent, so don't be surprised if you don't see a ton of data right away. You should start to see data stream into application insights, though pretty quickly after you set it up for your application. So now that we have insights up and running on our Web application, I want to dive you through a few key areas where you can investigate how your application is working. The first thing I want to point out is the live metrics option in a similar way to what you have in looking at a real time stream of your application logs. Here, you can see a real time stream of the data of users that are interacting with your application so similar to before I'm going to go just off screen and actually interact with our Web application, and I'm going to reload production here a few times, and we'll start to see that data stream in in real time, giving us information on the telemetry that's coming in. We also have the ability here to go in and look at failures that are happening. So in this case there is a 404 forgetting the robots file for this specific application, and we can see that failure here. We can see that it's happened one time overall, and so we get back our top three response codes and MAWR information about any failures that are happening. We also convey Oh, in and get a more detailed view of our performance so we can begin to go in and see the overall operations. In this case, we can see the 23 times that users were actually getting our index page and the 15 times that were coming in for the style sheet. We can see the average duration, which can help us sculpt star application to be as efficient as it can be. So insights. Here are things that are pulled out of the data that is being analyzed by Azure, and this is where application insights becomes very interesting, because if I click on this, it's going to let us know the things that are common amongst requests, and here we can see that the client city is Cleveland, and that makes sense because I'm in Cleveland, Tennessee. And I'm the one interacting with this application. So this gives you insights. That would be hard for you, defined by simply going through and looking at log data. So this is application insights, and it is a powerful platform for you to analyze your application. And Azure has made it easier than ever to integrate with your know J s application. In addition to providing this you in the portal for you to explore this data. You also can query this data through the rest ap I so you could begin toe work this into your own custom dashboard for your application. And maybe you build one of your organization to have all of the operational data for all of your applications. There's a lot you can do with application insights. But when we pair that with our ability to view our log data for the application, we gain the ability to both monitor and debug our applications. Running in azure APP service

Summary
[Autogenerated] So we have covered quite a bit in terms of being able to run our applications in the cloud. The things that happen post deployment here within this module. So let's take a look back at what we've covered. First of all, we looked at the concept of application life cycle, and we talked about how we can deploy our application into the cloud using deployment slots in a separate environment such as a staging environment. And we can preview it fully in the cloud before we choose to deploy it out to our end users. And at the point we're ready to deploy it. We could utilise deployment slots within APP service to create a zero downtime deployment. Then we looked at how we can review metrics for an APP service application, and we looked at metrics both around. Our Web application as well is with our APP service plan, and we saw ways that we could go in and chart even multiple metrics at the same time and even pin those to a dashboard that we can go back and view later. And then we looked at how we can create metric based alerts for our application so If we want to go in and know if there are a certain number of http errors and we wanna have azure reach out to us directly and let us know either through email or SMS or push notification or even a voice call. We can do that utilizing the alert service with an azure and then we configured scaling for an application using auto scaling. We talked about both scaling up as well as scaling out, and we talked about how, if we have an increased number of users, scaling out gives us the ability with auto scaling toe. Have this happen automatically for us and we defined rules based on CPU utilization, but we also saw many different metrics that we could use to create our auto scaling rules. Then we debugged an application, and we did this in a couple of different ways. First of all, we were able to look at our live log data so that we could analyse any problems that are application was having. And in addition to this, we utilized application insights by utilizing the new feature to integrate it without even having to build it into our note application. And then we were able to examine in the portal the insights provided through this service

Cloud Databases
Overview
[Autogenerated] Now chances are when you're building note applications on the cloud, at some point you're going to need to connect to a database. Now, from what we've learned so far, you could easily spend up your own virtual server and install whatever database you want before a variety of reasons. That's not always the best choice. So we're going to look at some cloud based options that our platform options that we can leverage and let's talk about what we're going to implement. First of all, we're going to be looking at a more traditional database, but one that has been moved toe work well in the cloud. And that's going to be the Azure sequel database, which is gonna be a sequel server compatible relation ALS database. But in addition to that, we're going to look at a different kind of database, one that was actually made mawr for the cloud from the beginning. And that's going to be azure cosmos DB, which is a fully managed database. And it has Mongo DB compatibility as well as other types of compatibility that we'll talk about in just a bit. So over this module, here's what we're gonna be doing first of all, we're gonna be creating and configuring an azure sequel database and all the things that need to go into that. Then we will be connecting to our Azure sequel database from our know J s application. And once we get that in place, will then be creating and configuring an azure Cosmos DB database and connecting to that and working with that from our know J s application.

Creating an Azure SQL Database
[Autogenerated] So now we're gonna walk through the process of creating our first Azure sequel database. However, before we do, we're going to need to know some things about the service. So, first of all, this service enables you to have sequel service compatibility in a managed cloud service. So if you're used to working with sequel server, this becomes a great choice for you. Next it provides two different purchasing models. Now it's important to note, no matter which model you choose, you're getting the same service. It just changes the way that resource is air allocated for your databases. So first we have the V Corps model and this is the newer model. And then we have the D. T you model, and we'll talk in a minute about what a d t you is. And this is the model that was around since the service was launched. Now, at any point, your computer or serviced here can be updated. This gives you the ability to scale within the cloud, and every Azure sequel database needs to be associated with an azure sequel server. But this isn't an actual server. We'll talk in a minute about what this means Now let's look at the D. T you or database throughput unit. This is a combined metric, and it's gonna measure your overall performance of a database, and it combines factors like CPU memory as well as read and write capacity into a single metric. And part of the confusion around this metric is actually what led to the new model, which is the V Corps model. Let's look at these two different models. So first, with the D. T you model, as mentioned, there is less visibility over the underlying hardware as well as the performance capabilities. So if you have a database that is running in your own data center and you want to move it into the cloud, if you know roughly how many eye ops or input output operations per second you're looking at and you know how much memory you need toe actually facilitate the level of performance that you need. It's gonna be really hard for you to calculate that in terms of DT use. Then that takes us to the V Corps model, and this is what covers a detailed cost model. Now, one of the interesting things about the V Corps model as well is it gives us two different options within that cost model. The first is provisioned, and the second is a serverless option. And the serverless option is really interesting because it enables you to pause a database for a period of time if there's no activity and then restarted once the activity comes up again, in some cases, this congrats lee. Reduce the cost of your application if your use case fits. Now, let's talk a little bit about an azure sequel server because in reality this is not a server at all. It's just a logical construct for managing one or more of your Azure sequel databases. And what it does is it centralizes management of things like you're Loggins and your firewall rules, as well as your threat detection policies and fail over groups. Now let's talk. Just amend about some theory around this, because sequel server is a database that was born outside of the cloud. We bring some of these concepts over, even to when it's running in a cloud capacity, even though this is a different service. When we moved to Cosmos TV, you're going to see what it looks like when you have a database that was actually born in the cloud. It's gonna work just a little bit differently. But here, let's go through the process of creating our first Azure sequel database. So we'll be creating a new database as well as the server that goes along with it. And then we'll be configuring, compute and storage capabilities for the database. So I'm here in the portal and you can see that sequel databases is an option that I have here in my shortcut list. I'm gonna go ahead and click on this, and when I do, you'll see I don't currently have any sequel databases, so I'm going to click add, And from here, I need to select the subscription and the resource group. Now I get a chance to give my database and name, and we're going to call this Web app. DB. Now I have the option here to select an existing server. I don't have one, so I'm going to hit, create new, and I'm going to need to give this a name. Now. This will resolve into a domain name, so I need to make sure that this is unique and we'll call this P s Web app. Db I need to give a log in name Well, is a password. And then I need to select the location which in this case will be East us. Now that I've selected my server, I get the opportunity here to say whether or not I want to use a sequel Elastic pool. Now we're not going to cover that within this course. However, if you're managing multiple databases within the cloud, this could be a great way for you to pull. The resource is that you're using across multiple databases and scale them in a way that could save you some money in the long term. Next, let's look at our compute and storage options. So here I can see the V Corps options for my Azure sequel database, and I can see that there are three different high level tears here. General purpose, hyper scale and business critical. And we have some information around the number of eye ops that each option supports, as well as the expected Layton see, and obviously here, with business critical, we get the best performance up to about 205,000 I ops and late and see between one and two milliseconds. But here within the general purposed here we have some different compute years we can look at as well, and that is both provisioned and serve Earless. And we mentioned these options earlier. Provisioned allows us to have our database always up and running well. Serverless enables us to pause our database if it's not being used, and because of that, it has a different pricing model. We're actually charged per second that we actually have that database up and running. And within each of these tears, there are different configuration values for R V Corps based options. It's going to be the number of V cores as well as our database max size. But for what we're doing here just for a development database, there's a tear that's gonna work really well for us, and that's going to be under the D. T. You based options. So I'm going to click here to look at the D. T you based options, and I'm going to select the basic tear now. The basic tear here comes with five DT use, which would not be enough for a production database, but for our purposes, it'll work great I'll also keep the data max size here at one gigabytes, and this comes out to be about $5 per month USD, and that should be sufficient for what we're looking to do. So once I have this selected, I'm going to hit apply. Now that I've configured the compute and storage capabilities of my database, I'm now ready to create it. So I'm going to hit, review and create everything here. Looks like what I needed to. So I'm now gonna hit, create, And now I can see that my database has completed. And next we'll look at how we actually connect and work with our database in the cloud.

Connecting and Configuring Azure SQL
[Autogenerated] So now that we have our database up and running in the cloud, we're now going to actually connect and configure our database. So here, within this demo, were first gonna be configuring firewall rules for our Azure sequel server. And then we'll be installing the M s sequel extension for visual studio code will then be creating an application specific database user and creating a table for application data. So I'm here in the portal with the database that I just launched. Now, as usual, we have some valuable information here on the overview page. We can see our server name, for example, as well as seeing the subscription and the resource group that were leveraging Know if we scroll down, we can see information here like R D T you percentage. Now, I haven't done anything yet, So this is just currently at 0%. I could click into this graph and from here, even going and set an alert rule based on my d t you usage. So I know if I'm getting to a point where I potentially need to scale my database, but for now, we'll just go back to our overview page. I also want to note some other features that are listed here on the overview page. You can see to begin with that transparent data encryption is enabled by default for this database. That means our database is going to be encrypted at rest. Now, all the data in Transit four Azure sequel server is automatically encrypted as well. Now, there are other features that we could choose to enable, like advanced data security, automatic tuning, auditing, geo replication, even dynamic data masking, which would allow us to limit exposure of sensitive information to Onley specific users. Now, in this case, we're going to choose to connect to our database through an extension we're going to install in visual studio code to begin to set up our database for us to use. However, before we can do that, we're going to need to set up our server firewall. So I'm going to click the option here to set server firewall. And in this case, I need to just add my current client I p. So I'm going to click this option here. And so now that I've added this in, I can now hit Save, and this will update the firewall so that I will be able to access this Azure sequel database. Now, The next step is I'm going to move over to visual studio code. Now from here, I'm going to navigate to the extensions tab. And here I'm going to search for M s sequel. And I can see the option here for sequel server and we're going to install this. Now I'm going to navigate back to my project, and I'm going to create a new file in my project called Set Up. Not sequel. Now the first thing you'll notice here is there is an option in the bottom. Right That says disconnected for M s sequel. We're gonna go ahead and click on this is this is going to guide us through the process of connecting to our database. Now, the first thing we need to do here is we need to create a connection profile. Now we need to give it our server name. I'm now gonna navigate, pack over the portal and copy the host name of our database. And so from here, I can pace that in now. The next thing we need to do is we need to enter the database name And so in this case, the database name is going to be Web app DB So I'll copy that, and then we'll pace that in. In this case, we can choose between using the sequel Log in or the Integrated Law again, and we're gonna choose sequel Log in. Now we need to enter my user name. Now, this is going to be the administrative user that we created when we created our Azure sequel server. And then we need to enter the password. And here we conduce Whether or not we save the password, I'm going to say no for now. And then we get to enter a display name for this connection profile, and we'll just call this Web app TB. And now this lets us know that we have connected to our Azure sequel database, and now we can begin running queries. Now the first thing we're going to do is we need to create an application user, so best practices would indicate that we don't use our administrative user within our node application. We need to create a user that is specific to our note application so we can limit its privileges just of what it needs for that application. So we're gonna go ahead and create a user, and here I'm going to paste in a password that I generated. Perfect. Now we should be able to go to this particular line. We can actually highlight this and right click and go to execute query, and we can see here that are query has completed successfully. Now, the next thing we need to do is we need to utilize some stored procedures to give some access to the user that we just created. And so we're gonna use SP ad, remember? And in this case, we're going to add it to the role data reader and then we're going to do the same thing. Four Data writer. Now we should be able to highlight these two queries and execute, and we can see that these also completed successfully. Now that we have those things in place, we can now create a table that we can interact with from our node application. Now we're going to try to keep this a simple as possible because this is not a course on interacting with Azure sequel server. So we're going to create a simple database table that weaken. Simply query from our note application and in this case will call our table users and the 1st 1 to give it an I. D. It's gonna be an end, and this is going to be an identity, Which means that azure sequel server is actually going to generate this value for us. And we'll make this our primary key. Then we need to add in a name and we're just gonna make this an in Hvar car to 55 and then we'll do the same for an email. Perfect. So now we should be able to highlight this query right? Click and execute, and we can see that this also completed successfully. Now we should be able to go in and select all the data from this particular table. It shouldn't return any records, but we should see the table structure come back as planned. And, as expected, we see the results come back. There aren't any results. But we do see the I. D name and email column that we previously created. So here, within this clip, we've been able to set up our firewall rules, install the M s sequel extension for visual studio code create an application specific user and to create our table. So next we're gonna begin interacting with this table from within our know J s application.

Reading and Writing Data to Azure SQL
[Autogenerated] So now that we have our database up and running and we've been able to create a table for us to work and we're now going to read and write data to Azure Sequel So here within this demo, we first will be installing a package for sequel server connectivity to know J s. Then after that, will be running queries from our no Js application against Azure sequel. So I'm here with in my application, and I've added a new file user db dot Js under the data directory. And this will be where we interact with our Azure sequel database. Now, I've already added in some code in here. The first is we're going to import a module called Tedious. Now, this is a bit of a play on words. The protocol for communicating with our Azure sequel database is TDs or tabular data stream. And so the tedious module for node is something that knows how to speak in that protocol. Now, you only to install that that's simply NPM install tedious and then dash dash save. I've already installed this in my particular project and you can see here I'm already pulling it into this file Now we're grabbing two objects out of that package the connection and the request object. The next thing you'll see here is we've added in a config object. Now, this CONFIG object allows us toe actually pass and information about our database so that it can create a connection. And this includes information around authentication, our server name, our database name and things like setting encryption to be true and how it handles the server certificate and then some options for how it packages up our data, such as road collection on request completion. Now, that last option makes sense because we're not returning a ton of data. If you were dealing with a lot of data, you might want to handle that differently. But this will simplify the code that we're using here within this project. Now, I do need to mention something here we normally would not hard code values like our user name, password server and database into our application. We would choose to instead past those in as application variables, which we've demonstrated before, but simply to get to the interaction with the database. I've added those in here for simplicity, but in your production application be sure that you're not implementing him in this manner. Now the first thing we're going to want to do is we're going to want to be able to get a connection to our database. Now the tedious library does not implement promises. So what I'm going to be doing here within this particular file is I'm going to be wrapping the asynchronous calls that I have to my database with promises so that when I'm implementing this in my routes, I can use the A sink and await syntax. So here I've added in a new method that's wrapped with a new promise, and what we're going to do first is we're going to need to create a connection based on our config. Now, this connection object is going to dispatch an event when it either succeeds or fails in connecting to the database. And so we're gonna be listening for that. We'll need to have a callback that listens to see if there was an air. So if there's an air, we're going to actually reject the promise and pass back the air. However, if there's not, we're gonna resolve the promise, and we're going to return the connection. Perfect. Now we have a potential connection to our database. Now, the next thing we need to be able to do besides connecting is we need to be able to execute the query. So we have a new method here, execute query. And just as before, we're returning a new promise. Now what we're going to need to do within this particular method, we're going to need to wrap this in a try catch block. And the reason is is because we could have a failure in getting our connection, But we're going to use the get connection method that we just created. Now that we have that we're going to create a new request object. And the first thing we're going to need to pass in is the sequel that were passing into our execute query method. And then we're gonna need to create a call back, and this callback is expecting three different values. The first is a potential air. Then the row count. And finally the rose that have returned from the query. Now that we have that in place within here, we're going to need to do what we did before. We're gonna check and see if there is an air. And if there is, we're gonna reject the promise. Otherwise, we're going to resolve the promise with an object that's going to include both the row count as well as the rose. Now, while we've created a request object, we haven't executed it yet. So the next step is gonna be to use our connection and we're going to execute this sequel statement. Now, the last thing we need to do here is we need to actually catch if there's an air, because we could have an error that actually happens from getting our connection. And so if that happens, we also will reject the promise and simply return this error. Perfect. Now we've created the two methods that we need to get connection and execute Query. Now that we have that in place, we're going to create two different exported functions, one that will create some users and then one that will fetch users from that table. And so here I've actually pasted those in because we have three different lines here within our create user sequel statement. And so we should see three different users. David, Jane and Edward added into our users table. And then for our query users method, What we should see is all of the return values from the table. So now I'm going to save our user db dot Js file And now we're going to navigate over to our users route. So the first thing we need to do here within our users route is we need to import our user db file. Now from here, we're going to actually take and make this and a sink function, and we're gonna remove what was in there Previously, we're gonna wrap this in a try catch block. The first thing we're going to do is we're going to get all of the users, and here we're concerned with the rose that are returned. So we're going to pull Rose out of the object that's return. And now we're going to say await user db dot where users And then what we're going to do from here is we're actually going to send those rows back now. We're not gonna create some HTML pages to do this for us. This is gonna work a little bit more like an a P i n point where we're simply going to see the data that's returned now. It's always possible we could have an heir. So we need to catch that air and in this case will return and air so the user can see that there is an air. So in this case, will set the return status code to be 500. If we have an error and we'll send back the air message now, this allows us to view the users that are stored within the database. Now. The next thing we need to do is we need to create a call that's going to enable us to actually put users into the database. And so we're going to have a very similar method here, but in this case were interested in the row count that's returning from our call, and this will use our create users method. And then we're going to return a message, and we're going to say number of users added, and then we'll use our account and now we can save now that we've added this interaction with our address equal database, we should be able to go under our terminal and from here we should be able to say in PM start and we're just gonna run this locally. I'm now gonna navigate over to my browser, and from here I should be able to run local host 3000 slash users, and we're going to see that we have an empty set return Now. This is great because this means we are communicating with our Azure sequel database, and it's returning an empty set. Now. The next thing we can do is navigate over to the terminal, and from here we can choose to use Curl to send a put request to slash users. And this should add in our three users into the database. And we can see here that it says number of users added is three. So now if I navigate back over to the browser and if I reload, we'll see that we are returning all three of our users from our Azure sequel database, along with some metadata for what is returned. So here we've been able to install the tedious library. We've used it to communicate with our Azure sequel database, and we have both written to and then read data from our database in the cloud

Creating an Azure Cosmos DB Account
[Autogenerated] So now we're gonna walk through the process of creating an azure cosmos TB account. So here, within this demo, we are first going to be creating that account. But we're also gonna be reviewing one of the unique characteristics of azure cosmos TB, which are the different AP I options that are available for it. So let's dive in. So I'm here in the portal and I'm going to search here for Cosmos TB. I'm gonna click on it now. From here, you can see that I don't currently have any cosmos DB accounts. So I'm going to go ahead and select, add So the first thing I'll need to do, like with any resource is will need to be sure I have my subscription and my resource groups set. Next, I'll need to give an account name and we'll call this Web app Cosmos DB. Now we have an option here to select the A P I. We can see that the default value here is sequel, but there also is support for Mongo, DB Cassandra Azure Table and Gremlin. So this enables you to work with this database in a variety of different ways, and this is one of the unique characteristics of cosmos DB So if you're used to working with mongo DB in your note applications and you're using maybe a popular framework like mongoose or other ways that you connect with mongo DB, you can choose to utilise cosmos TV basically as a mongo database for your application. In the same way, if you're using a graph database and using the Gremlin language, you can use that for Cosmos DB as well. Now we're gonna choose to keep this as the core sequel a P I for Cosmos TB. But you could choose to select another one if that's what you're used to working with. Now, the next thing we're going to do here is we're going to go in and select our location. I'm going to select East us now. One of the differences that we have when dealing with a database like Cosmos DB that was built for the cloud is we're really paying here not for our database were paying for the performance that we're choosing toe leverage and then to some extent, the amount of storage that we're gonna have within our database. So what we need to do here is. Give it some information around how were planning to use our database. In this case, we're going to choose toe, have non production selected. However, if we did go in select production, we would see some of the values below this change, including the ability to leverage geo redundancy, meaning that we would have multiple versions of our database running globally that would stay synchronized as well as concepts like multi region rights. In this case, we're not going to choose to leverage this for our development database. So I will select non production. Now that I have this in place, I can now go in and hit create. We can see here that we have passed validation. So now hit, create. And we can see that the deployment for our Cosmos DB database is underway and in the next clip will be walking through the process of how we actually create our first collection within Cosmos db

Creating and Scaling a Cosmos DB Container
[Autogenerated] So now that we've created our cosmos TV account, we're now gonna walk through the process of creating our first Cosmo DB container and talking about scaling our capabilities on Cosmos TV. So here, within this demo, we will first create that initial container, and then we will look at how we configure the throughput for a container. So let's dive in. So here I can see that my deployment has completed for my cosmos TB account. I'm not gonna click the option to go to the resource from here. Within the overview pain, I can see the normal information I would expect to see about any resource within azure, including the resource group and the subscription. But one of the things here that is missing is a host name that I would use to connect to it. And in place of that, we have a you are I. And the reason is is because cosmos db you interact with it primarily through http, as opposed to through a protocol like TDs. So in this case, you can consider it basically like a big rest a p I that we can call to interact with our data now the first thing you'll need to do in working with Cosmos TB is to create a container, so I'm going to click on the option to add a container. Now the first step we have and adding a container is we need to either create a new database I d or use an existing one. I don't have one yet, so I'm gonna create a new one and we'll call this Web app. Cosmos TB. Now we have an option here to provisioned throughput at the database level or at the container level. Now I'm not going to do it at the database level, so I'll unclipped that, and we'll come back to the concept of throughput in just a minute. We don't need to add a container. We'll call this Web app container, and then we need to add in a partition key. And this is really interesting, because when you're working with a no sequel database, it needs toe have a way to spread your data out across multiple different servers, and the way it does that is using a partition key. In this case, we're going to use course I D as our partition key. Now we're gonna be Creating a hierarchy, of course, is similar to what you would see it plural site, for example, where we will have courses and then within courses, you'll have modules and within modules you will have clips. So we're gonna use that because that is the type of data that can be stored well here within a document database like we have with Cosmos DB. But the system needs to know how to spread that data out across multiple servers. And so we're gonna use course I d here. Now let's talk about some potential pitfalls you could have with the partition. Keep if you have a piece of data that's going to be the same. Not one of the examples that it showed here before I entered. In course I d was using a ZIP code. Well, that works great if your users air spread out all over the country, for example, if you're here in the United States or if you're somewhere else, you're using the postal code. If you have people that are spread out, that's gonna be fine. But let's say that a majority of your users all live within a couple of different zip codes. Well, you're limiting how much we can, actually what we call shard, the data in between multiple servers. So we need to be sure that the value that we pick here is going to have a lot of different types of values. And so we're gonna choose course I d. Now the next thing we have here is the ability to configure our throughput based on something called argues or request units. Now a single argue is represented by a one kilobyte document that is actually retrieved from the system within one second. That's a single are you? So you might need, Mawr argues. If you're either having more requests than one or if you have data that's larger than one kilobyte, and so because of that you need to select either a manual amount or you can utilize a new feature called autopilot that will help guide that value for you. However, in this case, we're going to stick with the manual option, and we're going to choose to set it at 400 are use. That's a good amount for our development server. However, that probably would not be enough for a production environment, depending on the amount of requests you're going to be receiving. You can see that this is estimated to be about $23 a month based on 400 argues, this is gonna be sufficient for us, so we'll go ahead and click. OK, so we can see now that the container creation process has completed and it currently has the Data Explorer option selected here in the left pane. If I expand this out, it's going to show me my database and then within it, it's going to show me my container and then within the container. I have the option to go in and see the items, and I can see that I can query for those items using a sequel like Query language. Now, I don't currently have any items that are included. I do want to call out a few other things here. We do have the ability to go in and define stored procedures as well as user defined functions. And those will actually run within cosmos DB. And we can define those in javascript. So next we're gonna be walking through the process of using our cosmos db container within our know J s application

Using Cosmos DB in a Node.js Application
[Autogenerated] So now that we have our container, we're gonna be utilizing Cosmos TV within our know J s application. And so we will first be installing the JavaScript Azure sdk for Cosmos TB. And then we're gonna be configuring a connection to Cosmos TV from and no Js application. And then we're gonna be reading documents from a Cosmos TV container and riding documents to a cosmos db container. So let's dive in now. I'm here in V s code, and I want to call out a few things that I've added first, I've added in a new route called Courses, so I'll be able to access this both using a get request, any put request at slash courses and will implement both of those in just a minute. So here, that's added within my app dot Js file. Now, the next thing I want to do is show you the data that we're gonna be working with and that's gonna be in this course is not Jason file. That's in the data directory. Now I've added in sample information for a couple, of course, is that I have here on plural site and I've added the men in this nested structure. So we start off with our course and within a course you have modules and then within modules that you have clips and this is gonna work. Well, it's just some sample data weaken store within Cosmos DB. So the next thing that we need to do from here is we need to navigate over and look at our courses that Js file. So from within here, I've implemented two different routes. The first is using a get request, and in this case, we're calling our courses DB, which we're going to implement in just a minute. And that's the file that will contain all of the logic for interacting with Cosmos TB. But we're going to call a method called query courses, and that's going to return to us a list of items that are stored within our container. We're then going to send that back to the browser and just is before this will be more like an A P. I call where we're just going to send back the data. We're not going to construct HTML pages and tables toe list this data back and then here were catching any errors, and we're gonna be sending a 500 status and the air message. Now, as a note here, we could use the default air handler within express and simply passed us on using the next function. But since this isn't a course unexpressed, I didn't want to spend too much time dealing with the specifics of Express. Now the next thing here that we have is we have a put request and this is going to actually create our courses. So we're calling a method here, create courses within courses, DB and it's gonna return an object that will contain within it an item count property. And within there, we're going to return a message that says, Hey, here's the number of courses that have been added and then we're catching any errors that might happen. So this is what we're going to be implementing. So to make this happen, we now need to navigate over to the courses. Db dot Js file Now here I have done two things. First, I have actually pulled in the Cosmos client and this is using the JavaScript, azure sdk and specifically the module for cosmos. And so for this to work for you, you're going to first need to install it, which is going to be in PM install at azure slash cosmos dash, dash save. And this will actually save it within your package dot Jason file. I've already installed it, so I don't need to install that again. And I'm also pulling in the courses data, which is going to be the courses dot Jason file here within the same directory. Now that I have that in place, I need to start the process now of connecting to our container. So the first step is we need to initialize our cosmos client not to do that. We're going to need two pieces of information the endpoint which we have seen and the key. So I'm gonna now navigate over into the portal and so I can see here within the portal that, under my recent resource is I have my web app Cosmos db. So I'm gonna go ahead and click on that, but you could just as easily search for Cosmos and get to it that way. Now the first thing I want to note here is we do have Are you? Are I? We will need that value, so I'm gonna go ahead and copy that value and then we'll navigate back over to V s code and here will insert are in point. Now, I'm gonna navigate back to the portal, and from here, I'm going to scroll here in the left pane down two keys, so you'll note here that you do have a primary and a secondary key. Now, they both work the same. They both give you the same kind of access, but by having two keys, it gives you the ability to rotate keys. So if your application is using your primary key, you could then switch it over to use your secondary key. And then you could regenerate your primary key to be a new key. And in this way, it enables you to be able to shift in and out different keys over time. So your application isn't relying on the same key for an extended period of time, and this is just a security best practice. But for us, we're going to use our primary key, and we're now gonna navigate back over to V s code. So from here, we're gonna paste our primary key in, and now the next thing we need to do is we need to remember the name that we have for our database, and our container will need both of those to be able to get a reference to our container. So from here, our data base I d. Was Web app Cosmos, TB and our container I d was web app container. Now, the next thing we're going to do is we're gonna have a variable that will store a reference to our container, and then we're gonna create an A sink method that will get a reference to that container because we'll need that to either query our container items or to insert a new item into our container. So we'll call this get container. It's gonna be a sink method. And one of the differences that you'll notice is the sdk that we have now for azure. It utilizes promises. So we're not gonna have to do some of the heavy lifting that we had to do when we were working with users and our Azure sequel database. Now what we're gonna do here is we're gonna say if that local value is not defined for container, we're going to go out and get a reference to the container, and that's going to be a synchronous that we're gonna say await. Then we're gonna say client database and we're gonna pass in our data base, I d. And then we're going to say container and passing our container, i d. And now that gives us a reference to our container. And we're just gonna return that. And by storing this locally, we're not gonna have to go out and fetch this data every single time. If you remember, the first method that we need to implement is query courses. And so this is something we're gonna export. It's gonna be in a sink function. We're not gonna pass in any arguments to it. First, we need to get that container reference, and we just gonna call that see locally, were going to say, Oh, wait, get container! And then from within here, we now can actually query our container. Now we're gonna be getting all of the items out of our container Now. You would want to be careful in doing that if there was a lot of data. But for our small sample data set, this will work just fine. It's gonna return an object that's gonna have a resource is property within it. And so we're gonna deconstruct that pullout Resource is, And then we're going to say, Oh, wait, because this is an asynchronous function and we're going to say si dot items and this gets us to our items in our container, and then we're gonna say, Read all and fetch all. Now that we have. Our resource is we cannot return those. Now. The next method that we need to implement is what we're going to use to create our courses. This also is gonna be in a sink method and have no arguments, and we're also gonna get our local container now. What we're gonna do next is we need to loop over each of the courses that we've defined within our Jason file, and we need to store each of these in our container Now. Once we do this, we want to wait for that process to complete before we move on and then return the number of items that were stored within our container. So this is an asynchronous operation. We're going to start with a weight and then we're going to use the method promise dot all, and this will allow us to pass in an array of promises, and it will wait for all of them to complete before proceeding. Now we're gonna take our courses data. And if we wanted to call a method on each item in the array and then take the return value, which in this case will be a promise and then package it up into an array, we would use the method map and we're gonna pass in a lambda expression here. And so we're gonna have a course that gets passed in to this function. And what we're gonna do is we're now going to store it in our container, and we do that by saying, si dot items dot create and then we'll pass in the course. So in this manner were able to store each of the different values four our courses into our container. Next, we're going to return an object that will contain within it an item count, and this will be courses data dot length perfect. So we've now implemented the two different methods that we need in a rout. So the next thing I'm going to do is I'm gonna actually start this locally, So we'll just say in PM start. And now I'm gonna navigate over to the browser. So if this works is planned, I should be able to navigate to local host 3000 slash courses and we should see an empty array. Now, that's perfect, because that means we are communicating with Cosmos DB, even though we haven't put any data in place yet. Now, the next thing I'm going to do is I'm going to navigate over to the terminal and what I'm gonna do here within the terminal is I'm going to use Curl to send a put request to local host 3000 slash courses. And we can see here that the number of courses that have been added is too. So now I'm going to navigate back over to the browser. So now if I reload this, we'll actually see our courses data return. And you can see now that the course is data that we put in, has returned, but not just the data that we put in. There's some other values there as well. So let's now navigate back over to the portal and let's go look under the data explore. I'm going to open up my Web app container and then we're going to look at items now to give us just a little bit more room. I'm gonna collapse the menu here on the side and I can see that I have two items that have returned. I'm just gonna click on one of them so we'll be able to see here first. We have the information that we included, like course I d course name, course, author, the modules. But if I scroll to the bottom here, you'll see that there are several other values that have been added and as well So cosmos DB adds its own I d. And we haven't e tag as well as a time stamp for when it was added in along with some other values. And this is a part of how cosmos DB manages the data that it puts into a container. Now that we've added this in were next going to look at considerations for how we monitor in scale our cosmos db container in the cloud

Monitoring Cosmos DB
[Autogenerated] So now we're gonna talk about how we monitor Cosmos TB now that we have a container up and running and we actually have data that we can interact with, So here, within this demo, we will first be reviewing the metrics that are provided four cosmos TB in azure. Then we're gonna be scaling Cosmos TV after creation. So we know how to set the throughput when we launch our container. But how do we scale it once we have it up and running, Then we're gonna be looking at how we can configure alerts for Cosmos TB. So let's dive in. So I'm starting here in the overview pain and let's scroll down here in the overview pain. And if you see here, we now have a chart that's going to show us the total number of requests. It's also gonna point out the different http status codes that were returned. Now there's one that you need to watch out for, And that's gonna be the http status 4 29 That means too many requests. And what that means is, is that you haven't allocated enough are used for the load that you're currently seeing. So if you wanna be able to keep an eye on that? If we scroll down here in the left pane, we're going to get to the section that allows us to see metrics. And so if I click into metrics here, first of all, it's gonna give us a high level overview of what are average Throughput is in terms of our use. It's also going to let us know our data size as well as our index size. We do have a specific chart that is going to show us the number of requests that we have for our exceeded capacity. Which means that HTTP status for 29 we don't currently have any of those requests were actually not anywhere near our are you limit here on our development database. But this is where you could monitor this information. And if you wanted to be able to keep an eye on this data, you actually could go into alerts and we could go in here and create a new alert rule. Now you can see here that it has pre selected our cosmos TB account. Now we need to add in a condition, and in this case, we're gonna be looking for total request units. But we aren't really interested in the total request units. We want to filter this down and have this be just units that haven't http status code of for 29. So I'm going to go down here to status code and we can see here it doesn't list that particular status code we can add it in, and then we can be sure that it selected and that's going to filter down the signal so that it's just listening for that for 29 status code. Now, the next thing we need to do is sadly threshold. And in this case, we're going to say, if this is greater than zero, we wanna have an alert. And once we have that in place, we can hit done, and it's gonna let us know here. This will be about a 10 cent a month cost in USD for this particular alert. Now, I'm not gonna go through with the creation process because we would need to connect this to an action group which we've already done. But this illustrates how we can utilize the alerts capability to let us know if we're running out of request units. Now, if we do get an alert that fires were going to potentially need to scale our cosmos db container, some gonna go to scale here. I can see here that my current throughput in our use is set to be 400. Now, I could just as easily go up here and choose to increase this. Let's say to 1200. And if we then went in and saved, we would actually have now our container set toe have three times the initial throughput that was configured when we created it. And you can use those alerts to guide that process for you as you work with your cosmos DB databases. So now we've been able to monitor and set up alerts in scale our cosmos TB in the cloud.

Summary
[Autogenerated] So let's take a minute and look back at what we have been able to do here within this particular module. So we've implemented two different databases. Now, before we go in and talk about the specifics of what we did with each, let's take a minute and review the difference between what we're doing when we're doing an infrastructure as a service approach. Or we're deploying a database within our own data center versus the services that we were leveraging here. Aspects like security, like patching aspects like how we scale databases. We have seen that all of these are managed with Azure sequel and Cosmos TB. We gain the ability to simply go in, make an adjustment within the portal, and the rest of the management is something we don't have to worry with. So, irrespective of what database you choose to work with, you're gonna have a lot of advantages by using a platform as a service type approach over doing it yourself. But here within this particular module, we were first able to create and configure an Azure sequel database, and we talked about how the Azure sequel server is just a logical container that wraps around the databases that we create. And we were able to utilize the tedious library to actually connect to our Azure sequel database from our no Js application. And we were able to both store data in the database and then fetch that data and utilize it within our express application. And we did the same thing for Cosmos db. We created and configured both a database and a container for cosmos T B. In this case, we chose to use the sequel AP I but we looked at all the different AP I options that are available for Cosmos TB. And then we utilized the azure sdk for JavaScript, specifically the Cosmos DB module to connect to our Cosmos TV database, and we were able to push our Jason documents into the database and then read them back from within our express application.

Cloud Storage
Overview
[Autogenerated] next, we're gonna be talking about how we leverage storage in the cloud. First of all, let's talk about some of the components that are required when you're looking at cloud storage. So we need to have things like data, durability, meaning We need to know that when we put our data in the cloud that it's going to be secure irrespective of hardware failures that might happen. We also want to have secure and configurable access. We need to make sure that only the people that we want to have access to our data can get access to it. We also need to have global scale ability because we want to be able to distribute our content so that people, irrespective of where they are in the world, can have access to it. And as a part of that, we need to have the infrastructure managed that is supporting all of this. It needs to be continually updated so we don't have security vulnerabilities, and so that even if we have things like disk failures, we still have our content out there in the world. We also need to have data accessibility meaning here that irrespective of what type of platform were interacting with there were building applications on. We need to have a way to both upload data into the cloud and then retrieve data from the cloud. So let's talk about the different services that azure provides. That helps with this. So here, within the azure core storage services, we first have azure blobs, and this is a massively scalable binary data storage service. Now, blob stands for binary large objects. But again, you can theoretically store any size data inside of this service. We also have azure cues, and this is a manage message queue for asynchronous communication. Now, we're not going to be covering cues here within this course, but it will be covered later. Within this path, this is an important concept that no Js developers need to be familiar with. And then we have azure tables, which is a no sequel key value scheme, Ellis Data Store four structure data. And if you remember, we talked about cosmos Deby having an a p I that allows it toe work like azure tables. So depending on what you're looking to do, this might be something you could consider. But we're primarily going to be focused here within this course on utilizing the blob storage service. So azure blob storage is the primary object storage solution in the cloud, and it can support blobs of up to 4.7 terabytes. Now, this is specifically for something called block blobs, which is the type of blobs will be dealing with here in this course. Now it is highly available, and it does provide durability and strong consistency for the data stored within the service. So let's talk a little bit about the different concepts within azure blob storage. So first we have our azure storage account. Let's say, for example, we have a storage account PS Web app. Now within our account. We can then have multiple containers, and each of these containers are still connected back to that single account. So there is some configuration that has done at the account level some at the container level. Now, within our containers, we can have specific blobs, which are the binary data or the files that were uploading into the service, and these are connected to a container, and then you have some configuration that happens at the blob level. So let's talk about how we're going to apply this over the course of this module. So first of all, we're gonna be creating an azure storage account. And within the azure storage account, we're gonna be reviewing pricing for blob storage. So you understand the different variables that are at play when we're talking about pricing, then will be configuring an azure storage container. And then we'll be uploading files from our no Js application into that container, and then we'll be creating and using a shared access signature or s A s four controlled access to the files that we've uploaded into the cloud.

Creating a Storage Account
[Autogenerated] So now that we have an overview of azure storage, we're gonna dive in and create our first storage account. So here, within this demo, were first going to be creating a new storage account. And as a part of that will be reviewing the different configuration options that are available for a storage account. And then to understand the pricing impact of some of those choices were gonna be exploring the azure pricing calculator for azure storage. So let's dive in. So I'm here within the portal and I'm first gonna navigate to storage account. Now, I already have a storage account that was created when I set up the Azure Cloud show. But I'm gonna create a new one that will use with our Web application. So I'm going to click add now from within here, just like any resource. I'm going to select my subscription and my resource group, and then I need to go in and give my storage account and name. Now from here, I need to select a location, and I'm going to keep it in the East us. Now there are several different options that I can select for my storage account first we have performance, and this allows me to select between standard, which is going to use normal hard disk drives and premium, which will use SSD drives now. The next thing we need to do is we need to select an account kind Now. I would recommend keeping storage V two instead of selecting one of the other options, because this is going to provide the most capabilities for you now. The next thing we need to do is select an access tear, and I'm going to keep this here as hot. Now, if you had data that didn't need to be accessed frequently at all, you could potentially choose cool. And there's even another access to your that you can configure at the blob level, which is archive, and that changes the way that were actually charged for how we access our data and we'll dive more into that. When we look at the price and calculator in just a bit now there's one option that I skipped here that we need to take a look at, and that's the option for replication and there's a lot of different options to consider here, so let's go back to the slides and just explore these different options. Now, one of the great things about azure storage is that no matter what option you choose for replication, your data is gonna be copied to at least three different locations when you uploaded into azure storage. Now what happens in terms of where that data stored and how many regions that spreads across, That's what leads to the different options that we're going to review to. The first option is locally redundant storage or L R s. This is the cheapest option, and this gives you the ability to store your data within three different places in a single physical location. So within a single data center, and then we have a zone redundant storage or ZR s. It's still stored within a single region. But instead of being stored within one physical location, this is going to be stored across three different availability zones. So this gives you a level of protection. If something were to happen and we were to lose an entire data center, then we have geo redundant storage, or GRS, and this is the first place where we start to work in a secondary region. So we have our data stored within a single physical location, three different places and then, in addition to that, it also is stored within a single physical location in a secondary region. Then we have Geo zone redundant storage, and this combines the concepts we've talked about so far. So with this approach, it's stored within three different availability zones within your primary region. But then it is also stored within a secondary region as well. Now let's look at some different options for not only how we can store our data in multiple places, but where we can read data from multiple regions. And this brings us to to additional options that we have. And the first is read access for Geo Redundant Storage or our A. G. R s. So with this model were ableto have Jordan the storage where it's stored within a single physical location in a primary region. It's also stored in another region, and we can read from either region. So if we were to lose an entire region and we had users routed to our data with this approach, we could still route them to the data within our secondary region. And then we have read access for Geo zone redundant storage or are a GZ R s. And with this particular option you have your data stored across multiple availability zones within your primary region and you have a stored within a secondary region, and you can route users to read data from either region. So now let's navigate back over to the portal. Now, in this case, we're going to choose locally redundant storage or LRS because we're just using it for our sample Web application. But before we create our storage account, let's understand some of how we're actually charged for how we use our storage account. So I'm now going to navigate to the azure pricing calculator. And if you were to go into Google and search for azure pricing calculator, this is probably gonna be the first option that comes up Now. What I'm gonna do here is I'm going to select the option for storage accounts and then I'm going to click view, and this gives us an overview of how were charged for using our storage account. Now I'm gonna keep the default values here, which is gonna be a general purpose V two storage account in the East us using block blob storage with a standard performance tear. It currently has lRS set for redundancy, and it has the hot access to your selected. So if we just use the pay as you go option for this and restoring 1000 gigabytes, we're gonna pay roughly $20.80 for storage, and then you can see we're gonna pay about 50 cents for right operations about 50 cents for list and create container operations about four cents for read operations. So on and so forth, you can see that there are several different things that factor in to what we pay for our storage. And this leads to a monthly cost of about $21.84. Now, if we were to change our redundancy option from LRs toe r A g. R s and you'll notice some of the options aren't included here, That's because some of the options that I've reviewed with you are currently in preview. But if we did the r. A. G. R s approach, we would see that our total cost has increased. We can see that just storage has increased to $58.90 and we can see our total monthly cost increasing to be $80.94. Now, let's see how some other things potentially factor into this as well. So I'm going to switch from R A g. R s back to LRs. But now we're gonna change from a hot tear to a cool tear. Now, with this particular approach, we see our cost for storage decreasing. This is down to $15.20. But we'll see now that our operations against our data are now quite a bit more expensive. And so this leads us to be about $26.80. But if we have more operations, that's going to increase the cost. Now, the reason is is because with our cool access to your or especially are archived here, you're actually paying more for the operation that you perform against the data because the expectation is if you're using the cool or the archived here, that you're not gonna be accessing that data very frequently, if at all. And so that's how you can choose to leverage those access tears because we can see, for example, that are storage of 1000 gigabytes. If we're using the archived here goes down to be just 99 cents. But we see those operations again being pretty expensive. Especially are read and high priority read options if we have those selected. So in this case, if we want to be able to store 1000 gigabytes, we want a store it in the hot access to year, and we want to use our lRS redundancy. We can expect to have a total monthly cost to be somewhere around $21.84 which, if we think about all of the capabilities that are included, that's still pretty impressive that we're able to store that much data for that amount of money. So now let's navigate back over to the portal, and we're gonna go in and review and create our storage account. So now we've started the process of deploying our storage account, and next we're gonna dive in and be creating our first container

Managing Storage with the Storage Explorer
[Autogenerated] Now that we have a storage account, we can begin working with azure storage. So here, within this demo were first could be creating a new address storage container, reviewing the different access level settings for an azure storage container, and will be installing and using the Azure Storage Explorer application. So let's dive in so I can see that my storage account has completed it's deployment process. So I'm going to click on the option here to go to the resource so I can see here in the overview blade for my storage account, I get high level information that I would expect to see, including my resource group. In subscription, I can scroll down and see metrics that are specific to how I'm using the storage service. But in this case, I'm going to scroll back up and select the option here for containers. I don't have any containers that have been created just yet, so I'm going to click on the option to add a new container. We're gonna give this one a name of user images, and now we get the option to set a public access level. So by default we have private selected and That's actually what we're going to be using. But there are other options. We could choose to use the blob option, which gives anonymous read access for blobs on Lee. Or we could go in at a container level and have anonymous read access for containers and blobs, which would give users the ability to go in enlist and see all the contents of those containers. Now you might want to choose toe. Have the blob option if you're uploading, for example, different assets that you want to be able to pull in from a public website. Or you can utilize the container option. If you want to be able to store a large amount of files and let other users go in and actually see that list and download those now for our use case, we're gonna be adding on to our node application and letting users upload an image store that an azure storage and then generating something called a shared access signature or S A S so that we can give them temporary access to the file but without having to make it public to the rest of the world. So to do that, we're gonna keep private here selected as an option, and then we'll hit Create. Now, Once we have our container created, we can now click into it, and we can now do several things from within this view, including upload files directly into our container or go in and actually change the access level. Now, in this case, we're going to utilize a different solution for being able to manage storage. And that's going to be the azure Storage Explorer. So as your storage Explorer is an application that's provided by Microsoft that lets you go in and interact with your azure storage account but utilizing a desktop application as opposed to your browser and it's available for Mac Windows and Lennox. So once you install it, you'll be able to interact with your storage account. No, I've already installed this here on my machine, and I have a Mac, so I'm gonna go ahead and launch it, and I've already logged in to my account. And so, with a high level, I can see here the difference storage accounts that I have configured. I can also see that there's a preview that lets me interact with both cosmos TB as well as Data Lake Storage Gen one directly from here within the storage Explorer. I'm gonna go ahead and select my PS Web app. I'm going to right click and refresh this because we've added a container and so I can look here under blob containers and I can see now my user images container that we've just created. So from here, you can go in and manage the files that you have included. You can copy you RL's. You can clone files and give them new names. There's some actions here that you can do that would be a little bit more difficult to do within the browser. And so this is just another tool that you can use to interact with your azure storage account. So coming up next, we're gonna be interacting with our azure storage account from are no Js application. We're gonna give users the ability to upload their own images, store those inside of azure storage, and then we're gonna be generating our shared access signature so that they can view those images from within the storage account

Uploading a Blob from Node.js
[Autogenerated] So now that we have a container, we're gonna begin toe upload data from our no Js application. Over the course of this demo, we first will be integrating multi part uploads, indoor know J s application, and then we'll be connecting to our azure storage account from no Js will then be up loading blob data into our container and then we'll be utilizing the u. R L four are uploaded data, so let's dive in. So I'm here in V s code and I'm in my application, and the first thing I'm going to need to do is to install several different packages. Let me walk you through these. First we have one called Moulter and one called into stream. Both of these factor into how we're doing multi part uploads within express. And they're really specific to that. They don't have to do with how we're interacting with azure blob storage. We're then gonna be utilizing the azure blob storage STK a library called moment that is gonna help us with some date manipulation. And you you i d that will use to generate unique ideas for each image that's uploaded. Now I already have these things installed. So I'm just gonna close out of the terminal and I'm here in my app dot Js file And I've tried to do some of the basic work already so we can focus just on the code that's going to allow us to interact with our azure storage account. But here within the app dot Js file, I have added in a new images router, and this is going to be handling the slash images route. So if I go and take a look at this router, we're gonna be handling three different requests. Burst for just r get request that slash images, we're gonna be rendering a new view called images and we'll go take a look at that in just a minute. But that's going to just render a form that will allow a user toe upload an image for the post request. That's where the form is going to submit the data to. We're doing something here. We're passing in some or middleware. This is gonna be an upload strategy. And if I look here at the top of the file, we can see that we're utilizing Moulter to create this upload strategy. We're having it store the data from the uploaded image in memory, and we're telling it that we're expecting a single file with the name of bile. What we're doing from here within our post request is we're going to be utilizing a method called upload image that we're gonna have to create in just a minute that will send the data off toe azure blob storage. And if that works successfully, we're gonna redirect the user to slash images, slash show and then slashed the i. D. That was generated for the image. And that's the final request we need to take a look at. So that's what we're doing here in the final one. We're going to be calling a method called Get Image. You are I And we're gonna be getting back the ur eye for the image that we just uploaded. We're gonna pass that into a view called show image and that will just have an image tag that will be able to then display the image that we just uploaded. So let's quickly take a look at those views. So here, within images, we just have a form that has a single file upload, and we are making sure that we're setting this to be multi part form data because we are handling some binary data with this form. Now, the next thing I want to point out is our show image view. And this is a very basic view that's just going to allow us to view that image were passing in that image source value into this particular template. So now that we've seen those things, it's now navigate to the file that's going to allow us to interact with our azure storage account. Now, here, I want to point out that we're pulling in several different objects from the azure blob storage sdk were using Blob service Client Blob s A s permissions and generate Blob s A s query parameters. We're also pulling in both moment and you you i d. Now the first thing we're gonna need to do is to authenticate. Now, as we've mentioned earlier, it's not a best practice to hard code things that connection strings or credentials into your application. A matter of fact, you shouldn't do it. However, for the purposes of this demo, we're including it here. But you would probably want to include this inside of an application environment variable for your web application. So now I'm gonna navigate though back over to the portal to get this connection string. So I'm here at my azure storage account and I'm going to go here under the left pane toe access keys Now, just like we had with Cosmos TV. We have multiple keys that are available here, but in this case, we don't just want the key. We want to get the connection string. So I'm gonna copy the connection string for Key one. And now I'll navigate back over to V s code. So I'm gonna go ahead and paste in my connection string and then below this, I'm gonna enter in our container name, which was user images. So next I need to add in a function that's gonna let me have access to the client that allows me to interact with my container. Some first gonna create a local variable, and then we're gonna create an asynchronous function, and the first thing we're gonna do is check and see if that local value has been set, because if we've already said it, we don't need to go back and do this again. What we're gonna do here first is we're gonna utilize a static method on Blob's service client that allows us to pass in that connection string and get back and authenticated Blob's service client. And this is gonna be at our account level. Now the next thing we need to do is we can use that to actually get a reference to our container client and this is also a synchronous. And then we're gonna pass in the name of our container, and now we have access to our container client. So now we could just return that client Now that we have that in place, we can now transition to looking at our upload image method. So the first thing we need to do here within our upload image function is we need to get a reference to that container client, and that's gonna be an asynchronous operation. The next thing we need to do is we need to generate a file I d. Now the next thing we need to do from here is we need to get a reference to a client that will allow us to work with this specific file within our azure storage container So what we're gonna do here is we're gonna create something called our Block Blob client, and that's going to be equal to our container client. And then we're gonna pass in the name of the file, which is gonna be our file I d. Now, once we have that, we're able to perform the asynchronous operation of Upload Stream and then we're just gonna pass in the stream That was included as an argument to this function. Now, the last thing we need to do is we need to return an I d. And that's just gonna be the idea of our file, which is file i d. Now that we have that in place, we've completed our upload image function. Now, the next thing we need to do is we need to get our image. You are I and we're gonna be passing in that file I d. As an argument into this function. So the first thing we're gonna need to do here is to get a reference to our container client And then, just as before, we need to get a reference to our block blob client. But in this case, instead of being based on the file I d That's just gonna be based on the i d. Now that we have that we can return the u r L because that is actually a property on our block blob client. So now we can save. Now we've included what we need to get this to run. So I'm gonna go here to the terminal and from here in the terminal, we're gonna start our application. I'm gonna open up the browser, but I should be able to navigate toe local host 3000 slash images and I should see our form. I'm gonna go ahead and choose a file. I have a single image here that I'll upload and we'll hit Upload. Now we see that everything returns. But here we don't have access to the image because if you remember, we configured our container toe, have private access. So if I go over back to the portal, I could go back to the overview blade and go under containers. I can not click on my user images and I can see that we have indeed uploaded from are no Js application. This particular file Now what we're going to do next is we're going to configure the shared access signature so we can complete the process and allow the users to view this image within our node application.

Generating Shared Access Signatures
[Autogenerated] now, the last step in making our application work correctly is to generate something called a shared access signature. So here in this demo, were first gonna be reviewing the shared access signature options from within the portal. And then we'll be taking that knowledge and configuring a shared access signature from our no Js application. So I'm here in the portal, and here is the image that we had just uploaded in the previous demo. I'm able to select this. I'm gonna minimize this left pain here, and one of the options that you'll see here at the top is generate s A s. So I'm gonna click on that now. What this does is this allows us to configure someone else toe have access to a specific item within our azure storage container with the different settings that are included here. We can first go in and set, for example, permissions, and that could be read, create right or delete, or any combination of those in this case will just leave it at Reed. We also consent a start and an end time for that permission. So we could say that maybe it starts now, and maybe it's going to go for another six hours or eight hours. We also can even specify the I P addresses that are allowed toe access, that particular resource using this shared access signature as well as the protocols. And then we can specify whether or not this uses our first or second key that is included for azure storage account. So if we were to do this and just keep all the default values right now, if I went in and hit the button to generate a SAS token and a girl, we would then see what the token is. And then we would see a you Earl. And so if I were to actually select this and copy it and go into a new tab, I could actually then see this as the image that I had uploaded previously. Now the reason that this doesn't open up in my browser directly, it's simply because we didn't add a file extension on this. It doesn't know the mime type of the specific file, but everything comes through. I get the proper permissions. Now here comes the question. How do we now integrate this into our know J s application? And that's what we're gonna dive into next. So I'm gonna go ahead and move over to V s code. The first thing I need to do is I need to generate a new function here within my image. Db dot Js file and this is going to be get s a s query string, and we're going to need to values to arguments to make this work. We're first going to need our file i d. And then the next thing we're going to need is a credential so that we can actually sign this. So what I need to do from here is first created Paramus object. And the first thing I need to include here within this parameter object is the container name that we've already included this So I'm just gonna reference the variable that we already have. The next thing we need to do is put in the blob name. Now, this blob name is the file idea that gets passed in. So we'll go ahead and add that in now. From here. The next thing we need to include are the permissions, and in this case, we simply want to give them read permissions and so there's a little helper blob s A s permissions and then we're going to say parse. And in this case, we're gonna pass in our and that will let it know that we just want them to have read permissions after permissions. We need to specify when this starts and we'll just pass in a new date object to indicate that it's going to start when we create it. And then we're going to set the expiry date. And this is where we're going to utilize moment. So here we're going to say moment and then we're gonna say add 30 and minutes and this allows us to to specify a time. That's 30 minutes in the future. Now, with this, any user that has this girl would be able to view this resource for 30 minutes. There are other things we could add in if you remember, including limiting the protocol. So maybe forcing them to look at it via https or we could even limit by I p address. Now that we have this, we're ready to actually generate these query parameters. And so we're gonna call this method from the azure blob storage STK generate blob s A s query parameters. Now, this is going to take two different values. The first is going to be the parameters that we just created. And the next is gonna be the credential that we passed in as an argument to dysfunction. Now that we have that we're gonna utilize to string toe, actually get back the query string. So now we're gonna add this into our get image. You are. I function, we need to go to our get image. You are I function include the value that we were already including, which is the u R l from our block blob client. And then we're gonna add a question mark because we're gonna add our query string after this, and then we're gonna call the method that we just generated and that's going to be get s a s query string. We're gonna need to pass in two things. The first is gonna be the i D. Which that's not a problem. We have that that's passed in as an argument. And then the next thing we're gonna need to pass in is the credential. But we can go to our container client and from our container client there is a credential property that's actually on that client. So now that we have that, we should be able to save this and now go back and run our application. And so I'm just gonna enter in PM start. And now I'm gonna navigate over to the browser so should be able to navigate to local host 3000 slash images and then we'll upload a file. I'll pick the same file I picked before, and now we'll upload. And when we do, we can see the image that we uploaded has been included, and that is visible here because we generate that shared access signature. If I go in and inspect this, we're able to see that that you are. L includes the information that we passed in, including when it starts and when it expires. But it also includes a signature, and this is actually signed using the key that we have for azure storage account, which means that no one can manipulate these values, actually wouldn't work if we went in and changed any of the values that we have for example, the expiration time. So through this, we're able to configure access to the items that we have within our azure storage account, using a shared access signature

Summary
[Autogenerated] So let's quickly review what we have been able to cover within this module. So first of all, we created an azure storage account. We talked about some of the benefits that come with using a mature storage platform in the cloud. And when we created our account, we looked at different options that we have for things like our performance here, or are geo redundancy options. We then reviewed pricing four azure blob storage, and we utilized the azure pricing calculator in our browser to do that. And we looked at how changing things like our geo redundancy option can affect what we pay as well as our accessed here. So then we created our first azure storage container, and in this case we chose to set our privacy option to be private so that no one by default had any access to the items within our container. And then we were able to upload files from a no Js application into our storage container, and we utilized the azure blob storage sdk to do that. And then we were able to create a shared access signature for controlled access to the items within our container and that included first us doing that in the portal, but then we actually replicated that by doing it within our no Js application.

Azure Functions
Introduction to Azure Functions
[Autogenerated] So next we're gonna talk about a new concept, and that is the concept of azure functions. Now let's go back and look at our different cloud computing models that we've been discussing. When we started off with this particular course, we looked at infrastructure as a service. We were able to launch our own virtual machine. Then we looked at azure APP service being a platform as a service option. We also talked about the software as a service approach where, really there's no configuration. You might have very little control, but it's very easy for you to implement. Now we're gonna be looking at an azure function app, and this is going to be something that still has elements of both platform as a service. But it's also much closer to a software as a service approach because we have to configure even less than we did before. Let's introduce this concept of server lis compute, and so serverless compute is the ability to execute our compute tasks in an on demand manner, without configuring any of the underlying infrastructure, how we scale it or even our life cycle management. So we use this often when we're looking at event driven work flows and will showcase why, throughout the course of this module, let's look at some different benefits of using azure functions. No. One of the things that we have is that we have automated scaling, so whether or not we need to execute this one time or a 1,000,000 times, the service itself knows how to scale it accordingly. Now this does depend on your hosting plan, and we'll talk about some of that in just a minute. It also comes with bindings, too many azure services. So if you want to do something like trigger a function to be executed when you upload something into blob storage that in a lot of ways is just baked into the service, there also is an integrated development process. So when we looked at how we leveraged no Js on our virtual server, we had to implement things like how it was going toe log. We would need to implement things like tracing and inspection, and all of that is included within something like azure app service. But it goes a step further here within azure functions, and we have automatic integrations with many of those different services for how we get information on the functions that are run. It also has wide platform support. And let's take a minute to look at what those different platforms are so we can choose to leverage and azure function that is written in C sharp job script F sharp Java power shell, python and typescript. So we have a wide array of support on azure functions. Now let's look at the three different hosting options that you have when you're creating an azure functions app. So first you have the dedicated plan. Now I've included this here in a different color than the later options, because this option is different now. At its core, an azure function app in some ways is just a version of an APP service app. And with the dedicated plan, you're actually choosing to use an APP service plan as opposed to taking full advantage of the server lis capabilities. Now there might be reasons why you'd want to do this if you have an APP service plan that you're already paying for, because with an APP service plan you're paying for those resource is whether you're using them or not, you might have some extra room on that plan, and you just want to run your functions on there, and that's totally fine. But with the next two options, we get to fully take advantage of the server lis concept, so the next one is consumption plan, and with this plan, you have your infrastructure dynamically added and removed based on demand. Now the premium plan is just an updated version of the consumption plan. You get that same model with the premium plan, but you also can integrate your V nets and you can eliminate cold starts. And that's a concept will talk about here in just a minute. Now let's look at this concept of an azure function app, So the way that it's organized is you have your initial function app and let's say we have one called PS test function app. Now, within a function app, you have functions that are running, and maybe we have two different functions here within our function app, that one that's processing images and one that's updating our user info. Now let's talk about the two different things that make up an azure function. The first thing is probably no surprise to you and that is our function code written in any of the languages or platforms we had mentioned earlier. But the next thing and this is critical is how our function is triggered and our input and output bindings. Now we're not gonna be talking fully about triggers and bindings just yet will introduce the concept, but we'll be covering that in more detail later within this module. So next let's talk just a little bit about the life cycle to help understand how some of this plays out. So let's say we do have our azure function app, and within it we have two functions are processed image and update user info functions well. We have our service here, and let's say that we're triggering our functions based off of an http call, which is one of the trigger options that we have available to us within azure functions. So let's say we get a Web call that comes in well, it's going to handle making sure that there is virtual infrastructure ready to handle that request now, the first time that request is made, it's going to need to pull the code and configuration for our function into that virtual server and that first time it's what we call a cold start. It's gonna take just a little bit longer to do that. Then it will, with subsequent requests. Now it's gonna keep our function available for a period of time. But let's say what happens if we get a time of additional requests in? Well, in that case, it actually could spin up another virtual instance and move over another copy of our function and maybe even another one and add in another. But over time, as the request diminish, it would also manage the process of actually removing our functions in the infrastructure that is supporting it. So let's talk about how we're gonna bring this concept to life here within our module. So first we're gonna be creating our own azure functions app, and then we're going to be creating and configuring an azure function in the portal. So one of the great things about the services it's very easy to get started with you can take advantage of an editing capability directly within the portal. Then we're gonna be reviewing and configuring input and output bindings for an azure function will then be implementing a local development workflow for azure functions. Because if you're a developer, chances are you don't want to do all of your development in the browser, you have tools like V s code that you're used to using. We're gonna talk about how we then shift that workflow over to be a local development workflow. We're gonna look at how we trigger an azure function from a file upload on blob storage, and then will be configuring the output of an azure function to be stored in blob storage.

Azure Functions Cost Model
[Autogenerated] so before we actually go in and create any resource is within our azure subscription, Let's dive in and look at how we actually pay for azure functions. So I'm here in the azure pricing calculator that we utilized earlier. Within this course, I'm now going to scroll down, and this time I'm going to select Azure functions and then I'll select view from within here. I'm gonna initially update the region to be east us. And I want to keep the tear at consumption, although you could certainly select premium here if you wanted to. If you're interested in the dedicated tear, you can simply go to your APP service plan here within the calculator and determine the pricing from there. But here, we're going to be looking at least serverless model of pricing. Now, with this model, we're actually paying for two different things. One is we're paying for the number of requests, So if we have a 1,000,000 executions per month, we would have a 1,000,000 requests and there is a fee per request. Then we have another fee, which is the amount of gigabytes per second of execution. We take up with our functions. So if We had one function and it took a full second to execute and we set aside one gigabyte of memory. That would be one gigabytes second, and you could have a function take up mawr or less than the gigabytes. Second. And there's a nice calculator here to help us determine how much we would be leveraging. So let's say we want to have 512 megabytes available to us. Let's say we have a function that takes a full second and let's say we have one million executions of that function now. If we were to do this, we would be paying a grand total of $1.60 for our monthly cost. Now, part of the reason that that's all we're paying is we get the 1st 400,000 gigabytes seconds and our 1st 1 million executions completely for free. But if we were to do now two million executions, we would see that this goes up to $9.80. Now that's still a minuscule amount compared to what we're getting with this service. So now that we have an understanding that we pay with our azure functions based on the number of executions and based on the number of gigabytes seconds we consume, let's dive in and create our first as your function app.

Creating an Azure Function App
[Autogenerated] So our first step in creating our first azure function is to create an azure function app. So here, within this particular clip, we're gonna be walking through the process of first creating a new azure function app from the portal. Although you certainly could do it from the command line as well and will be reviewing the options for the hosting of our azure functions. So let's dive in. So I'm here in the portal and I'm going to go search for function app. I'll select it, and from here I need to add a new function app. So I'll hit ad, so you'll begin to notice some similarities between a function app, an APP service happen. In many ways, a function app is just a different type of app, service app. But I'm going to go in here and I'm actually going to create a new resource group. We'll call this PS as your functions and we need to give our function app a name and we're gonna call this PS azure functions app. Now, I need to first select my runtime stack because in this case, I am going to use code and not a docker container will select node and we can see here that version 12 is pre selected. And that's what we want to use. Then next we need to select the region. And so I'm going to set this to be East us. And then I'm gonna click on next to go look at the hosting options so you'll see here that it is creating a new storage account for us. Actually uses this for some of its own bookkeeping and to keep our function up and running. Now, the next option we have here is for the operating system. Now it's important to note it is going to pre select the operating system for you, based on what it thinks will work best for the runtime that you have selected. So, for example, if I were to go in right now and select Lennox, there are some capabilities of the Functions service that wouldn't be available to me as opposed to just keeping it on windows. So I'm gonna keep it on windows for now. Now you can see next that here are the different plans that we discussed. We have the consumption option, which is the server lis option. We have premium and we have the APP service plan. We're gonna choose to keep this at the consumption option because we do want to take advantage of those serverless concepts that we mentioned previously. Now, once we have these things selected, weaken, select, review and create, and then once it passes, validation will create our function app. And now that our deployment has completed, I'm gonna click on the option to go to the resource. Now, depending on when you actually view this particular module, you might or might not see this option. Microsoft is in the process of implementing a new azure functions management experience that looks a lot more like the rest of the portal. I'm going to be using that. So if you see this option, you can get to the same view that I will have by simply clicking to preview the new function management experience and from within here we can see an overview blade that's very much like the other resource is that we've worked with. So next we're going to utilize the portal here to create our first azure function

Creating an Azure Function in the Portal
[Autogenerated] So now that we have an azure function app up and running, we're gonna be creating our first azure function in the portal. So first will be creating that new function in the portal, and then we'll even be able to have an edit experience with our code in the portal. And then we'll be able to test and validate that our functions are working as planned. So let's dive it. So I'm here in the portal, and I concurrently see the overview blade for my azure function app. Now, what I want to do here is I want to go under functions here on the left. I can see now that I don't have any functions. And again, an azure function app can have multiple functions associated with it. I'm gonna go ahead and hit the ad option. Now, from here, I get some templates that I can select from, and this begins to address the concept of triggers and bindings which will talk more about within the next clip. Now, one of the options we have is to use an http trigger and that's what we want because we're just gonna use a normal Web call to be able to trigger our function to execute. But let's quickly look at some of the other options that are included. We do have triggers that can fire based on a timer. We can have something come through. If we put something in azure que storage as well as the service bus, we can look at event hub Io t hub. There's a lot of different options. So these are things that are pre defined that we can simply integrate with. And so, for this template, we're gonna choose the http trigger option. Next, we need to give our function of name, and we're just gonna call this test function now. The next thing we need to do is we need to set the authorization level. Now, there's three different options that we can consider here. So first we could choose anonymous. And if we chose anonymous, anyone could execute our function if they knew the girl. Next, we have function level authorization. So with the function authorization level, we would have a key that is specific to this function that would be needed before you can execute it. And then with admin, we have some admin keys that are associated with our functions app that if we said it is that you have to use one of those keys to execute the function. So I'm just gonna keep it at function because we don't want this to be anonymous, and then I'll hit, create function and now it has automatically taken us into our function here within the portal. So first of all, here within the overview blade, we can get the function app that it's associated with, and we can quickly link over toe application insights for our functions app. But from here, I'm gonna go over to code and test, and you can see here that it's got some Java script coat very similar to some of the JavaScript code. We already have written within this particular course. And what it's doing here is it's going to be looking for us to pass in a name either in the body or as a query parameter, and that it's going to return something that says hello and then the name that we passed in so a very basic hello world function, but I want to point out a few things here. First of all, I want to point out the context argument that's passed into this function now, irrespective of how our functions are triggered, they are always going to have a context object. And this is what gives us access to our bindings, which will talk more about within the next clip. It also gives us access to things like the ability to log we can see here we're using context dot log, for example. Now we have something else included here, and that is the request or R E Q variable that's actually passed in. And this is what's going to give us access to that http request that actually triggered our function. Execution. Now what comes after that context variable is going to be dependent on what has actually triggered our function. And so we'll see in the next clip how we can configure different types of triggers and how that affects our functions. Now from here, one of the things we can do is we can go to get function u R l and you can see here that it's just using a default key, and it's actually gonna pre populate a u R L that allows us to execute the function. And so it takes care of putting that authorization string after the initial Earl. And so I'm going to open up a new tab and I'm just gonna paste this in. And when I do, I'm going to see that it's gonna return an error. In this case, it says, Hey, you need to pass a name on the query string or in the request body. So what we can do is we can go up here and simply add in an ampersand and will say name equals and will say, David. And now it's gonna say, Hello, David. So now we've been able to execute our function using the URL. Now one of the things I want to point out as we also have access to some log information directly from here within the portal. So now, if I were to go in and I would actually run this again, we'd actually go back here within the portal, and we can see the information that's coming through from application insights about our function that was just executed. But for now, I'm gonna go ahead and close this now the last thing I want to point out before we finish up this particular clip is this function Keys option here on the left. So here, within the function keys option. This is where we can actually define new keys. Revoked previous keys, renew keys that will enable people to be able to access our functions. And here is the default key. And this is what was actually injected into the U. R l that we utilize previously to test our function. Now, next, we're gonna be looking at how we can configure our triggers and bindings for our azure functions.

Triggers and Bindings
[Autogenerated] So next I want to talk through two concepts that we have introduced, but we haven't really dove into fully, and that's the concepts of triggers and bindings. So let's take a look at a sample azure function now, the one we created earlier we called test function. And we know that there was some code written that responded when we made an http request to a specific girl. So in this case, we would have our function code, and then we would have a trigger. And each trigger has a type. And in our case, this one was an http request. And so we knew that these two were connected. So causing our trigger to fire would then result in our function firing so at a high level, this is how azure functions work. So the first concept we need to talk about is a trigger and a trigger is what causes a function to execute. Now, functions need to have exactly one trigger, and triggers can come from defined actions like, for example, if you're making an http request or they can be integrations from other azure services like blob storage. Now, triggers can provide input data into the function. Now. We saw that with our example because we were able to get the http request within our azure function. And we can have triggers from things like an http request or from blob storage, but also from things like Cosmos TB, for example, inserting a new document into a container or event grid or Q storage or even i o T. Hub. So there are many, many integrations that we can perform to build these event driven server LIS applications. Now let's expand beyond the concepts of triggers and start looking at bindings. So let me just give you an example. Let's say here that we have a specific A p I and let's say we want to update our photo info. Maybe we want to create a thumbnail and then add some new tags that users have entered in about a specific photo. And let's say that this is roughly the U. R L structure. So we're gonna be passing in the i d of the photo when the user is making this a P I request. Well, when they're making this, http requests for the a p I we're gonna need to do several things first, we're gonna need to go get the picture. So if we're gonna be able to create a thumb now, we need to get the data out of blob storage for the photo that they uploaded next. If we're storing data about a photo like maybe the user has entered in specific tags for the photo, we're probably going to store that in something like Cosmos DB. So we're going to go retrieve the document from Cosmos TB, And then if we're actually passing in new information, we're gonna need toe update both the picture, and we're gonna need to update the document data. So maybe with the picture, we're storing a thumbnail in a separate area of blob storage and then with the data, were updating those tags and then saving that back into cosmos TB. Now, you might say, Well, this is perfectly logical. I know I can use the Azure sdk to do all of these things so I could build all of that into my function. But before we look too deeply into that, let's talk about the concept of bindings and we'll talk about how it changes how we would solve a problem like this so bindings provided to clear it of way so you can connect. Other resource is from azure to your azure function. Now you can configure them to be either input or output bindings or in some cases, both. But you don't have to have these these air an optional feature of azure functions. So let's look at how this would play out. First of all, we would have our function code and we could give it a name update, photo info. This would still be an A P. I call. So we're gonna have an http request trigger, and we know that these things were going to be connected. But now, instead of us having to write all of the code to go and fetch data out of blob storage and to go fetch data from Cosmos TB, we could configure what are called input bindings. And so in this case, the way we could do it is we could take that I d. That gets pulled from the specific AP I call. And if we're using that as the file name and blob storage, we could simply wire that together in a way that I'll show you later and say, Okay, I want you to go get this file. And then we could also say, if we're using this as an I d within a collection in cosmos TB, we could say we want you to also retrieve this so that before we even get our function to be called, that data is already included as parameters into the function. You might say, Wow, that saves a ton of steps that saves a lot of boilerplate code and you'd be right. But then, if you're thinking that you still need to go in and write the logic to save that data back out, it really hasn't saved you a ton of time. However, we can now integrate what we call output bindings. And so, with output bindings, what it enables us to do is to say, if we put our data in a specific place that's provided to us within the context object, it will a synchronously. After we complete our function, execution, save that data to the correct place, whether it be blob storage or cosmos TB. So if we play this out first, we would have our http request triggered based on the user hitting that specific U R L Then we would have our input bindings, gather the needed data and put it together so that it could be sent into our function. Our function code would then execute. And once our function is done, executing azure would then take the data that we have placed in specific places for our output bindings and then save the data out for both our blob storage and cosmos db output binding. So next we're gonna move forward with implementing not this specific solution. But we're going to move forward with doing local development for azure functions and then build out a simplified version of this flow.

Preparing for Local Development
[Autogenerated] Now, up to this point, you've done all of your development for azure functions within the portal. But your developer and you have tools that you like to use, and chances are you're not going to want to do all of your development from your browser. So here, within this particular clip, I'm going to walk you through the things you need to install so you can do a local development for your azure functions. And so, first of all, we'll be looking at how we installed the azure functions core tools, and this is essential. This gives you a CLI way to interact with the Azure Function Service, and then we'll be installing the Azure Functions V s code extension. So let's dive in. So first I'm here at the page on Get Hub for azure functions, core tools. And this is a set of CLI tools, four azure functions. And if you scroll down here on the read me foul, you'll get specific instructions on how to install it for your platform. So on Windows, you can install it within PM on Mac. It's best to install it with home brew, and they're even our instructions here for Lennox. Now, in this case, be sure that you're installing the latest version, which for me at the time of recording this course is version three. Now, I've already gone through the process here of installing it on my Mac with Home brew. So next I'm gonna navigate over to the terminal and from within here. If it's installed, I should be able to type and funk F u N C. And I'll now see the output from the cli tool. This gives me the ability to do things like funk in it, which would be able to create a new function app. And then, if I have a function app, I could type in funk new to create a new function from a template. There's a lot of things that are included here and up to recently, this was really your only way of interacting locally with the Function service. But now there is an extension for V s code that wraps around these tools. That gives you a much better overall experience for creating your function. APS. So what I'm gonna do from here is I'm first gonna create a new directory, and then I'm going to launch of the S code. So here I have V s code. Open it to the new directory that I just created. I'm now gonna navigate over to the extensions pain, and from here, I'm going to search for azure functions. So here you can see the azure functions extension for V s code. Now it relies on the azure functions core tools, and it will actually install it for you if you haven't installed it. But either way, you'll have access to all of the capabilities of that Seelye tool. But here, within V s code. So here, in the next clip, we're going to use this extension to create a new azure functions app, and we will be configuring both triggers and output bindings.

Local Development for Azure Functions
[Autogenerated] So now we're going to actually implement a local development workflow for a new azure function app. So let's talk about what we're going to build now. If you remember, we are utilizing blob storage within our Web application, and what we want to do here is we want to have a trigger that works with that Web application. So every time we upload a new image, it will create this trigger that will then call a specific azure function that will create a thumbnail for image. So what we want to implement here within this particular clip, this will have a trigger that triggers from blob storage and then executes our function code. We won't be able to save anything yet. We'll be implementing that within the next clip. Let's talk about what will be doing. First of all, we're gonna be creating a new function app within V s code, and we're also gonna be configuring a function with a blob storage trigger. And then we'll be configuring this function for local testing and we'll actually verify that it works locally before we publish it to the cloud. So let's dive in. So first I'm here in the portal and I want to review our storage account. And just as a quick refresher here within my storage account PS Web app. I have a container called user images, and this is where we actually upload images from our web application. Now, as of right now, we have one image that has been uploaded into this container. So now let's navigate over to V s code. So I'm here with NVs code in my empty directory. Now, the first thing I'm going to do is I'm going to navigate here under the azure tab and you can see I have functions open. Now, if I open this up, I'm going to notice the function app that we previously created. And within this particular function app, we have one function that is our test function. And this test function utilizes an http trigger. Now what we're going to do is we're going to start by creating a new app and running at locally before we publish it into the cloud. So I'm going to click the option here to create a new project, and we're going to keep it here within this particular directory. Now we need to enter in our language, which for us will be Java script. And then we get to choose between the different templates that are provided. Now, for now, we're going to choose the azure blob storage trigger. But then we'll be updating this configuration. We're just going to call this our blob trigger, and then we need to tell it where it's going to get its setting for how it will connect to our storage account. Now we're gonna have it create a new setting. And so from here, we're going to choose our storage account, which is P s Web app, and then we need to tell it the name. Now, if you remember, this is why I went and looked at our storage a kind ahead of time. We have a user images, and then within here, we're going to have a name value. That name value is going to be the you you I d. That we generate when we upload the image from our Web application. Now, in this case, it's only going to look within this particular container, which is what we want, because we're actually going to save our thumbnail out into a different container. Now, here it's letting us know that it has created this project. And so, from within here we can see that it has generated an initial function for us. You know, a few things I want to point out. First of all is it is passing in both our context value, but also we have a blob value that's actually coming into our function, and this will be the bites of the file that's actually stored within our blob storage account. Now to see why that's there, let's navigate here over to the files Explorer. So first we're looking at our index dot Js file. But let's also look here at our function dot Jason file and here within this file, this is where we're configuring are bindings, and in this case, we have our trigger. We can see here that the direction is in and this is named my blob. Now what it's going to be doing here is it's going to be looking for that path value within our user images container. Now it's also using a connection here, which is P s web app. Underscore storage. Now, if I go and look at my local settings dot Jason file here, I can see that PS Web app underscore storage and I can see the connection string here for my storage account. This is how it's actually getting access to that particular account. And the extension here within V s code just automated the process of setting that value for us. So with those things in place, we're now able to run our function. But before we do that, let's go ahead and add some specific logic into our function. Now, the first thing I'm going to add in is I'm going to add in a try catch block. Now, the next thing I'm going to do is I'm going to add in some logic to begin to create a thumbnail. Now, as mentioned, we're not going to implement how we're going to actually save this thumbnail were just want tohave the initial trigger fire here within this particular clip. But to do this free sizing of the image, I'm going to be utilizing a library called Jim. So I'm gonna open up the terminal and one of the first things were going to do is we're going to install this library that we're going to need gym and then we'll save that. And now I can actually close the terminal. And so the first thing we're going to need to do is we're going to need to actually bring in Jim now, the next thing we're going to need to do is we're gonna need to read in this blob value that's being passed in, and we're going to need to connect it to Jim. So here I'm actually bringing in the bites were reading it from Jim, and then we're choosing to actually go in and resize the image. Now, the logic here is currently saying that we're going to resize this toe, have a with of 300 pixels and then automatically calculate the height of the image based on the original aspect ratio. Now, the next thing we're doing is we're getting an output buffer. We could choose to save this file locally, but remember, we're dealing with some compute resources here that could likely be thrown away in the near future once this function finishes executing. So we need to store this somewhere more permanent. So we're going to save this back into blob storage. But we're not gonna be implementing that within this clip now, the next thing we need to do is we need to log if we have any errors that happen in this process. So the next step is we want to be able to test our azure function locally. So to do that, we can take advantage of both the extension that has been provided within V s code as well as the azure functions, core tools that we have installed on this machine to run this locally and even the trigger will be integrated in with our local version of our function. So I'm going to go here to the run options and there is one already configured which is attached to note functions. And that's what we want. So I'll go ahead and hit the run button, and the next thing that's going to do is it wants me to select a storage account. So I'm gonna select a storage account. I'm just going to select one of the ones that I already have configured here, and then it's going to start the process of setting the environment up so that I contest this function locally, so I'm gonna add a break point in at the first line here within the try block. And now I'm gonna open up our web application. Now, I still have my web application running locally, so I'm going to open up local host, poor 3000 and the images route. I'm gonna choose a file and then hit open, and then I'll hit upload. So now I'm gonna navigate back over to V s code, and now I can see that it has triggered the break point. So first of all, we can see from the log statement that we have processed the Blob. It gives us the information about the blob file name and the number of bytes that we're dealing with. Now, from here, I'm gonna go ahead and just push through this particular break point, and we should see this complete. And there weren't any heirs. So we are successfully able to get our output buffer. And within the next clip will be expanding upon what we've done here to integrate in an output binding to take care of storing this back into azure blob storage

Utilizing a Blob Storage Output Binding
[Autogenerated] So now we're gonna pick up where we left off and integrate our output binding into our azure function. So just as a reminder, here's what we have Currently we have a trigger that is a blob storage trigger that gets fired when we upload an image from our Web application. And then our function code is able to create a thumbnail from the image that we load in. What we need to figure out a way to do is to add in an output binding so we can save our data back to blob storage. So end to end. Here's how this will work. We will upload something from our Web application which will cause are triggered a fire which will then cause our function code to execute within our function code. We will generate that thumbnail, and then we'll put it in the right place within our output binding so that our output binding that is able to fire and save our data into blob storage once our function code has completed execution. So here within this demo were first gonna be adding an output binding to our azure function to make this work. And we're also going to be publishing our local development project to the cloud because right now it just exists on our local machine. So let's dive in. So I want to start off here within my storage account. Now, I have one container user images that has a couple of images in it that we've uploaded already. Now what we're gonna be doing when we configure our output binding is we're gonna have our output binding save into a container called thumbnails. Now, if our output binding works correctly, that container should be created automatically for us. So now let's navigate back over to V s code. So here is my function code. And just as a reminder, the last step that we took within our function code was to create an output buffer. Now, this output buffer is the raw bites of our resized image, specifically in JPEG format. And this is what we need to figure out a way to save into blob storage. Now, our function dot Jason files where we define our triggers as well as our bindings, both input and output. Now, I could add our output binding in here directly, but there also is an easier way with the extension that we've installed for azure functions. So I'm going to navigate to the azure extension. I'm not going to go to my function and I'm going to right click and say add binding. I'll set this as an out binding and then I'm going to select Azure blob storage because we want to save our data out to azure blob storage. And I'm going to keep the name here. Output blob will need to remember this because we'll need to reference this in our code. Then we need to specify where we want it to be saved. Now, as I mentioned, we're gonna have a container called thumbnails and we're gonna use the same name parameter that we used when we were looking at our trigger. This way will be able to use the same you, you i d, that we're defining for the image as the name of the thumbnail. Now I need to specify where it's going to find the connection string for our storage account. We've already created this and this is the PS Web app underscore storage. And now we can see with just that we've added in the needed code for our output binding here within our function dot Jason file. Now I'm gonna navigate back over to my function code, so we just need to add in a single line of code to make this work now. I mentioned earlier that all of our bindings air accessible under the context object. Now, if you remember, we named our output binding here output blob. And we simply need to assign our output buffer to this value. Now, in doing this, it will actually save this out to blob storage once our function runs. So I'm gonna go ahead and run my function here locally. So now I'm gonna navigate over to my browser, So I have my web application pulled up. It is also still running locally, so I'm gonna select choose file. We'll select the image we've been using, and then we'll upload it. And here we can see the image Justus we've done previously. So now I'm gonna navigate back over to V s code. So here we can see within the log output NVs code. We can see the image has been processed. We have a new image here that starts with the characters. 79 b two. So now I should be able to navigate over and see the created thumbnail within our portal. So, first I'm here at the list of containers within the portal. I should be able to refresh this and we can see that are thumbnails. Container has been created and we can see that we do have a new file that has been uploaded 79 B two. So we're gonna go ahead and click into this, will then download it, and then we want to launch this file. And so here we can see that our thumbnail has been created successfully and save within the correct container within our storage account. So now I'm going to navigate back over to V s code. Now, since we've created are into in function and since we have verified that it's working appropriately, the next thing that we need to do is we need to actually create this as a function app in the cloud and not just on our local machine. So I'm going to stop execution and detach my function here, and then I'm going to navigate back over to the azure extension. So from here within my azure extension, I have the option to actually deploy my local project as an azure function and so I'm going to select that option. We'll select the option here to create a new function app and will give it the name PS Web function app. We'll select the runtime. We say that it's going to be in the East us and now it is creating our function app in the cloud and deploying our function code into it. And so now we have completed the process of deploying our function app into the cloud. So now we have completed our goals For this clip, we've been ableto add our output binding, verify that it's working correctly and we have now published our function app into the cloud.

Summary
[Autogenerated] So we've covered quite a bit here from beginning to end within this module. So let's quickly review what we have been able to accomplish. First of all, we created our first azure function app, and we did that initially within the portal. And that is a part of that. We created and configured our first azure function directly within the portal. Now, the first function that we created utilized an http trigger and we talked about different authorization levels for how we could access that. And then we were able to actually test our function from the portal we reviewed and configured input and output bindings for an azure function. We talked about the purpose of triggers as well as input and output bindings, and that their configured within your function dot Jason file. And then we implemented our local development workflow four azure functions. We installed both the azure functions, core tools, CLI as well as the extension with NVs code. And ultimately we were able to trigger an azure function from a file upload on blob storage. So we worked with the previous Web application that we had created, so that every time we upload a new image, we were able to seamlessly create a thumbnail and save that back into azure blob storage

Continuous Integration
Introduction
[Autogenerated] so now that we're able to deploy our solutions into the cloud next, we need to look at the concept of continuous integration. Now, according to Thought works, continuous integration is a development practice that requires developers to integrate code into a shared repository several times a day. Each check in is then verified by an automated build, allowing teams to detect problems early. And this has been a best practice for some time, so that when developers commit code, you can check to make sure that it builds and passes tests. But then, if we choose to take this a step further, we have seen the emergence of another paradigm, and that is continuous delivery. And this is a development process where an application is able to be deployed continually into a production like environment through an automated process that includes building testing as well as deployment. Now let's see an example of this to really understand what this looks like when we put it into practice. So within a continuous delivery workflow, you taken initial trigger, which is a get commit or in whatever source control repository you're using, and then from that commit, you're able to actually trigger the process to initially build your application and then to run a set of tests, unit tests and integration tests that enable you to know if your application is working as it should be working. And then from there, actually deployed the solution. And so you can end up with situations where an application might be deployed hundreds of times per day. That because of the integrated test process, you can make sure that you're not disturbing the intended function of the application with the commit. So here, within this module, we're gonna be walking through the integration of a continuous delivery pipeline for your application. So first, we're gonna be reviewing the capability of a service called Azure Dev ABS. And then we're gonna be creating an azure Dev ops organization. And Project from there will be implementing an azure develops pipeline will then be integrating JavaScript unit tests into a pipeline. And ultimately we will be automating the deployment of an APP service app and an azure function app from Azure Dev ops

Azure DevOps Overview
[Autogenerated] So in the previous clip, we introduced the service Azure Dev Ops and now we're going to dive in a little bit more and understand what is provided with this service. So overall, Azure Dev ops is really not just a single service. It is a suite of developer services, and it enables organizations to plan, build, test and deploy their solution. And that's true whether we're looking at on premise or the cloud. Previously, this suite of services was known as visual studio team services or V S. T s. But within this suite of services, we're really looking at several different services. First, we have azure boards and then we have azure repose. We have azure pipelines as well as test plans, and we have azure artifacts. So let's look at each of those in turn. First, we have azure boards and within azure boards. This gives you the ability to plan and track development work for teams, and it enables you to track multiple types of work. And this could be high level work like epics and features or lower level like user stories and tasks. And it supports agile methodologies you can support both scram and Kon Bon with the capabilities that are provided for you within azure boards. Now, we're not gonna be using azure boards here within our demo, but I want you to understand the capabilities that are provided with this service. And then we have azure test plans, and this is another one we're not going to be using, but at a high level. It gives you the ability to manage manual application test plans, and it enables you to share your test plans and test suites in your test cases, even with stakeholders within your organization. And it supports the ability to do plan testing, user acceptance, testing and even some exploratory tests. Next, we have azure repose, and we will be integrating in with azure repose here within our demo. And what it does is it provides source control for your development projects, and it supports that by including support for both. Get as well as team foundation version control or TF V. C. Then from here, we have azure artifacts. So as your artifacts integrates package management into your continuous delivery pipeline and so within this, if your organization uses package management solutions like Maven or in PM, which again. If you're in no Js developer, chances are you're leveraging in PM or Newgate or Python with Pip. You can integrate in your private packages into a private package repository so that you can integrate that in with other projects that are leveraging those packages. But what we're really gonna be focused on within this particular module is utilizing azure pipelines and as your pipelines enables you to have a continuous delivery pipeline for most any development platform and out of the box is supports utilizing at build process on Windows Lennox or even Mac OS, which is really helpful if you have people within your organization building Mac or IOS application, and from a source control perspective, it integrates with azure repos, as you would expect, but also with solutions like Get hub and bit bucket. So we're gonna be working to implement a continuous delivery pipeline for both our function and our APP service app here within this module

Getting Started with Azure DevOps
[Autogenerated] So next we're going to start working within Azure Dev Ops toe work towards our goal of creating a continuous delivery pipeline for our application. But first I need to talk about a few different data types within Azure Dev ops. So, first of all, at the highest level, we have an organization. You'll need to create at least one organization as the container for your projects. And this is the construct for organizing groups of related projects within the overall tool suite. Now within organizations, we have projects as mentioned and this is the container for your work that you're doing within Azure Dev ops, and this is what gives you access to the different services within the tool suite. Then, in addition to an organization and a project, you have the concept of teams and this is the container. For groups of users. However, this is primarily leveraged within. Azure boards were not going to be using azure boards, so we're not going to be setting up any teams. For the purpose of this demo will just need to create a single organization and then a single project within that organization. Now the documentation contains a lot of information on how you can use organizations and projects to help best organize the work that you're doing. But for now, let's take a look at what we're going to be covering within this demo. So first we're gonna be creating a new Azure Dev ops organization and then within that will be creating our first as your Dev Ops project. So let's dive in. So I'm here within the portal and the first thing I want to do is I'm gonna actually search for Azure Dev ops from within the search bar. And I'm going to go toe Azure Dev ops organizations. It's gonna give us a splash screen and we're gonna have an option here to go look at our Azure de Bob's Organizations. Now, Azure Dev Ops exists under dev dot azure dot com. It's letting me know here that I have an account, but I haven't yet created an organization. And so I'm gonna go ahead and select the option here to create a new organization within Azure Dev ops. And so we're going to need to give it a name and we're gonna call ours PS web, app, organ and in this case will choose to host our projects in central us. Now it's important to note here, as your Dev Ops is not available in every region. And so this will be where we choose to host our projects from will hit continue. So now that we've created our Azure Dev ops organization, we're now able to get our first glimpse of the tool. Now here along the left pain, we'll see our organizations listed. Now For the purposes of this demo, we only need a single organization. And to be honest, most companies air only going to need a single organization as well. However, you could choose to use different strategies for your organization. For example, if you're a part of a large enterprise organization, you might choose to divide your groups of projects into separate organizations. But for us, we're gonna be interested in just creating our first project underneath this organization. I'm going to choose to leave the visibility of this new project as private. Now, if you have any projects that you want to have the open source and be available to anyone on the Internet, for them to be able to go in and view the project, you can choose to select public. We're gonna call this PS function app, and then we'll hit, create project. So now that we have our organization and project created in the next clip, we're gonna be walking through the process of connecting our function app in with azure repose and then creating an initial azure pipeline.

Creating a Pipeline
[Autogenerated] So now we're gonna walk through the process of creating our first pipeline within Azure Dev ops. So first, we're gonna be integrating our azure function app in tow, Azure repose, and then will be creating that initial azure pipeline for that function app. So let's dive in. So I'm here in the portal and the first thing I'm going to do as I'm going to navigate over to my function APS from here, I'm actually going to create a new version of my function app. Some gonna hit ad, Then I'm gonna create a new resource group. Then we need to give it a name. The next thing we need to do is set our runtime stack. We'll set the version and believe it is 12 and then we need to set our region. Then we'll go to the hosting options. Now there's a great template that is provided by Azure Dev ops for integrating with Lennox function APS and so we're gonna choose to use that. So in this case, we're going to select Lennox as our operating system. Now, this will give us the benefit of being able to use that template. However, we won't get some of the abilities that we have with a Windows operating system for editing our function within the portal. So I'm going to navigate now to use a storage account that I already have. And then we want to be sure that we have the consumption plan typeset. So now we'll hit, review and create. So now that the deployment is underway for our new function, app lets navigate over into Azure Dev ops. So I'm here within the project that we created previously PS function app. And so now I'm gonna navigate under repose. Now, from repose, we get the ability to integrate code that we have already created and add this as a new remote. So we're gonna go ahead and copy this clone. You are l four, our repository. Now we have the ability here to choose https or weaken set up keys and use. Ssh! I'm gonna generate get credentials and here will utilize the https clone URL We also need to note are password here because this will be the only time we're able to actually see it. Next, I'm gonna jump over into V s code. So here, within V s code, I have the function that we created previously that creates the thumbnail and has the trigger from azure blob storage As a part of creating my function environment locally, it created a git repository for me. Now, what I need to do here is I need to add in our azure repo as a remote. So we'll set this as the origin and then we'll hit enter. Now that we've added this in, I'm gonna choose to go ahead and add in. All of my changes will call this our initial commit and then we'll push. Now it's going to need our password and we're gonna enter the password that we got from within Azure Dev ops. And now we should be able to navigate over to Azure Dev ops and see the code from our project. And so here I'm going to refresh this view and we now concede the files from within our PS function APP project. Now, the next thing we need to do is navigate over here in the left pane to pipelines and we're going to select the option to create a pipeline. We're gonna let it know that our code is in azure repose and then we'll select our function app. You can see that we have an option here for no Js function app to Lennox on Azure. So let's select that option. We're gonna need to select our subscription and then it's going to want us to sign in. From here, we can select our function app name and here we can see RPS Lennox function app that we created right before we launched into Azure Dev ops. Well, now hit, validate and configure. So within Azure Dev ops pipelines are defined in Gammel and here we can see initially that we're triggering this pipeline based off of a commit to our master branch. It's then, including some variables that define things like our azure subscription and the function app that we're gonna be deploying to now within any pipeline, we're gonna have multiple stages. This pipeline comes pre configured with two different stages, build and deploy. We can see here that are steps included within our build stage, our first getting no Js. Then we're gonna build any extensions if they're included. We're going to run in PM install as well as in P m run build and in PM run test, if they are present and will be integrating in a bit with in PM run test and will be publishing our test results from here. We're then going toe archive. Our files will upload them, then into a staging directory and then we have our deploy stage where it will walk through the process of deploying our azure function app. So from here, we should be able to just save and run. Now, once we do this, it's going to commit the Yamil file for our pipeline within our repository. Now it is cute up our pipeline for execution. We can see the two different stages build and deploy. And if I mouse here over the icon next to the build stage, it's letting us know that it is actually running this step in the pipeline. And now we can see that our pipeline has completed its execution for both our build and deploy stage. So simply by going through these steps, we've integrated our function app in with azure repose, and we've created a build pipeline that will now execute every time we commit to the Master branch and deploy our azure function app.

Including Test Results in a Pipeline
[Autogenerated] So now that we have our application building on commit, we now want to integrate a level of testing into our function app. So first, within this demo will be integrating unit tests into a function app. Now, this isn't a course on writing unit tests, so we're simply going to integrate a sample test. And then once we have that in place, we'll be publishing our test results to our azure pipeline. So let's dive in. So I'm here with NVS code and the first thing that I need to do as I need to install some packages within PM So I'm going to be installing Mocha as my testing framework chai for some assertions and Mocha J Unit reporter. And this is critical because this is going to be the reporter that will be ableto export data in J unit format so that our azure pipeline can pick up on it. So now I'll save those dependencies. Next, I'm going to create a test directory and create a sample test file. So within this test file, I'm going to include an assertion that assumes that one plus one should equal to, and we know obviously that that's correct. so this will work as a basic test for us. And in a minute, it will also allow us to see what happens when a test fails from here. I'm going to navigate over to our package about Jason file. So if you remember in our pipeline it is executing our test script now. Currently, we don't have any tests integrated, but we're going to change that. So from within here, we're first going to tell Mocha to run our sample test file. And we're going to tell it to use the reporter that we pulled in. Now that we have this in place, we should be able to test this locally and we can see that it does run our test suite and we have a test results dot xml file as the output perfect. So now that we have what we need, we need to figure out a way to integrate this into our pipeline. Now our pipeline is a yam Oh, file that is integrated into our repository. However, I'm gonna have to pull from my remote repository to get access to that. Before I do that, I want to be sure I can commit the current changes that I've made, so I'm gonna go to my source control. So first, I'm gonna add my test results file to get ignore, and then I'm going to choose to stage all of these changes and will commit them. Now we'll pull and then we'll push. So now if I navigate back over to my files, I can see my azure pipelines file. So from here, I'm going to navigate toe where we're actually running our tests so I can see here we have in PM, run test after in PM, Run, build. Now, in this case, we don't have anything we need to build, but we will be running our test suite. So I'm going to add one task here, and this is called published test results. Now it's going to look for the J unit output of our tests in that test results dot xml file. And now that I made this change, I can actually commit this along with the other changes that I've added and it should be able to run my tests and automatically pick up this published test results task. So we'll go ahead and commit this and now I'll push. And so Now I'm going to navigate over to my Azure Dev Ops project. And so now I can see that adding tests result to pipeline is currently executing. So I'm gonna go ahead and open up this execution of my pipeline, and now I can see that my run has completed for my pipeline. I also can see here that 100% of my tests have passed, which again we only have a single test. But this shows us how we can integrate our test results into our pipeline. Now, next, let's see what happens when we have a failing test. So I'm going to navigate back over to V s code. So next I'm gonna navigate back to my sample test file, and here I'm going to say that one plus one is going to equal zero, which we know will fail. So now I'm gonna go ahead and commit this change. We'll save that and then we'll push. Now I'll navigate back over toe as word of obs so I can see now that I'm having another run of my pipeline and now we'll let this finish executing and now we can see that we had a failing test. If we scroll down, we can see here that 0% of our tests passed. In addition, we skipped are deploy stage because our build stage failed. So now if we're able to write tests that can properly test our function app, we can make sure that we pass our entire test suite before we deploy our azure function app into production. So within the next clip, we're gonna walk through the process of creating a pipeline for our web application and in this case will be showing how you can integrate it in with a repository from get hub.

Integrating a Github Repository
[Autogenerated] So now that we've created a pipeline for our function app, we're gonna be doing the same thing for our Web application. However, instead of leveraging azure repose, we're gonna be leveraging Get hub! Because I know many people already have their entire development workflow within get hub and it integrates well with Azure Dev ops. First, we're gonna be linking a get hub account to an azure pipeline and then we're gonna be creating a pipeline four our APP service, Web app. So first, I'm here in get hub and I'm gonna be creating a new repository, and we're just gonna say PS Web app. And then I'm gonna set this to be a private repository because I want to show you that you can integrate with private repositories within get hub. Then we'll hit the option here to create repository. So in this case, I'm gonna be using the ssh option, some going to copy that and then I'm gonna navigate back over to the S code. So from here with NVs code, I'm gonna go ahead and add this as a remote. Now we've added that in now. I already have my ssh keys configured with get hub. So that's why I don't have to include any authentication information here. Next. I'm gonna go here under my source control tab. And what we're gonna do is we're going to push will select origin. And now I should be able to navigate back over to get up. And if I reload the tab, I should see that I've pulled my web application code in now. From here, let's navigate over toe Azure Dev Ops. Now, if you remember, we had created an organization PS, Web app orig within our organization in Azure Dev ops. We had a project PS function app. Well, now we're going to create a new project. Although if we wanted to, we could add another pipeline into the project that we already have. But here, for organizational purposes, all create a new project. We're gonna call this P s Web app and we'll choose to keep the visibility here as private and then I'll hit Create. Now that my project is created, I'm going to go to pipelines now if you remember with our last project, we started in repose but here were choosing to use get hub instead of using azure repose. I'm gonna create a new pipeline. And here I'm gonna let it know now that instead of having our code and azure repose, it isn't Get hub. So I'm going to select that option now what it's gonna ask here if you haven't connected your get hub account into Azure Dev ops, you're going to have to authorize your account first. Once that's in place, you'll be able to see a listing of your repositories here. I'm going to select the repository that we just created. And then what I need to do here within get hub is it's gonna have me specify the exact repositories that I wanted to have permissions to. So if I scroll down here to the bottom, I can see here that I only wanted to have access to specific repositories. I'm gonna go ahead and approve the options that are selected here and now it's gonna navigate me back over to Azure Dev ops. It's gonna want me to pick my account, and now we're able to configure our pipeline. Now we're going to select the option here for no Js Express Web app to Lennox on Azure. When we do, it's gonna ask us to select our subscription and then we continue. Now here it's going to need us to sign in again. Once we sign it again, will be able to select our Web application name and here will select PS node Web app. Then we'll hit, validate and configure. Now, from here, it's gonna have us review the pipeline Yamil, that it generated for us. We can see here that we are triggering on our master branch. We have our variables configured and this looks very similar to what we included with our function app. But if I scroll all the way to the bottom, we can see here that within our deploy step, we're deploying an azure web app. We can see that we have our subscription app type app, name runtime stack the package and the startup command included here, so we can customize any of these options for how we're deploying it into our app service app. Now, all of this looks good. I'm not gonna make any changes. So I'm going to select, save and run. Now it's going to commit this Gamel file into a repository will keep the default values here and then we'll hit save and run. Now our pipeline is now created and it's starting its first run. So now I'm going to navigate back over to V s code. Now back here in V s code. The first thing I need to do is I need to pull because here we have had our Yamma pipeline file added into the repository. Once I do, I should be able to navigate back over and see my azure pop lines dot Yamil file. Now, we're not going to make any changes to this pipeline file. However, I showed you in the previous clip how you can integrate your unit testing into your pipeline and you could choose to do the exact same thing here. What I am going to do, though, is I'm going to add one simple change. I'm going to go here underneath the message that we added previously, and we're going to add in just another bit of text. We're just going to say here that this is deployed via azure pipelines. Now I'm going to commit this locally first, and then in a minute we're going to actually push this change and verify that we're able to deploy this using our pipeline. But first, let's go check on our pipeline build process. So here, within our azure pipeline, we can see that the deploy process has completed for our web application. Now, if I go over here to the tab that I've preloaded in our Web application, I should be able to reload this and we can see that we do have our This is production message. So this is the current version of our Web application. However, to test out our pipeline, let's deploy our new change. That includes the other message that we've added and below this. So now I'm going to navigate back over into V s code. So from here, I'm not gonna take the changes that I've made, and I'll push those. And now I'll navigate back over to my browser. So here I'm going to go under my pipeline and we can see that we do indeed have a new execution of our pipeline. Now let's talk about why that's significant. So in addition to being able to work with azure repose and being able to trigger pipeline runs based on those repose, we can get the exact same integration from within. Get hub now by doing this, we don't have to completely change around our development workflow. If within your organization, you're already working within get hub. So we'll go ahead and look at this pipeline execution. And now we can see that our pipeline run has completed. I should now be able to go here to my APP service application Reload. And here we can see our message saying that this was deployed via azure pipelines. So here we've been able to utilize a repository from within Get hub. We've been able to connect that and authorize that to our Azure Dev ops account. We've been able to create a pipeline for our APP service web application and we verify that we can deploy our changes simply by committing to our master branch on get up.

Summary
[Autogenerated] so we've covered quite a bit around the concepts of continuous integration and continuous delivery here within this module. Just as a reminder, we talked about a workflow here where we start with a get commit or commit to any source control system, and from there were able to build and then test and then ultimately deploy our applications without any manual steps in the process. So through this, we've been able to look first at the different capabilities of Azure Dev ops. So we talked about the ways that within Azure Dev ops, you can create organizations and projects and teams and we looked at the five different services that were included. And from there we specifically integrated with the repose. And with the pipeline service, we created our Azure Dev Ops organization and an initial project. From there we were able to implement a pipeline. We saw how azure has templates that we can leverage to create pipelines within a yam Oh, file format. We also manually edited that Yamil template for our function app so that we could add in unit test reporting. And through that we were able to integrate our JavaScript unit tests into that pipeline and we were able to see the percentage of tests that we had passing based off of the J unit reporter that we added into our application. We also automated the deployment of both an APP service APP and an azure function app showing that we can, with the cloud based application, have continuous delivery directly from our source control repository.
