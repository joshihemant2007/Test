At the heart of application development is a desire to rapidly create scalable and high-quality applications. In this course, Using the Serverless Framework with Node.js on AWS, you'll learn how to leverage the Serverless Framework to develop those kinds of applications. First, you'll explore the basic application model of serverless applications within the framework. Next, you'll uncover how to development and deploy fullstack serverless applications and microservices on AWS using the framework. Finally, you'll delve into how serverless applications can be used to create resilient fault-tolerant architectures. When youâ€™re finished with this course, you'll have a foundational knowledge of serverless application development using the framework, that will help you as you move forward to creating future serverless applications. Software required: Serverless Framework v1.x, Postman, an AWS Account, Node.js v6.10.3, and npm 3.10.10.

Course Overview
Course Overview
Hi everyone. My name is Fernando Medina Corey, and I'm a Pluralsight author. Welcome to my course on Using the Serverless Framework with Node. js on AWS. In this course, we're going to learn how to use the world's leading framework for serverless development, the serverless framework. Some of the major topics that we'll cover in this course include the serverless framework's application model, developing and deploying our own applications on AWS using the framework, how we can build highly scalable event-driven services, and how we could create resilient fault-tolerant application architectures using the framework. By the end of this course, you'll know all the fundamental concepts you'll need to work with the framework and have practiced each of them by creating your own serverless services, full stack applications, and CRUD APIs. Before beginning the course, you should be somewhat familiar with JavaScript, HTML, and the command-line, though all the commands, code, and installation instructions will be provided for you during the course. You may also want to take my Introduction to AWS Lambda course here on Pluralsight to get a more fundamental understanding of serverless applications. I hope you'll join me on this journey to learn about serverless application development with my course on using the serverless framework with Node. js on AWS here at Pluralsight.

The State of Serverless
Serverless Context - Why Do You Need Frameworks?
Hello, my name is Fernando Medina Corey, and I'm a data engineer. Welcome to my course on Using the Serverless Framework with Node. js on AWS. Let's take a look at what we'll be doing in module one, the State of Serverless. In this module, we'll start by looking at prerequisites for this course, so you know what to expect as you work on projects and listen to modules. After that, we'll look at some of the problems that serverless application developers commonly see, and how serverless frameworks can help with those. We'll touch on a few common serverless frameworks to give you an idea of the landscape before we really hone in on the benefits of the serverless framework itself. Then, we'll overview the concepts we'll be learning throughout this course, and how you can apply them to your work. And finally, we'll look at the schedule for the rest of the course, including the project we'll be building and what new skills we'll have by the end of this course. So let's get started. Though we will be going through demo projects at a granular level, it will be useful to have some background experience before taking this course. Specifically, you should already have an AWS account, and understand some AWS basics. You should also probably understand some basic JavaScript syntax, as we'll be working with Node. js heavily in this course. Because we'll be relying on AWS lambda heavily in this course, it will be useful for you to have a better understanding of it. If you haven't already, I'd suggest taking my Introduction to AWS Lambda course here on Pluralsight. And finally, some exposure to the command-line will really help you succeed when we're working on examples. I want to quickly clarify something that may already have some of you confused. There is a distinction between the serverless framework, which is the specific framework we'll be using in this course provided by serverless. com, and serverless frameworks more generally, which are a general term for any framework that focuses on the Functions as a Service or 'Serverless' paradigm. With that out of the way, let's take a really brief review of traditional versus serverless architectures. This course will cover very few of the basic concepts of serverless applications. For more on that, I'd suggest again my AWS developer course, An Introduction to AWS Lambda. But for now, here's some mandatory context on the need for serverless frameworks. In more traditional applications, you see a tendency to host entire applications, or at least particular application components, on managed servers. You might have business logic, a relational database, and even access keys all set up on the same physical or virtual server. In the serverless model, you might break up each component of your application into separate services, all handling particular tasks. One might manage notifications, another might deal with database updates, and another might manage file storage and retrieval. Now, each of these components would be isolated services, each running their own serverless functions. Now, this is a great benefit to interoperability in separation of concerns, but it does add some organizational challenges to application development. Now this is the interesting part, and where serverless frameworks start to show their utility in how they organize and standardize this development process. I'm going to assume that you're already on board with the benefits of serverless infrastructures, like only paying for what you use in terms of your server utilization, and decreased server configuration and management times. I will try and convince you that serverless frameworks can be a great way to help the architecture serverless applications. So what benefits do you get from using a serverless framework? The first is a common set of standards and organization for your code base. This is especially important in a micro-service-oriented world when you break out more and more pieces of code to operate separate, but cooperative services. With a standardized code base, it becomes easier to read, review, and maintain the code that you write, which is critical to the development process. The next benefit is tooling. Frameworks give you a powerful set of tools to interact with infrastructure providers, scaffold your code and services, and also add in functionality as you see fit. One of the other primary benefits of using frameworks is the community surrounding them. When building projects within a framework, you'll have the benefit of having a group of others to ask questions, share solutions, and springboard your development process. So this still leaves the question, why the serverless framework? What else is out there? Some companies develop and support their own frameworks, like Bustle. com's shep. You also have frameworks like Sparta, which is specifically for development in Go, or the AWS and Python-only framework, Zappa. There are certainly a variety of frameworks, and each of these might suit different use cases, so how do we settle on a single framework? One of the first things to consider when choosing a framework is whether or not it offers good core functionality, and does things like manage your code base, or manage the deployments to whatever infrastructure provider you're using. You'll also want to know the level of support for multiple languages that the framework covers, or at least know that it works well with your development language. Additionally, you want to make sure it integrates with the infrastructure as a service provider that you use. And finally, the framework itself should either offer significant functionality, or be able to improve and adapt itself through things like plug-ins. Next, let's take a look at why we might use the serverless framework specifically.

Why Use the Serverless Framework?
As I mentioned earlier, community is a huge part of choosing which framework to use, so let's take a look at some GitHub metrics that serve as proxies for the quality and size of the development communities around these frameworks. We'll be looking at watchers, stars, and forks. Each of these metrics should indicate relatively how many fellow developers are interested in the project as of July 2017. Now some serverless frameworks are supported by smaller groups of developers, or specific companies. Or even if they have larger communities, they may only support a single language, and can potentially not be the one you're interested in using. If GitHub can tell us anything, it's that the serverless framework easily has the largest development community out of any serverless framework. It's not a surprise, either, as the company behind the framework is dedicated to the growth and maintenance of the framework itself, rather than the framework being an offshoot of some other development practice or project. But don't take my word for it. In June 2017, the technology analyst firm Red Monk evaluated several serverless projects on GitHub over time, and found that the growth and stars for the serverless framework vastly outpaced all of the projects that it was evaluating within the category. This interest comes not only from smaller startup companies, like CrazyLister and Ibotta, but it also comes from highly established companies, like Coca-Cola, Expedia, and Nordstrom. Now that we understand how popular the serverless framework has become, let's take a quick look at some of the key features of the framework. One of the key tooling features that the serverless framework provides is a highly structured development process, which we'll look at more shortly. This process makes it easier to develop new services, and for developers to quickly gain familiarity with the project. As part of the structure development process, the framework also manages all the infrastructure required for deploying a project. Because it manages this process, it starts to significantly reduce the complexity of infrastructure providers. Rather than having to learn the nuances of multiple APIs, you can learn the serverless framework commands and that should automate the deployment process across multiple infrastructure providers. Further, the framework is somewhat infrastructure independent. The code structure and deployment process doesn't change very much at all when switching between infrastructure providers. While you may have to switch certain libraries, the structure remains the same. Speaking of infrastructure providers, in addition to using it with AWS, the serverless framework also has support from Microsoft Azure, IBM's Open Whisk, and Google Cloud Platform. Across these infrastructure platforms, the framework has support for a variety of languages, including Node. js, Java, C#, PHP, and Python. Now I don't want to convince you that this is a one-size-fits-all kind of solution. There are potential drawbacks when it comes to using serverless technologies, and frameworks in particular. Some of the most common criticisms of the framework is that it obscures the backend infrastructure behind each of your services. Now this could technically be both a benefit or a drawback, depending on whether or not the obscurity increases productivity, or leads to the potential misunderstandings of the underlying technologies. I'd say it's important to still have an understanding of the infrastructure underneath the framework's automation processes, and keep that in mind when building applications. The framework is also somewhat permission hungry. In development environments that are highly locked down, it can be difficult to use the framework to manage your development processes, because of the broad permissions required to create and destroy infrastructure. There are also potential challenges for maintaining compliance with things like HIPPA. Because serverless services are by definition somewhat distributed between many managed services, it's important to see which ones qualify for compliance and which ones do not. The development process is also very opinionated. There's a specific way to write and structure your code, and similarly, there's a specific way to deploy it. This again can be a benefit or a drawback, depending on your use case. So with this basic context out of the way, let's take a look at the projects and skills that we'll be developing throughout the course.

Key Course Concepts
Now that you have a basic understanding of the need for the serverless framework and the context of serverless applications that it falls within, let's take a look at what we'll be learning in this course. In Up and Running with the Serverless Framework on AWS, we'll take a look at the installation of the serverless framework, the application model that it uses, and the development process. Then, we'll take a look at deploying a text notification website with a serverless HTTP API. In this, we'll be using plugins, APIs, and third party integrations. Then, we'll look at scaffolding and deploying our own serverless CRUD APIs. Specifically, we'll look at how serverless deals with CRUD APIs, databases, and also how we can test and debug our applications, especially when they incorporate multiple parts. And finally, we'll take a look at what is multi-platform serverless, examine some of the risks that you have when you work with infrastructure as a service provider, and also see how we can build multi-platform and resilient architectures, as well as changing some of our code to work to be more infrastructure agnostic. In this course, we'll be building out a few examples, centering around our fake company, The Woof Garden LLC. For this, we'll have a few different course scenarios that we'll use. Sometimes we'll be building out services for the company, or be building out technical components of things that they might use. We also might be building internal tools that they can leverage to do different tasks. So don't be surprised if you see several images of my pets throughout the course. As a quick reminder, while the serverless framework works with many languages and infrastructure providers, in this course we'll be exclusively using it with Node. js on AWS. Now let's take a quick look at the projects we'll be building with these two in subsequent modules.

Planning Course Projects
It's really important to have concrete demonstrations of what you've learned after finishing a course. You can use these to showcase your knowledge to current or future employers, or just generally be proud with what you've built. So let's take a look at some of the serverless framework projects that you'll have built by the end of this course. In our Up and Running module, we'll be building out an email reminder system. The reminders will be triggered by scheduled events. These events will activate our lambda handler function. When the function is triggered by a Cron event, it will create the email and send it at that specific time. When the function is triggered by a rate event, which runs in regular intervals, but at a specific time, it will create and then send another email. To send emails, we'll be using AWS's simple email service, which we'll cover in the module. In the next module, we'll be creating a website for The Woof Garden to send and receive text notifications. This will involve deploying a static website on AWS that includes a submission form and a Google recaptcha validation. The website will submit form content through our serverless HTTP API, and have that content processed by the handler function. That function will first check the Google recaptcha API to make sure that the recaptcha is valid. If it isn't valid, the function will return and serve us an error back to the website. If it is valid, it will process the remaining form content and use a stored environment variable to make a call to the Twilio SMS API. This will then send a text message to the cell number specified on the website. In the module after that, we'll be creating our own serverless API to manage CRUD operations for a PET database. There will be multiple lambda functions in order to manage the creation of new entries, reading specific entries and listing all entries out, updating existing entries, and deleting old entries. All of these operations will integrate with a DynamoDB service resource to store and manage the data persistence. So let's take a quick look at what we've done in this module. We now know more about the serverless landscape, and how the serverless framework fits within it. We also know how infrastructure as a service infrastructures interact and relate with the framework. And we're hopefully excited to build our new projects and skills throughout the course. In the next module, we'll be getting set up with the serverless framework on our machines, and we'll write and deploy our first serverless project. See you soon.

Up and Running with the Serverless Framework on AWS
Module Concepts and Installation Demo Outline
Hello, I'm Fernando Medina Corey, and welcome to module two, Up and Running with the Serverless Framework on AWS. In this module, we'll cover some of the installation steps for the serverless framework, some differences you might see between operating systems, and how to get set up with an AWS account if you haven't already. Then we'll look at the serverless framework's application model, where we will learn about functions, events, resources, and services, and how they work within the serverless framework. After that, we'll review some serverless development basics, including required configuration, how to deploy our code, and how we can think about testing it in the future. Toward the end of the module, we'll deploy our first project, a scheduled email reminder using AWS SES and Cloud Watch. So let's get started.

Getting Started with AWS
The first thing you need to do is to create an AWS account. You can do this by going to aws. amazon. com/free. You'll press the Create a new AWS account button, and continue through the registration process. Once you're done, you'll log into the console. In the console, you'll look through the available services for IAM, or Identity and Access Management. You'll click on IAM, and then navigate to the Users tab. In that part of the console, you'll create a new user with the Add user button. As I mentioned before, the serverless framework is fairly permission hungry, so we'll be creating a user that has admin rights. Importantly, you want to make this user have programmatic access to the AWS API. You'll also probably want to have this user have access to the console, so you can sign in and enable some features later. In order to go through this whole process, you may also have to create some groups and assign the user to the group itself. This process is covered in more detail in my AWS Lambda course, You're also welcome to review the AWS documentation for how to create a user with admin rights that has access to the AWS CLI and the console. When the user is created, you'll copy down the access key ID and secret access key values that you see here. You'll also have to press the Show values button in order to see the secret. Copy those values down somewhere where you can use them when you use the AWS configure command later on.

Installing the Serverless Framework on Windows
Now let's take a look at how we can install Node on Windows. The easiest to do this is probably to Google for Node version 6. 10. 3. On the Node page for this version, there'll be a download link if you scroll down the page. Depending on your operating system, you might need a different installer, but we'll be using the 64-bit Windows Installer. After the download is complete, open up the Installer and go through the installation steps. When the installation is complete, you should be able to open PowerShell or the Command Prompt and enter in node -v to see the version of Node you've installed. If you want to confirm that Node is working, simply type in Node and hit Enter, and enter in some JavaScript code that will be print something to the console like this. You can exit out of Node by pressing Ctrl+C twice, or following the instructions provided. You can also check the version of npm that you've installed with Node. If you've installed in npm successfully, you should now be able to install the serverless framework. You can do this with the npm install command. Use npm install serverless -g. This should install serverless globally, then you can check the version of serverless that you have installed with serverless -v. You can also use the sls command, which will substitute for any serverless commands that you enter in. So in this case, you'd use sls -v. Now let's move onto installing the AWS command-line on Windows. There are a few ways to install this. I'd suggest looking up the AWS documentation for installing the command-line on Windows. The easiest way to install the AWS command-line on Windows is probably to use the provided MSI Installer. However, if you'd like to be able to update the command-line more easily more frequently, then I'd suggest looking into how to install it with PIP, as this will be able to install and upgrade the command-line package more frequently. After you've installed the AWS command-line, you can check the version with aws --version. Once you've confirmed that it's installed, you should use the aws configure command and enter in the credentials that you got from the earlier steps. As always, if you need additional clarification on this, you're welcome to look at the installation steps in my earlier course, An Introduction to AWS Lambda.

Installing the Serverless Framework on OS X
Probably the easiest way to get started with Node in npm on MAC OSX is to use nvm, the Node Version Manager. To do this, you can go to GitHub and look for creationix's nvm. You can see the URL here. You'll scroll down the instructions and see the command to install it. Run that command in the terminal, and then you should be able to use nvm to install Node. Run nvm install node to install the latest version of node. Then you can check the version that you have on your operating system. In this case, we want another version of Node, so we'll try using nvm use and entering in that version number. As you can see, if we haven't used nvm to install that version already, we're going to need to do that with the nvm install command. You can do this now. With the 6. 10. 3 version of Node installed, and the corresponding version of npm, we can go ahead and install the serverless framework. Let's do this now with npm install serverless -g. This will install the serverless framework globally. With the installation complete, you can check your version of the serverless framework using serverless -v. You can also use the sls command for shorthand for any other serverless commands. If for some reason nvm isn't an option for you, you can also download versions of Node and npm from the nodejs. org website. You can do this by going to the website, clicking on the downloads page, and then proceeding to download and install Node on your operating system. You'll need to go through the provided installation guide after downloading and double-clicking on the file. You can use pip3 or pip install awscli with the upgrade and user flag in order to install the aws command-line. If this is your first time getting set up with the aws cli, you'll also need to run the aws configure command, and enter in credentials from the aws console. You should have these credentials from the earlier step on creating an AWS account and getting your access keys.

Installing the Serverless Framework on Ubuntu
Now let's get started on installing our dependencies on Ubuntu. The first thing you'll do is use Bash to run the Ubuntu install -bash script. That should set you up with nvm, the Node Version Manager, on Ubuntu. Once you're done with the install script, you'll close that terminal and open a new one. Now we can use nvm to install the version of Node that we want, using nvm install 6. 10. 3. Then you can check the Node version with node -v. You can also check the npm version with npm -v. When you see the version shown here, go ahead and continue. Now we can install the serverless framework with npm install serverless -g to install the serverless framework globally. When the process completes, you can check the version of the serverless framework that you have with serverless -v. You can also use the sls command to substitute for the words serverless and any of the commands you use with the framework. Try it now with sls -v. Now let's install the AWS CLI. The easiest way to do this is to check first if you have PIP installed on Ubuntu. You can do this with pip --version. It should come with most installations of Ubuntu. If you do have it, you can use pip install --upgrade --user awscli, and this should install the most recent version of the AWS CLI for you. After it's installed, you should use aws configure to set up the AWS access keys and AWS secret keys that you got in the earlier part of this module.

The Serverless Framework Application Model
Now that you're all set up with the serverless framework and your AWS account, let's get started learning the serverless application model. Every serverless framework application has the same components. They have functions, which execute code in response to specific events that trigger them. Now these events can be anything ranging from an HTTP request, a scheduled event, or a specific infrastructure provider event, like an AWS S3 file upload. Serverless framework applications also frequently use resources. Resources are other infrastructure components that your application depends on. An example of a resource in the AWS infrastructure could be something like a DynamoDB table, or an AWS simple storage service bucket. In order to keep all of these moving parts straight, the serverless framework has a concept of services. And these services encapsulate your functions, events, and resources, so that you can logically separate them from other services. While it's more than likely that within one full-blown serverless application you'll have many services, this really helps to organize everything that you're doing. So let's take a look at some of these concepts in a little more detail. Now when we say serverless functions, we mean something specific depending on the infrastructure provider the framework is interacting with. We could use offerings like IBM's Open Whisk, Google Cloud Functions, or Microsoft's Azure Functions. But in this course, we'll be relying exclusively on AWS Lambda, the serverless function equivalent of the other service providers. What events you can use in your applications also depends on the platform that you're using. In AWS, you can trigger functions with events from the AWS API Gateway, scheduled events like Cron, from file uploads or changes that happen in AWS S3 buckets, from CloudWatch logs in AWS, and from updates of things like DynamoDB tables or even from IOT devices and things like Alexa Skills. The most commonly used serverless resources are things like we mentioned already, like DynamoDB as a serverless database, and S3 as a file storage service. However, you can also have any other AWS resource that can be described in a cloud formation template. Now this can be anything ranging from CloudFront Distribution networks to speed up delivery of files to your users, or an Amazon Cognito User Pool to manage the users of mobile applications. And the serverless framework documentation describes a service like a project. It's where you define your AWS Lambda Functions, the events that trigger them, and any AWS infrastructure resources they require, all in a file called serverless. yml. Services all share a similar set of characteristics. All of them have the same logical structure to organize your entire code base. Services also have a single configuration file that shares the same structure inside of that configuration file. Services that require REST APIs also all share the same endpoint URLs for those endpoints. And finally, each service shares the same resources amongst its functions. So we've kind of hinted at this, but let's look at what's actually in each of these services. Also services contain a serverless. yml file, which is the main configuration file, and defines all the configuration for our service. They also contain all the function code that will be triggered in response to events, as well as any of the dependencies that the code may require. But let's take a look at an example to see this in a little more detail. In a simple Node. js service, we might see two files, serverless. yml for the service configuration, and handler. js, which actually contains the function code. In a medium-sized application with multiple services, we would see several directories for each service. Now each of those services will have serverless. yml configuration files, and in this example, The Woof Garden pet service interacts with the pets database, and that database would be a resource required for this purpose. This service would also have all functions used to perform CRUD operations within the handler. js file. Keep in mind that function code being in a file called handler is purely by convention, and this is convention that we'll be breaking later when you split up CRUD operations into multiple files. All dependencies for this function live within the node_modules folder. There's also the package. json file used to specify the node dependencies required for the service. Another service in this application used for reminder notifications would have a completely independent serverless. yml file, different function code in handler. js and more than likely different dependencies. Now let's take a closer look at each of these application components. The serverless. yml file contains all your configuration for the service written in. yml. Now. yml is fairly easy to learn, but for the most part we'll just be modifying existing examples. So don't worry too much about this. The first set of things configured by serverless. yml is the service itself. It defines a name for the service, determines which provider and runtime this service will be using, and specifies files or directories that explicitly should or shouldn't be included when packaging the function code. It also defines any resources required by the service. In this example, we see it defining an imaginary users table, defining the structure of the table, and determining the level of read and write capacity that we should provide the table with. Critically, serverless. yml also defines all the functions within the service, the events that will trigger them, and any configuration required for the functions themselves. Each serverless function in AWS' Node environment has an event, a context, and a callback. The event object contains data passed into the function from whatever event triggers it. Most often, functions will process the event object and determine how to act from that information. The context object contains a variety of information about the function, such as the function's name, the function memory limit, and the log group name for the function. It also provides a method to get the remaining time left for the function before timeout would occur. And finally, the callback is used to pass back information to the function caller. Many serverless functions make use of dependencies to enhance their functionality. These dependencies can be anything from snippets of custom code included in the same folder, to full-blown npm packages. These npm packages can also be serverless framework plugins that enhance and extend the functionality of the framework itself. Okay, we've covered a lot of concepts, so let's quickly visualize everything before we start developing our own serverless services. First, let's think about a service responsible for CRUD operations on some database. Now, every service is a shared configuration file, so we'll imagine this as the structure of our service. And in this case, we'd have several functions, each one to manage different CRUD operations. We might also require specific libraries to work properly with the database. In order to trigger these functions, we'll likely need to have some sort of web form to create new entries. These forms would send events through the AWS API gateway and serve as our events. And finally, we might have a database resource to store all this information for our service. In another example where we're creating a reminder service, we'd have the same structure for our configuration and our service, but we might only have one function. Now, the function might use fewer libraries, and have a different set of events on a schedule rather than associated with an API. Finally, it might store email reminder templates in a different kind of resource entirely, such as AWS S3. So you'll start to see that while the types of applications can differ substantially, you'll generally have the same structure and underlying concepts behind everything. So, now that we've got a strong understanding of the concept used by the serverless framework, let's take a look at how we can develop our applications.

Developing Serverless Services
Developing applications with the serverless framework from scratch has a few specific steps. Let's take a look at each of them now before we apply them to our first project. First, we'll highlight how to start up your service and install your dependencies. Then we'll look at some configuration settings. And while the next step would be writing our own function code, we'll postpone that for the demo that we're creating at the end of the module. We'll also highlight how the serverless deployment process works, and some options you have for deploying your applications, as well as some things to think about so you can test your applications effectively later on. The first thing you would usually do to create a new service is to create a new service from a template. You could use this command to create a simple node. js service in a directory called servicename. After you create the node service, you would run an npm init and then npm install any node dependencies that you wanted to use. Next, you'd open up your serverless. yml file and specify the details of the service, which at a minimum would include the service name, the provider and runtime, and the functions that you're using in your service, along with the events that trigger them. In this example, the filename would be handler. js, and the function name in that file would be woofGardenEmailer. As you can see, we also set a scheduled event that runs every three days. We'll come back to this configuration in a moment when we're writing our application. Again, since we'll be writing our own code in a moment, we'll get back to that part later. But for now, let's look at what happens when we're all done writing our service and we'd like to deploy it. When we're ready, we can run the serverless deploy command. This command will deploy or update our entire application. We'll always need to run the specific command the first time we deploy our service. Remember, if you're feeling lazy, you can always replace serverless with sls and any serverless command. After we deploy our service for the first time, we can also run sls deploy function, and pass in a function name as an argument. This deploys only the function and skips the rest of the deployment process. This speeds up large deployments considerably. Function names are case sensitive, so be sure to match the function name in the serverless. yml file exactly. Also, keep in mind that the function name in serverless. yml is not necessarily the same as the name of the function inside your handler. js file. This might be a little confusing at the moment, but I promise I'll go into more detail when we work on our demo. Finally, you can also use the sls deploy package command to specify our particular package to deploy to AWS. But we won't be using this option in the course, as it's primarily reserved for integration with continuous integration and continuous deployment pipelines. You may also want to remove your service in the future. You can do this with the sls or serverless remove command. But what actually happens in the background when we run these commands? Let's take a look. The first thing that happens when we run a command like serverless deploy is that the framework reviews our serverless. yml file, and then creates a CloudFormation template from that file. It also zips up the function code and dependencies and sends that over to AWS S3 for later use. The CloudFormation template is then passed to AWS to create any resources for the service that we need. This includes things like APIs, databases, or other resources required for the service. CloudFormation also then creates the functions themselves from the zipped deployment packages that were stored in S3. And when everything is finished, the serverless framework displays the successful status of the deployment. Now let's briefly go over how we can set our function code up to be more testable in the future. In general, we'll want to look farther ahead than this course when building the serverless framework applications. To do this, we'll want to think about writing testable function code. Notice I didn't say testable lambda functions. This is because our function code could be usable on other infrastructure providers entirely. On this subject, try to separate business logic out from the infrastructure specific code that you're writing. For example, if in the future you like to replace a particular component that sends emails for another one completely on a different infrastructure provider, it will be much easier if the business logic is already separated from the mailer itself. This will also make it easier to run unit tests for the business logic and then develop integration tests with the services used. With all of this information in mind, let's make our first service.

Creating Your First Serverless Framework Service
In this demo, we'll be creating an email reminder system that's triggered by two scheduled events. One will be a RAID event and the other will be a Cron event. Now, both of those triggers will invoke a handler function. That function will determine which of the two triggers is invoking it from the information provided through the event. It will create one type of email when the Cron event triggers it and send out that email, and then another type of email when the RAID event triggers it, and send that second type of email. So let's look at the steps to create our first serverless service. First, we'll create the service from scratch using the boilerplate templates that the framework provides. Then, we'll update our serverless. yml file to scaffold out the details of our service. After that, we'll update the contents of our handler. js file with our own function code, and then set up AWS's simple email service in the AWS console in order to send ourselves emails. Finally, we'll make sure we have all of the dependencies squared away before we deploy our service and invoke a sample function. The first thing we'll need to do will be to create a template for our service. We'll do this with the serverless create --template and used the aws-nodejs parameter, as well as the --path to specify where the service will be installed in serverless-reminders. As you can see, we now have a serverless-reminders folder. In that directory, we start out with a handler. js file, and a serverless. yml file. Let's take a look at the serverless. yml file now. The only thing you'll be keeping in the serverless. yml file is the service specific portion, as well as the provider section that you see here. Go ahead and remove everything else from the serverless. yml file. When you're done, you should have something that looks like this. At any point, if you get stuck, you can reference the demos for this module in order to copy and paste over the correct code for this serverless. yml file, or any other file that we're using in this module. Next, we'll add in some IAM statements that we'll need for this demo to function. This essentially will allow the functions to access AWS' simple email service, or SES. After that, we'll add in the functions that we'll be using in this demo. We'll start with the dailyReminder function, which has a handler, and the handler file is called handler. js, so in this case, it'll start with handler. The function within that handler is sendReminderDaily, which we'll be using as the second part of that handler. Once we're done with that, we'll go ahead and add the event that will trigger the function itself. That event will be a scheduled event, which will use the rate option in AWS, and in the case of this function, we'll be using it every day, so we'll put 1 day. We'll also add a second function, which will be our weekendReminder. We'll call this weekendReminder. We'll add a handler, which also exists in the hanlder. js file, in this case being sendReminderWeekend. We'll add a Cron event for this function, so that it runs on Sunday and Saturday at a specific time on the weekend. Keep in mind that when you're setting these times, it'll be in UTC, not near a specific time zone. Now let's take a look at our handler. js file. To expedite this process, I won't have you write all the code for the handler from scratch. You can open up the demos file that you have for this module, and inside it you should see a folder with three files that we'll need to copy into our new service. The first one is the handler. js file, and the other two files just to speed things up for later, are two dependencies we'll be using to create email templates. The first is dailyReminder. html, and the second is weekendReminder. html. You're welcome to reference any of the other files as you see fit. However, we'll primarily be working with the ones I've mentioned. The rest are used by npm to install the entire project. But because we'll be creating this more from scratch, we'll be going through this bit by bit instead of actually installing it with npm install. Now let's take a look at handler. js. In this file, we export two functions that are almost identical. The first is the sendReminderDaily function, which we've configured to run on a daily basis within serverless. yml. As you can see, it has several requirements that it needs to work. The first is the aws-sdk, which is used to create an ses client for sending emails via this function. Fortunately, we don't actually need to include this dependency when building our function. Within the execution environment in AWS, we get it by default. We'll also need fs, which we'll be using to access the local filesystem and load in our email templates. After that, we set the email address that we'll be sending and receiving email to and from, and then create the parameters we'll be passing in to AWS. SES. The main things to notice in these parameters are that we send both the option of an HTML and plain text email, and that we're using the same email to send and receive mail. This repeated email is because while we're starting out with SES, we have to verify all to and from addresses as an AWS precaution to avoid spam. You can remove this barrier for full-blown applications, but in this course we won't be getting into those details. Farther down on the code, we'll use the ses. sendEmail method with the parameters that we've created to send an email via AWS. And finally, in typical node fashion, we'll write out what we'd like to happen in the case of errors and successes. You'll see almost identical code below in the second function, with the key exceptions of the function's name and the filesystem location that it loads its email template from. Now let's verify an email on the ses portion of the AWS console. Scroll through the AWS services and look for the Simple Email Service. Open that up and click on the email address section. My console will look slightly different than yours, because I've already verified emails. But you should also see the button to verify a new email address. Click that and enter the email you'd like to use. Then check that email and follow the verification instructions provided by AWS. After that, you should be able to refresh the page and see the verified address. At this point, we're ready to deploy our first service. You can open up the terminal again and get back to the top level of our service directory. You should be able to list out the files and see our HTML templates, a handler. js file, and our serverless. yml file. Now we can run serverless deploy. This will package up our service, create a CloudFormation stack, upload a deployment artifact to S3, and generally manage the entire AWS deployment process for our new service. When it's finished, you should see an overview of the deployment status that includes the new functions we've just deployed. Let's try testing them now. We'll run serverless invoke --function, and we'll use the dailyReminder function to run that. We'll also include the --log flag in order to make sure that any logging information is printed to the console. This should run a test of the function and display the runtime information. And here's a great example of how logging out the runtime information when invoking a function can be useful. If we review this error, we can see that we neglected to update our handler. js file with a proper verified email, so let's update that and try again. Here's the verified email we need to update. Now that we've updated and saved our handler file, let's update the specific function we'll be using. We can save ourselves some time by deploying that specific function with serverless deploy --function daily reminder. This will update and deploy all the dependencies for this particular function without doing the entire deployment process. Especially in larger services, this saves us a lot of time. Now if we retest the function, we see it works correctly. If we load up our email, we can see this successfully send to email. Now let's take a look at the AWS Lambda console. You can load up the homepage and find the section for lambda. On this page, we can see the functions that the framework has created for us. If we click into those, we can also see the CloudWatch events these functions now have to be triggered by. But what happens if we want to tear down the service completely? Well, we can do this with the serverless remove command. If we check the console again, all of our functions and infrastructure should have disappeared. But of course you might want the occasional reminder to work with the serverless framework and see cute puppies while you're doing so. So feel free to go back and edit the schedule and the re-deploy the entire application again with serverless deploy. Wow, so we covered a lot in that last module. We learned all about the core serverless framework concepts of functions, events, resources, and services. We also learned a variety of serverless framework skills, including the processes and best practices that we can use to develop, configure, deploy, and test our services. And we did all that while deploying our first serverless service. With that heavy learning load out of the way, let's take a look at what's coming up. In the next module, we'll be looking at serverless APIs and applications, and how we can build those using the serverless framework. So see you there.

Deploying a Text Notification Website with a Serverless HTTP API
Module Overview and Serverless Plugins
Hello, I'm Fernando Medina Corey, and welcome to my course on the serverless framework. In this module, we'll be deploying a text notification website with a serverless HTTP API. In this module, we'll learn about serverless plugins. We'll start with an overview of the serverless plugin model before taking a closer look at serverless-finch, a plugin we'll be using to deploy static websites to AWS. Then we'll learn about serverless APIs. We'll overview all the components that make up a serverless API on AWS, but we'll also take a closer look at one of the key components, the AWS API Gateway. Finally, we'll take an in-depth look at some of the configuration required to make these HTTP APIs function. After that, we'll get ready to create our module project. We'll get set up with Google Recaptcha and Twilio, which we'll need for the project. We'll scaffold out a service, review our function code, configure our API endpoints, and finally, we'll deploy the project to AWS. So let's get started. Let's take a look at how the functionality of the serverless framework can be extended using plugins. The serverless framework is merely a group of plugins that are provided at the core. These come together to create a set of functional commands for the terminal. But what can plugins do for us? Well, they can extend the utility of existing commands, or when we want new functionality entirely, we can create new plugins. And each of these in turn give us more command-line options and functionality. External plugins are added on a per service basis and are not usually applied globally. If you want to work with a plugin, make sure you're in your services root directory, and then install the corresponding plugin with the help of npm. We can install plugins on a service specific basis with npm install. If the purpose of the plugin is to augment the development process, we probably want to save it as a dev dependency, as shown here with the --save-dev flag. The next step after installing the plugin is to set any configuration that may be required for it to work. At a minimum, this usually includes creating a section of our serverless. yml file that specifies the plugin name, and may also mean we need a custom section to specify specific configuration details that the plug-in uses, but we'll look at specific examples of this in a moment. Let's take a look at serverless finch, the serverless plugin we'll be using in this module to deploy a static site frontend to our application. The first thing we'll need are some pre-made static site files, including HTML, CSS styles, and imagery. To save us some work, I'll be providing us with a template website that you'll be able to use. With this, we'll be able to start testing our site locally to make sure it looks and works as expected. Then we'll use the serverless finch plugin to move the static site files to AWS S3 and set them up as a static website. At that point, the website will be available to the internet at large, and any users who'd like to see it will be able to access it. We'll get into the details of using serverless finch more during the demo, but for now, let's take a look at serverless APIs.

Exploring Serverless APIs
In order for the website front-end to do anything meaningful, we'll also need to create an API that the front-end can interact with to send and receive data. Let's take a quick look at how the serverless framework allows us to accomplish this. The serverless framework helps us create serverless APIs by managing all the components that we might need in order to create them. The first thing that a serverless API would need to deal with is an incoming HTTP request. In order to handle that within the AWS ecosystem, the serverless framework uses the AWS API Gateway. API Gateway handles all these incoming HTTP requests to its web endpoints, and then passes them along to lambda functions. Those functions can use managed resources ranging from backing databases to identity management services, or other logic to process those requests as needed by the application and return a response. Because API Gateway is a core part of the serverless APIs that we're using on AWS, let's take a quick look at what it is and how it works. AWS describes the API Gateway as a service that enables developers to create, publish, maintain, monitor, and secure APIs at any scale. So why use the API Gateway when building your serverless applications? Well, first, the service makes it super easy to create REST APIs for both web and mobile applications. These APIs are fully managed by AWS, making them easy to scale and removing a large amount of the overhead required to secure them. That's not to say security isn't something that you have to consider when creating an API, just that AWS takes care of some of those layers for you. API Gateway also easily integrates with AWS lambda, and the serverless framework seamlessly handles this integration for you without having to dive into too many of the detailed configuration steps that would otherwise be required while working directly with the AWS services themselves. Another nice thing about the API Gateway is that the pricing is completely dependent on the usage. One million API requests are included with your first year of using AWS under the free tier, and as of August 2017, you're only charged between $3. 50 and $4. 25 USD per million requests. Keep in mind, there is also a charge associated with the amount of data that you transfer with the API Gateway, which at low levels of use is usually between 9 and 14 cents per gigabyte. Finally, if you'd like to cache information, you can also pay roughly between $0. 02 and $0. 04 per gigabyte per hour across a selection of cache sizes. Now let's take a closer look at what configuration we'll need to use in our serverless. yml file in order to get our serverless APIs functional In our serverless. yml file, you'll need to specify functions as usual. In this case, we have a serverless function create, which has a lambda handler and a folder called pets. The filename would be create. js, and the function inside that would be called create. If this seems a little abstract right now, don't worry too much. We'll be taking a closer look at configuration just like this during our demo for this module. Now the way we set up our API is by specifying HTTP in the event section. All we need to define there will be a path, an HTTP method, and the cross origin resource sharing, or cors, setting. In this case, we'll enable this, because we want to be able to use the API from other domains other than the one that AWS creates it within. This configuration would allow us to deploy an API endpoint that accepts posted JSON and passes it along to a lambda function for processing. So what if we wanted to retrieve specific information from our API? In that case, we might need to pass an ID value through the API, so it can pass the ID to a lambda function, and then run a lookup on the other end. In these cases, a few things would change. First, we'd need a new serverless function in the pets directory, in this case, in a file get. js, we'd use the function get. We also modified the endpoint path from the previous example, and add a required id parameter in curly brackets. The API Gateway will pass this value along, and we'll use the id value to look up the pet within the backing dataset. We'll also need to make sure the HTTP method would change to get, which is the expected method for retrieving data in a way that would cause no change to the underlying service. When your API requires additional access to external services, you may also need to manage environment variables. Within AWS, environment variables can be set once in the serverless. yml file, and then referenced within the function code of a service. Here's an example of how you'd make an access key value accessible as an environment variable. We'll also see how to load these values into our function code within the demo later in the module.

Creating Your Text Notification Website
Now that you know some of the required steps for us to set up a serverless API, let's get started on our project for this module. As a quick reminder of what we're building, we'll create a static frontend that has a recaptcha validation test and form fields, and is configured to send post requests to our serverless API. That API will send the input it receives to a lambda handler function. The handler will send a request to the Google recaptcha API and process the input. If it fails, it will return that failure to the lambda function, which will send that back to the requester via the API. On success, it will send a request to the Twilio API, which will send a text using the information included in the form fields. Before we start this demo, please be aware that the API keys and secrets that you use in this course should be kept out of source control. Especially your AWS access and Twilio keys, as they could be used to send messages on your behalf, or take actions in the AWS console. So when iterating on these applications, be sure not to accidentally commit these credentials. In this demo, we'll be deploying a full-stack serverless application. We'll start by gathering our dependencies, including working with Google Recaptcha to make sure we'll be able to use that in our site, setting up a Twilio account in order to get credentials to send text messages, and then scaffolding out our service with the code provided for this module. Then, we'll review all the function code that we'll be using, the API endpoints that we'll be deploying, then we'll take a moment and deploy our project as a whole, including the static site and the front-end, and our serverless APIs in the back-end. So let's dive into our demo. First, let's get our Google Recaptcha credentials. The application uses reCAPTCHA V2 with the I'm not a robot checkbox and verification. You can sign up for reCAPTCHA V2 with a Google account at this URL. During the sign up process for reCAPTCHA V2, make sure to include amazonaws on the list of domains. This will allow your reCAPTCHA to load on the AWS static website, which we'll be deploying. Then accept the terms of service and press the Register button. Copy down the recaptcha data site key and secret values somewhere you can reference. We'll be adding those to a configuration file in the moment. Next, let's create our Twilio account at this URL. The first time you create an account, you'll need to verify the phone number you'll be using with the account. But in exchange, you'll be getting a few dollars worth of trial credits to use with this application. You can give your project whatever name you'd prefer. When you're done signing up, click the Show API Credentials link in the top right. Then copy down the account SID and auth token values. You'll need to press the eye icon to show your auth token. Once you're done with this, you can get your first Twilio phone number by going to this URL and clicking the Get your first Twilio phone number button. Choose the phone number that they offer you, and then copy down that phone number, as we'll be using that later. Next, we'll be reviewing some of the code we're using in this demo. I'm using the tree utility to show you each of the files within this folder, but this isn't required for this course. The first one we'll be opening up will be serverless. yml. Go ahead and do that now. In serverless. yml, replace the default values in the environment section with your newly acquired Twilio and recaptcha values. Also be sure to replace the final configuration value of your unique bucket name here with a unique S3 bucket name to deploy your application in. This value has to be different from all other S3 buckets, because S3 buckets are globally unique across every bucket in existence. So I'd suggest something like fernando-node-text-message-app-20170801 to make sure you don't run into any name conflicts. Consider adding the name of your favorite animal to the end just to be safe. Make sure that you're copying the Google secret and not the site key. And also keep in mind to put in your Twilio phone number in this file, not your personal cell phone. When you're done, make sure to save the file. Next, let's take a look at the abridged version of how the handler. js file validates things coming in through the API Gateway. First, we load in our environment variables and required libraries. Then we add the headers constant of Access-Control-Allow-Origin *. This allows us to make sure that we don't run into issues with cross origin resource sharing. After that, we create a Google recaptcha payload from the event data, and then we send that payload over to Google and check whether or not this recaptcha has passed. If the Google recaptcha is valid, then we create a payload for the Twilio API, and we attempt to send our SMS message. If the message is successfully sent, then we send the results back to the caller. After we're done reviewing the handler. js file, let's go back to our terminal. Run npm install from the top level folder of this project in order to install the dependencies we need. This will install the Twilio helper, the requests package, serverless finch, and any dependencies that they need. Now with all our dependencies installed and our code set up, let's deploy our serverless API using serverless deploy. After the deployment is complete, be sure to copy the API Gateway value that is returned to the console. It should look something like this. Let's open up the lambda section of the AWS console one more time. You should see a new function listed. If we click into that function, you can scroll down and see that it's been configured with environment variables that were included with our serverless. yml file. Now let's go to the API Gateway section of the console. If you do this, you can see that the API Gateway has successfully deployed and configured an API endpoint for us. But with all of this deployed, how are we going to actually use this in an application? Because we created our recaptcha to be used within the domain Amazon AWS, we'll actually be taking advantage of AWS static site hosting. To speed up this process, we'll use two things, a prebuilt static website template that's included in the demos code that you've downloaded for this module, and the serverless finch plugin for the serverless framework, which we'll be using to deploy our static site. First, we need to prep our static site for deployment. Use a text editor to open up the index file at client/dist/index. html. We'll be making two changes. First, replace your Google recaptcha site key text with the data site key value from the Google recaptcha step earlier. This will have the static site load in your specific Google recaptcha for validation. This value is different from the secret value, which we just copied down and put into serverless. yml. Next, change the API endpoint URL to the value that you copy down from our deployment process. This will make your static site post to the API endpoint you just deployed. After making these changes and saving the index. html file, let's use serverless finch to deploy our static site. You can do this with the serverless client deploy command. This will deploy the contents of our client/dist folder as a static site in an S3 bucket on AWS. When this process is completed, it should output the location of our deployed project. Copy that down, and paste it into your browser to take a look. You should see a page that looks something like this. Go ahead and enter in your personal phone number and a message to send to yourself. You'll need to use your personal phone number, because other numbers aren't validated and your account is still in a trial phase. Once you're done entering a message, then complete the Google recaptcha portion and send the message. You should see a success status and get the message on your cell phone. If you'd like, you can go back to the AWS console and take a look at S3. Within S3, you should see a new bucket that we're using for our static site hosting. This bucket contains all the static site files from client/dist and was created by the serverless finch plugin. And that's it. We've just deployed our first full stack serverless application using the serverless framework. In this module, we covered several serverless concepts. We took a look at plugins and how we can use them to extend and enhance the functionality of the serverless framework. We also took a look at the process for API development and configuration, as well as some deployment options that we have for when we want to get our services up and running. We also learned some new serverless skills; specifically, we implemented our own serverless APIs. We also deployed a full stack serverless application, complete with the front-end application that we deployed with the serverless finch plugin, through to the back-end serverless API that allowed us to integrate with other third-party services. Coming up in the next module, we'll be working with serverless CRUD applications and how we can use APIs and databases in combination with the serverless framework. I'll see you in the next module.

Scaffolding and Deploying Serverless CRUD APIs
DynamoDB Overview and Resource Configuration
Welcome to the next part of my course on the serverless framework. My name is Fernando Medina Corey, and in this module, we'll be looking at scaffolding and deploying serverless CRUD APIs. We'll start looking at how we can deploy serverless CRUD APIs on AWS. We'll see how we can configure the API Gateway endpoints for CRUD operations. We'll also look at serverless databases, in this case, DynamoDB. In order to work with Dynamo, we'll also need to look at how to integrate database resources using the serverless framework. After that, we'll review and build a serverless pet database. And finally, we'll test and debug our database. We'll learn particular tools and strategies frequently needed when debugging serverless applications. All serverless framework CRUD operations on AWS usually happen with the same workflow. First, HTTP requests are made by a browser or another HTTP client. Those requests are sent to the AWS API Gateway, where the request data is processed and directed to a specific lambda function, depending on the API Gateway endpoint requested. Each potential CRUD operation will have its own function. If the API Gateway receives a request to a create endpoint, it will route that information to the create function. Alternatively, if API Gateway receives a request to delete some value, it will route that to the delete function. Each of these functions will process the JSON payloads that come in via the API Gateway in order to validate them and take the appropriate actions on the backing DynamoDB database. Because we're going to be relying on DynamoDB heavily for our CRUD database, let's take a closer look at what it is and how it works. Amazon describes DynamoDB as a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. So how do we use Dynamo in our applications? DynamoDB shares several things in common with other data stores you might be familiar with. Similar to other databases, DynamoDB stores data in tables. Each of these tables contains multiple items, and in each of these items, you have one or more attributes. Attributes are the fundamental data element in DynamoDB and are in many ways similar to fields or columns in other database systems. The main distinction is that as long as items in this same table have a different value for their primary key attribute, they don't need to have any other attributes in common. In this way, DynamoDB is schema-less. Now, the attributes nor their data types need to be defined beforehand, and each item can have its own distinct attributes. All items in DynamoDB must have distinct primary keys, but each table can have these keys made in one of two ways. The table can use a simple primary key, which uses only a single attribute such as an id as a partition key. In this case, no two items can have the same partition key or value. Or, the primary key can be a composite primary key made up of a partition key and a sort key. In this case, you could have multiple items with the same partition key, as long as they all had different sort keys. An example of this might be if you had a table of pet ids and a sort key of all the dates that they attended daycare. That way you could query for all the entries for a single pet id, or for a range of dates a specific pet was in attendance. DynamoDB has a few features that are particularly useful for serverless applications. The first is auto scaling. DynamoDB allows you to configure your table to automatically scale up your capacity as needed within a particular range, in this way meeting your demand. It will also automatically scale down up to four times per day. Dynamo also allows you to closely monitor the throughput of your applications. With its read and write capacity units, you can specify exactly the thresholds of traffic you'd like to be able to accommodate and pay accordingly. Additionally, if you're able to consistently predict your need for Dynamo at higher levels of use, you can also take advantage of reserved capacity. Reserved capacity allows you to pay up front for capacity that you know you'll use at a drastically reduced price. So if your workloads are consistent enough, you can get some serious savings from this feature. Finally, according to AWS, DynamoDB can also be HIPPA compliant, which makes it useable on a broader spectrum of applications. It's always good practice to review the details from AWS directly to determine how best to meet the legal requirements like this. But before you think I'm pushing DynamoDB on you regardless of your use case, here's a few things to keep in mind when designing future applications. First, NoSQL isn't relational. DynamoDB requires you to design tables to be queried with a partition and sort key only. Scanning through the entire table is a very costly operation that becomes cost prohibitive as the table size grows. So if you're used to relational databases like MySQL, keep in mind that this is a different animal entirely. So when designing your tables, keep in mind that design decisions matter. When designing your application or your table, you're stuck with the primary keys you create. You don't have options to change and optimize your table design going forward without completely recreating the DynamoDB table. So keep in mind that the table design is somewhat rigid, and be sure to optimize your tables up front whenever possible. Additionally, because the way DynamoDB partitions keys, you have to be careful to pick uniform partition keys. DynamoDB spreads your read and write capacity over automatically created partitions as it scales. So for example, if some primary key value is queried at a much higher frequency than others, the performance of the partition that that value is located in might suffer because it's shared with other values in that partition, and it can't borrow capacity from other partitions without increasing the overall limits on the entire table. Essentially what this means is that as you grow, hot keys become an issue at scale. So be aware of this when creating your own applications. Let's quickly look at how much DynamoDB costs. DynamoDB has a fairly straightforward pricing model. The first part of the charges is for write capacity. Write capacity for DynamoDB is charged about $0. 47 per write capacity unit per month. This is roughly equivalent to 1 write per second, or 2. 5 million writes per month of items that average about 1 KB in size. You also get 25 free write capacity units per month with your AWS free tier account. The next charge is for reads from your Dynamo table. These are charged at about $0. 09 per capacity unit per month, which can actually amount to as many as 5. 2 million reads per month of items, averaging about 4 KB in size. 25 read capacity units are also included within your account for AWS free tier. The last main charge for Dynamo is in relation to the amount of storage used. You pay $0. 25 per gigabyte per month on the total amount of data stored in Dynamo. You also get 25 GB per month for free included within the AWS free tier. There's one quick side note that needs to be made about the read types that DynamoDB offers. The first type of read is eventually consistent. Eventually consistent reads may miss writes that were made recently on the table you're reading from. They maximize throughput and can happen twice per second on items less than 4 KB for each read capacity unit. These are the default type of read, and because you get more reads per second for the same capacity, they're cheaper overall. Now, strongly consistent reads on the other hand are guaranteed to reflect the most recent writes to a table. Because of this, they require twice the read capacity of regular reads and are effectively more costly. Another Dynamo feature you may want to use is DynamoDB streams. This feature allows you to take action on changes that occur on a Dynamo table. Pretend you have a stream of incoming requests to Dynamo. You can use the streams feature to identify changes on this table and trigger a lambda function to review that change. The function can then process the change and determine if it should take an action. For example, in the case of an additional new entry to a users table, we might trigger a lambda function to send an email notification to the new user. Here's a few key takeaways for DynamoDB while you work with it on your own applications. The free tier is very generous. You can build applications that can handle about 200 million requests per month and use up to 25 GB of storage without needing to pay AWS a penny. Don't be afraid to play around a little bit with this to get the hang of it. Keep in mind that your reads aren't strongly consistent by default. So if you have a highly sensitive application that needs strong consistency for its reads, be sure to use it where appropriate. Design all your tables thinking about potential issues down the line. Could you have some primary keys reference significantly more than others? Should you be using a sort key? Additionally, if you do need to take action on changes to a database, make sure you think about using DynamoDB streams. Now let's take a look at the configuration details of setting up a DynamoDB resource within the serverless framework. The configuration for a serverless CRUD API would have a few additional components when compared to other services. First, it requires an additional section in the configuration called iamRoleStatements. Within that, we add an IAM, or Identity and Access Management policy, that can be used by the service to access DynamoDB. In this specific instance, we're allowing it to access our DynamoDB pet table. This is a slight oversimplification, but we'll take a look at some actual code just a little bit later in this module. We'll also need to create a resources section that names each of the resources used. In this case, we defined a resource called pet table. We describe it as a DynamoDB table that has the DeletionPolicy of Retain. Now this means that when we deploy, remove, and modify our service, the table data will not be deleted. This is usually a good idea when working with services that rely on data persistence, because it allows us to avoid destroying all of our information by accident. The latter half of the resources configuration defines some properties of the DynamoDB table. Specifically, it creates attribute definitions for the table, and it defines an attribute ID, which is a string. It also specifies the id attribute will be used as the schema key, and uses the HASH KeyType. This means that the id value is the partition key for the table, and the table is using a simple primary key. We could also add another defined attribute using a range KeyType, which would make this a composite primary key, but we'll skip that in this application. The configuration also sets the provisioned read and write capacity of the table to 1, which is conveniently within the AWS free tier. Finally, it sets the name of the DynamoDB table within AWS, which matches the name of the table in the IAMRoleStatement. Now let's quickly refresh our memories about the demo we'll be building out in this module, and then we'll implement the sort of service ourselves.

Configuring HTTP CRUD APIs with the Serverless Framework
In this demo, we'll be creating a serverless API to handle incoming HTTP requests. That API will allow us to create, read, update, and delete items inside of a DynamoDB table that we'll be using to store information on pets. Let's take a look at each of the steps we'll need to get through this and make it happen. To start this demo, we'll be creating a serverless pet database. To do this, we'll need to start a new serverless project. We'll do this by copying an example project from our demo files. Then, we'll review the serverless. yml configuration file and all the code for the service. After that, we'll install the requirements for the project using npm, and then deploy the project to AWS. Then we'll confirm that our API endpoints are functional, and we'll review the effects in the AWS console. First, go ahead and download the demos. zip file for this module, then unzip the code and change directories into the serverless node REST API folder. I'm currently showing you the contents of the folder with tree, but again, you don't need to use this for this module. The first thing you'll want to do is open up our configuration file for the demo, the serverless. yml file. This configuration file takes advantage of some parameterization that the serverless framework allows us to do. The first thing you'll notice is that the DynamoDB table doesn't have a specific name like pets table. The reason for this is because it's naming it with the parameters that are already included in the framework configuration. It's pulling the self:service parameter, which means that it'll be populating the serverless node REST API value. It's also including the stage parameters, which will pull in the stage that this is going to be deployed in. You'll also notice that we have a more extensive set of permissions in the iamRoleStatement. This is because our service will need a variety of permissions to take every action on this DynamoDB table. The resource itself that these permissions are granted on also uses the same parameterization as we see above in Dynamo table. While you won't need to make any changes to any of these values, you can choose to rename your service if you prefer it to be named something else. Just keep that in mind when we're going forward in the examples. Now let's take a look at the functions for our service. The first function in our service is the create function. Its handler is located in the pets directory, in a create. js file, and it has the function name of create. Now we'll take a close look at this when we get back to our terminal. The events that will trigger any of these functions are HTTP events. In this case, that means the API Gateway will process and forward these along to our functions. For the create function, you can see that the method is post directly to the pets endpoint. This is because the post method is the expected method when you're changing data within the database. We see a very similar case below with the list function. In this case, the location of the function changes slightly to the list. js file, and the method also changes to get. That's because we'll be getting back information without making a change to the underlying service. If we scroll down slightly, we can see the get function as well. This is almost identical to the list function, as it also uses the get method. However, it takes a specific ID, and in this case, will allow us to pass an id value into the API Gateway, which will be processed and look up specific within the database, rather than returning a list of all the values within the database, which our list function did. Finally, we have the update and delete functions. These do about what you'd expect. They're both contained within the pets folder in their respective update and delete. js files, and the function names within those files are update and delete. Additionally, they each have their own respective method. In the case of the update function, it uses put, which is the expected method when changing information in the database, but not adding a new entry itself. The delete function uses the delete method, which is expected for deleting information. And lastly, you can also notice that both of these take a specific id similar to the way that get did. That's because each of them will be updating a specific value within the database, rather than deleting or updating the entire database as a whole. If you scroll down a little bit, you should see the resources section. In the resources section, we see something similar to the examples we were looking at earlier. We create a serverless framework resource called PetsTable, which is an AWS DynamoDB table with a deletion policy that allows it to keep its data even when the serverless framework is redeploying and making changes. It uses the same attribute model that we looked at in earlier examples, and also has the same provision throughput. The only difference here from the earlier part of the module is that the table name will be reflective of the provider and the DynamoDB table name, which we defined slightly earlier in this configuration file. So now let's take a look at the function code within each of our function files.

Reviewing Your API Function Code
Now if you take a look at how we structured our function code, you'll notice that all of it is within the pets directory, and this is reflective of our serverless. yml file. All of the configuration for the handlers had the pets/ at the beginning of them. This is because they're all in the pets directory. Now this is purely by convention. You could also have each of these functions one level up and not use the pets directory at all. Alternatively, you could spread them out into smaller and smaller directories and continue to add slashes to direct the path within the serverless. yml file. Really, it's just up to you to define the best way to actually structure your code for whatever example you're using. Now let's open up the pets directory and look at each of our function files. Now each of our functions has some similarities. In each of them, we bring in the required dependencies that we'll be using throughout the function. In the create function, we used the uuid dependency, which we'll be using to create consistently unique IDs for each of the items that we'll be adding to DynamoDB. In all of our functions, we also bring in the aws-sdk, which will allow us to interact with DynamoDB. We also always bring in the DynamoDB document client, which when we're working with JavaScript allows us to more easily interact with DynamoDB. In this case, whenever a create function is run, it saves the timestamp information of the first run. It also parses the JSON event. body and stores it as data. Then it checks to make sure that there's a petName and a petBreed, both of type string, in order to continue going through the function. When that results in a success, we can scroll down slightly and see that we'll be creating a params object, which we'll be using later in the function. That params objects models what's expected to pass into DynamoDB in order to create an entry in a table. First it passes in the DYNAMODB_TABLE name environment variable that we're getting from the configuration in serverless. yml, and it creates a new ID from UUID using the v1 method, and also adds in a petName and a petBreed, as well as the createdAt timestamp. Once it creates those params, it passes them through the DynamoDB put method in order to save that information in DynamoDB. Then, if successful, it passes back a successful response using the callback. The delete function essentially uses a bunch of the same information. It uses the same dependencies, also uses the process environment variable, the DynamoDB table name, and then takes the id value that's passed in through the event. It uses all of this to create a parameters object, which it then passes to the DynamoDB delete method, and then returns the response code depending on success or failure. The get function also does something fairly similar. It creates the same parameters object and then passes that into the dynamoDb. get method. Afterward, it processes that result and returns it with a response. The list function does something very similar. It passes in the DynamoDB TableName and then scans the entire table. The results of the entire scan are then passed back in the response. This includes all the items that are within the DynamoDB table, which means that as this table grows, this operation will become more expensive and contain a lot of additional data. And finally, update. js does something similar to what we were seeing in create. js. It creates a timestamp and loads the data in from the event. It then validates to make sure that we have the petName and petBreed correctly recorded as strings. And then it creates a params object and again uses the DynamoDB table name and some information of the event to specify the key that we see within the DynamoDB table. The only unique thing that this function does is that it uses some DynamoDB specific syntax in order to reference the petName, petBreed, and other values within the table. This essentially allows us to specify what values within the table will be updated, and makes sure to override all of these new values. Finally, it passes in that constructed parameters object to the DynamoDB update operation, and then returns the result as we expected with a success or failure using the callback.

Deploying Your CRUD API
Now if we go back to the terminal, let's take a look at how we can install the dependencies for this project. First, just double-check that you have the correct version of npm and Node running. You can use npm --version to see the version of npm. You can also use node --version to see the correct version of Node. You should be seeing Node version 6. 10. 3 and npm version 3. 10. 10. While it is possible that you'll be able to install the dependencies correctly using a different version, I'd suggest using these just to keep our system requirements as identical as possible. As with any npm package, you'll be able to install your dependencies using npm install. This will reference the information in the package. json file. This should install our dependencies fairly quickly, as the only dependency we're really using is the UUID dependency that we looked at earlier when reviewing our function code. Now you're ready to deploy your application. So go ahead and use sls deploy. You could also use serverless deploy. When this is completed, you should see a variety of endpoints that have been created by the framework. You should also see that these functions were created in accordance with the endpoints. Go ahead and copy down the API URL, because we'll be using that to test the API in a moment. Now let's take a quick look at the AWS console. I just want to show you that our DynamoDB table is created as expected. Now let's scroll down slightly and go to the DynamoDB section of the console. Then we'll click on the tables tab. When we load up the tables portion in the DynamoDB console, we should see our table name. If we scroll to the right, we'll see the total read and write capacity of that table. In a UNIX environment, you can test your API using curl. You can use -X to specify that you want to test it with a POST request. Then you'll paste your specific API URL. Next, you'll add in the --data flag and then use the single quote and add JSON that looks like this, and another single quote in order to specify the data that you'll be passing along to the API endpoint. After you hit Enter, you should see a response that looks something like this. This means that your data was posted successfully, but let's go ahead and check on that in the AWS console. In the AWS console, you should be able to navigate to the table that you created previously. After doing that, go ahead and take a look at the Items tab. On the Items tab, you should see that you've just added an item successfully to the table. If this wasn't successful, I'd suggest double-checking the format of the data that you're passing in, and also making sure that you have the correct API URL from the results of the serverless deploy command. You can always run serverless deploy again, and then copy down those API values. You can also use Postman in any environment in order to test your API. You can get Postman from getpostman. com, and use it on any mainstream operating system. The first thing you'll need to do will be to change the GET dropdown to POST. Then you'll paste in the specific API endpoint that you were using with the curl command into the section here. After that, you'll go to the headers section and specify a content type of application/json. Then you'll go to the body section, select the raw button, and make sure that you pick application/json from the dropdown menu. Now we'll enter in the JSON without the single quotes, and in doing this, we'll be able to add another entry to our database by pressing the Send button. If we scroll down slightly, we'll also see the results from the API. This should also indicate that we successfully wrote our data to DynamoDB. But let's go ahead and open up the console one more time in that Items tab, and also take a look after refreshing the page. As you can see, we've successfully written a second entry to our database. Now let's take a look at the next portion of our module, in which we'll be looking at how we can test and debug our CRUD applications in AWS.

Essential Debugging Tools and Resources
Even with the great tooling of the serverless framework, there's a few other tools that you'll want some experience with to save yourself time while you're creating serverless applications on AWS. The first is AWS CloudWatch logs. CloudWatch is a monitoring service for AWS, and the logs it has for lambda will help you track down errors that occur during function execution. While you can run serverless invoke with the --log flag like we did in an earlier demo, you can't watch every function execute in your terminal, so being able to review the logs after the execution will help you find errors that you missed. You should also consider using Postman to help test your serverless APIs. This tool allows you to create and test HTTP requests with JSON text bodies in order to test your applications are working correctly. Finally, I'd also suggest getting familiar with the serverless framework GitHub Issues section. Because the framework is evolving so quickly and adding significant functionality, that's a great place to look for issues that others might have already encountered with the framework. There's also a lot of other resources that we'll take a look at that are listed there. In this demo, we'll get familiar with some of the debugging methodologies we can use to test our services. First, we'll make a change to our CRUD API, which breaks the function that writes the new entries to the database. Then we'll re-deploy it with that quirk in order to see how we could investigate and fix the issue. We'll check to make sure we're forming the request correctly, and see what responses we're getting back from the API. We'll do this using both curl and Postman. Then, we'll log into the AWS console and review some of the CloudWatch logs for our broken function. After that, we'll isolate the problem that we created, fix the issue, and then redeploy our service. So let's get to it. Within our create. js file, scroll down to line 33. On that line, go ahead and add an extra t to the end of put. Don't forget to save the file after you've done this. Back in the terminal, you can specify serverless deploy --function and use create for the function. After this is deployed, be sure you copy down the full URL for the POST endpoint. Back in Postman, go ahead and make sure that the API endpoint that you copied down just now is the same as what's in the section here. Then change the values for the petName and petBreed that you were going to add to the database earlier, and go ahead and hit the Send button. You should see this message here. Now internal server error doesn't actually tell us that much information, but it does us something useful. It means we have to take a look at the lambda CloudWatch execution logs to get a little more information about this error. Back in the AWS console, go ahead and navigate to the lambda section. In the lambda section, you might have quite a few functions by now, so go ahead and filter for the word create. You should then see the function that we have here. After you've clicked on that, you should see a page that looks like this. On that page, we'll go to the monitoring section. If you scroll down slightly in the monitoring section, you should see an invocation that just resulted in an error. You can see this on the chart on the far right. Go ahead and click the View CloudWatch logs button. These are the log streams for your function invocations. You can go ahead and look more in depth at specific runs of your function by clicking on the most recent log stream. Be sure to sort by the last event time in order to find that most recent invocation. This is especially important if you have a lot of logs. As you can see, the error log information is surfaced here. You can see there's an error that has to do with DynamoDB putt not being a function. Well, this is obviously because we changed it earlier in the module. In this case, we see that our function started correctly, but that there was an error within our create function. Now this function is located at pets/create. js. In that file, we also see that the error occurs on line 33 at position 12, which would indicate exactly where we need to go to fix this issue, or at least investigate it further. Now let's go ahead and load that file back up and delete the extra t and save it. After we've done that, we can go ahead and run the serverless or sls deploy command with the --function flag, and pass in create, which is the function that we just mangled in order to test it. Now let's go back to Postman and go ahead and try the same request again. After we hit send, we should see this response here. This indicates that we successfully fixed and redeployed our function. Another important resource for you as you continue to debug and work with the serverless framework, is the GitHub page of the serverless framework itself. This can be found at this URL. On that page, you can take a look at GitHub issues and other information about the framework itself. You can also navigate to the serverless website at serverless. com to get the most up-to-date documentation. You can also look at the serverless framework's forums at forums. serverless. com. Additionally, you can take a look at the gitter. im for the serverless framework, which can be found at this URL. If you want more real-time responses, I'd definitely suggest the community in the gitter. im page. In this module, we learned several new serverless concepts. We learned about multiple-endpoint serverless APIs that can be used to perform multiple tasks. We also learned about serverless databases, and how we can include them within our serverless framework services. In addition to these concepts, we picked up some new skills. We implemented a CRUD API that used lambda, the API Gateway, and DynamoDB. We also deployed a full stack serverless application with data persistence. And we learned how to use several testing and debugging tools, including CloudWatch, curl, Postman, and GitHub. Coming up in the next module, we'll take a look at multi-platform serverless and what that means, as well as where we can go next after we learned all these concepts.

What Is Multi-platform Serverless?
IaaS Outages and Risks
Hello, I'm Fernando Medina Corey, and welcome to this module of my course on the serverless framework. In this module, we'll look at where a serverless is moving with multi-platform serverless architectures. We'll start this module with some context by looking IaaS Outages and Risks. We'll look at cloud provider failures, including notable failures of Azure, Google Cloud Platform, and AWS. We'll review these failures and look at some key takeaways from all of them. Then we'll move into looking directly at multi-platform architectures. We'll overview what multi-platform architectures are, and the components that they're comprised of. Then we'll move into other forms of resilient architectures and provide examples of how we can add fault tolerance to our applications. After that, we'll review a few of the basic best practices in this area before we look at what we can do with our newfound skills. So let's get started. Using any technology has benefits and risks. The same is true with infrastructure as a service providers. While you gain a huge amount of benefit in terms of functionality, you do give up some control to the internals of your systems. And when a serious provider happens, you end up only being able to report the problem and twiddle your thumbs. So here are a few examples of some of the things that could go wrong with infrastructure providers. In November of 2014, Microsoft Azure made a configuration change to improve the performance of Azure storage. Usually these kinds of changes are flighted and incrementally rolled out in different areas to try to preemptively spot any issues. Unfortunately in this case, an engineer inadvertently skipped the flighting process and deployed a bug. Before this could be fixed, it caused connections to Azure Storage and VMs to fail and took several hours to fully resolve. In a more recent example, in April of 2016, Google Cloud platform was making a configuration change to traffic routing on its network. This change caused an issue, because it was made in only one configuration file and not another. Because of this inconsistency, the system was supposed to notice this error with the canary test and fall back to the previous state. But while the canary test noticed an unsaved configuration, a bug in the management software ignored it and deployed the change anyway. At this point, connections began to fail across Google compute instances, which launched a series of internal alerts at Google. These alerts were investigated and then fixed by the engineering team there. And here's one final example, just in case you think I'm showing favoritism to AWS. On February 28, 2017, an engineer in the AWS billing department tried to remove a few servers related to work they were doing. Unfortunately, the command used by the engineer had a typo, which dramatically changed the outcome and removed many servers, including servers used by Core AWS simple storage service subsystems. This removal caused connections to S3 to fail, which also cascaded down into errors for services that relied on the AWS S3 service, which was a substantial number of AWS services. So why do I show you all these examples? It's not to convince you that using infrastructure providers is a bad idea; in fact, the vast majority of companies that aren't massive enterprises are probably better off outsourcing their cloud needs to an infrastructure provider. The main reason I show you this is to make sure that you realize that bugs happen, and you need to take appropriate precautions for your organization. That might mean planning for cases like these and it might not. Another important detail here is that the principle of least privilege makes for safer systems. In many of these examples, reducing the ability of humans or programs to take certain actions may have avoided this issue entirely. And this is an important thing to bring to your own organizations. In general, it's also important to have a plan for failures. For some, this might simply mean that you're ready to send a tweet or email about your infrastructure provider having issues and apologize to your users. It also might mean that you need to plan a place to mitigate issues like this entirely, and finally, it's important to realize that even IaaS providers can fail. It doesn't happen very often, but it does happen. Next, we'll take a look at how we can try and design systems within the serverless framework that avoid these issues when they do come up.

Multi-platform Architectures
If we want to design applications that are resilient to provider failure, we need to start thinking about how to use infrastructure providers redundantly. Let's take a look at what I mean. First, let's imagine we have a sample serverless service. This service relies on AWS DynamoDB tables and the API Gateway. Let's say we're running our application code one day and AWS has an outage related to the services that we're using. We decide that the outage is bad enough that next time we need to try and replicate our service on another provider. So what do we do to prepare for this? First, there's the issue of replicating the data between data stores. For now let's assume that we take advantage of a solution that dual writes the data and that we're able to do this. The next problem is that the function code we're using is written specific for AWS and relies on AWS libraries and SDKs. We can't assume that another provider will be able to work with our service code, so how do we solve this problem? Well, we need to abstract our code. Rather than writing functions and services that only work with specific writers, we can write more generalized code that relies on a build and configuration process to be bundled with infrastructure specific libraries. That way if one infrastructure provider goes under, we're still able to keep our service running on another provider. When we think about building applications like this that can integrate with multiple clouds, one of the key issues is how we get these clouds to work together. Fortunately for us, the folks at Serverless Inc have been working on that too. In August 2017, Serverless Inc, the company behind the serverless framework, announced a tool that they're backing called the event gateway. This tool allows you to share cross cloud functions and events with AWS Lambda, Microsoft Azure, IBM Open Whisk, and Google Cloud Platform. Now this sounds perfect for developing these infrastructure independent applications, but because this tool is so new, I won't be covering it in detail within this course. But let's quickly take a high level look at how it works. With the event gateway, you can collect events from multiple clouds and centralize them through the tool. This allows you to receive the events on any cloud, not just the one they originated on. This gives you some incredible flexibility when it comes to determining which cloud should deal with which events under what circumstances. It also allows you to trigger functions on any cloud to respond to events from any cloud. Additionally, it can centralize these events from disparate cloud providers to get a better understanding of all the data flowing through everywhere. Teams can take a closer look at whatever events they'd like and even add additional functions and services on top of those existing events. Now to me this all sounds super exciting, but be sure to check back in on this project after folks have started testing its abilities and adopting it for their purposes. As for any new project, only time will really tell how it evolves. Next, let's take a look at some components and tools that we'll need while developing multi-platform services. For any multi-platform service, you're going to need to start with generalized function code. The code will perform the core business logic of your application. It will need to be supported by platform-specific libraries, enrich the functionality on a platform-specific basis. This might range from libraries you develop to help work with infrastructure providers to tooling that you use to help monitor code on one provider versus another. You'll also likely need the set of SDKs specific to the infrastructure provider you're working with. You may need to use your own libraries to abstract these SDKs and make generalized code possible. And finally, you'll have to evaluate what specific IaaS services that you'll be able to use to substitute for one another. For example, you'll need to ask the question, what can you use to substitute for the API Gateway or DynamoDB on a separate provider? But right now all of this is kind of conceptual. So in a moment, we'll take a look at some specific ways we can improve the resilience of our applications.

Other Resilient Architectures
Now let's take a look at specific examples of resilient multi-platform architectures. We'll start by looking at an example from Auth0, in which they use multiple infrastructure providers to maintain high availability services for their products. Then we'll look at how we can use AWS availability zones and regions to gain some redundancy just within AWS. And finally, we'll look more in depth about how you can use DNS failover to instrument resilient and fault tolerant services. In a talk at ScaleConf, the Auth0 Director of Engineering said that to improve redundancy and availability, we run Amazon on the West coast, we run Azure on the East coast, and if one fails, we just move over to the other one. We have Amazon as our primary. In practice, this means that Auth0 creates a fully redundant application in both AWS and Azure. They also mentioned that they keep a redundant data store within the Google Cloud Platform. They also keep DNS setup that points to the primary in most times, but in the cases of failures, they keep a very low time to live on this DNS, so that services don't continue to reference a failed service, and it fails over to the other provider in case of issues. This allows them to consistently deliver their services across the globe, even in the face of issues with infrastructure providers. But what if our organization is glued to AWS and doesn't want to move at all? Well, there are still some things we can do to prepare for the worst. AWS has the concept of regions. In each of these regions are several availability zones. An example region is us-east-1, which contains several availability zones, including us-east-1a, b, and c. Regions are geographically separated areas that are usually chosen by developers for proximity to clients that you're serving. A us-east-1 region would be faster for users in Philadelphia, than, say, folks in central Europe. These regions span the globe, and can be found in places like the United States, Ireland, and Tokyo. Within each of these regions, you'll also find availability zones, or AZs. These AZs are geographically separated within a region in order to attempt to avoid issues like floods, fires, or other natural disasters. There's a few key ways we can take advantage of this in our applications. The first is multi availability zone redundancy. An example of this would be if we were using AWS Elastic Compute Cloud, and spun up one EC2 instance explicitly within availability zone A and another instance in availability zone B. Fortunately for us, there are also some services like AWS S3, which are automatically redundant in these availability zones by default. There's also the option to replicate our applications or across regions entirely. Now this is not done automatically by AWS, and we would need to explicitly replicate all our data, files, and services between these regions. Having this sort of redundancy would actually have made an application withstand the earlier mentioned issue on AWS S3, as that was specific to the us-east-1 region. And finally, you could take both of these approaches and be redundant both within availability zones and regions. So now let's change gears here and see how we can use DNS failovers to improve our applications. Let's imagine we have a user making a connection into an application. That connection uses domain name servers, or DNS, to send the traffic where it needs to go. Let's imagine that in the first case the traffic is reviewed and routed and eventually sent over to a service running on Azure. Let's also pretend that there's a service configuration error on the application deployed to Azure, and it starts sending back server errors or isn't even connecting at all. Now every time we make a request of the service, our HTTP client will look to see if the time to live value DNS has been given has expired. This TTL, or time to live, is the amount of time which we wait before asking DNS again to show us where to go in case it has changed. In these cases, the TTL must be very low to make sure that users don't keep getting the same address for a service that isn't working. In the case where the service is failing, we can monitor the health metrics like 500 errors and automatically change the DNS setting to start sending traffic a different direction if we see too many errors on the provider. After these changes are made, we might have a second connection attempt, which again goes to DNS, and then reviews the new settings for routing, and is directed to a duplicate of the initial service on AWS. In this way, we're keeping a redundant service around for emergencies that can be swapped to automatically in the case of failures. Now it's impossible to reliably give you all the specific best practices for your current organization and infrastructure provider, so I'll leave you with a few practices to keep in mind when making decisions going forward. First, assess your risks. Determine what needs your organization has. Maybe you don't actually need to plan for the edge cases for when providers fail. Alternatively, maybe you work in an incredibly sensitive industry or organization, and you need to be aware of everything and take precautions for cases exactly like this. Next, make some backups. While it's fair to say assess your risks and only focus on the things that matter, modern cloud storage is incredibly cheap, and it's likely that there's something you should really be backing up somewhere. Importantly, you want to make and test your backups to make sure they work. And finally, after doing a review and determining what reasonable risks you can take given the situation, make sure to put in appropriate failsafes. Plan some time with your organization to make and test those changes and be sure that they address what you want them to, or else they're not worth putting in place at all. Congratulations! You've done so much in this course. In this module alone you've learned new serverless concepts related to multi-platform architectures. You've also reviewed some of the examples of other resilient architectures, and learned some best practices, so that you can implement within your future applications. So what next? Well, you're ready to make your own applications with the serverless framework. You'd also start to experiment with the other ways of building serverless applications. To learn more about lambda, you could check out my Introduction to AWS Lambda course here on Pluralsight, or you could start playing around with the serverless framework in other programming languages or with different providers. There's a lot of possibilities here, and I hope you're excited for them. I hope you enjoyed this course on the serverless framework. Please feel free to reach out to me on Twitter, or to ask any questions you might have about the course. Thanks again for watching, and I hope to see you in another course soon.
