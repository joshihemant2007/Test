In this course, Zero to Production Node.js on Amazon Web Services, Kevin Whinnery guides you through the steps necessary to build, test, deploy, and scale Node.js web applications in production on Amazon Web Services. You’ll gain the skills you need to build a Node.js production environment that's productive in development and won't fail under a modest load. To accomplish this, you’ll walk through a sample project which uses a structure that's similar to what you might find in a production Node.js application, and which you can build on as a template for your own apps. You'll also explore several modules including Express, Sequelize, and Grunt, to help you understand the different pieces involved in application deployment.

The Web Server
Introduction
(Introduction) Oh yeah, very, very stoked to be here. Hi, as Mark said, my name is Kevin Whinnery. I work for an outfit called Twilio. We make an API that lets you send and receive text messages, make and receive phone calls, and embed chat and video calling and any kind of communication experience that you might imagine into your applications. And I lead a team called developer education, and we build the website which serves twilio. com/docs and also manage a couple other internal services as well. And yeah, I'm very, very stoked to be here. One of my favorite parts about the job is when I can get in front of groups of folks and work through some content and facilitate some learning on your part on a subject that I like to talk about a lot, which is Node. js. Just a quick refresher on the agenda, we are going to be here for two days, so make sure that you take care of yourself, take breaks whenever you need to because it definitely going to be a lot of content and a lot of code for over the course of the next couple of days. And today, we're going to start off by getting to know some folks here in the room and also hopefully online a little bit. Looks like we have a good set of folks, 52 users right now online, which is pretty awesome. And then over the course of the next couple days we'll be talking about a Node. js application code base. And I'll get into some of the theory around that after we talk about the agenda here, but we'll kind of start with the basics of just running a Node. js web server, talk a little bit about some of the build tools and the automation that's built into the project that we'll be looking at. We'll take a break after, and we'll be writing code, a lot of code, in between each of these presentations. Then we'll talk about the data tier of the application and some of the choices we made there, and then we'll cap off the day by actually deploying this application to AWS. So, if you haven't already, now would be an excellent time or at some point in the near future to sign up for an AWS account. Everything that we'll do today can be done with the free tier of services. So you can sign up for an account, and then we'll actually kind of together go through the steps that you need to accomplish to get this application configured and deployed. And we'll be doing a little more manually than you might after you've done this a few times, but configuring the stack manually has some advantages in learning kind of what some of the moving parts are. So, we'll be kind of configuring our production environment and kind of understanding what all those different moving pieces are. Then tomorrow we'll be talking about the front end, so we'll talk about some of the front-end toolchain bits that we're using to build and compile our JavaScript and CSS assets for the front end. And then we'll talk about how we actually build the front end with Vue. js. Our sample application uses Vue. It's a --- it's not quite the behemoth of say React or Angular or one of those, but the toolchain and the general techniques of building an application like this will be similar across all those. And if you haven't checked out Vue before, hopefully it'll kind of whet your appetite for what I think is kind of a nice middle ground between very large frameworks like React, which kind of assert themselves into your application in a big way, and something maybe lightweight like jQuery, I think it kind of sits in an interesting middle ground between those two frameworks. We'll talk a little bit about how we create a real-time component to this application and some of the things that we have to work through in our production environment on AWS to make sure that works correctly. We'll take a little break, and then we'll finish up the afternoon by looking at some of the lesser known --- the lesser celebrated parts of web development, which are the things that you have to do after the application is deployed to make sure that it stays running in production, that it can handle a reasonable amount of load, that it is actually solving problems for your users and how to kind of instrument your application to understand how users are interacting with it. So we'll talk a little bit about those production monitoring and load testing tools and also dig into some web analytic stuff as well. And then afterwards I will definitely stick around for as long as folks want to for Q&A on both days. And we should have enough time in between each of the presentations and the brief demos to answer any questions that folks might have as well, and I'm happy to dive as deep as I can into those different areas. So, the title of this course is Zero to Production with Node. js, and the reason I wanted to approach the workshop like this in a slightly different way, Mark said I actually did do a workshop many years ago which focused on just a sort of basic introduction to Node. js and some of the key modules that you're likely to use in your journey as a Node. js developer, but that is really like kind of the easy part, right? Like writing code that does stuff is something that we're all reasonably good at. We can download a module. We can dig through the documentation. We can figure out how it works. What is not necessarily super well understood is how you fit all those pieces together in an actual code base that builds an application and solves a problem for a user. So, the goal of this workshop is to get everybody in here walking away with a starting point. I imagine that all of us will want to customize some of the components that we'll find here quite a bit. But I would like us to at least walk away with a starting point that has productive development environment that you can iterate on quickly as you're building features and also one that won't fall over under a reasonable amount of load. We're not going to be creating a deployment infrastructure to run Netflix or Twitter, but we should be able to understand what the performance ceiling is of our application, and we'll hopefully leave today with the tools to understand what that is. So, we'll be walking through a sample project, which uses a structure which is very similar to what you might find in a production Node application, uses a lot of the same techniques and tools, and is something that you can build on as a template to build your own production Node application. Over the next two days, you'll notice we're not going too deep on any one module. We'll talking a little bit about Express, we'll be talking a little bit about Sequelize and Postgres, we'll be talking a little bit about Grunt and npm run scripts and some of the command line tooling, but again, the objective here is to get a sense of like what are all the pieces that you'll need to bring together to actually deploy a Node. js web application. The other bit that I wanted to expose you to with this content is usage of newer ES2015, ES2016, whatever you want to call it, features of the JavaScript programming language that are showing up in Node 6, which is the version of Node that I wrote tested this content against. And using transforms, we can actually also use that same code in the browser, so I wanted to give you a little bit of exposure to those new JavaScript programming language features that you may or may not have started to use in your own applications already. But mostly, the most important bit is that bit down on the bottom there, is taking the time to write code. The collective opportunity cost of everybody here in the room and of the 50 people online is tremendous. There's nothing more valuable in the world than an hour of developer's time. So number one, I really appreciate you spending that time here with me and learning about Node. js. And number two, I think that the best investment of that time is just getting soak time writing code. There's no substitute for actually digging into code, debugging stuff, referencing documentation, figuring things out. So this workshop experience has been optimized to allow you to write lots of code with support from everybody here in this room and support from me, but really to get you that time in repetition. You will be your own best teacher here in the next two days.

The Sample Application
But again, the goal is to get a 10, 000 foot view of what a production Node code base could look like and some of the tools that you're going to need. So, in order to do that, I did put together a sample application that is based on a sample application that you've seen probably 100 times before, which is TodoMVC. I called it TodoMVC++. Thank you, I'll show myself out. But what the application does is not the important bit. Basically, what this thing is going to do is present a rich front end that shows a todo list that you can edit. Most of the time this TodoMVC application just lives on the client side and then persists stuff to local storage. We've extended it to actually store those todo list items in a database, which is driven by an Express-based API layer. So, the what of the application is not as important as the how, so that's we're going to be focusing most of our time on. So, that's why I went with a sample application that we already understand so that we can focus the layout of the project and how we go about our business of building the project. We will dive into a few specific details, especially tomorrow when we talk about Vue. js and how Vue. js handles rich front-end development tasks, things like that, but we probably won't go too deep into any one tool. Again, the focus is on that 10, 000 foot view. Mirror the techniques that you might really use in your Node. js application. I'm sure you'll riff on what we put forward. It also doesn't do everything that you might do in production. Just at some point you have to draw the line in a learning experience like this one, so there's stuff that you're going to want to do in production that you won't see in here. So we're not going to do the DNS configuration to route a domain to our elastic low balancer instance. We don't implement cache-busting file names based on the hashes of CSS and JavaScript content. So there's definitely areas in which we can go further in our road to production, but this is kind of a baseline that you can use to build out those other things as well. It's going to do most of the big things the way that you're probably going to want to approach it. The code is structured using the Google JavaScript Style Guide. I don't know if it's the best one, but it's the one I use, and as long as you have a style guide, I feel like that's pretty good. And it also combines a set of technologies that I use personally and have used for a long time. And again, there might be some that you prefer over another. Maybe you think hapi is better than Express, and maybe that's true, but there's --- this is a set of technologies that work together pretty well, and I invite you to riff on that as we go through.

Exercise 0: Setting up Shop
So the first thing that I want to do before we get started is to ensure everybody is at a good place with their local development environments. So that GitHub repository that I sent out before contains what should be at this point a fully functional Node. js web application that you can run locally. So, if you head out, if you haven't gone there already, it's actually up on my GitHub repository, guthub. com/kwhinnery/todomvc-plusplus, and thanks to some great QA from here in the room earlier I believe that README should take you all the way through actually getting this thing set up locally on your laptop, and I think that is the first step that we need to accomplish before we get on to some of the other things. So we'll take as much time as we need, hopefully not to much time, just to make sure that we get everybody with a functioning local development environment so they can start playing along at home. So, let's go ahead and do that. And I'll be coming around to help folks as they might need help. The one potential tripping point for a local development environment may or may not be getting like Postgres up and running. Postgres can be kind of a bear to configure for local development, so if you want to take sort of a shortcut, there is a service out there you can sign up for called ElephantSQL, and you can actually provision a shared Postgres database for free, and we can drop that database configuration directly into your application. So, I would invite you to go through the README, start taking those steps if you haven't already, and while you're doing that, I'll very briefly kind of go through the structure of the application project as it exists today so you have a sense of where to look for what. So, I'll browse it out here on GitHub, and the bits that I'll draw your attention to right away are this bit, the Gruntfile, which we'll dive into a little bit later, and that's sort of the nexus for most of the administrative commands that we'll run for this application. Grunt is a general purpose task runner, again, that we'll dive into in more depth later. We have a package. json, which contains all of our project's dependencies that you'll be installing, and then we have a bin directory, which contains the JavaScript file which actually launches the HTTP server, which powers our application. And then we also have --- right now we just have one custom Grunt task located in this directory as well, but anything having to do with any custom code we're going to write for the command line lives in there. The config directory, this is how we use it. It's kind of --- it's sort of loosely based on Django configuration, but we have a this index. js, which has soft of the global defaults for the application's configuration, and then we have a configuration files with overrides for the development production and test environments. So you can dig through that if you like. But we also have this file, user. save. js, which doesn't have anything in it right now, but if you create a user. js for local development, you can override any of the configuration settings there, such as a custom database URL perhaps, if you go the route of spinning up an ElphantSQL database rather than having a local one. So, configuration lives there. We have this db directory, and the primary bit that you'll be interested in is this is where we've configured our ORM to place the migration files for this application. We are using a relational database for this application, so as we modify the database, we'll have to create migrations to move the database from one state to another. If you've used frameworks like Rails or maybe Hibernate before, you're kind of familiar with this concept. The src directory is where most of the fun stuff lives. The browser directory has all of the front-end JavaScript code and SCSS files. The static directory contains things that will be copied over verbatim into the web route of your application. The server directory contains our Express web app, our Mocha tests for our back-end code. It also contains the model directory, which configures our database connection and defines the model objects for our application. We have controllers, which have the routes for our application, and this todos controller, which contains --- which actually powers all of the routes in the application. If you don't know what these things are, don't worry. We'll kind of dive into more of them as we go. And then we also have the views directory, which actually has the HTML which will server the front end for this application. And there is a tiny bit of shared code. In my experience, the opportunity for code reuse on client and server is not maybe as great as you would hope. The bigger win is the ability to write the same kind of code on both client and the server, but there is a tiny bit of affordance for shared code across platforms. And if you have any questions about other stuff as we go along, I'd be happy to go specifically into any decisions or the other bits there, but many of them we'll go over as we go through the course. So, at this point, what I would like to do is ensure that everybody gets the application going locally to the point where they can run Grunt in the top-level application directory, which will fire up a couple of neat tools, which will monitor your application project for changes and then execute compilation steps or relaunching the application based on changes. So what you should be able to do with a running Postgres database and a valid database connection is have a persistent set of todos that will stick with you across page refreshes because in this instance they're actually being stored in a Postgres database. So this is the step that I would like to get everybody to, so we'll spend a little bit of time doing that before we dive into some of the code that drives the web server. Alright, so I re-mic'd myself because I was talking about the option of using ElphantSQL rather than a local Postgres database. It looks like my configuration might not be totally sound as yet because I haven't run the migrations because I changed my database URL, but I didn't run the migrations. So, I'm going run sequalize db:migrate, and that should go across the network and update my ElephantSQL database. Show us where you changed the config to. And again, where I changed that config was I created a file called user. js in the config directory, and I didn't override for this databaseUrl property. And you can actually copy the configuration, like say from the test environment and just bring that over, and then you can paste in that Postgres URL you get from ElephantSQL. And now when I refresh, I get an empty list of todos, which is okay. And if I go back to the UI, I should start being able to pop in some todo list items. So the steps there would be to update the configuration to your ElephantSQL databaseUrl, to go out to the terminal and execute sequalize db:migrate, and then you can rerun your development server with Grunt, and you should be off to the races from there. What is the get path to ElephantSQL? It is --- if you go to elephantsql. com, you can sign up for an account, and then you can create your own. What are the limitations on ElephantSQL as far use? The free database is only 20MB, so that would be the big one right there, but you can create multiple free databases if you would like. Sharding across frees. (Laughing) Yes, maybe it has some other fun features as well, but use this a couple times in workshops just for something quick and dirty. Alright, so we have a couple folks online reporting an issue that was --- if you loaded up the README earlier, we actually discovered some bugs in it, I apologize, here in the room earlier this morning. If you're loading up the page and you don't see --- and the style sheets aren't being loaded, run grunt collect_static, and collect_static has and underscore between the collect and static. The README on GitHub has been updated to reflect this, but that'll copy over the static CSS files that you need to be in that public directory. So, the README has been updated to reflect that. What I'll show one more time is the fastest path to success with Postgres, which I think is where most people are tripping up. If you're on the repo and following the README, we'll start kind of right from the beginning. You're still going to want to do this. After you download the code, if you haven't already, you'll want to do npm install in the directory in which you downloaded the code. You'll want to do npm install -g grunt-cli sequelize-cli, and that'll put two new commands on your system path. One is called Grunt, and that's the task runner that we're going to be using, and another is called Sequelize, and that will control the database migrations and the other stuff that you'll need to do later. And then for setting up the database, if you don't want to go the route or were not successful going the route of getting a local Postgres database going with these instructions, simply forego these instructions and create a database using ElephantSQL, which you can sign up for a free account at elephantsql. com. Then, once you have signed up for one, the first thing you're going to see is a URL for your database info, and you can use this connection string in lieu of a local Postgres database. So to connect to this database rather than one that's running locally, inside your application folder here, and this is the code that you downloaded from GitHub or you cloned when you got the repository, you'll create a file called user. js, and you can initialize the contents of that file with what's in the test. js file or similar, and then you will copy in your ElephantSQL database string into this file, and then the application will connect to that ElephantSQL database rather than one that's running locally. After you do that, then you can continue along with rest of the instructions in the README. So you will then do sequelize db:migrate, and then you will do grunt collect_static, and then at that point you should be able to run the application locally just by running the grunt command. So, we'll make sure that we get everybody to that step, but again, I think the fastest path to success is signing up for --- if you're struggling with Postgres locally, sign up for an account here on ElephantSQL and just go ahead and use that. It'll be totally fine for our local development purposes. Alright, so we will come around, make sure if anybody still needs help after, and when we get into the next exercise, we'll make sure everybody starts by getting the local development environment ready to go.

Serving HTTP Requests with Express
The first part that we'll start to break down in our application stack is one of the most important components that we have in the stack, which is the Express web server. It handles incoming HTTP requests and renders HTTP responses with HTML and JSON and is kind of a core component of this and any other Node. js web application. It's very similar --- it's very similar in style to say Sinatra, if you've worked with that framework in the Ruby world or maybe Nancy from the C# world or any number of other HTTP microframeworks out there in that it is very, very minimal, and it's also by far the most popular HTTP request handling library out there. And loath to call it a web framework because it's not really a framework. It's a module that's really good at routing HTTP requests to a chunk of JavaScript code that you would like to run on the server. Unlike a framework like Rails or ASP. NET MVC or Spring, batteries are absolutely not included with Express. Anything you want Express to do, you have to install as an additional middleware or some other add-on to the software. So, there's good things and bad things about this. The good thing is that you never end up with more bloat in your application than you absolutely require because you're consciously choosing every single piece of software that's executed when you're handling an HTTP request. The downside is that there are no intelligent defaults. So, if you want to do something like, oh, I don't know, parse incoming post requests from a client, you need to install an extra piece of middleware to do that, which we have in our sample application. So there's good things and bad things about that design. The reason why I still like it very much today is because it's not magical. It's very easy for me to reason about what this piece of software is actually doing. And the reason for that is it's all based on a middleware stack. Before we go any further, does anybody know what this picture is a reference to by any chance? No? Alright. So what you all need to do today when you go home is fire up your Kindle and download Ready Player One is the name of the book, and this is the stacks, which is a vertical trailer home --- trailer park, which is featured in Ready Player One. Fantastic book. You will not be sorry that you did that. But the middleware stack is all Express is. It's just executing a set of JavaScript functions to process an incoming HTTP request, which is coming into your web application. So, when your web app receives a HTTP request on port 3000 or wherever it's running, usually what happens is Express will execute something called a middleware, and that's just a fancy name for a JavaScript function which takes in a HTTP request, executes some logic, and either returns a response or passes it ahead to the next handler in the middleware stack. So in a typical Express application you have some number of global middleware that are going to run on almost every request or every request, and we'll see how we configure that here in a second. We'll also have some specific routes in our applications, so we'll want to handle specific URLs with specific logic for maybe getting a todo item out our database or executing other logic when a certain path is requested within the application. And then if the global middleware and the application routes are unable to process the request or God forbid an error happens during the execution of those middleware, we can have other middleware that's lower down in our stack that will catch errors like a 404 Not Found, like if we don't have a handler to handle a certain kind of request, or a 500 error if one of our other middlewares throws an exception. So we have this stack of JavaScript functions that are executed sequentially. At some point, someone is going to return a response of some kind, hopefully, is kind of how this works. And if you've done any server programming or developed rack applications for Ruby, you might be kind of familiar with this concept of a middleware stack.

The Web Server Code Demo
For those of you who haven't maybe seen Express before, I'm just going to do a very brief demo of some of the key things that you'll want to accomplish when you're building an Express application. So, I'm just going to do one from scratch to kind of demonstrate how Express works. And here in this directory I've actually already downloaded the Express module so I can write Node program that's going to using it. So, I'm going to create a new file, index. js, which is going to be our simple Express application, and I'm going to open that up, and in my text editor here I'm going to be able to require the Express module. And I'm going to repeat some things that if you've been doing Node for a while you probably already know, so I apologize for that, but there are lots of folks in the room and online who some of this stuff might be new. So I'll break it down as best as I can, and feel free to shoot in any questions that you have along the way. So the first thing I'm going to do is require Express as a CommonJS module. And the way that --- and this is the modularization mechanism provided natively in Node. js. And if we could in the text editor too, if we can bump some _____. Sure thing. I could probably go a little bit bigger. Thank you. Alright, so there is a global function in a Node program called require, and when I pass in the name, it's going to try to find a module of that name. So if it's just a string like this, like express, it's going to reference the Node module path, and the way that that is resolved is in a couple of ways. First locally, here in this directory, if there is a node_modules folder, it's going to look in here and say like, oh, there's something called express in this node_modules folder, so I'm going to load that module from there. And after that, if it doesn't find express in a local node_modules folder, it'll actually start walking up the folders up above that looking for Node modules, and failing that, it will look for globally installed Node modules. So if you installed Express globally, it will require that version of the module. If you want to require local modules that you've created, which will do here in a second, there's a slightly different syntax for that. But now I've created this express object, which represents the Express library. And I'm going to create an application just using the express constructor, or express builder, I would say. It's not really a constructor function. And with that application, I can start defining routes to handle incoming HTTP requests, or I can start configuring middleware. But before I do that, the express module does have the capacity to start like a built-in HTTP server, which we'll do on port 3000. So that'll actually start listening for requests. Typically you don't use the built-in HTTP server like this, but for demonstration purposes we'll do that. And there's a couple ways where I can define handlers in my application. And app. use is a way that I can define a middleware function for my application. So use expects a function as an argument, and a middleware function will have arity of three, so it'll take three arguments, which is going to be the request, the response, and next. And next is a callback function that is going to be called to continue on processing the request after the middleware has finished executing. So in this case, a common use case for a middleware is to like check for an authenticated user and then add that user's information to the current request for downstream processing. So I could say request. user = name Kevin or something like that. Maybe I would check the cookies associated with the request and get some data there, but I would do some processing on the request and/or the response, and then whenever I'm done executing that logic, I'll just call next, and then the request will continue processing. And then when I want to handle specific --- uh yes, question. Is there a meaningful difference between declaring your app variable using a let versus a var? The difference with let is block scope. It's essentially the scope in which the variable is declared. So with var, the scope is always going to be like whatever the enclosing meaning of this would be. So like if this code were to execute in the browser outside of a function scope, if I did var app, that would declare the app variable globally. Let declares variables within the scope of a block. So in this case, this method is visible to my entire application, but if it was declared within an if block or some other kind of block within the application, it would be only scoped --- it would only be visible within that block. So you can basically use it in place of var, and it usually means what you want it to mean, which is declare this variable inside the current block scope. So, and the const is for variables that you do not --- that you don't expect to change. They should be immutable, essentially. So, we create the application, we define some middleware, and then we can handle specific HTTP methods on the application as well. So when a browser makes a request to our web application, that's an HTTP get request. So on our application object, I can say get. Whenever there's a get request to /hello in my web application, I want to execute some logic. And in this case, I'm just going to create a callback function with a request and response. Technically, I don't even need these other parens, but it's a hard habit to break, and I'm probably not ever going to use it, never going to break it I don't think. But at this point, I'm probably going to render an actual response to this route. So, now I'm going to say response. send, and by --- and response. send will send back different content types depending on the kind of object that you pass it. If you pass it a string, it will assume that you're trying to send HTML back to the client. If you pass it a JavaScript object, it'll serialize it to JSON and assume you want to send JSON back to the client. So in this case, I'm going to send back a string that says Hello request. user. name, and it's going to use that value which was defined in the middleware to actually insert that name into the HTML response that I'm sending back. And this is another newish feature that I won't beat in, but it's possible to do string interpolation in this version of Node using the backticks rather than the single ticks. So, it's just one of those niceties that is snuck into the language recently. So if I save that up and run the program --- No. Oh yeah, I'm actually already running something on port 3000. That would be why. Okay, so now I have a little Express application running on port 3000, and if I visit hello in the browser, I see Hello Kevin, which is based on the result of that middleware stack that I defined. Now, the way --- and typically if you as an application developer are creating your own middleware, you don't probably want to define middleware in this way. Most of the middleware that you see that you'll use from npm or elsewhere in the ecosystem will actually define a constructor function, which returns a function that you can use as middleware. So in this case, I'll just demonstrate that technique really quickly by creating a new file. We'll call it middleware. js. And the --- and in this case, I'm going create a function that's going to accept some arguments and then produce a middleware function as a response. So, I'm going to do a function called create, and it'll accept an argument like a username. And this create function is itself going to return a function, which will be the actual middleware that we --- that's actually not necessary. So, this will be the actual middleware function that you can mount in your application. So, if we copy over kind of what we did here, we get request. user. name. So we put that processing logic inside of this middleware function, excuse me, just do that, but now instead of like hard coding the name that we send back, we'll just pass in the username that was configured here. And in this way, you can essentially parameterize the creation of your middleware so you can have middleware behave differently based on the arguments that are passed in. So typically, this is how you're going to want to construct some middleware. And then to actually make that create function the public interface to my module, I will assign that function that I just created to module. exports. So now I can require my own middleware by passing in a relative path to the module that I'd like to require. So, in this case, it's going to be in the same directory, and it was called middleware. So, I'm going to require that middleware function, and now, rather than having the function in line there, I'm going to say --- I'm going to change up my middleware to pass in a different name. And now, if I kill that process and restart it, go back out to my server, I have a parameterized middleware that does something slightly different. So again, if you're developing your own middleware, that's typically the way that you're going to structure it with a factory function that will create --- that will in fact return a function based on the parameters that you pass in. Alright, any questions on Express or middleware right away before we keep trucking or things that you'd like further information on before we try it ourselves?

Alternatives to Express
What we are going to do first is actually look at a few things that you could use other than Express. And over the next couple days, like as we introduce technology in this stack, I'll try to talk about some of the other choices that are out there because, again, I think one of the strengths of the Node ecosystem is there are a million ways to do everything, while at the same time a weakness of the Node ecosystem is that there are million ways to do everything. So, I will try to cover some of the other things that you can use. Hapi is another framework which is focused on creating server-side APIs, although it can do server-side HTML rendering just like any other framework out there. It's used in a lot of popular Node web applications and has a lot of --- has a lot more features than Express, a lot more behavior out of the box, which can a be a good or bad thing depending on how you look at it, but it's another mature robust framework out there that's definitely worth your consideration. On the sort of the furthest end of spectrum from Express is a framework called Sails, which strives to be sort of a full stack framework in the tradition of Ruby on Rails. So it provides its own ORM, its own configuration system, and sort of tries to pack in all the things that you probably need for a dynamic web application into the same package. And it's pretty okay. I know a few people who have used it. And if more of a full stack framework appeals to you, it's definitely worth checking out. The caveat that I'll put out there, which is again in my old age the way I choose software is essentially on like how well it's documented, like how much effort do I have to expend to do something useful with this piece of software. And Sails is documented okay, but the thing that's kind of a bummer is like you don't find a lot resources for it. Like there aren't a lot of people writing Sails tutorials or sample applications and that sort of thing, so you'll find yourself in the wilderness quite a lot when you're using Sails. The other one that I would throw out there, which is sort of a forward-looking framework built by some of the same people who built Express is called Koa. And Koa had in the past created a generator-based API for a serving HTTP requests. They're now moving towards like fully embracing Promises and the async await syntax that's emerging in newer versions of JavaScript, but it's definitely a bleeding edge type thing. And again, because I'm a crusty old man like that, that bleeding edge stuff gets old for me pretty fast, but it's definitely a cool one that's worth monitoring that could definitely pick up steam, so.

Exercise 1: Hacking on Express
Now, we get to what I hope will be the fun part, one of the fun parts of the day, one of many, hopefully, where we spend a lot of time actually hacking on this application. Now, unfortunately you guys have already found a few bugs that I did not intend to put in the application, specifically around the README and the setup experience. However, there are actually a lot of things in this application that leaves something to be desired. In fact, you could say that I have issues, in fact, many of them, seven at the moment that I could use your help on. So what I would like to do over the next, we'll see how long it takes, it may be 20 minutes to a half hour, maybe more, we can kind of play it by ear, I have three GitHub issues here labeled exercise1, and those are GitHub --- those issues are around features or problems with the application that I would like for us to collectively add. So, what I would like for you to do is to pick one of these issues or get through all of them if you're feeling particularly industrious and try to add the feature or correct the problem that is described in one of these GitHub issues. So, for example, issue #7 is for a feature addition. So at Twilio, all of our HTTP APIs will return a header called X-Shenanigans-None because one of our core values as a company is no shenanigans, so we return this header with every response from the API. But sadly, this API for todos does not return the X-Shenanigans header. So we should look at expanding the implementation we have with Express to add that header to every request that serves our todos API. So we could send it back with every request, but I've scoped it here to say like only do this for the todos route, see if we can for those API requests return back this X-Shenanigans header. So that's one of the things you can tackle. Another thing you might want to tackle is this piece, which is not sending a Powered By Express header with every response. So, if you were to look here at this application, which is already running in the Network tab in Web Inspector, where is this going to be? Where's my? There we go. I just have to click on the resource there. So if you look at the headers that are sent back with the response, there's a X-Powered-By: Express header that gets returned by default in an Express application. And it's innocuous enough, but when you're taking an Express application to production, usually you don't want to include that extra information because it's possible that an evildoer could launch a targeted attack on your application with the knowledge of what web framework you're using on the back end. So, it's not the hugest deal in the world, but if --- it's a good thing not to do in a production Node application. So removing or disabling that header is a good thing to do. The other piece that you might take a look at is the logging middleware for Express. So right now we don't use very much middleware in the application, and one of the things that would be nice to do would be to actually to log the HTTP traffic that's coming into the application. So there's lots of modules out there that do this. I've linked to one called morgan, which is very popular to use with Express applications. So adding some HTTP request logging middleware would be a nice feature to take on as well. So, if you would like to take one of these on, you can just hack on it locally, and if you are comfortable with the Git workflow, you can actually send a pull request to this repository with the implementation of the feature that you implemented. And if we have in 20 to 30 minutes time a few pull requests that show how to do this, I'll go head and merge them in; otherwise, I will cheat, and I will show you how it is to implement, like we actually implement each of these bits. But I would vastly prefer to merge a pull request from one of you, so if you would like to take on one of three of these issues in the next 20 to 30 minutes, we'll say 30 minutes because there are still a few folks that are getting their environments set up, but we'll play it by ear. No sooner than 30 minutes will we reconvene and talk about some of the solutions.

Exercise 1 Solution
You all have risen to the challenge very effectively here. Good job team. I knew you could do it. I had nothing but the utmost confidence. I perused the pull request that came in for these specific issues, and we have great solutions for all three issues. So we have two that are clearly winners, like they're what I was looking for, and then we have one that is correct and one that is more correct, so we'll look at both the correct one and the more correct one. So #1 we got from juan267 who I imagine is hanging out on the live stream. So what's up buddy? Good job. This is the most minimal edition of the morgan logger that we could've added. So what he did was he npm installed that module using probably the --save command, or he might've added manually. That also is a possibility. But when you know that you want to add another module to your project's dependencies, you can execute this command, npm install, and then --save will actually write the dependency to your package. json so that the next time another developer npm installs your project they will get that dependency as well. So, it's always a best practice when you add a dependency to add it to the package. json as well, and npm install --save is usually the best way to do that and lock in the current version of the module. So probably what we had here was an npm install --save morgan, which would add that dependency to our package. json as we have here. And then we require the morgan module, and by the default behavior of morgan is to log HTTP requests to standard out, and he's mounted that middleware there in front of just about everything else here. So, we will get that HTTP logging output on every request to the server. So, I'm going to go ahead and +1 that, so +1 for me, and I'm going to merge that down, so good work. And we'll pull that down here in just a little bit. Alright, so the other one we did was to take a look at removing that X-Powered-By --- actually, we'll do that last because that has the two answers. The other one was adding and X-Shenanigans-None header to the response. So we have a couple of submissions there, so we'll go to this one from James that fixes #7, and this is probably the lightest weight way of doing it. In the webapp. js he added a new middleware, just declared it in line, which is probably fine. You could've created another file, probably would've been overkill here. He sets the X-Shenanigans header on the response using response. set, which is a method available on an Express decorated HTTP response object, and then he calls next to move onto the next piece of middleware. And that's all we need to do to add that header to the todos routes. So I'm going to +1 that, great work, and I'm going to merge it down. Awesome! So now for the removing the Powered-By Express issue, we had a couple approaches, both of which are --- both of which work, one of which is slightly more correct, so let's see if I can find the one. Okay. So this is the version which is correct but not as correct as it could be. So here we've defined a global middleware, which is going to be executed on every request that will remove that header X-Powered-By from every response that we send back. And that will have the desired effect, but it will also execute on every single request. So, it's a little bit insufficient. We don't necessarily need to take that header off on every single request to the server. So probably what we want to do is actually disable that option in Express. So I think we have a pull request here which does just that, so within the configuration options for Express. So Express has this app. --- on an Express app has a set and a get function, and those will set configuration properties and also get configuration properties. In this case, he is setting a configuration property, this x-powered-by header to false so Express will never send it at all. Yes, question. Is there any meaningful difference between doing this method of. set for the header to false and doing. disable of the entire header? App. disable x-powered-by specifically. This one's not a header. It's a config value. Okay. So this one's on app whereas the other one was on response. Right, but you can also do, at least according to the Interwebs, you can do app. disable x-powered-by in the same area in webapp. js in the rest of the middleware. Yeah, I believe there's also a signature there for disable for Boolean properties. So like this is expressly just setting a property to false. Disable, I believe, toggles a Boolean property like x-powered-by to false, so they're equivalent in this case. So the correctest might've been app. disable x-powered-by, and it would've saved us a couple characters there if we wanted to go that way. So, go back out here. I've got some +1s. James added some docs. Look at you. Good work. I'm going to add my own +1, and we're going to merge it down. Super job! So now, if we go back out here, we've got some add logging middleware. I thought we would've closed this out, and so. I think we just didn't mention it in the commit message, which is fine. I'm going to go ahead and close this out. So, 30 minutes later we just knocked out two or three GitHub issues on our project, so great work. We're going to do that again a couple more times here today, and then I've got a fresh batch that I'm going to load in there tomorrow for you guys to go crazy on as well. So, knocked this first one out of the park. Very nicely done.

Build Tools
NPM Scripts and Grunt
We did a little bit of hacking on Express. Now we're going to move into a part of the stack that we've already been using, but we're going to try to dive in a little bit to understand better how these things work and what they can do for us. We've been executing these commands, these migration commands, these Grunt commands, and these are provided by some of the build tooling that we have in this project. Now build tooling, there's a lot of different choices out there, but the two that I keep coming back to are npm scripts and Grunt, and Grunt tasks in general. And I'll kind of take you through what each of these things do. So, npm scripts are commands that you register in the package. json of your Node project and can execute arbitrary shell commands. So, it's essentially like adding aliases for commands that you could run from the command line yourself. However, there's a couple things that you get for free when you do an npm script like this. Probably the most notable one is that if you have npm installed locally, any modules which should be command line utilities that have some kind of command line option, those utilities will be added to your system path when you run a script through an npm script. And we'll see what we mean by that here in a moment. It's also a conventional way of interacting with Node projects. So the npm defines a set of, I don't know, about 15 different scripts that it sort of expects to possibly be present in a package. json. There's a npm start, a npm test, a npm after install. There's a few like lifecycle hooks and other bits that npm will sort of conventionally expect to possibly be present in a package. json. So by conforming to that convention, another Node developer who may or may not have ever seen your project before knows well if I execute npm start, like probably something is suppose to happen. And in fact, lots of Platform as a Service providers including Elastic Beanstalk, which is the bit that we'll be using later on this afternoon, will look for that npm start script as the primary way to kick off your Node. js web application process. So having these npm scripts is usually a good idea anyway. And then the other one that I use is Grunt. And I'm already seeing some questions in the chat about Grunt over gulp or whatever, and we'll talk about some of the conventions. It largely does come down to preference. Gulp, in some scenarios, you know, performs really well, so that's really great. Webpack you can actually use with Grunt, and it's really good at doing like a subset of frontendy stuff. So there's lots of different choices out there, but the reason why I kind of stick with Grunt is the mature plugin ecosystem. There's a ton of plugins out there that do 99% of the things that I want to do, and it's better than other solutions out there, I think, at like synchronously orchestrating tasks. So if you want to do this, then this, then this or do a couple things concurrently and then another thing, Grunt makes that easiest. And there are other tools out there all that stuff is still totally possible, but the code becomes, for at least for my simple brain, possibly a little bit more confusing. And it's, you know, --- there are lots of people out there, in fact, like the Lloyd who lives here in town in the Twin Cities does Node DevOps for Walmart, and he, for most of their orchestration stuff, his like method, his weapon of choice is like Bash scripts. Like they just have like a ton of like really Bash scripts, which like run all of their deployments and do all that stuff. So ultimately it comes down to preference, but the thing that I think Grunt helps with over like just pure Bash scripts and npm scripts is orchestrating and like building tasks that work together.

NPM Scripts and Elastic Beanstalk
I mentioned this before, but the npm scripts have a special relationship with Elastic Beanstalk, which is going to be our deployment environment here this afternoon. The three npm scripts that Elastic Beanstalk cares about is npm start, which is going to be the command that's run to actually start up the web process that's going to serve requests on your EC2 instance; prestart, which is the command to run prior to the start; and poststart, which, as you might imagine, is command run after the start command. So, wherever we have an npm start is going to be the way that Elastic Beanstalk launches our Node process. In our example, we're just launching our server process with the node command, but you might imagine we could do other things like start it with a process manager like PM2 or nodemon or forever or something like that rather than just using Node directly. So this is the opportunity we have to kind of configure how our Node process is run.

Build Tools Code Demo
So at this point, I'd like to do a quick demo of how you create scripts that run in both of these environments. So, I'll start off by doing a run-through of the tests for the application that we already have to demonstrate some built-in npm scripts. So, first, oops, I'm going to pull down all those tasty bug fixes that you guys worked so hard on. And now if I come back out here, let's move this back down to a single column and see if we can blow it up a little bit. Oh wait, not that one, this one. Alright, so the ones that we have in our project right now are the npm start, which executes our server process, then we also have this guy, npm test, which is another one of those conventional bits that npm knows to execute. And in this case, we're setting the current Node environment to test, which is going to change our application's configuration a little bit. Mostly I think it just hits the test database rather than the development one. It's going to run a command called mocha, which is a testing framework for Node. js, and it's going to run all of the tests in this test directory. Now if we go out to the terminal here and execute mocha, we can see that the mocha command isn't found, like it isn't installed globally, it's not on the system path, but if I run npm test, --- No! What's that? Server model is db. So I have a problem with my --- problem with the test. Clearly I haven't run them enough lately. Going to have to fix that. But what did happen is the mocha command was run being passed this argument. So, the npm scripts, in addition to whatever is on the system path normally, will also look another place for binary commands. So if I look in the node_modules directory, there's this subdirectory called. bin, and that contains all of the potential command line functions that were installed by any of my local packages. So here is that mocha command which will invoke the test harness which I clearly have to fix. So, a npm script is a way that you can run a script using a locally installed Node module so that somebody who wants to run that task doesn't have to install your command line utility globally, which is kind of gross. All they have to do is clone your repository, run npm whatever, and all of the command line utilities that are necessary to execute those scripts will automatically be added to the path, but not pollute the global system path. So that's the primary benefit of using the npm scripts. So in addition to these scripts in the package. json, we have the ability to create Grunt tasks. Now the sort of conventional location for configuration of Grunt is in a Gruntfile. js at the root directory of your application, and within this directory when the grunt command is executed will load up different tasks that our Grunt task runner can perform. So we have browserify and sass targets, which we'll talk about tomorrow, but we also have a couple of bits that we've already been using, which is this watch and nodemon command. So, watch is a third-party module that's available for Grunt, and you can say that --- you can basically specify different targets within this that will watch this set of files. So in this case, if any of our browser JavaScript changes, we'll execute the browserify task. And in this case, if any of our Sass files changes, we will execute the sass:dev task. We also have the nodemon task, and nodemon, even if you install it globally standalone, it's a really useful utility for monitoring a Node process. And if JavaScript files change within the directory that Node process was run, you can automatically restart that Node process. So it's a very handy development tool that will I'm sure be in your belt if it isn't already, but there's also a Grunt plugin for it. So in this case, we pass in the script that we would like to execute, and then by default nodemon will monitor the entire current directory for changes to JavaScript files. But in this case, we told it to ignore the node_modules folder, so we don't care about any changes to the dependencies. We ignore the public folder, which is where Grunt will put our compiled static assets. And then we also ignore any browser-specific JavaScript because that's not going to impact server code at all. And then down here at the bottom, we have this concurrent task, which is again another third-party plugin which will run these two tasks, nodemon and watch, concurrently so that we can both recompile our static assets and watch our server-side code for changes at the same time when we run this task. Because otherwise, unless a task returns something, Grunt won't move on to the next task. So in this case, if we want both of these tasks to execute concurrently, we have to use this special module to make them run at the same time. And then down here is where we actually configure Grunt to load in those third-party tasks, so we have grunt-sass and browserify, we also have watch, and nodemon, and concurrent, and then we also have shell, which just executes shell commands which is a fairly simple bit to do. And then we --- the default command is what happens when you just run Grunt with no arguments. And in our case, what we want to do is first we want to compile --- like this is like essentially the way that we provided to run our development web server. So what we're going to do first is we're going to compile a development version of our style sheets, we're going to run our JavaScripts through Browserify, and then we're going to start up both our watch and our nodemon tasks so that we can continually recompile and rerun our Node app as we run. And we also have some custom tasks, which are just collections of other tasks. So this collect_static task, which you had to run eventually to get your local development environment to work, runs, and init_static task, which we'll look at in a second, and then it runs the sass task, but it's for dist rather than dev, which will actually minify the output at CSS as well, and then we have the browserify task there as well. And this init_static task, if you're paying very, very close attention, you might notice has not been defined anywhere in our Gruntfile. That's because it's a custom task that we wrote. So what we can do is tell Grunt to look for more tasks in another directory that we specify. So in this case, we're going to say that our custom tasks are going to be located potentially in our bin/tasks directory. So if we expand that, we can see that we have this custom Grunt task here. The exports for a function here, it got a little sloppy. I forgot my const up at the top there for my module requires. Apologies there. But we have a custom task here, so we call registerTask, init_static, we include a basic description of that task in case anybody runs grunt help, and then we have the actual function which executes, which is going to destroy the public directory if it already exists, recreate it, copy over the static, like the unmodified static assets from our src directory, and then copy those over as a part of that task. So, you can do any kind of functionality this way. For instance, like we in our workshop, we're going to being using the Elastic Beanstalk command line utility directly to deploy, but for our application we have Grunt tasks that manage lots of different parts of the deploy process for us using the AWS, SDK for Node, and sometimes the Elastic Beanstalk command line utility directly. So using --- so that's one of the things I like about Grunt is just a general purpose task runner for Node code, so we use it for all kinds of stuff as a result there. Any questions on npm scripts or Grunt tasks? Yeah, question go ahead. Question, when you do the grunt load npm tasks --- Oh yeah, let me go there. --- does it automatically include all the grunt-sass, grunt-broswerify things when you do a grunt without any parameters? So when you do a grunt without any parameters, what's going to happen is Grunt is going to look for a task called default. And in this case, I specified the default task to be like these other tasks, like the sass task, the browserify task, and the concurrent tasks. And the way that Grunt acquires those abilities is through these lines of code here. We register these additional task types from npm. Like we've installed a grunt-sass plugin and a grunt-browserify plugin. And in the initConfig, which is this --- which is usually the first part of a Gruntfile that you see, this is where we configure all of those plugins. So here's configuration for browserify, for sass, for watch, nodemon, and the rest of them. So after you type grunt, look for that default task. Then it executes this list of tasks, which it knows how to do because we loaded those tasks from npm here and configured how they should operate up here. A few people are asking for your slides. Yeah, sure thing. I can --- I'll just like create a PDF and maybe check it into the GitHub repo. Perfect. So we'll do that as soon as we can after we break down here. Other questions before we move on, on the Grunt or npm stuff? Is there a good online resource for finding the different available Grunt modules that are npm modules that we might want to include? The Grunt site does have a number of them that you might find interesting. Also, the npm package registry. So if you go the npmjs. org and just search for grunt, there's --- you can kind of search for the kind of stuff you're looking for. That's not going to probably lend too much, but like let's say I want to find like grunt webpack, like this is what you would use if you wanted to do webpack stuff from the context of your Gruntfile. So like usually if you search for like grunt whatever, that's one of the reasons why I still use it, if I search for grunt whatever, there's like a 90% chance I find something that kind of does what I need to do. Grunt website has a plugin search that will --- I think it just goes over npm as well. Yeah, Yeah. And you don't have to type grunt. Exactly. Even better. Some people are saying that they sent your PR to fix the tests. Oh nice, nice. Appreciate that. I'll go take a look after we get working. I bet I'll be able to merge it down right away.

Alternatives and Friends of Grunt
So the last bit before I turn you loose on some scripting tasks, which I'm kind of weird in that like command line scripting is like my favorite part of Node, like I love to do stuff on the command line. I don't know what my problem is. There's lots of alternatives out there. We already started talking about some of them. Gulp is probably the other major one. The big advantage of gulp over something like Grunt is the fact that it uses the Node. js stream's interface to stream data from one plugin to another so that you don't have to wait for one task to be completely finished before you can start streaming data to another task. And that is really cool, and it does speed things up oftentimes quite a lot, but again, old man Kevin kind of thinks it's harder to reason about how streams work and write custom tasks that execute in the order that I expect using the streaming interface. So I did gulp for a long time, and then I kind of found myself coming back to Grunt. Again, not because it's bad, just because my simple mind was able to comprehend what was going on in Grunt a little bit more easily. But it's superfast, and it also has a very nice ecosystem of tools. I think Grunt, Grunt has been around longer, so I think there's probably more out there for Grunt, but gulp probably is like cooler at this point, so there's maybe more new stuff being invented for gulp these days. Webpack is a tool that's out there that some folks have kind of used like as a front end for executing tasks like for compiling their front-end assets. And it's really awesome, like it does a lot of really cool stuff. It's got like really cool plugins. The way that you would implement like those cache-buster URLs that I was talking about briefly earlier, I think the easiest way to do it is with webpack through Grunt. So there's a few --- there's lots of really great uses for it, but I think it's not necessarily as good of a general purpose task runner as Grunt is. Like I said, we use Grunt for all kinds of stuff from deploy tasks to like just simple scripting stuff, and that's not really what webpack is for. So using webpack from Grunt I think is the winning combo for me. Yeah, do we have another question? Yeah, what happens if you don't define a default task in the Gruntfile? If you don't define a default task and you just run Grunt, it will say that there's no default task defined, and it'll error out. That's what'll happen if you don't have a default defined.

Exercise 2: Enhancing Your Build Tools
Now it's time for the next exercise portion of our morning, where you fix more stuff in the example code project. So to correspond with exercise #2, we've got some more GitHub issues for you to take a look at. They are tagged with exercise2. And the first one is a bit that probably needs to be implemented before we take this thing fully to production, which is a task which will --- or a task to modify our collect_static task, which we already have, in addition to running our application JavaScript through Browserify to then run it through Uglify as well. So compressing the JavaScript source into a minified JavaScript source file that we can use in production and putting it into the public directory just like we do with the rest of the collect_static command. So spoiler alert, there is a Grunt plugin called Uglify, which is probably the place to start barking up if you want to take a crack at that task. So that is number 1. For bonus point actually on this task, I didn't actually call it out here, but if you look in the EJS template, which renders our TodoMVC page, there's a little bit of conditional logic in there that inserts a minified script, or like a script tag to the minified version of like app. css if we're in the production environment. But the JavaScript is just app. js every time, so maybe we also want to in the production environment include the minified. js file rather than fully inflated one with the source maps and everything. So that is another bit you might want to take care of during this particular issue. The other one that I put out there was to add a npm start script that is dedicated just for running migrations. I think you'll probably notice something in the package. json that already does run the migrations, but poststart, which is what it's currently assigned to, can do lots of other things. So it might be nice to have a npm script which just runs migrations using the Sequelize command line tool. So for that you'll have to learn a slightly different incantation to actually run that and make it work. So adding a special purpose migration command to our npm scripts will be another thing you could work on. And those are the two I've got, but I'm getting like unsolicated pull requests for all kinds of stuff, which is awesome. I'm very stoked at what you guys are doing with this.

Exercise 2 Solution
I think we'll go head and go over some of the results. It looks like we had folks step up to the challenge once again with some solutions that are looking correct, like something we can ship. So, the first one I wanted to check was the pull request from Lucas here in the room that added the Uglify configuration to the application. So, what Lucas did here, bump that up a little bit, so what Lucas did in the Gruntfile is he added a new configuration for the uglify task. He created a target called my_target, not a super descriptive name, but I think it does work. We can probably follow along and figure out something for that. I think in this case, maybe like dist or something like that would be appropriate. But in this case, we are going to be taking app. js and writing it out to app. js. Now, in this case, I probably would've added like a. min. js or something to the end to sort of specify it, that it would be the minified version rather than the regular uncompressed version, but that's generally what we're looking for. We need this file, which is public/app. js to be uglified, and in this case we're actually renaming it --- or we're going to output to the same file. Probably we'd want to do a different file, but that's generally what we need. And then the --- we also have the uglify task loaded in from grunt-contrib, which is exactly what we need, and then we also add in the uglify task to the collect_static command, which was the desired enhancement that we had to the project. We also required the 2. 0 version of Uglify, which I think sounds find. And actually, here I just pulled down Lucas's branch locally so I could check it out. So if we run grunt collect_static, that's going to run our browserify task, and then we also see that we ran the uglify task successfully. And then if we look at our public directory, which is the desired output, we definitely have some uglified JavaScript in there. So a couple of bits to tweak, but that definitely works, so like excellent work Lucus! Very nicely done. How did he know to get the version 2. 0. whatever of Uglify? My guess is that what he did was do npm install --save grunt-contrib-uglify, which will add the latest stable version of that folder to npm, your package. json, excuse me. So that's totally good. I think we'll --- I may make a few minor modifications before I push it into master, but that's what we need, so you definitely get the free taco there. That's a lie. There are no free tacos. I probably should've brought tacos. That would've been a good idea. Alright, so let's take a look at one of the other solutions that we had. So that was what we're looking for Uglify, and then we needed a npm script to actually run the migrations. So, we have one pull request for that, so let's take a look. Alright, this pull request also includes the Uglify solution, which is just fine. Usually, we want to keep one pull request to one issue, but I won't wrist slap very hard for that here. I appreciate --- It didn't originally. What's that? It didn't originally. I think he just forget to branch. It's okay. That's just fine. Alright, so the key bit that we have here is the migrate command was added here. Maybe it'll be a little bit easier if we actually, you know, view the file. So, we had this prestart command, and that's actually going to be executed on our Elastic Beanstalk instance before the server process is started to run any pending database migrations, but we also wanted to have like a custom script because that prestart could eventually do other things besides just run the migrations. So we created another task called migrate, which also used the local version of Sequelize to run the migrations. So, just for the sake of expedience I'll copy that just to see what that's going to do. And the thing I was hinting at in the comments, if you were kind of reading through that thread, is that the way that you invoke like nonstandard npm scripts is slightly different than for those that you would run otherwise. So if say like npm start, that's going to run the migrations, and it's going to start up my development server --- my server process on port 3000. So npm already knows how to npm start, but if I say npm migrate, it's going to say well, I don't know, I don't know what a migrate is. So it's not one of the sort of built-in commands that npm knows about. So rather than saying npm migrate, I'm going to say npm run-script migrate, and then that's going to run my migrations using the built-in Sequelize module. So if you add commands to the package. json that aren't part of the known set of npm commands, you do have to add in that extra run-script prefix before you can run those commands. So that was the bit that I wanted to point out there.

The Database
Sequelize and PostgreSQL
We've got two more big meaty sections here this afternoon before I turn you loose. The first is digging into the data tier of the application that we've been looking at today. And the --- in this case, we're actually going be looking at how to use Node. js with a relational database. Anyway, so the stack we're going to look at is one that's actually less heralded in Node because most of the NoSQL databases out there are, because they sort of natively speak JSON, they end up being a really good fit for Node. js. But for certain use cases, there are, you know, a relational database does make a lot of sense, and particularly Postgres is actually a really feature-rich relational database, which can do a lot of interesting stuff that JavaScript can take advantage of, particularly with its JSON fields allowing you within a table to have a rich query --- or like an unstructured queryable column with JSON data embedded in it. So, Postgres still a really great choice for Node. js development, and specifically, it allows us to use RDS, which is the managed relational database service from Amazon which is very performant, highly available, all those things. So if you're going to be using a relational database of any kind with Node. js, the chances are you're probably going to go one of two routes. You're going to either use a driver for your database directly, so just use PG or Postgres or a MySQL driver directly, maybe execute raw SQL against it or maybe use a SQL builder. But if you're using a ORM in Node. js, you're probably going to use Sequelize. Sequelize is definitely the most feature-rich ORM that's out there for Node. js today. So, if you're looking for the type of experience that you might've enjoyed with say Active Record, which is kind of, for me, like the gold standard to which I will compare all ORMs, it definitely has its problems, but I feel like it's a great piece of software, it's probably the closest you're going to get to that in the Node. js world. That said, this is another situation where if you choose to use Sequelize, the support that you're going to find out there on the Interwebs is not awesome, like there aren't a lot of tutorials, there aren't a lot of resources, there aren't a lot of sample apps that have been built using Sequelize, so you're going to be a little bit in the wilderness if you use Sequelize. The documentation is pretty okay, but there are a lot of things that go unsaid that you occasionally have to dive into the source to figure it out. But all that said, it definitely is of all the ORMs out there on top of a relational database, I think it's definitely the creme de la creme. But yeah, as I mentioned, isn't SQL for old people? Like shouldn't we be using NoSQL databases? And there's some reasons why you might want to consider going with a NoSQL database for Node, mostly because the support for NoSQL databases in Node is much better, like there's a lot more folks using those technologies, a lot more resources available, and the libraries are a little bit better. For instance, like Mongoose, which is the ODM, the Object Document Mapper, for MongoDB I think is particularly good. But the main reason that I've gone with SQL is that Postgres itself is an awesome feature-rich database, and RDS was a good choice within our Amazon-centric ecosystem. It was going to be the best data store solution for us. So the opportunity to go with RDS was definitely a driving factor in making that choice.

The Database Code Demo
What I'd like to now is take you through some of the database code in the sample application and talk about how some of these things are laid out. I know we've kind of dug into this a little bit, but I want to take a moment to sort of step you through the key bits that make this go in our TodoMVC application. So, the first thing I'm going to point out is this file, the sequelizerc dot file at the top level of the directory. So, when you're using the Sequelize CLI to execute commands in your dev environment, this file configures some of the key bits about that experience, like where your database configuration is going to live, when the CLI tool generates migrations, where it's going to put those migrations, and where it's going to look for models that have been defined or that will be defined using the CLI. So this file is there for when you do execute those Sequelize CLI commands at the top-level directory. It's going to take these settings into effect. The other places where you'll find like database related logic in the application is here in the db directory. Sequelize requires like database configuration to be specified in kind of a specific way, which is kind of incongruent with how we're doing configuration elsewhere in the app. So we have this database. js file, which spoon feeds Sequelize configuration in the format in which it is more comfortable. So we actually read in the real configuration values from our config setup as we've been doing already so far, but then we populate these things, like the databaseUrl and the dialect that we use for Sequelize here. The other bit you'll notice is the migration's folder. So this is where when you generate a new model or you generate a new migration, which mutates the state of a database from one state to another, those files are going to be generated here. Much like other migration systems that you might've used in other environments, a migration file name is built of a couple different bits. The first is a timestamp, which is the actual identifier of record for this migration, and this is how the Sequelize, or the ORM engine, knows which migrations have been applied and which migrations have not been applied. So this is important and is generated by the --- by Sequelize when you create a new migration. And then the second part is simply descriptive. So create-todo could be any string, you know, fluffy bunny one, two, three. It doesn't really matter. It should be descriptive to say what the migration is about, so second part of the migration is for humans. And every migration exports a JavaScript object that has two functions on it. One is called up, and the other is called down. So when we are migrating the database to the next version, the up function is called, and that function is passed a Sequelize query interface, which has methods that let you create tables or add columns to existing tables. So as you're mutating the data model over time, you'll be able to specify these functions to go up and down. So here, right now this application only has one migration, but all of you are going to change that in just a little while by adding --- updating the data model a little bit. So we're creating a new table called Todos for our todo items, and then we're also specifying the down command so when we're rolling back this migration we're just going to drop the table that we created. So, when you generate migrations to update the data model, you'll be using these migration files to do it. Now the other place where you'll run into database code in the application is in the models folder. And there's a couple things that happen here which are kind of a twist on like how you'll see things structured in the Sequelize documentation. So, this file, db. js, creates a single instance of a Sequelize object, and that Sequelize object takes in the constructor a Postgres databaseUrl that will have the username and password embedded inside of it. So this is where we're authenticating against the database, and we're also passing in the options for that connection to the database, which we're loading in from our configuration. And in our case, the configuration that we're passing in is pretty much the defaults that Sequelize provides. The development Postgres URL is configured here, and these database options are arguments to the Sequelize constructor saying we want to use the Postgres dialect, we don't want --- by default we don't want to log every SQL statement out the console, although in the development environment you can see that we actually override that and say yes we do want to log SQL statements out to the console. And then we also configure Sequelize's connection pool, which specifies how long a connection is held that's idle before it's deleted from the pool, and then the maximum number and the minimum number of concurrent connections to the database that are allowed, and those are the default values for Sequelize which you can tune to your requirements if you feel that is necessary. So, this database object is used to define all the other models in our application, and right now there's only one. We have a Todo model where again we're going to be requiring the Sequelize module because we're going to need some class-level variables off of that as we define the model, and here we're going to create a new model. And right now the only attribute we're adding is title, although the models by default also have an ID, which is an auto-incrementing primary key in the database, and it also has a created at and an updated at timestamp, which is added automatically by Sequelize. So, we export that model object, and then that model object is what we use elsewhere in our application to actually do queries and update data. So, here in the todos controller we require that model, and when we fetch all Todos we use the query interface attached to the model to find all the Todos with a certain limit, and then we can findById, create, and destroy and do all those other operations within our backend API here. So, this is the, kind of the interface that we have put together. We're going to be expanding on this data model a little bit with a migration. Some of the gotchas to be aware of here, so rather than something like Active Record where this sort of canonical source of truth about the current state of the model and the properties that its supports lives in the database. In the model itself, you don't actually configure what all the properties are. Like you might configure validations and other logic about the model, but you don't sort of enumerate here are all the properties that are a part of the model. In Sequelize, you actually do have to do that. So as you're creating migrations that mutate the state of the underlying tables that back your models, you'll have to keep that in sync with what is actually defined here in the model. So if you create a migration that adds a property to a model, you'll have to make sure to add that same property here in your model declaration. So one thing that is potentially somewhat onerous, it's that way in other ORM systems, but again, if you're coming from Active Record, that might be a little different than what you're used to. Now the --- when we had you set up your development environment, the two command line utilities that you set up were the Sequelize CLI and the Grunt CLI. We installed those globally. So the Sequelize command line utility, if you just run sequelize --help, you'll see all the different generators and other options that the command line interface gives you. And the help text is actually pretty good. Like that's where I go to a lot to see what is going to be possible with the command line utility. Most of the commands have some decent help text that show you how to use these different commands, actually give you some examples of how to generate models of various kinds. So as you're kind of exploring this, I would definitely recommend that you take advantage of these help operations. So for example, for model:create, which I suspect is something that you're going to be interested in, sequelize, and then we can just basically append help to the beginning of that, that's going to actually show you the commands that you pass in to generate like say a user model with certain attributes, and it'll show you like the actual JavaScript that'll get spat out by that command as well. So, definitely make use of those help texts as you're going through and doing stuff with the ORM. The other bit that you'll want to take advantage of while you're hacking on it is the Sequelize documentation posted on Read the Docs. Specifically, I'll point you to the migrations documentation because we will be looking at expanding on our model a little bit. So you're going to be generating migration, updating the model, running the migration, and enhancing what the application is --- or what the model is capable of. So, definitely will be a resource worth consulting. So, are there any questions right away about Sequelize and kind of how it fits into this application or other general questions? Obviously, there's a lot to learn about the Sequelize API, which is why we're going to start off with a task that will get you familiar with that by actually wrestling with a real task rather than putting a bunch of method names in slides that none of you will remember. I'm going to be giving you a task that will require acquiring an operational knowledge of how this ORM works.

Alternative Databases
Before we dive into that, quickly going through some alternatives. I think probably the most popular alternative in the Node world is MongoDB, which is really great. It natively speaks JSON, it's pretty fast, and there's no migrations because it's, obviously it's a NoSQL database and can sort of store document objects whose content can change without necessitating a database migration. Also Mongoose, as I mentioned, the ODM on top of MongoDB, is probably much better than any similar tool in Node. js right now. There's a lot more resources out there on how to use it. So, there is definitely that advantage. Using an ORM like Sequelize is definitely a choice that you would make if using a relational database is important to you for some reason, as it is for us, and if you like the features that Postgres offers, which are many and great.

Exercise 3: Enhancing Your Data Model
So now we're going to be moving on to our third exercise of the day, which is enhancing our data model to use some additional properties. So, if we head out to GitHub, you'll notice that there are two issues remaining. Both are tagged with exercise3, and both will get you into a little bit meatier parts of this application. The first is --- and it could be that you'll be able to knock these out both at once because maybe you'll want to write a test which exercises your new functionality that you're adding. But the first issue that I have is that right now there's no way to track the status of a Todo model once you mark it as complete. So, many of you were astute enough to note that when we actually run our application here in development mode, when you mark a todo item as complete and refresh the page, it doesn't actually do anything. Now the user interface part of this we'll actually get to tomorrow. That'll be sort of our entree into learning a little bit about how Vue. js works. However, the backend part of this is something that I'd like for us to address today. So, what I would like for folks to do is to take the model for the todo and add Boolean flag for whether or not it's been completed. And in addition to adding that flag, we also need to modify our API in Express, implemented with Express, to accept a completed field along with the current title field, which is the only one that's persistent at the moment. And again, I wouldn't spend too much time, or I wouldn't spend any time, because we'll do that tomorrow, trying to update the user interface. I would just look to test these URLs directly. And if you're into doing that, the go-to for me --- if you like cURL, that's great, but if you're using Chrome, there is an app called Postman. And Postman I feel is kind of the bee's knees if you're testing a REST API because it gives you a nice GUI that let's you construct HTTP requests with different HTTP methods against your backend. So here, and it sort of saves your history as well, which is also kind of nice. So here we can do a GET request against /todos and hit Send, and then we can see the JSON that comes back from the server. We can do a POST request to todos, and then along with that pass in some form encoded. And they do need to be form encoded, by the way, if you do decide to use this tool because that's the way that we're parsing the body. If we pass in some values like hey there Front End Masters and hit Send, we can create a todo that way. We get that representation back. And if we list them out again, we can see that the Front End Master's record is in the database. So after you add that property, using something like Postman to test the API would be useful. In addition to that, the second issue that I would like for folks to take a look at during this time dovetails kind of nicely into that, which is adding some integration tests which we don't have yet in the application. We have like one kind of crappy unit test for our backend right now. Test coverage in this application is not great yet. So we could definitely do more unit tests as we add more functionality to the application, but if our test pyramid should be fat at the bottom with unit tests and kind of skinnier in the middle with integration tests, that's the part of the pyramid that we're going to be implementing next. And then, of course, at the top of the pyramid you have some end-to-end tests, which we will talk about later. But we're, at this point, looking to add to the middle of that test pyramid. Adding an integration test where we're testing that controller logic is working model logic in the way that we expect. So, I would like for us to write an integration test that covers our controller logic. So, write at least one test. The challenge here is to write at least one test for the todos controller that exercises one of the CRUD operations. Now to do that, you're going to have mock an Express request and response object and provide some values that can be used to satisfy the requirements of the controller logic that we have in that we have in that file. So that'll be part of the challenge there. So we'll probably for this, since this is a meatier task, we'll take for sure 30 minutes. We'll probably say 40 minutes to tackle this because this will be the first time that you've engaged with these technologies. And you have any questions on the testing front, you can just create --- if you'd like, you can create a new file in the test directory here, and if you npm test, any test logic that you run will be picked up and executed. So, if you'd like to take a crack at that first before modifying the model at all, you can certainly do that, or if you'd like to modify the model first and update the API with this new property, you can take a crack at that first. But I think it would be good opportunity to try to take a crack at both of these tasks. So, adding one integration test for the controller and then also the addition to the Todo model that we've been talking about.

Exercise 3 Solution, Part 1
It was a lot of code to bite off, but I think it gave --- hopefully gave you a sense like the end-to-end process of like migrating a database in this way. So it was about half and half, so what I think we'll do is actually live code this solution together so we understand how to --- like all the pieces of code that are necessary to make this change happen. So, the first bit that you're going to want to do, like if I'm going to want to update my data model, the first thing I want to is generate a migration, and that piece of code or that documentation I put in the chat kind of takes you through how that works using Sequelize. So I'm going to use the sequelize command line utility, and then it has an option called migration:create, and that's going to create a new migration file for me in my project. So, let me close some of this stuff down. Alright, so again, when we have a migration like this, a migration is going to consist of two operations, the up and the down operation. So when I am migrating my database up, when I am modifying it from its previous state, all of the logic to do that is going to be within the up function, and when I --- if for some reason I need to roll that database change back, all the logic to do that is going to be in my down function. And one of the mistakes I saw of people making was trying to like recreate the table for a user within the up function, adding in that Boolean field that we had before. So, totally understandable why you'd think so, like you want to update the definition of this model, but with a database you probably already have a database full of like hundreds of thousands of todos or whatever, and you can't recreate that database table or you'll lose all the data. So what you want to do is modify it in place, and that is the role of code in a migration. So in the up function I want to add a column to my existing database. So the way that I do that, and this is again in the documentation that I put in the chat, is using queryInterface. addColumn. And this takes a couple of arguments. The first is going to be the name of an existing table, which I believe is Todos. I can double check that actually in my model file. Todo actually it might be called. So that's going to be the name of my table, and then it's going to be the name of the attribute that I want to add, and in this case I'll call it completed. And then finally, I need the data type that I want it to be, and in this case I'm going to make it a Boolean, so I'll say Sequelize. BOOLEAN type. So I'm going to add a column to the Todo table called completed, and it's going to be of type BOOLEAN. And then in the down I'm going to do something very similar. So rather than add column, I'm going to remove column, and the two parameters I'm going to pass in again are the name of the table and then the actual --- just the name of the property I want to remove. So we have Todo and then the name of column. Alright. So, now I've got my migration logic in a place where I feel like I want it. Now what I want to do is, you know that timestamp that gets generated for the migration is important because it lets us --- Sequelize know like which migrations have actually been applied because each migration is ordered chronologically, so that I don't want to touch, but the second part is just for humans to make it more readable. So I'm going to call this migration add-completed-column or something like that. I had Todos for the table name instead of the Todo. It is Todos. Is it Todos? If you look at your other migration, it has an s on it. Oh, okay. So I'm sure I would've figured that out when my code didn't run, but I appreciate the heads up. So, can you explain again why would the down add the column? Wouldn't it remove a column? Oh, I'm sorry. That was another typo. It was copy/paste error on my part. Thank you guys. Appreciate it. You are helping me debug left, right, and center. So yeah, the up is going to add the column, and the down is going to remove the column. So that is my bad. That is definitely the danger doing it live. So my migration is created, I've given it a human readable label, and then I can head out to the command line again, and I'm going to run sequelize db:migrate. Do you want to be allowing null as a value for the completed attribute here or does it not matter in this instance? So that's a question you have to answer like based on your application logic. So if it is very important that this value is not null and you want to have that constraint in the database, in your migration you'll also have to update all of the other columns or all of the other rows in the database to have either a true or false value for that. So, you'd have to code that into the migration. In this case, we are going to allow a null, and we're not going to mutate the rest of the table, but you can kind of decide based on the needs of your application. That's definitely something you have to think through. Alright, so I've run my migration, so now if I check my database and I --- this is another little tool that I use a lot when I'm using Postgres locally, it's called Postico, I don't know what exactly it's suppose to be called, but it's a nice Mac GUI over my database, and it lets me sort of take a look at the current status of my database. And I think actually my database configuration might still be --- yeah, it's actually set up to go to this other thing. So I'm just going to delete that because I don't care about that, and I'll run my migration again, and this will actually target my local database. And I'll hit Refresh, and now I have my completed column, but it's null because I didn't do any mutation of the existing data. So now my database is in a good state. Now I need to update my model to reflect this new value. I missed the command for the update database. What was that again? It was sequelize db:migrate. It was the same command that you ran to run the first migration initially, but now it's going to pick up that there's a new migration that it needs to run against the database. Or npm run migrate. Exactly. We added that feature in earlier today. Good point. So, we run the migrations, and now we have this new column. Now we need to update our model declaration in JavaScript to be aware of this new property to make this property persistent. So we'll add a completed attribute to the model, which again is going to be of type BOOLEAN.

Exercise 3 Solution, Part 2
Now we should be able to start using the model with this new property. So the first thing I might try is to run my tests, npm test, and it does actually successfully create a TODO item, but I might sort of modify that to also add in a completed flag. So, in addition to creating a todo with the title Text, I think I actually want completed there, right? Right! I'm going to create a todo with both the title and the completed, and I'll add another assertion there that says todo. completed should equal false. And there's a bunch of ways to do that assertion, but that one works okay. But this is one way for me to just quick verify that the table has been sort of updated and the model is kind of doing what I expect. These are fairly shallow tests again, but it's the first step. Now what I need to do is update my REST API endpoints to actually update these properties on the models as well. So for that I'm going to open up my Todo routes, and for both the update and the create operation I'm going to have to accept another parameter for the completed flag essentially. So for an update, we'll do this first. I'm going to look for a request body. Or excuse me, I'm actually going to do that after my 404 check. So, I'm going to set the completed property of my model to be equal to the result of a Boolean check. So if request. body. completed equals the string true, that's going to set completed to be true for all other values of that component that is going to be --- that's going to evaluate to false. There's other ways to do this. This is a fairly quick and dirty implementation, but it's one way to make that piece happen. And then in my create I'm going to do something similar. So, in this case, oops, I'm going to set that completed property again based on the result of that Boolean comparison, whether or not it's true, and then it should be updated in my database accordingly. So those are the two like REST API endpoints I need to update. Now I'm going to run my application and make sure that my REST API is doing what I feel it should do. So I'll start up the dev server, and now you can verify this using cURL or whatever HTTP client you want. Like I said, I'm kind of a fan of Postman, so I'm going to do it that way. So if I start off by sending a GET to my input, I can see that I have all of my todos in the database, but for most of them the completed attribute is set to null. So let's create a new one with that attribute in place. So, in addition to the body, the title, excuse me, I'm going to pass in a completed string, and I'll initially set it to false. And I created that value in the database, so completed is set to false. I could create another one set to true, send that in, and then, again, if it's anything but the string true, it's going to default to false. So now if I list those items in the database, the most recently created ones are there with their proper values. And I can test the update by changing the --- by using the PUT. PUT is the operation that I'm using to update the models. And in this case, the title is going to be whatever I want that to be, and completed is going to be true. I can send that, and once again I can see like id 11 has been updated, but that had one already, so let's do like maybe id #2. (Working) And hit Send, and then that's been updated as well. Alright, so I've --- once again, the steps were creating a migration, handling, adding, and removing the column whether or not we're migrating the database up or down. After we did that, we updated our actual Todo model to have the property that we added to the database. We updated our test, and then we updated our Express controller logic to take advantage of that new property which wasn't there before. Cool! Any questions on any of that code? It is --- there were a number of steps there that you had to discover, so not surprised that a good chunk of people --- yeah, there was definitely a lot to grok there if you haven't done it before. Anybody in the chat? Maybe we'll give them a chance to --- Okay, if you have any other questions on the Sequelize side of things, I'd be happy to answer them. Sequelize is a fairly full featured ORM. It can do complex relationships and can use database-specific features like JSON fields and Postgres. So if you're interested in digging deeper, it's definitely going to be your best bet for an ORM if you're not going to go with the driver, a driver directly.

Production Environment
Elastic Beanstalk and RDS
That was a spin through the data model, but now we're going to get into one of the parts that I think might be new to more folks in the room. Who here today deploys code like on AWS? I try. You tried a little bit. That's okay, totally fine. So the --- so what we're going to go over here is a stack that we can use in AWS using a few of their more popular product offerings to stand up this application in production. Now this is definitely a piddly process. The reason why I think not too many people have deployed code into production on Amazon is because it's kind of a pain in the butt. There are a lot of different things you have to learn. There's a lot of steps that you can screw up. There's security groups and IM users and configurations and command line interfaces, and there's a hundred different ways you could screw it up. So what we're going to do is go through one pipeline to get a Node. js app deployed on AWS using a combination of Elastic Beanstalk, which is essentially managed Amazon EC2 instances, and they're talking to an RDS database on the backend and that are part of an Auto Scaling group that we can configure to add more instances to our application, if necessary, to handle bursts in traffic and can scale down based on our requirements as well. So we'll talk a little bit about the production environment that we're going to create at a high level, talk about some AWS terms, and then what we're actually going to do is step through the provisioning of a new environment, which is going to take a while, so we'll kind of do it together over the course the exercise. Hopefully with the magic of television we can cut this up a little bit later because each step does tend to take a little while. But we --- the goal is for everybody to get the current running version of their application deployed onto their own personal AWS account. That is the desired end state for today, okay. Alright, let's do this thing. So, first we'll kind of talk about the stack that we're choosing and kind of where it sits. Like on the spectrum of possible production deployment environments for your web application code, there's kind of two ends of the spectrum that you might be familiar with. Like there's stuff that is completely 100% managed, and you're really only interacting with it via API, things like Firebase, which you see on the far right of the screen. Firebase is you use an SDK to sync data to their backend, and you don't actually manage any infrastructure at all. You're just using their backend as a service. There have been other things like Parse and Kinvay and lots of other vendors in this space that have tried to like remove the need for a backend in your application at all. On the other extreme, there is virtual hardware that you manage yourself, like DigitalOcean, virtual private servers, or Amazon EC2 instances, which are servers that you configure and deploy code to and SSH into as if you are managing a fleet of physical servers. Sort of more towards the middle is something like Heroku where you are running an actual web application, but it's on a fully managed stack, like you can't SSH into the server which is running your code. And the way that your code is run is actually completely abstracted to you. You say Heroku, here's my application, run it for me, and that works out really well. And then kind of in between is this guy, this weirdo called Amazon Elastic Beanstalk, and this sort of the Amazon answer to Heroku, but it's much closer to like managed infrastructure on your end than it is to Heroku where there's sort of a veil of secrecy which you can never penetrate. So, the environment for this application that we're going to try out is a combination, or the two key technologies are Elastic Beanstalk and Amazon RDS, or Relational Database Service. So, Elastic Beanstalk is interesting in that it's more configurable than most other platforms as a service. Like at the end of the day, even though you're using Elastic Beanstalk to configure it, you're just running EC2 instances that you can SSH into that are just like Linux servers that are ultimately under your control. There's some management software that is running on them, and the way that you interact with them can be a little wonky sometimes. But ultimately you have a lot more control over the actual hardware that your code is running on than you would in something like Heroku. Also, because it's part of like the Amazon ecosystem and you can manage it through the administrative interface, it's easier to make Elastic Beanstalk play with other Amazon resources. And I finger quote easy because if you've ever been in the Amazon administrative interface before, you will realize that nothing is eve easy ever. But if once you do figure it out, like it is actually pretty cool sometimes, like the way that you can set up DNS with Route 53 and use S3 buckets and CloudFront for your static assets, like there's lots of cool stuff that you can do, but the learning curve is incredibly high. And RDS is a pretty good service. It's a performant way to run your database. It's obviously highly available, and it has managed software updates and snapshots, so a lot of the questions about reliability of your database kind of go away by using this managed service, which has been working out pretty well. So, this is kind of the topology of the solution that we're going to create today. So, the key technologies in the Amazon world that we're going to be employing are an Elastic Load Balancer, an Elastic Beanstalk environment, which provisions Amazon EC2 instances within what's called an Auto Scaling group. So, we can define rules that say like alright, once our --- the instances in our application are at like 60% usage of CPU, that's the time to spin up another instance of the application. So, the Elastic Load Balancer is what will accept incoming HTTP traffic from the outside world, and then the Elastic Load Balancer will delegate those HTTP requests to instances within our Auto Scaling group, which again is managed by Elastic Beanstalk. And then all of our EC2 instances are talking to an RDS instance that we also create within our Amazon account. So again, at a high level, Elastic Load Balancer is shunting off HTTP traffic to all the different EC2 instances within our Auto Scaling group, all of which are communicating with an RDS instance on the backend. So, that's what we're trying to create, at least at a very high level.

Provisioning an Environment
Provisioning an environment is, is again kind of a journey, so we're going to do that together, and we'll do it kind of manually so we can see the different pieces and how they fit together. But at a high level, you're first going to decide which AWS region you're going to run your code in. It's usually best to be geographically close to where your users are, and there's ways to run your application in multiple regions, which we're not going to get into. But we'll select a region, whether it's US East Virginia or US West to Oregon or whatever, it doesn't really matter, and then we will then create a user with permissions to allow us to essentially create the Amazon EC2 environment that we need and allow us to deploy new versions of our application and things like that. So we'll create an AWS user, and that user will be assigned credentials that we will use to configure a command line client to access Amazon on our behalf, which is part of the exercise that we'll be doing shortly. Next, we're going to be creating an Elastic Beanstalk environment, and environment is basically a fully functional version of your --- like a fully functional like execution environment for your application. So typically you'll have multiple environments. You might have like a development, a staging, and a production environment, and those environments might be configured slightly differently. They might use different sized EC2 instances, use different sizes or configurations of RDS, that sort of thing. So we'll create an environment, in this case, we'll just create one environment to start with, and then that will be added to a security group. And a security group is a construct within Amazon that defines like which resources within your Amazon account can communicate with one another. So, our RDS instance and our Elastic Beanstalk instances, like the EC2 instances managed there, will be part of the same security group, which means that our instances can connect to our RDS instance that we'll create, but the outside world can't directly connect to the RDS instance. So we'll create both an EB environment and an RDS instance, add them to the same security group, and then we have to configure that security group to allow incoming connections to Postgres. And then we need to deploy our application, an application version, and then potentially we profit. So it's a long journey, and there's a lot of things that you can screw up, which is why I've been super, super nervous about this component of the workshop, but I have done it three or four times over the course of the week, so I'm like 94% sure this is going to work. So let's do this thing.

Production Environment Demo, Part 1
What I'm going to do is basically deploy our TodoMVC application to a production environment, and there's going to be some kind of bumps along the way because I'm going to actually manually configure a few things after the application has been deployed. But if you have an AWS account already, you can feel free to follow along because these will be the same steps that you'll need to reproduce after as we're trying to get the application deployed yourself. So, does everybody here --- does everybody have the, excuse me, an Amazon account, like at least an Amazon developer account? So we did go as far to sign up there. So that next thing that you'll want to do, and I'll drop this in the chat, is the Elastic Beanstalk Command Line Interface is something that you'll need to install. It's a Python utility; it can be installed on any platform. I'm looking at the docs right now, but let me drop it in the chat. (Working) And if you have Homebrew, it's actually fairly easy to get installed. There's actually a brew package that will do it right up for you. If you also have Python installed, you can install it with pip as well. So, let me go ahead and drop that in. So if you want to go through the process of starting to get that installed, I will sort of slowly go into the Amazon console and give you a little chance to get that installed as we go. Because the first stop is going to be --- like once you sign in to your Amazon account, which I am right now, you'll probably see a dashboard that looks something like this. I'll deploy to a region that I haven't deployed to yet maybe, or I guess could go US East. Doesn't really matter. Alright, so I'm going to be creating resources within the US East region, but first I need to start by creating an API user for myself. So from the dashboard, I'm going to go down to Identity & Access Management, and these users are actually shared across your account, so it doesn't matter which region your in for this particular one. So, the first thing that you're going to want to do is go down here to Users, and Create a New User, and you can give them a username of some kind. So like I'll go through and actually create one, although I actually have done this numerous times, so I already had one created. So once you create a user, you'll have the opportunity here in the browser to see their security credentials, which is going to be their secret key and like a user ID. So you're going to want download those credentials and keep those in a safe location because it's not possible to retrieve them. If you lose them, you're going to have to regenerate credentials for the user for security reasons. So once you create a user, download your credentials. Store those in a safe location. It looks like Bohdan is saying that he always gets scared in the AWS console, and that is wise, as well you should. It's kind of a scary place. But hopefully we'll leave today with it being a little bit less scary. So once we have that user created, we're going to want to add that user to a group. And this is different from the security groups that we'll be talking about later, but it's essentially a user group which gives them certain permissions within the AWS infrastructure. So in may case, I only have group defined, which is Admins, which kind of gives GodMode access to a user for all of the resources in my AWS account. As practical matter, most organizations you'll work in that use Amazon will have security groups with much more granular restrictions, but if we look at that briefly just to give you an idea of what it is, let's see, so we can see the users that are in a group, and its permissions. And Amazon has essentially like a language, a DSL for determining like what resources a given group or a given user has access to, and the administrator access policy, it's JSON based obviously, and for the administrators basically says that you can allow like all kinds of actions on every kind of resource. So it's the most permissive one, but you can make these very tightly scoped to specific products, to specific instances of services as granular as you feel like you need those services to be. So, once you have created, excuse me, a user, added them to a group which has sufficient permission to --- yes, question? Some people are asking you like how you're going through this. Do you just want them to watch for now and then they're going to go through it themselves? Some people are behind, but … Yeah, I mean I think that if you want to follow along, that is totally fine. The first page of the documentation that I put in the chat will also take you through the same process. So, if you want to --- for me personally, I like to see it and then do it, but if you --- I would recommend that you probably watch at this time and then try to recreate it. Because if we all try to do it together, like it's not going to --- like I don't think it'll work. So we'll go through the process, I'll show you the steps, and then I'll help each of you get through it with the rest of the time we have here this afternoon. Alright, so we create a user, we download our credentials, we add that user to a group or give that user permissions to access the resources that it's going to need.

Production Environment Demo, Part 2
Now we're actually ready to start creating the Elastic Beanstalk application, and for that I'm actually going to go back to the command line. And for extra, to kind of show you what this looks like end to end, I'm actually going to clone this project fresh to show you exactly like what would happen if like you were executing this as we usually do like on a continuous integration server. Like these are the steps that would happen to actually deploy this application with the exception of actually initializing the application. So, I'm going to go --- oops. (Working) As you can see, I've practiced this once before, once or twice. Alright, so I'm pulling down the application, and when I'm ready to actually start deploying this thing, the first thing I'm going to do is I'm going to cd, oops, into the application directory, and I'm going to type the command. Like at this point, I assume that you've installed the --- we have the AWS CLI installed, and when we do, that's going to put this eb command on our system path, and this is going to be the primary way in which we interact with the Elastic Beanstalk service. So the first thing you're going to do is eb init, and that is going to prompt you for a region in which you would like to deploy your application. And in this case, I'll go with US East Northern Virginia. It doesn't it really matter. And then you can --- if you already have an existing Elastic Beanstalk application that you're looking to associate this project with, you can choose that; otherwise, you can create a new application which we're going to do here. Alright, and this one we're going to call todomvc-plusplus, which is a good enough default name. And then it's going to try to detect the type of application you're looking to deploy. So for Elastic Beanstalk, it supports certain types of applications for deployment. Node. js is one of them. I'm going to say yes. And then it'll ask you if you want to set up SSH for your instances, which will allow you to associate the public key on your laptop with your instances so you can actually SSH into them. You can totally do that if you would like. For demonstration purposes, I'm going to say no, and now we're all set up. The one step that didn't happen for me, because I've already done this, is I was not prompted for my AWS access key and secret. So those credentials that you downloaded from the console before, you'll be prompted to enter those in during the setup process when do eb init. Alright, so now I have my application created, and now I need to create an environment, and this is actually going to create EC2 instances that will run my application. So to do that, I'm just going to say eb create, and it's going to prompt me for an environment name. The environment name is generated by default. It's just the name of your application -dev. You call it whatever you want, but I'll --- maybe I'll just call it todomvc and not associate it with an environment. And then you can enter a CNAME prefix, which is going to be like the public URL for your Elastic Load Balancer, which will send traffic to your actual Node application. Todomvc would be fine for that. For the load balancer, the classic configuration is going to be fine. You can do either one. The options are slightly different based on the region. If you did Oregon, like I think there's a different default, for instance. So, now what is happening is something that is actually ultimately going to fail because our application needs a little bit more configuration before it'll actually run. But what's going to happen is that Elastic Beanstalk is going to provision an environment that we can administer through the admin interface. It's going to zip up the current contents of our application directory, it's going to create a zip file of that, it's going to upload that archive to S3, and then that archive is actually going to be deployed to the EC2 instances that are being created for Elastic Beanstalk right now. So this is going to take a little bit, but there's a little of information that we can already start to glean from the output here. It's creating a new environment for us. So it's already created our S3 bucket, and it has created a security group for our Elastic Load Balancer, which is not something we care about because we're actually more interested in the security group for the Elastic Beanstalk instances themselves, which we'll be able to get at the end of this process. So this does take a while, so this would be a good point if you wanted to continue along your journey. If you've already downloaded your credentials, going in to your application project directory, doing eb init, going through the motions of initializing your application project, and then doing an eb create to create an environment to actually deploy our application to. So, got the environment going, again this is going to take a little bit, but the thing that we would like to see get created is a security group for our actual Elastic Beanstalk instances. So, once we have that, then we can create the RDS instance, which is actually going to talk to --- that our instances are actually going to talk to, our EC2 instances. (Waiting) So, what did the 0 say to the 8? Anybody know this one? Nice belt. Yes! Be here all week. Alright, so now we can see that an actual EC2 instance has now been created with this identifier. By default we're only going to have the one instance created that we can deploy our application to. Alright, so we've got our Auto Scaling group created. The instance is being added to it. Chugging, chugging, chugging. So it says it's safe to Ctrl+C, but what would happen at this point if you did that? You'd have to start over, right? No, it would keep going. The logs are still being piped. Like if we Ctrl+C this, we could actually go into the console, which maybe we'll do right now. If we go to the Elastic Beanstalk console, we can see that the todomvc environment is gray because it's still initializing, but we could kind of view the logs and see what's happening there. So there's --- oh good, we do have our security group already. I must've missed that.

Production Environment Demo, Part 3
There's a lot of different configuration options here, which I'll get into in a little bit. But now that we have the security group created, I'm going to go ahead and create an Amazon RDS instance to associate with this environment. So it is possible to create an RDS instance within the Elastic Beanstalk administrative interface, but they recommend against doing that for a production configuration because environments are not necessarily durable, like they do get created and deleted with more frequency. So the typical way that you're instructed to set this up is to create an RDS instance independently and then add it to the security group associated with your Elastic Beanstalk EC2 instances so they can talk to each other. So that's what we're going to do right now. So, we'll scroll down on the console home page and go to RDS, and we'll go down to Instances, and we are going to Launch a DB Instance. And in this case, we are going to choose Prosgres, and to stay in the Free Tier we'll use this Dev/Test version of Postgres. And then the other handy option here is Only show options that are eligible for the Free Tier. So for just messing around, it's good to check that out. And we can leave the defaults here for the most part. So the Database Instance Identifier, we will call Todos or todomvc, it doesn't really matter, and then we're going to configure a username and password for our database. Now in this case, because our database isn't open to the world, these aren't necessarily super secret squirrel values, so I'm going to choose awsuser and awspassword for these values. (Working) And now I'm going to select the security group to add this instance to. I can leave those default settings, but I need to be able to choose the security group associated with my eb instance. So, let's see, awseb security group. I forget what it actually was. Let me go to my output here. Might have to go refresh my memory on which one that was. And as we can see, our environment is degraded right now because it's under configured, and we still need to do a few things. I'm going to go in here, go into my instance configuration, and see that security group is like something that looks like that, x2iaq. And we don't want the load balancer security group. We want the Elastic Beanstalk security group because it's those EC2 instances that need to actually talk to the database. And now these values matter. If you remember like the database name that we have configured by default in our configuration, I believe is just called todos, so that's what we're going to call it in our actual application as well. We can leave the default port, and a lot of these other options we can tweak a little bit, but right now we can launch our instance. And it is going to take a few minutes. That is not a lie. So that instance is being spun up. It's going to be added to the security group that our Elastic Beanstalk application can access. So while that's happening, let's head back into the console for Elastic Beanstalk and do some configuration that we needed to take care of anyway. So, for the --- at the software level, there's a couple things we need to do. So the software configuration refers to the actual Linux operating environment that our application is executing in, and there's some other properties that we can tweak in here that'll be useful. So, for a Node. js instance, you have the option of using either NGINX or Apache as a proxy server for your Node. js application, or you can use no proxy server at all, but probably you do want to have that. And then you can specify which Node version you'd like to run on your instances, and the most recent version of Node that you can elect for your instance right now is 6. 2. 2. There's a listing of this in the documentation that you can refer to. So we'll update that to the latest Node version that we can. We'll leave this blank because we have an npm start script, which is going to specify the command which actually launches our web application. And now when we scroll down a little bit. There are a couple of pieces that we do want to configure. So one of the things we want to make sure we do is have our static assets served by NGINX rather than Node because NGINX is going to do a much faster and better job of serving those up for us. So we are going to map the static path so any incoming HTTP requests that go to /static will be mapped to a virtual directory within our application. In this case, that's going to be the public directory, which is generated by our glup, or excuse me, our Grunt tasks. So, we're going to go ahead and add that. And we also need a couple environment variables. So we are actually going to be populating our RDS connection string here, but we don't have that quite yet. So, what we'll do for now is configure a few other bits we need. Number one is we need to create an environment variable indicating that our NODE_ENV is production. We also need to indicate that for npm they only --- npm should only install the direct dependencies of our project, not the dev dependencies. They don't need to install all the Grunt crap that we need for local development. They just need the production dependencies. So, I'm going to specify NPM_CONFIG_PRODUCTION equals true. And then just as a placeholder I'm going to put in RDS_CONNECTION_URL, and that's going to be the actual database connection string for the Prostgres database that we just kicked off the creation of a little while ago. So let's go over here and view our database instances and see how we're doing. So it looks like our instance is not quite created yet. So we don't have an endpoint for it as yet. So once it's actually created, we'll have an endpoint that our Elastic Beanstalk application can actually address in this application. So, hasn't been created yet, so we'll check back here in a little bit. But for now we'll apply those configuration changes, and whenever we change the configuration like this, the environment is going to start an update. So, if you only have one instance, then Elastic Beanstalk will spin up another instance and begin the update, and then once the update is complete, it'll start accepting all of the traffic, and then other instances that need to be updated will be updated after that. Yeah, so this is going down. We'll check in again on our RDS instance. Still creating. Alright, so once we have the RDS URL that we can add to our configuration, we are going to be mostly ready to actually deploy. So to get ready for that, we're going to run the grunt collect_static command. What's going on? Unable to find local grunt. Oh yeah, we need to npm install. Alright, so these steps that are actually preparing the application, in our case, happen on a continuous integration server. So when we push a commit to master, our CI server is going to create all of our static assets and do whatever command line functions are necessary to get our application ready to deploy. And then we'll actually zip up a deployment artifact that we will send to Elastic Beanstalk from our CI server. We'll send that to S3, and then we will kick off a deploy of that artifact that we created. In our case, we're going to be doing the creation of the zip file right here on my laptop, but typically what you would do is set up a continuous integration server that would actually execute all of these tasks for you. So we're doing the npm install, getting all of this ready so we can execute our Grunt task to create our static assets and the public directory where all of those will be held. So, while that's happening, we can check back in on RDS land. So it's backing up, which means it's pretty close. That's good. Alright, and while that's happening, there is one last piece of configuration that we need to do. We're going to go into our EC2 configuration. (Working) Let's see, where is this? Clusters, Task Definitions, Repositories. Oh wait, that's ECS, not EC2. That is for Docker containers. I'm not interested in Docker containers right now. Alright, so we're going to go into our Security Group configuration for our EC2 instances and look for some security groups. So, we have two security groups that were created for our todomvc application. One was created for our Elastic Load Balancer configuration, and one is created for our EC2 instances. So I'm going to take a look at this one. And yep, I think that's the security group we're looking for. Now I need to go over here to the Inbound tab. I see a lot of heads and hands here. I know. I feel your pain, but the initial setup is definitely where most of the pain is. So, what we're going to do is Edit inbound rules, and we already have a rule that's going to allow HTTP requests on port 80 to go into the --- to be accepted for this security group. We're going to add a rule, it's going to be for PostgresSQL, and we are are going to put in the security group again for our eb instance, so that's going to be this one, this AWSEBSecurityGroup. And we'll hit Save. This is being recorded. Don't worry. You can go back and check it out. Alright, so now let's see if we have our RDS instance ready to go. Awesome, it looks like it is available. So now if we go to the configuration details, we can see we actually have an endpoint for it, which is great. That means we can configure an environment variable in our EC2 instances which contain the connection string for the database.

Production Environment Demo, Part 4
So, we're going to go in here, into our configuration --- software configuration and our RDS_CONNECTION_URL, and we're going to create a Postgres connection string just like we have been for local development, only this the one that we're going to use in production. Our username is going to be awsuser, awspassword, and that's going to be at the URL for that RDS instance that we just saw, and then /todos is going to be the name of the database. We'll hit Apply. And now we should have the necessary environment configuration to actually get this bad boy going. So once this environment update is complete, we're going to do the penultimate step, which is an eb deploy. So be ready to do that here in a just a minute. So with the environment updating, we can't actually kick off a deploy. And we can see that right now the application has not been successfully deployed yet, so we can't see anything on our Elastic Beanstalk instance. A couple of questions are coming in here. Where's the best and safest place to have Elastic Beanstalk pull in the AWS access key and secret access key? When you go through the eb init process, it's going to actually save your access key and secret in a DOT file like in your home directory, and that's generally speaking where I --- it's where like everybody who deploys to Elastic Beanstalk on our team keeps it, in like a. aws directory essentially in your home directory, so that's where that goes. And the next question, is it safe to put the db username and password in an EC2 environment variable for production? If you've configured your RDS instance to not accept traffic from the outside as we have, like it's not possible to connect to the database outside of the current VPC. In which case it's not the end of the world to have the connection string as an environment variable, but what we typically do is --- like there's a way to store secure credentials in a S3 bucket and then load them into the system environment. So rather than like managing it through the administrative interface directly, you could pull in credential information from a S3 bucket so that it's not accessible directly through the console interface. So there's different ways to approach that. This is probably --- again, because it's not publicly accessible, it's not the end of the world, but probably the best place to do it is in a S3 bucket like in an XML or JSON file or something like that. Then it will read those in from your application. Alright, so now our environment is actually in a decent state, so now we've actually npm install. I think we still need to do grunt collect_static to get everything ready to go. Now we can do an eb deploy, and that's going to package up all of the application logic in our directory to a zip file. It's going to create a new application version, which we can see through the administrative interface, and ultimately it's going to deploy that new version of the application to our instances. And what we saw before --- what they're going to do once the application version has been deployed is run these commands that we set up. So we have a npm prestart command, which is going to run our migrations against the database. So if there's any available migrations to run, those will get run prior to the application being deployed. And then we have a start command which will run our server process and start listening for inbound connections on the configured HTTP port. So, if all goes according to plan, we should have a new version to look at here in a few minutes. Alright, so I'm afraid to look, but that looks like --- suspiciously like a todo, and it seems like we are successfully running an application in production on Elastic Beanstalk. Did you get up there and add some? Good for you. Good looking out. So now we have our RDS instance connected to all of our EC2 instances in the same security group. We have an Elastic Load Balancer, which is what this URL is pointing to, is accepting traffic from the outside world, and it is routing it to all the instances in our Elastic Beanstalk configuration, and our Elastic Beanstalk again is talking back and forth with a RDS database. So, that's all it takes to deploy an application on Elastic Beanstalk, but the good part is once you get all of this set up, really all you're going to be doing going forward is eb deploy. Like you're going to be zipping up your application into an archive, you're going to be deploying a new version, the new version will get installed on all your instances, and you can sort of go about your business. There are ways to do a lot of the things that we did manually via API, but I wanted to go through kind of all the different steps so you could understand hopefully a little better what it's going to take to actually get a new environment provisioned on Elastic Beanstalk. Phil asks, do you recommend having a staging environment before going to a production environment on AWS? Absolutely! You definitely want to do that. So right now we only have one environment that we can push to, but it's possible to deploy to multiple environments. So it's definitely good to verify your changes in a stage environment as we definitely always do prior going to production. The other thing that's actually kind of nifty about Elastic Beanstalk is you have the ability to blue-green deploys. So you can deploy your application code to an environment, which is essentially the standby environment, you can validate that everything is good to go in that environment, and then in the administrative interface or via API you can actually change this public-facing URL. You can swap URLs with another environment. So you can with 0 downtime just convert to another environment that's already been deployed and verified to work like directly through the interface, which is actually rad. We didn't actually take advantage of that because we don't have access to the console because when you work in a large organization things can be pretty locked down. But it's something that we'd like to explore doing because it's definitely something that can be done via API as well. Going into the dashboard here, let me just point out a few bits in the administrative interface for Elastic Beanstalk that might be useful. In the --- with all of your applications, there's multiple potential versions of the application that could be deployed. So, if I'm at this level looking at my todomvc-plusplus application, I can see all the environments that are running this application, and I can also say see all the application versions. So this is the current version of the application that's deployed to production, so you can see it's currently deployed to this environment. And I can deploy past versions of the application. This is how you would do a rollback like if something --- if some stuff hit the fan. You could roll back to a previous deployed version of the application. So there are a few things you can do here. In the environment configuration, there's also a few bits that are worth calling out. The log interface, you can actually view the last 100 lines of the logs directly within the administrative interface, but we also have this eb logs, oops, eb logs command that we can run from the command line interface, which will give us the last 100 lines of a couple of different logs. It'll give us the output from our Node. js application, and it'll also give us like the logs from the deployment commands that are happening on our EC2 instances. So here we have --- we can see the output of our var migrations, and then we start to see the morgan logging that you guys were nice enough add into our application earlier this morning. So you can retrieve the logs in that way. The other bits worth showing off in the Scaling configuration, this is where you can start to toy with the settings for your application. We can sort of mandate that there's never going to be fewer than two instances --- two EC2 instances running the application at any given time. We can put an upper bound on the number of instances that can be running the app. We can configure different availability zones. And we can also configure different scaling triggers. So we can say that when the network bandwidth reaches a certain level, that is the trigger point at which we can spin up another instance. So there's a lot of capability that we have there. And then on the instances themselves, this is where you can actually configure the type of AWS or type of EC2 instance that will be running your code. The default is a t2. micro, which is pretty small. It has like one virtual core in like 513 MGs of memory, I think, and it kind of goes up from there. So you can configure the actual machines that are running your application based on your performance requirements. And we've already kind of looked into the Software Configuration, which is another bit. We'll also look at some of the health check features that are present here tomorrow when we talk about like monitoring this application environment. There are a few other things that you can mess with. For updates and deployments, by default a third of your fleet of instances, whatever that happens to be, if it's just three, it'll be just one of them, but essentially like when you're doing a deployment, that's how many of those instances will be taken out of the load balancer to perform your application update at a time. And once that one instance has been updated, it's going to be put back in the load balancer, and the other instances will be taken out, and that instance will start accepting the traffic from the load balancer. And you can configure like how much of your fleet you want to take out and update and put back into the load balancer there. Alright, so that was a tremendous pile of stuff to do, but for anyone who wants to attempt it, I will take you through the steps. The documentation or the place to start that I put in the chat was starting with the EB CLI, then, again, I think actually bringing up the 10, 000 foot view of this process here. Within a region, you'll create a user, you'll download those user's credentials, you're going to create an Elastic Beanstalk environment from the command line, you'll create an RDS instance through the administrative interface, and you'll configure a security group to allow incoming connections. So there are a few pieces of that process that are a little bit putzy, so I'm happy to help you through any of those individual pieces if you'd like to give it a shot yourself. But again, I think the benefit here is that this is being recorded, and all of you can watch that ordeal again at another time when you're actually going through this and clicking the button. So there are definitely a lot of pieces to set up, but hopefully that gives you a sense of like what the moving parts are during the process.

Alternative Production Environments
And as we've always done, we talk about the alternatives. Obviously, if you go with a Platform as a Service like Heroku, it's fully managed, your DevOps responsibilities are virtually nil, but it tends to cost a lot of money at scale, whereas if you go to the other extreme, managing your own infrastructure, what you don't get is this orchestration software. So like when you do an EB deploy, like there's no management software that's going to take some of your instances out of the fleet, update them, put them back in, and do those changes for you. So, there's lots of other open source software in Amazon tools that you can use to accomplish this, but that is essentially the value that Elastic Beanstalk provides is that even though it is kind of a pain to set up, once it is set up, the orchestration that you get is actually pretty strong. There's just generally if --- the more control you have over the infrastructure, the more DevOps you're going to have to do. And really what this comes down to is resources, right? Like if operating your service at scale is more expensive than the time you might spend on DevOps, then that's the time when you're going to want to probably manage your infrastructure versus if time is more precious than the --- time is more expensive than the amount of money you would spend on server infrastructure, then going with something like a pass is probably a wiser choice. And Elastic Beanstalk is kind of in the middle. It's the gateway between something like Heroku and a fully self-managed EC2 environment.

Exercise 4: Deploying the Application, Part 1
I think what we can do now is folks that want to take the plunge can start doing that. I'll come around and help anybody who wants to go through the actual deploy process and/or answer any questions about anything else that we had today. If you are planning on doing the Elastic Beanstalk deployment, we just sat and did it for a good 30 minutes, so that's probably about the budget that you would set at this point for getting that going, which is totally cool, and it's kind of why we're here, or one of the reasons why we're here. So if you're down to give it a shot, I will handhold all of your way through that process as best I can. And you're planning on writing some documentation about this too? Yeah, as part of the written materials, I'd like to do some written stuff on this as well. So all the stuff that was just deployed, is that all free then to run, or is it hourly? Is there any hourly? All the stuff that I used was within the Free Tier, like all of the instances were t2. micros. So yeah, this was within the Free Tier, but you can sort of dial up any of the individual pieces to more expensive. So you could run a ton of little application like this on Free Tier, and then it's just when you want to dial them up is when the charges will start coming in? Yeah, yeah, and I think like the Free Tier is --- my understanding is like you have 12 months with it or something, and then you start paying regardless. It's --- but yeah, I think at this level, and I think there might be limitations on numbers as well. You can have 750 hours. Seven hundred and fifty hours a month is the Free Tier limit. So it's one instance in 31 days. If you've got two instances, it's 15 days. Of computer time essentially. Yeah. Gotcha. So you want to make sure you aren't just spinning these up all the time. Not willy-nilly, no. I'd be lying if I said I didn't have like a monthly like $15. 00 Amazon bill that I just have no idea like where I got it from. That's what I'm afraid of. You can just opt for the $15 thousand enterprise one. Yeah, I don't know man. But no, I think I just have enough little stuff where I've ran out of the Free Tier on a couple accounts. For those of us that wanted to go through this, I mean could you go through that list like slower, the whole setup? You went through it fast, and I thought there was no way I could be here and watching at the same time. Yep, absolutely. I can definitely do that. I can kind of go back into the environment that we just created and show you some of the bits that we had to configure, like kind of from the beginning, if that helps. Alright, so great request is to kind of retrace the steps of that deploy now that we, like we --- there's a lot of down time in there where we might have missed some things, so I'll just go through and retrace the steps of the deployment from top to bottom, or provisioning the new environment. So the first thing that I did was here in the console I go down to my Identity & Access Management interface, and if I don't have one already, I'm going to create a user for use in essentially deploying stuff, like getting credentials to deploy stuff to Amazon. So we create a new user with a username, and I'll just delete that one right away. So we create the user, and then we download the security credentials for that user so we can use that authenticate the Elastic Beanstalk command line interface later. After we create that user --- here I'll just --- I'm going to delete that right away. Alright, so after we create that user, we want to add that user to a group, so we check that user, and then we say Add User to Groups. I think I actually already added it to the only group that I had, like this one. But if I do like … So how do you download the credentials? After you create, I'll do it one more time, so after you create the user, there's a button here that says Download Credentials. Okay, I was trying to get it from an existing user, so I have to. Yeah, from an existing --- you only have one opportunity to download credentials for a user after you create them; otherwise, you have to like regenerate credentials for a user. And then for a new user that's been created, you add them to group. In my case, I only have one group, and that's like the super admin group, so you would choose to add them to that. Did you have to create that ahead of time, or is that something that we'll have to do? Yes, you will have to create that group. So when you create the new group, you can say like, you can call it SomeGroup, and then after you create the group, you can choose like a preset policy from a list that will apply to that group. AdministratorAccess is the GodMode policy. And then there are lots of other more granular policies you can choose from as well. So you create that group, and then you can --- all the users you assign to that group will have that set of permissions. So it doesn't matter the name of the group really. It just matters that that user is a member of that group and that have admin privileges? Yep, and that they have whatever privileges on whatever AWS resources they need to access, and using the super admin policy means that they have everything. So once you have a user created who's assigned to a group with a sufficiently permissive policy, then you're ready to start cranking out some actual stuff.

Exercise 4: Deploying the Application, Part 2
So it's possible to go to create environments through the console directly, but what we did during our exercise was actually create it from the command line using the Elastic Beanstalk CLI. So we cloned our application, we entered that application, and then we started off with eb init. And eb init is what took us through choosing an AWS region to run our application in, it's where we created an environment, excuse me, and it's where we sort of would configure our credentials for our AWS user that we used to use the Elastic Beanstalk command line interface. And then after, we ran eb init. So when you're going through that, it asks for this AWS access ID. Where do you get that from? When you downloaded your credentials before, like in the console when you created the user, that download would have a CSV in it with like an access key and a secret value. Where does that get stored if you had a user already created? If you had a user already created, but you didn't download the credentials, you need to regenerate credentials for the user, and whatever credentials they had previously would no longer be valid. And you can do that from the user? Yes, you can. So for individual users, like here's --- here I'll do a different one. So here's the user, a user I created for an earlier test. So here is their credentials. You can see the access key again, but the secret you'll never see again. But what you can do is you can create a new access key, and you can download those credentials, or you can click on this link, and you can actually show them in the browser. So you have a chance to create like a new set of keys as well. Does that kind of make sense? So you do --- so you create an access key, and then you download the credentials there, and then that file is the one that you put in when you do the eb init. Yeah, you don't put in the file, but you use the access key and the secret strings from that file that you downloaded. So here I'll just download this, and I'll show you what I mean. So I just created these access keys, and then that's going to be downloaded in like a CSV. So if I open that CSV with like a text editor, it's got my user name, my access key, and my secret, and those are the values that I will plug in during the eb init. Does that kind of make sense? So, the eb init is where we plug in our AWS credentials, and we sort of initialize our application. And then eb create is where we create an actual environment with EC2 instances in it that we can actually use to run our code. And once we --- and that took a very long time, but once it was created, we ended up with this. We went into --- when we went into Elastic Beanstalk, now all the sudden we have this environment. And the way we did before was under configured, so it wasn't okay right away. But we have this environment which is actually running EC2 instances which have our code. So on the configuration side, what we needed to do is go into our Instance configuration and check out like what EC2 security group the EC2 instances managed by Elastic Beanstalk are a member of. And that is a value that we had to use a few other places, if you remember. The first was when we created a new RDS instance, which is what we did next. So we went into RDS, and we created an instance for PostgreSQL, and during the configuration process we added that RDS instance to the same security group that our Elastic Beanstalk EC2 instances are a part of. So that essentially enables them to communicate with one another. Once we created that RDS instance, put it in the same security group, the other step we needed to do was to go into the EC2 configuration, go down to Security Groups, and then select, I believe it was, if I mouse over that, yeah, so we selected the same security group in this like administrative interface, and under this Inbound tab we needed to add another configuration for Postgres on port 5432 that would allow inbound connections for members of this security group. Is that a typical port for Postgres? What's that? Is that a typical port that's used? Yeah, 5432 is the, sort of the default. Standard, yeah.

Exercise 4: Deploying the Application, Part 3
And then once that security group mumbo jumbo was configured, then we --- at that point we pretty much had everything we needed to actually deploy the application. So we had to complete the configuration of our software. So if we go back to like our actual Elastic Beanstalk application in Configuration, we changed our software configuration. We changed the version of Node that we're running on our instances to 622. We created a virtual directory for our static assets so the NGINX proxy server on each of our instances will serve static assets rather than Node doing it. The Express static server is much slower than NGINX would be. So we configure that directory so NGINX will serve up our JavaScript and CSS and images and what have you. And then we created three environment properties. We set the NODE_ENV to production, we set the NPM_CONFIG_PRODUCTION value to true so that when Elastic Beanstalk does npm install on our application it only installs the dependencies that are necessary for production, and then we also created a Postgres connection URL, which we are here storing as a system environment variable that we created through the interface, but an alternate way that you could do that is to store this connection string somewhere else outside of the administrative interface. You could store it in an S3 bucket or something of that nature. Because this database can't accept connections from the public internet, it's like not the end of the world necessarily, but the proper way to do this with any secret value like this would be to have outside of the administrative interface. For our purposes, it's a decent solution. So, after that was configured --- Bruce asks, does anyone know what the needed environment properties are for the todomvc config screen? That would be these three. We need --- the environment variables that you need in production are the NODE_ENV set to production, the NPM_CONFIG_PRODUCTION flag set to true, and then a RDS_CONNECTION_URL, which is the database connection string for the actual Postgres database. And that URL string comes from --- the URL string for RDS comes from configuration for the RDS instance that we created. So if we go to our Instances over here, here's the one that we created. If we inspect its properties, we can see what's labeled as the Writer Endpoint over here, and that is essentially the internal address of that particular database. And when we created the database, we created a user, which we called awsuser. So we included those credentials in the format Postgres://username: password at this URL:5432 at this URL port 5432/todos, which is the name of the database that we needed to connect to. So that is the full genesis of that RDS connection string. So if you create an instance like that, and you're in development, and you just let it sit for a while, does it just go away after a month or something of inactivity, or does it stay there forever? It'll stay there forever. So yeah, you can --- the default configuration is that you can only ever have one instance running, so we can take a look at that. So, that is actually like an Auto Scaling or like a scaling configuration. So, if we go in here for Scaling, we have like the minimum instance count of 1, and then we also have the maximum of 4 configured right now. (Audience comment- inaudible) Yeah, I know. I'm too excited. I can't sit down --- there's --- with this must raw AWS action going on. So yeah, lots of yak shaving at the beginning. But when you get past that, as I said, we're really in the situation of building your application, zipping it up, and deploying it with eb deploy. There are different options you can do. You can deploy applications to different environments. There's lots of flexibility that you have with the platform, but in terms of the workflow, it improves significantly from here on out. So was that helpful to go through one more time? So yeah, there we have it. I think I've successfully made sure that like 50% of you will run screaming for tomorrow with dropping this AWS bomb on you today. But tomorrow, now that we have this bad boy up in production, we're going to be focusing mostly on more of the application stack. We'll be looking at the front-end toolchain. We'll be learning a little bit about Vue. js and how we implement a rich front end with Vue. We'll be adding some features. We might've successfully got our back-end application working to set that completed flag, but we still have to do some work in the user interface to make that happen, so we'll be doing some work with Vue. js tomorrow. We'll be talking a little bit about monitoring a production environment. We'll add some modest real-time features with Socket. IO to our todo application so we can see how that plays out in our production environment as well. Nothing too crazy, but we will be adding a little bit of real-time spice, and we'll be looking at how to implement web analytics. Because we're using Node and Browserify, we have kind of a nice way to implement Google Analytics in our application both for custom event tracking and for basic stuff like page views. So we'll implement universal analytics in the front end as well. So yeah, I think --- you think this is definitely the hardest part, so if you want to keep banging on it, we definitely still have some time. I'm happy to go through any of those steps again for anybody. Question if there is going to be merge for the exercise3 solution? And there definitely will be. I'll push out my local changes for the database piece in the migration. We have a question if we can work along with our local environment tomorrow? You absolutely can. You don't have to use the production environment if you don't want to tomorrow. You can just use your local development environment. Alright, with that, I think I'm going to un-mic, but I will stay on the chat and stay in the room for anybody that wants to keep on trucking. If you have questions on the deployment stuff or on the any of the Node stuff that we covered today, I'd be happy to answer any questions that you have or talk about specific scenarios that you're interested in. I know that was a lot, I hope it was helpful, but once you kind of get through all this stuff, as we'll actually see tomorrow, this environment can actually take a decent beating much more than you would think out of compute resources of this size. So we'll beat up on it a little bit tomorrow and see how that works as well.

Front End Toolchain
Day Two Agenda
Today, we're going to look a little bit at starting off with kind of a deeper dive on the front-end part of our application. So, we've sort of handwaved at the primary tools in the front-end toolchain a little bit, but I wanted to dive in a little bit further and kind of explain some of the things that are going on there and some of the tools you'll bring into bear. Did you have question, Mark? Yeah, Phil asked to see an example of how to properly store credentials in S3 like what was suggested yesterday. Yeah, yeah definitely. I don't have that in the materials, but maybe like over lunch I can whip something up to show that. It's really just a matter of like creating a S3 bucket, uploading a credential file there, and then accessing that S3 bucket from your instances. But that's essentially what it is, and that's --- there are different ways to accomplish that, but having an S3 bucket with restricted access such that only members of a certain, kind of like we do with RDS, such that only members of a certain group can have access to that bucket. So like in theory, you can configure it such that an administrator creates credentials, creates the connection strings. That administrator uploads those values to an S3 bucket, and only the instances running in production have access to actually read that S3 bucket. So me as the developer, like I never know what those values are, but when I deploy my application, I read in those values from S3 because I only have access to that bucket in production. And that's kind of what we do with our production environment in our ops team. They provision the credentials, and they give our production instances access to that bucket, and then we read in those credentials from there. So at a high level, that's how it works. But yeah, so I'll see if I can do a specific example, but that's essentially what it would be, provisioning an S3 bucket and giving permissions to the Node app to read from that bucket, but only in production. So again, I think what we're going to lead off with today is diving a little bit more into the front-end toolchain. We'll start off with a little look at Browserify and Sass, kind of the two primary tools we're using for our JavaScript and CSS code preprocessing. We'll dive into a few of those features, do a few little tweaks to our todomvc-plusplus application with those tools. And then we'll spend a good amount of time this morning, we might extend the workshop time a little bit since it'll be a new framework for most of you, talking about Vue. js. Maybe it's voo, maybe it's vue. I don't know. I always said vue. I thought it was just a cleaver way of saying vue. But we'll be extending Vue. js with a few more features that we've been lacking in the todo application like actually persisting the state of a todo item to the database and also playing a little bit with some of the capabilities Vue has on the Vue model and hopefully getting a little bit more familiar there. We'll also do a, kind of an extended live code walkthrough some of the key features of the framework just to give people a taste of how it works. After that, we'll dive into adding some real-time features to our application. Again, we'll do a little bit of a code walkthrough on how Socket. IO works at high level, and then we'll add a specific real-time feature into the todomvc application so we can see how we can maybe update that todo list in real time as folks are adding more items to the list. We'll take a little break. We may or may not start Socket. IO or finish Socket. IO before break. We'll kind of play that by ear. Then this afternoon we'll talk about a couple of tools that we use in our production applications for load testing. We'll take a quick look at a tool called Locust, which'll help you do some load testing on your web applications. And then we'll have you, in the exercise, actually integrate a tool that we found really useful called Rollbar, which is a service which tracks exceptions that are raised by your application and allows you to send out notifications with that information. So, we'll actually integrate that into our todomvc application as well. Then we'll wrap up with some web analytics basics. Google Analytics is a tool we use all the time, and I think it's sometimes poorly understood by developers, or maybe developers feel there's nothing you can that you glean that useful from this tool, so we'll talk a little bit about some of the information that you can get from the tool. We'll look at --- I'll try to equip you with a couple skills within the Google Analytics UI that you can use to track effectiveness of your web applications, and then we'll look at the technical implementation side and add a couple Analytics tracking features to our todo application. And then we'll wrap up with some Q&A, and if there are specific topics that we want to dive deeper on, maybe we can look at that. Yeah, maybe we can look at the S3 bucket bit there, maybe do some live coding there. But yeah, that's kind of what we're going to do here on day two, and it's going to be a lot of code, and I hope you enjoy doing it as much as I do.

Browserify
I think we'll start off with the two kind of heroes of our front-end toolchain, which are Browserify and Sass. And they help with a lot of different things that make our front-end code much more expressive to write and also more performant and feature rich. So Browserify and Sass are both preprocessors, Browserify for JavaScript and Sass for CSS. In our application, we're actually using Browserify as the primary mechanism by which we transform our JavaScript into actual cross-browser JavaScript code that can run across, again, across a variety of browsers. So Browserify provides a few things. It actually has a transform, which we'll talk about next, that runs it through another tool called Babel, which ultimately produces cross-platform JavaScript code that you can actually serve, which we've been doing this entire time with our todo application. Sass is a preprocessor for CSS. It takes a language that is a superset of CSS features and converts that to plain Vanilla CSS. So Broswerify, I think, is maybe one of the key technologies of the entire Node. js ecosystem. It's a critical component of being able to realize this idea of isomorphic JavaScript where you can write code that can run both on the client and on the server. And while as a practical matter you don't really that often share a ton of code between client and server unless it's pure JavaScript, the primary thing that Browserify provides is the ability to write your code the same way. So when you're writing a Vue. js component, you're still dealing with a CommonJS module, you're using the same language features that you can use on the server in Node, and Browserify makes all of that possible, so there's no context switching where you're going to the front and back end. If you've worked, as I have, in languages where JavaScript is purely on the front end and on the back end you're writing Ruby or Java or whatever, you know that there's a switch when you go from one language to the other, and Browserify makes that switch pretty much seamless, so it's a critical tool in my mind for the success of Node. js. So Browserify itself provides like one primary service for your application, which is to analyze the dependency structure of your code to see which modules are required, and then it builds a JavaScript bundle and a sort of integrated require system which will allow that your Node. js style CommonJS code to run in the browser. And that's pretty much what Browserify provides, but the other thing that it does is it provides an affordance for transformations and plugins. And the transformations that you can include along with Browserify do lots of very useful things. In our case, we're using a Browserify transform for Babel, so we're able to, during that process of packaging our JavaScript, convert code that is written on the server for a ES2015+ version of JavaScript, and we can actually take that code and convert it to regular ES5 code that can run on a variety of browsers. So the babelify transform provides that functionality for us. And that's just one of many different transforms that you can use. There are transforms that allow you to require like templates. So if you have Jade templates or Pug templates I guess they're calling it these days, you can actually require those files, and there's a transform that'll turn it into a JavaScript function that you can require directly in your JavaScript code. So there's a ton of really useful technologies and transforms out there that you can integrate into Browserify. The transforms are typically installed as peer modules of Browserify. Browserify itself doesn't ship with any of these transformations. And while you can execute Browserify directly from the command line, typically you can execute it with a variety of options. Typically, what ends up being expedient is configuring your Browserify transforms and options within the package. json for the project. And if you look in the todomvc-plusplus repo, you can see the configuration options for Browserify. So in this case, we're right now using two transforms. The first is browserify-shim. Sorry for the laser pointer usage for those online. But browserify-shim will --- helps you to shim in different modules that may exist as regular JavaScript modules so they can be required in your application. So in the configuration down below for broswerify-shim, we have the ability require jQuery in our CommonJS style browser code, but that's essentially going to be a reference to the window scoped dollar sign object to which jQuery is assigned when actually running in the browser. In addition to the shim transform, we also the babelify transform, which again is a framework for integrating lots of different features for advanced versions of ECMAScript into your application. In our current application, we're using the ES2015 presets, which add a lot of language features that don't yet exist in most browsers, but there's other presets that you can use. There's a popular React preset, which allows you to convert JSX files with integrated Vue code into regular JavaScript as a part of this process as well. So Browserify, key tool in our toolchain here. Where is that configuration file? That's actually going to be in package. json, so down near the bottom.

SASS and SASS Alternatives
The other key player in our front-end toolchain is Sass. Sass provides a superset of CSS features, as I said. If you write just plain CSS, that is also a valid Sass or SCSS style sheet, but it provides a lot more functionality that --- I'm sorry Browserify --- up in the chat James actually corrected me. The Browserify stuff in this project is largely --- is in the Gruntfile, but some of the transform configuration is in the package. json. So some of the Browserify transforms expect to find their configuration in package. json, which is why that's there. But the transformations are actually configured in the Gruntfile, I think, in this instance. Thank you for the clarification there. Alright, so in terms of Sass, we have --- it gives us the ability to better modularize our CSS essentially. It allows us to write our CSS like we would write our code. So, in our code we wouldn't write the same thing over and over and over again for different classes, but in CSS that's essentially what you have to do. If you want to use --- you'll find that you're using the font family declaration for multiple elements, and it's possible to write CSS such that you don't repeat yourself a lot, but invariably you end up doing that. So Sass provides the ability to create things called mixins, which allows you to have better modularization in your JavaScript code, or in your, excuse me, your Sass code. So you can essentially integrate sets of functionality into your CSS and like sort of reuse those mixins all across your Sass code. The other bit that it allows you to do is to specify variables and import external Sass style sheets basically anywhere within your Sass. Both are fairly critical features, especially like when you're talking about colors and other sort of sitewide design elements. Being able to assign those to variables that you can use in multiple places and then change once and then watch those changes propagate everywhere in your styles ends up being pretty clutch. Sass also comes with a fast C-based compiler, so it's fairly performant when you're actually compiling those style sheets as well. Can you talk a little bit about advantages or disadvantages of Sass over LESS? Yeah, I mean actually that's a --- we'll just skip ahead just a little bit. So it's fairly interchangeable with LESS in terms of style. I think it comes down to preference largely. Sass is cross-platform. I think LESS is Node. js only, if I'm not mistaken. You can probably use it everywhere. I'm sure it'll work just fine with other environments as well, but yeah. I'm going to extend that question. Why wouldn't you use something like Bootstrap 3 and jQuery UI for some of this stuff? So those tools, like the question if you didn't hear it was why wouldn't you Bootstrap 3 or jQuery UI for stuff like this? And those tools actually serve a slightly different purpose. So like jQuery UI does provide a set of CSS classes that you can apply to objects to make them look nice and turn into drop-downs and stuff like that. But for authoring your own styles for your own components, you probably will be authoring CSS from scratch anyway, and Bootstrap and jQuery UI help with some prebuilt classes that you can use for grids, and those tools work really well with Sass or LESS. In fact, Bootstrap, one of the things that you can do if you're using Sass, and I know at one point they had LESS styles as well, but I feel like at maybe at one point they converted to Sass. I'll have to check on that specifically. But if you actually download the Bootstrap Sass files, you could import them into your own styles and use only the parts of the Bootstrap styles that you're actually like leveraging in your application and which ends up being a little lighter weight than including a larger package that you didn't customize. There are also similar to Bootstrap is a framework called UIkit, which I would definitely recommend you check out, which provides a set of Sass, like Sass mixins and other bits that you can actually include in your own CSS to create like CSS3 transformations and useful things like that. So it provides useful mixins, responsive breakpoints, and stuff like that. Thank you. Yep, no problem. But yeah, so LESS serves largely the same purpose, and it's fairly interchangeable. At this point, it's --- for me, it's more a preference thing. I like Sass a little better. And webpack, as long we're here on the alternate tip, is --- also provides like module bundling functionality based on the new ECMAScript module system. It also provides some other module bundling features and functionality, also fully legit, also has a growing set of plugins. And I think at this point, like because Browserify has been around longer, I think it has a richer set of transformations and plugins that can be brought to bear using Browserify, but webpack has a large and thriving community of plugin developers as well. So again, fully a valid choice if you want to go in that direction.

Browserify Code Demo
Alright, so at this point I'll just poke out a couple bits to show you some features of these platforms. So, I'll go into the todomvc repository and just write some changes that I'm going to immediately throw away just to demonstrate some of the features that you can bring to bear. So for Browserify, I mentioned there's a lot of different transforms that you can use. And in fact, if you Google Browserify transforms, even if you misspell it, you're probably going to get it, you'll see this list of at least a few popular transformations. So we'll just kind of pick one out. I thought I was going to do --- there's a markdown one, which is kind of neat, which allows you to essentially require a markdown file as a string in your JavaScript application. So, just temporarily I'm going to install this without saving it because I'm not including the --save. I'm going to install the module locally, but it's not going to be added to my package. json, so none of you will get it if you were to clone the repository right away. And this is going to add a new transformation capability to our Browserify code. And you'll notice that the configuration isn't terribly well documented here, but if I go into, as James pointed out, my Gruntfile, I can see --- these are my Browserify options where I can pass in different bits, like whether or not I want to generate a source map, and then I specify a src file or multiple src files and a dest file for my bundle. This is kind of the core arguments you would pass in if you are executing Browserify like from the command line. But where the transformations are declared is here in the package. json. So here in the transform section I specify browserify-shim as one of them, I also specify babelify, and now I'm also going to drop in browserify-markdown as one of the transforms. So, if I fire up my --- What does browserify-markdown do? It allows you in your JavaScript code to require a markdown file and have that available in your JavaScript code as a string. Not terribly useful, not all the time, but just to kind of show you a feature that you can add to your transformation, I figured we would take a look at this one. So now I have this transformation added to my configuration. In my browser code, let's go into app. js say, and the feature that this transformation allows me to do is rather then just requiring JavaScript files, I can require markdown files, which will be converted to a string. It'll actually also be parsed into HTML if memory serves. So, just for demonstration purposes I'm going to require the readme for the project, and that's going to be up one directory. See if I can get the resolve right. So in src, I think it's one more, and then README. md. And when the application initializes, I'm just going to go ahead and console. log the contents of the readme file. So I'll start up my server, todomvc app. jsmust be object. Let's see, what did I mess up? Js/app. js must be object. (Working) Okie-dokie, so my markdown transform doesn't seem to be working because it's blowing up on this line. It could be that I didn't do this correctly. So up one directory would be browser, and then up another directory would be src, and then up. Yeah, no, that is the proper path. Well this is less impressive than I had hoped. (Working) Alright, I'll keep trucking for now, but the way that typically you would do this, like running it through Grunt you don't get the full output of Browserify, so you could run the Browserify command line utility to get a more detailed stack trace of why this isn't going, but I won't waste your time with that right now. So, I'll go ahead and take this out. But the idea there is that these additional transforms essentially give your code new capabilities that it didn't have before. So I'll get that running hopefully while you guys are at the break or while you guys are working on the exercises, figure out what went wrong there.

SASS Code Demo
The other bit that I was going to show is just a few minor features of SCSS. So here is the very simple CSS that we have for the application. Most of the styling for the todomvc application is provided by some external style sheets that we don't necessarily care about. Those are actually located here in the base and index style sheets. All of the different implementations of todomvc, of which there are many by the way, I don't know if I called that out yesterday, but if you're interested in seeing like the same application implemented across multiple frameworks, you can head out todomvc. com and kind of compare and contrast different approaches to this. So is this the place where I got the code if you haven't navigated out there already. So in our case, we actually don't have a lot of custom CSS yet, but some of the features that Sass provides you that you wouldn't necessarily have otherwise are the ability to create variables and mixins and to nest your CSS, which is actually one of the things that I find most useful. So, if our server --- let's go ahead and run our server. Oh, come on now. Unexpected character. Oh yeah, I didn't back out the JavaScript changes, so let's do that. Okay, super! Alright, so here's our todo application. As you can see, I was spamming in quite a few entries into the todo list earlier on. But when I'm writing my CSS code, I'm usually kind of concerned about the structure of my HTML. So I have this section where all of my todos are, I have this unordered list with all my todos, and I'm use to being able to write HTML in a nice nested structure like this, and Sass lets me write CSS in that nested structure as well. So in here, let's say I wanted to add some styling to the list items in my main class. So, I can come in here and write a selector for the main div much like you would in Vanilla CSS, and what I could do if I wanted to target the list items of this list, I could write a selector that went something like this. So for the unordered, excuse me, the unordered list items, these are the CSS classes I would like to apply. But that's not super dry because if I also wanted to style the unordered list itself, I'd have to another CSS declaration up there to do that. So one of the nice things that you can do in Sass is nest those declarations. So, within the main div I'm going to add some styles to the ul. Let's say --- this might break the layout, but I'll add a nice big padding all the way around the unordered list, and then for the list items I'm going to change the font-style obviously to Comic Sans MS to class it up a little bit. So our tooling found the changes to the Sass, automatically recompiled, and if I come back out here, looks like I needed to be more specific with my list item styling, but I've got my big padding around the ul. Obviously, we could just do that. I won't actually fix it. I just saw a tweet that important is kind of like importing ants --- Importing ants. --- into CSS. In fact, it must not be li, like there are probably --- oh, there's another div in here called the view and an input. And oh, it's a label, so I don't know if it's cascading down that far. So, maybe if I did the label. I just have to see it in Comic Sans now. I have to get my fix. Actually, it's because of your CSS properties use font-style. Font-family, oh yeah. There we go. Ooh there we go, much better, yes. So, that's one of my favorite features is that you can keep your CSS much drier by nesting it in this way. And let's say now that we've decided that Comic Sans is clearly the best font for this application, and we want to apply it globally, we can create a variable called appFont or something like that and essentially set that equal to this string. So we can use this Comic Sans MS font-family declaration, and we can use that variable here as the appFont. We could also say for our, I believe it's an h1, I'll double check. Let's say for the h1 in our application I want to do the same thing. I want to use the appFont font-family. So if I come out here and refresh, that is actually quite a bit better. I think they should've done that to begin with. So a few bits there are really useful. And let's say like your CSS is getting a little crazy. You feel the need to break it up a little bit. If you want to import CSS into regular CSS style sheets, there's one and only one place you can do that, which is at the top of the CSS file, and you can't do that nested within other CSS declarations. But what we can do with Sass is move this content out to something called a partial, which by convention is a file --- as a file name, we prefix it with an underscore. We could call this _main. scss. And here in that Sass file I can have these declarations. And not necessarily that you would structure it in this way, but now rather than declaring them in line, I can actually include the main. scss. Is that underscoring it? The underscore is conventional, it denotes a Sass file as a partial, and when you do the include, like you don't actually need to include the underscore. Include expected identifier was. What? Oh, you know what? I think I just goofed this up. I don't think I need the parentheses. I think I can just do that. Or maybe do I not even need that? I might have to --- what's that? Is it include or import? I thought it was include, and I think I do need the quotes. Yeah, I think it might be import. Yeah, there we go. Yeah, it's the danger of doing it live that's for sure. So now if I refresh my page, I still have my same stylings there. So, you can include like nested CSS in this way as well. So basically it gives your application CSS an entirely new bag of tricks that it didn't have before. What is it that's actually monitoring the Sass files and compiling it to CSS? Yeah, right on. So, in our Gruntfile, the default task which runs like the development app server is this one, the default task, and what's going to do is it's initially going to compile our Sass and run our JavaScript code through Browserify, like right away when we launch it, and then it's going to launch this task concurrent:dev, and that's going to do two things. It's going launch two tasks, one of which is nodemon, which is going to watch for changes to our server-side JavaScript, and the other is this guy. Where is concurrent? Where did I define that? Here we go. So, here's were we defined like the tasks that we want to run concurrently. One of them is watch, which is going to recompile our Sass in our front-end JavaScript, and then the other one is nodemon. So, it's really Grunt that's in the background watching for changes to your Sass files. Yeah, it's a Grunt plugin called watch. It's one of the --- it's in the contrib package here. So this watch plugin has that capability. It can run tasks when a certain condition is true.

Exercise 5: Building on Front End Tools
So we talked a little bit about some of the alternatives. So at this point, we're going to take a little bit of time to knock out a few more issues in our GitHub repository related to Broswerify and Sass. And these ones are fairly simple, should be a nice warmup for the rest of the day. So let's head out to GitHub and see which tasks have been flagged with exercise5. So, we have a couple of bits here. The Browserify-related task is to add ES2016 presets to our Babel configuration in addition the ES2015 presets that we're currently using. So our front-end JavaScript, clearly not progressive enough. Need to add in some additional next generation ECMAScript features. So, we are going to be changing our transformation configuration to allow us to include those ES2016 features as well. So that is one task to add to the configuration. The other is to update the styles for our todos page. And the thing that you're going to want to accomplish here is to ensure that the links in the footer --- so down here it's going to, I'm going to have to scroll way down. So these links in the footer right now are not colored significantly differently than the rest of the text, so what we'd like to do is make those links in the footer the same color as this header up at the top of the page. And I don't know about you, but that's really hard for me to read, so I think messing with the opacity or the color of that h1, deciding on one that's a little bit easier to read, and then essentially ensuring that the color is stored in a variable that we can use both in the header and in the links way down here at the footer. So it'll require to use at least that one feature of Sass. So I feel like it's something we can knock out in a relatively short period of time, so I'll think we'll take about 20 minutes and then come back as a group to knock it out. Yes, Mark. There's a question about is there a reload module that you recommend? There is LiveReload, is a popular one that you can use with Grunt. I don't use one. Like I work on a project that does use one, and it like, it just constantly pisses me off because reloading the page I'm on like every time I save a file just isn't the behavior I want usually. So I don't use one, but LiveReload is a popular one. So LiveReload would reload the entire page. I think hot reloading is reloading an individual module. Oh right, yeah. _____ which is typically people who use like hot reloading with webpacking, React, right? Yeah, I think that's a feature of those tools, so I don't have one that I recommend because I don't --- Yeah, I know that there's Hot Module Reloading upfront really that I know of. Yeah, I'm not familiar with that specifically. Yeah, I know that React tooling does that. It's especially to a great effect like if you're using React native, like it'll reload the code in the simulator, but there isn't one that I use. So, my answer is derp there, I guess. Alright, so let's go ahead and try to knock out those two issues in our GitHub repo. And yeah, we'll take about 20 minutes break to do that. We'll take a look at any pull requests that come in and merge those down, and we'll move on to some Vue. js stuff after that.

Exercise 5 Solution
It looks like we have a couple pull requests in to take a look at our two challenges. So, let's see what we've got, see if folks were able to crack this nut. I did notice that we had one contribution that is what we're looking for in addition to a couple stylistic fixes that I had actually mentioned yesterday, so I appreciate the help there. Well basically what we had to do to add the this Babel set of presets is number one install the preset set as a development dependency in our package. json, which we did here. So there are the ES2016 presets. We also added the presets, or we changed the presets configuration for babelify such that it would use both of those presets, both ES2015 and ES2016. So that is what we were looking for. And now when we compile our JavaScript, we'll have the ability to use those ES2016 features, which I think there's actually only like one or two major ones, in our Javascript files as well. So we won't immediately put it to work, but now our client-side JavaScript capabilities are just that much more elite. So I'm going to go ahead and +1 this guy, looks good to me, and I'm going to merge it down. So thank you very much. Good work there. Alright, we also have a pull request for the SCSS changes. We have the font color defined as a variable here, looks to be a gray, I'm sure it's perfectly readable, and then we're changing the color of the h1 to that, and then in the info we're actually changing all of the color to that rather than just the anchors, which I guess is fine. That's pretty close. I think what I was hoping for was we'd actually just get the anchors themselves, but that's pretty close. So, I'm going to say partial credit there. The primary bit is here. We define this shared color that we're then going to use two different places in our CSS declarations. So that is definitely what we're looking for there.

Vue.js
Building with Vue.js
We talked a little bit about some of the tools that we use that are sort of a framework agnostic to enhance what we can do in our front-end JavaScript and CSS. Now we're going to dive a little deeper into the core front-end framework that we're using to drive most of the interface to do MVC in this example, which is Vue. js. In Vue. js, if you remember from yesterday, I'm fond of talking about spectrums to kind of place different tools inside them. When you're thinking about sort of building a rich front-end, there are frameworks that do quite a lot for you. Up on the right, we have AngularJS, which with version 2. 0 and in previous versions, it really demands a lot of you as a developer. You really have to buy into the Angular way of writing applications, and you're probably going to write your entire front-end to be an Angular web application. And the same is true of React to a lesser degree. It is possible, I think more possible, and I shouldn't say it's impossible with Angular, but I see React employed more frequently to provide dynamic features to a component of a web application more so than I do on the Angular side. But again, you're kind of buying into the framework whole hog and writing all of your code to kind of specifically cater to that framework. And then on the sort of other end of the spectrum, you can write JavaScript yourself, you can write jQuery plugins that do all the fancy stuff on your page without really any set structure imposed by a framework, or you can use something like Backbone, which is quite a bit less opinionated and just provides sort of a minimum structure for how you can write the JavaScript which runs your front-end. And I think Vue actually --- one of the reasons I like it is it kind of sits in the middle of these approaches. It's something that is fairly easy to add in to any page, whether or not you're already using Vue. As a library itself, it's reasonably light weight. But, it also provides some of the key features that make React and Angular useful, like Ractive two-way data binding, a JavaScript object model that you can use to update your data and keep your data out of the Vue. So I think Vue kind of sits in an interesting middle ground between a large framework which can take over your front-end code and these other libraries which you use to implement parts of your logic. So, some of the key features. Again, the two-way data binding is definitely one of the highlights. And the object models, like the Vue models that you're interacting with to update the data which drives your user interface is just a plain JavaScript object, which I've always dug. There's not like a special scope object or any kind of fancy affordance by the framework, you're just interacting with a plain JavaScript object to update the data of your Vue. It also provides, much like Angular if you've used that before, HTML directives, which you can add into your markup to assign event handlers or bind event handlers to methods on your Vue. And you can build fully component-oriented applications, much like you can with React and Angular using Vue as well, if you decide to use it in that capacity.

Vue.js Code Demo
So rather than tell you about it, I thought I would write a little code to kind of demonstrate some of the key features of the framework that we're already taking advantage of in the TODO application in kind of a smaller chunk. So, I'll drop the JSFiddle here into the chat after we get through things here. But, I'm going to write a little bit of code here in this fiddle. I'm loading in Vue from npmcdn. com, which is actually kind of a nifty tool for loading in npm modules into your front-end code, especially in situations like this where you're wiping up a fiddle. So in any case, all I've started off with here is having Vue loaded in onto the global window scope, and then I've got a HTML element that I'm going to essentially bind my Vue component to. So, I'm going to create a new Vue. And what I'm going to endeavor, to do is to create a todo list interface which will show up in the lower right-hand corner of the screen over here. So, when I create a component, a Vue component, I'm going to bind it to an individual element, and here I pass in a selector for that component element. And, the first thing I'm probably going to want for my Vue is some kind of input, like a text field input, that's going to be the place where I actually type in my todo list item, and then I'll have a button --- hang on, I'll make this a little bigger. I'll have a button that you'll click to actually add the todo item. And then, I'll probably have an unordered list here of all the items on my todo list. So if I reload the page, I've got my user interface, but it doesn't actually do anything just yet. So the first bit that I'm going to do is declare a data attribute of my Vue component. So, this data attribute, when you declare it up front like this, basically allows Vue to monitor these properties of this data object. So if you make any changes to the properties of this object, those will update any user interface elements that are bound to those attributes. So, in this case, I'm going to create a data element for the todos, and maybe I'll seed it with one todo list item like get the milk, or something like that. So I've got --- oops, that's not what I wanted. I wanted that. So, I've got a Vue component with some basic data in it, and now I need to render that out to my user interface here. So, the way in which you do that in Vue is with an HTML directive, most of which for Vue. js are prefixed with a v-. And there's a directive called v-for which will loop over an array within your data model and output some template based on that. So, for todo in todos, which is this element of my data object down here, I'm going to render a list item, and then I'm going to use the double mustaches to output some data from my model. Then I'm just going to output todo. text. And when I reload that, I now get my list item updating. Now I need to kind of wire up my user interface here, such that I can enter a new todo list item, click the button, and something is going to happen. So for that, I'm going to do a couple of things. I'm going to bind the value of the input field to a property of my underlying data model. And the way that I do that in Vue is with this v-model directive. And I'm going to say I want to bind the value of this input field with a model field called newTodo, which doesn't yet exist, so I'm going to come down here and add that to my data model. And I'll initialize that to just be an empty string. And then to actually make it such that I can add new todo items, I have to make this button do something as well. There's also a directive for that, for binding event handlers on your HTML, and that directive is called v-on. And then after a colon, you can pass in the name of a DOM event that you're interested in binding. So v-on:click is going to be a model method that we haven't defined yet. We'll just call add. Now we need to go down here, back to our component, and in addition to tracking the element and the data associated with it, we're going to create some methods on our object, and we're going to create one called add. And what that add function is going to do, it's going to push a new item into our list of todos. So in this case, the text is going to be the current value of this. newTodo. So we're going to add that to our list of todos, and then we'll also blank out our newTodo item, setting that back to a blank string. So if I run that, I should be able to come over here and say new todo item, and click Enter, and it looks like by and large my little todo list is working. So, again what I think is really neat about this is with what is a very small amount of code and not having to change how I render my page very much, I get this really powerful data binding mechanism and event handling in this component which is pretty much completely decoupled from my HTML markup. So, there's lots more that Vue can do. For the exercise that I'm about to turn you loose on, you're going to want to definitely check out Vue's excellent guide, which kind of takes you through a lot of the core concepts you're going to need to learn to get up to speed on developing with Vue. And as always, we like to bring up the alternative slide here because there are always a million ways to do everything in the JavaScript world. We already kind of went over them. All of them are great alternatives to use. I explained some of the reasoning behind like why I'm kind of high on Vue because I feel like the value that it provides me is very high relative to what it asks of me in terms of how I structure my application, in terms of how many dependencies I need to load in. So, I think Vue is nice combination of features and weight. But, these other frameworks up here are also awesome. React is taking over the world. React Native is a really cool project especially. Ember does a lot of things really well. Backbone was kind of my previous go to for a lighter weight MVC-type thing. So all of these, totally awesome, you should use them if you like them if it makes you happy.

Exercise 6: Building a Rich Client
Now what I'd like you to do is we'll take at least a half hour this time because this is going to be a little bit trickier than both of these issues that we had before, which is updating the client-side code in our application in a couple of key ways. The GitHub issues for this are tagged as Exercise 6 and the two that I will ask of you are a couple meatier tasks. The first is to actually start persisting the completed status of a todo item using the REST API that we've set up on the back-end. Now, if you remember, we, yesterday, updated our application to accept a completed attribute in addition to the title. So our API and our back-end is fully set up to support this, but our front-end still doesn't do anything about this. So, our task would be to update the Vue. js application to persist that completed attribute, as well as the title. So that would be issue number 1. And then issue number 2, if you get through that, is to use some data binding features in Vue to add a feature to the application which it doesn't have today. The concept I had was adding a light and a dark theme to the todo list. So, what I'd like to do is like maybe underneath the todo list add a UI element, which is essentially a chooser of some kind which will let the user switch between a light theme and a dark theme for the CSS on the todo list. So the way that I imagine this will work, although feel free to riff if you would like, is that there will be some component, some HTML, links, or radio buttons, or whatever, underneath the todo list, you click one of them, and it will either toggle between like a light and a dark by adding a CSS class to the container element for the entire TODO list application. Then you'll also have to write the CSS to power a different theme for the application. So, whether it's a different background color, different fonts, or whatever, adding an alternate theme to the TODO list application. For bonus points, if you want to, after the user selects a theme, save that in local storage as a user preference. That I think would also be a pretty nifty feature. So, you can peel off whichever of those you feel is more interesting, or do both of them, which would be even better. And as I said, I think we'll take at least a half hour to work through those, as for many of you this will be the first time that you're really digging into Vue in any great detail. So take some time and see if you can knock out one of those. If you only choose one, the REST API one might be the more instructive of the two. But if you get through that pretty easily, the light/dark theme will introduce you to a few more features of Vue that you wouldn't be exposed to otherwise. A couple of questions. What's this in the scope of the Vue object? This is going to be the Vue component object itself. So the actual instance of the Vue component. That's what this is, and also incidentally, it's why when in our handler code, so if we look at the Vue here, we don't usually for the methods use the arrow syntax for the function here because we want Vue to be able to set this in the scope of the function to refer to the actual component instance. So, yeah, this refers to the actual Vue component instance. The other question was is Vue the primary framework we use at Twilio? My team does use Vue in our little chunk, but it's not the only framework in use at Twilio. Like in the developer console, they are now just like starting to introduce a lot of React stuff. There's also a lot of handwritten JavaScript in there. But the two primary ones in use that I know of are Vue and React at this point. We're the only ones using Vue though, probably because of me. But yeah, a lot more of Twilio is now being written in React as well on the front-end.

Exercise 6 Solution, Part 1
When we sent you off on this Vue. js journey, we had you try to change the application to take advantage of the API that we just put in place last time around to actually save the completed status of the todo list items. And so we actually do have, looking through, we have one pull request, which is great and definitely implements this in a way very similar to what I would recommend. So let's actually take a look and blow that up. So, what we have here in this pull request are a couple of changes. For the first is to our existing store utility class, or utility object, I guess. So, we had some logic before that would actually save every todo in the list if it was flagged with a Boolean property called modified. And this was just kind of a byproduct of the way TodoMVC was originally implemented where like the only way todo items were saved were as an entire list all at once. So, that's still the way that it works. So what we've done here is updated those Ajax requests to in addition to sending the title to the server, also sending the completed status of a todo item to the server as well. We also have that down here in the updates. So, both in the create and the update steps, we've just also started tracking the completion status of the todo. So that was step one that we had to do. Next, we had to modify the user interface to actually like save that property as a part of the interface when something was actually completed. So, what we did here, this is the actual Vue component, we added another instance function here. Actually, I might just open this file up so we can see what we did. So, there is a section of functions within a Vue component called methods which are callable as a part of event handling and other scenarios. So we had before an addTodo, a removeTodo, edit, doneEdit. And what we added here was a new method called complete which takes a todo that was completed. And the actual property completed of the todo is already being updated because it was bound to a property of the checkbox object. And maybe it would be easier to look at that here in the source for index. ejs. So, here is the for loop in which we're rendering all the individual list items. And inside each list item there is a div that contained the contents of the button, but also this checkbox toggle which was bound to the completed property of the todo model. So anytime that checkbox is updated, it's actually going to be updating the Boolean property completed on the underlying todo list item. So that property is already being updated. So what we did in this change set here is we take the completed todo, we flag it has having been modified, which is necessary to actually persist the save to the server, and then we called that same storage method that we had before, passing in the current list of todos. And because this todo was marked as modified, we're going to persist that change to the server. So that was the only thing we needed to change on the actual Vue instance itself. And then the third change in the pull request was also necessary. So we updated our service, which hits the back-end. We updated our controller, or our component logic, and we also updated our HTML here for the actual Vue. And it was reformatted a little bit, which is also good, get it closer to under 80 columns, which I appreciate. The primary change here was adding a new event listener on this object. So whenever the change event happens on this checkbox, we're going to call this complete function on the component, passing in the current todo that was updated. And there are a few ways you could have gone about this. Listening for the change event is great and probably maybe the cleanest. Another way would have been to attach a click handler to this input as well. So, here's our input checkbox. Listening for the change event and calling our function is a great solution. Another thing that you might have done is add a click handler function here, and then you could have called that complete function, passing in the current todo for this item. So, that would've been a response to the actual click, but listening for the change of the property is probably the best way to approach that. So, excellent work on that piece. That is precisely what I would've done in this situation so good work. And you did it much better than I would have done had I live coded it in front of you. So, I'm going to go ahead and apply a plus one to that. And since it's a bigger change, I'll pull it down locally and make sure that it's working properly before I merge it in, but it looks absolutely perfect at this point.

Exercise 6 Solution, Part 2
So that was the fix that we had submitted for completing that particular issue. And that was awesome. It looks like we might have some other ones here. So we have a slightly different one here. It looks like this was maybe not the theme piece, so we can actually do that together right now since I don't think we have a pull request for the theme challenge specifically. So, the other issue that we had open was the ability to change the current CSS class of the component --- let's see here, based on like some kind of selector, or radio button, or something that would indicate that we want to change like the current theme of the object. So, let's go ahead and implement that here together. So, at the end of the day what we want to do --- here is the todoapp section, which is the container for the entire interface. And if we look at our todo controller here, the element that we are binding to is this todoapp, which I'm sure many of you already figured out. Here, let me back out the changes here because I haven't done that. So, what we want to do is conditionally add a class to this section that kind of contains all of our component based on the property of a Vue model that will update through some kind of user interface component. So the way that Vue provides to do this is a couple of things. One thing you could do is you could have a mustache template here, which will put the value of a Vue model property directly into the class list of the element here. So I could have a themeClass like Vue model property and I could drop it directly in the class list there. The other thing I can do is I can use a bind directive, which is a little bit more flexible if you have to do complex things with the classes that you're adding based on the state of the model. So one of the ways that we have to do that is with this v-bind directive. We can bind the class of that object to be the result of an array of properties or of an object with properties that we can toggle true or false indicating whether or not that class should be selected. So in this case, I'll just pass in an array of potential model properties. In this case, I'll just call it dark because by default, we'll assume that the light theme is essentially the default and just add some additional styles for the dark theme. So, what we're going to --- excuse me, we'll just call that theme. So, we'll be setting that property based on the Vue model. So now I think that's pretty much all we're going to have to do here in the markup. If we go into the actual Vue, we're going to add another property called theme, and this is actually where we'll set the default. So, the CSS theme for light will be the value by default, or we could actually not have a light theme class at all and just leave it empty. But we'll have a light theme class just in case we decide we want to add some properties for that later. So, now we need to do --- actually, I lied. We do need to go back to the markup a little bit. So, down at the bottom here at the section that has the filters, we'll do something just quick and dirty. There are a few ways you could do this. You could do like an input radio button, which maybe I will do right, input radio with a name of theme and a value of say light. And, we'll default that to checked. And, in addition, we're going to bind that to that theme property that we just created. So that's going to be like our radio option for the light theme. And then we're going to do something similar for the dark theme, although this time it's not going to be checked and the value is going to be dark. So, we've updated our Vue model to track this new property called theme. We have a little bit of user interface here to allow people to select a radio button based on their current preference. It's going to start updating the model, which should start toggling this class up here at the top. So, let's fire that up, make sure that that worked. And again, because I was spamming my database, we're going to have a long way to scroll down to the bottom, but there is our light theme and our dark theme, which doesn't seem to be updating the way I'd thought. Let's see here, name, theme, value, dark. Actually, you know what? This is going to change that to be a Boolean rather than a string. So that's not exactly what I want in this instance. Alright. What did I goof up there? Let's see. Yeah, I'm not sure. That kind of matches my note over there so I'm going to have to dig into that. Does it need to be checkbox instead of radio? No, I think the radio is what I want because I only want like one of these to be selected at any given time. What is weirding me out a little bit is the fact that I can't update it. It could be some like z-indexing stuff. I'll just, as a Hail Mary, try this and then I will leave you alone to debug this. I do want to show you how this works though so I'll do this over the break. Alright. So there I think it might have been like a z-indexing issue in the CSS because I am able to click it now. So, now if everything worked according to plan, like if I inspect this component, open up the body, there is the current class and --- that's not exactly doing what I thought it would do. Let's see, value dark. Alright. So that binding isn't working exactly the way I wanted here. I thought that --- I'm going to have to figure out why that is. Man! That is a bummer. But the idea is we are going to be toggling this property, it's going to be updating the class on that DOM element, and then we can add CSS styles to make that work. So, during the next break I'll debug that. I'll check my gist that I had set up for that and figure out why that's not working, and we'll cover that when we come back. But the next piece that we are going to tackle was starting to dig into Socket. IO a little bit. Yes, question. A quick question about --- this was a little bit a while ago, but what about the allDone input? They're just asking what about the allDone input. Yeah, so --- I can ask them to clarify if --- Yeah, what is the allDone? What do we mean by the allDone input? I'll ask them. Okay. The main section right above the todos header is bound to an allDone v-model. Line 25, I think. I think maybe like is that the completed stuff? V-model allDone. Oh, toggle-all. I think it's if you want to mark all of them isn't it? Yeah, so that is essentially --- I think that's going to be like a Boolean toggle. Let's take a look at that. And he's just saying it's not saving it to the database. The allDone is not saving to the database. That's a good question. I'm not sure off the DOM what that's supposed to do because the allDone --- this is code that I took from the TodoMVC implementation, is not actually a model object. Oh, you know what it is? I think it's probably a computed property. So, allDone is this computed property here, which has a getter and a setter. So it can be treated as if it is a property of the Vue model associated with the component. And then when it's called in the Vue like it is over here, the get function is called. So this checkbox is toggled based on whether or not all of the current todo items have been completed. So that's where that's coming from. It's one of these computed properties. So if you did want to save all of them after doing that you would just update this forEach callback to mark that it's modified and all saved? Yes, yes, that's what we would do. So you would, in addition to completed for each of them, you would also mark them as modified and it would all go to the server based on the current implementation. So the setter is what happens on change. So like if I were to click this checkbox, then that's when the setter logic gets executed.

Realtime User Interfaces
Using Socket.io
The next challenge that we have is to start adding some real-time user interface capabilities to the application. And I think we'll probably take this one and like work into lunch if we're having lunch at noon. So we'll talk about this and set up the exercise, and then we can work on that kind of over lunch and maybe pick back up again at like right around 1:00 p. m. local time. So, Socket. IO is the one library I'll present here during the day for which there is no alternate. It's pretty much the de facto standard for adding real-time capabilities to a node app. There are a couple others out there that you can use, but Socket. IO is by far the most popular. And Socket. IO essentially provides two things to your application. It provides a client-side library that you'll load up in your HTML that will allow you to communicate with the server over a WebSocket, and it also provides a server-side API that you can use in your Node. js code to push data to connected clients over a WebSocket. And I think the best way to kind of go through this is to, again, show you the sort of HelloWorld of Socket. IO, which is a simple chat application.

Chat Application Code Demo, Part 1
If you want to build something similar, the Socket. IO documentation is actually fairly good on this front. So if you just go to socket. io, and click on Getting Started, you can do pretty much what I'm about to do here by going through this tutorial in setting up a basic chat application. So, what we're going to need to do to use Socket. IO --- I'll use it in a simple sort of stand-alone project first, and then I'll pose a challenge to the group to integrate Socket. IO in our TODO list application as well. So, first thing we're going to need to do, we already have, if memory serves, we have an Express module installed in this directory. Yep, there we go. But we don't yet have Socket. IO. So I'm going to install that now. And Socket. IO works alongside pretty much every node web framework. It requires an HTTP server instance to do things like serve up the client-side interface JavaScript to the browser and a few other bits. And I'll show you how to kind of set that up on the server-side first. So, if we open this up, this is the simple Express example that we looked at yesterday that showed us the basics of creating middleware. And in that application, we let Express create an HTTP server for us, but in this instance, we're actually not going to be doing that. So, let's go ahead and require the HTTP module from core node. And, we're also going to require the Socket. IO module. We're actually just going to create --- I'll require that, we'll actually require that here in a little bit. So now instead of calling listen on our Express app after we create our Express app, we're going to create an HTTP server. (Creating) And then, we're going to have that server listen on port 3000. And this time we'll pass in a little function that's going to say that our server is running just so we know that it's working fine. Alright, so we have our server there. Now that we have our Express application set up, we're also going to serve up some static assets so we can see what's happening on the HTML side of things. So we're going to use the built-in Express static server. And, we're going to server up the public directory which has yet to be created from within the current directory. So if there's any HTML, CSS, JavaScript, other assets, they'll be loaded from this public folder. Here, let me just pull this up. (Typing) Alright, so once we get that piece all set up, we have our regular, old Express app. Now we need to --- let's maybe start by creating that HTML application next that will be using the Socket. IO stuff on the client-side. So we'll create an index. html. And we'll just have some basic markup here, maybe an unordered list which will have our messages. And then we'll have a form or maybe even just an input type text. And then this is going to be the way in which users are going to interact with our application by typing in some chat messages. And then we'll have a button to actually send that message out to some folks. Alright, so we have our basic Express app running here. We actually don't need any of this anymore. And, if we've done everything correctly, we should have an Express app running. Oh yeah, we already have one running on port 3000 over here. Alright, so we should have a server running on port 3000 that just has basic HTML UI here that eventually will become a chat application.

Chat Application Code Demo, Part 2
Now we're ready to start adding in the Socket. IO pieces. So, on the server, we're going to create the Socket. IO object that's going to allow us to manage incoming WebSocket connections. So, we're going to create this IO object by requiring Socket. IO. And then to that, we're going to pass in the HTTP server that we just created, and that's going to allow Socket. IO to serve up the client-side JavaScript that we'll be using here in a second in addition to a few other bits. And, next, with that IO object, we're going to be listening for a connection event. So, whenever a new WebSocket connects, this function is going to be called with that socket. So, for now I'll just console. log socket connected. And, we'll start doing some more interesting stuff with that socket here in a minute. So that is the server-side component. Now on the client-side because --- one of the things Socket. IO provides us is like a dynamic script tag. So this route that I'm about to type in the source here is actually going to be generated by Socket. IO because this isn't a route that we created. So it's just going to be socket. io/socket. io. js. So that's going to load up a Socket. IO JavaScript object for us to start working with on the client-side. And then in my script tag here, this is where we'll write all the logic for our chat application, I'm just going to create a new instance of Socket. IO here on the client as well. We'll just call that socket, and then we'll use the window-scoped IO object to initialize it, which is what we get when we include that script tag. So I'll save that up. And I'll kill my server. And I'll actually run the script with nodemon so we don't have to restart it every time. And if we refresh the page, go back out here, we should have our message that the socket was connected. So now we have a connection between the client and the server. And now we can start doing some interesting stuff with it. So, the first bit that I think we'll do, we'll start off on the client-side. We can use the client-side socket connection to emit events that will be received on the server. So I'm going to say socket. emit message. And in addition to Socket. IO, maybe I'll just, for good measure, grab a version of jQuery to help me grab some stuff out of the DOM. So for that message, I'm going to actually grab the current value of the form field. So, I'm going to emit the message and then the second --- so the message is kind of the event that other sockets can listen for and the server can listen for, and the second argument is essentially the data value that you want to send along with it. So, for the input of the chat object, I'm just going to grab the current value of that. And that's going to be enough logic to emit the current value of that. We also want to wrap that in an event handler, so we have that send button, so on click. That's when we want to grab the current value of that. And we'll emit that value back to the server. And then we'll also reset the value of like our local chat box back to null. So, that'll be kind of the send part of the equation here. Then back on the server to actually listen for that incoming event, we're going to listen for an event on that socket object that we get when somebody connects to the server. So, when that socket receives a message event, we are going to execute this logic on the server. And for now, we'll just acknowledge that we got it by writing that out to the console. And again, if everything went according to plan, we'll send some messages, and hopefully out here --- oh, no! Missing handler. Console is not a function. Oops, oh yeah, console. log. That would be a more effective thing to try. So we'll send that. And then here we go, we have some messages being emitted in real-time from the client to the server. Now we've implemented one half of this, but what we haven't done is send this out to any other connected client. So, what we can do when we want to send a message to every other socket that's currently connected is use the IO object, this guy that we created here, and use a method called emit to fire off a message on that object that will be received by every client. So, it's very similar, actually, to what we did on the client-side emitting that event. So we're going to emit a message event, and then we'll just relay that message that we just got. Alright, so, now after we emit that to every connected socket, up here on the client, we're going to have another event handler here, so on that message event. This is going to be fired every time a message event is emitted from the server. And what we want to do is on that unordered list that we had, just append a list item to it. So for that, what do we call that? Messages. We'll actually start by creating a li, oops, li =, create a li item, and we'll set the text of that to be the message that we got back from the server. And then, we'll append that list item to the messages. So now what should be happening is we'll emit a message from the client, it will be received on the server, and then the server will emit that message back to every other connected client. So, let's reload and see if that is indeed what we experience. So it looks like we have a very crappy real-time chat application running in the browser. And now, one other tool that is a constant companion for me and for many other developers that work with webhooks or have to expose their local development machines to the internet is a little tool called ngrok. And ngrok basically allows my laptop to get a publically addressable URL on the internet. So, this will allow all of your to test my application along with me. So this tool I'll call ngrok http, and we'll want to expose port 3000. And then I'll reserve the subdomain of frontendmasters. So now, if you go to frontendmasters. ngrok. io, we should see that interface that we just saw. And I'll start sending some messages. And hopefully, some of you out there will start sending some messages back to us as well. Try to keep it PG-13, this is a family show. Alright, so we got some folks coming in here. So, there we have, again, the magic of Socket. IO for all to see. Certainly something that I'm sure folks have seen before, but in just a tiny amount of code, a few lines in the client, a few lines on the server, we were able to create this real-time interface for our application.

Exercise 7: Realtime Features
The issue that I have right now with the application, which will be labeled Exercise 7, is to update the todo list when a new todo is added via websocket. So, the way that the Vue application works right now is it appears that todo items are added to the database in real-time, but really what happens is we add them to an array, the array gets rendered in our interface, and then we make a request to the server which actually persists it to the database. So, the challenge that I would like to pose for all of you is to create a socket connection between the todo client and the server and whenever a new todo is added, and right now we can do it for an add, if you want to try to implement it for an update too that's totally fine, but we won't immediately add the todo list item to the array associated with the Vue component. We'll actually make a request to the server, save it in the database like we are right now, and then when the todo list item has been successfully saved, we will emit an event from the server to the client containing the saved todo list item which then will be added to the array and then represented in our user interface. So it'll be only when the todo list item has actually been persisted that it'll be displayed in the user interface, which hopefully should be pretty close to real-time. So after lunch, we're going to take a crack at that. And, yeah, that should be a good time and give you a chance to dig into Socket. IO a little bit. The challenge here is going to be how you structure your Socket. IO code. So, right here we kind of have everything in the same file and access to the HTTP server and all that jazz. But generally speaking, that's not the way you're going to want to handle it in your application. So, much as we have this webapp. js file, which kind of creates a reusable Express application, we're probably going to want to create another file which manages Socket. IO related data. As a little bit of a spoiler, probably what you're going to want to do is in a module, create an object which exports one function, which is a connection handler, which will be used by the socket server. So if I bring up this simple example, you probably want to create a module which has this function, which will be called every time a new socket connects to the application. That module, for the sake of argument, should probably create like a global registry of sockets of all the sockets that are currently connected to the system, and then that module should probably have another function which will allow the controllers to emit messages to all the currently connected sockets. So, you're going to be creating a CommonJS module that exports two functions. One is going to handle incoming socket connections, and the other is going to be used by controllers to emit messages to the currently connected sockets. And again, because that's a decent amount of code, I thought I would at least prime the pump a little bit with what I think would probably be a reasonable implementation. So, we'll break now for lunch, and then we'll take another half hour on top of that to tackle the challenge, and then we'll come back and look at the solution at 1:30 along with the Vue. js solution. I've got a quick question. Yeah. It seems like the whole Socket. IO kind of template is much cleaner than messaging, than a RESTful interface. I mean, what's the tradeoff between them? I know that Socket. IO is a lot faster. Between a RESTful interface and a socket connection? I mean it's kind of about the kind of client that you expect to connect. It's possible to do like authentication with websocket connections if that's something you want to do. But yeah, other than exposing an API that can be used by all kinds of clients, like not every kind of client can create a socket connection to your server and like create read, update, delete operations that way, having a RESTful interface is probably more accessible. But yeah, you could certainly architect your system in such a way where most of your average crud operations happen over a websocket if you want. If you're controlling both ends of the pipe, sockets would be efficient? Potentially, yeah. It's certainly is faster, but the support for like authentication especially is a little more fiddly with websockets than it is with REST. With REST, you have cookies where you can store authentication tokens and by default, you don't get that with a websocket connection. Like there are other modules you can use to get that type of functionality, but there's no reason why you couldn't implement a lot of your services over websockets if you wanted to.

Exercise 7 Solution
Before I went to break, I was kind of working a little bit on figuring out why my Vue. js theme switcher that I had attempted to demo a few minutes ago wasn't working, and it was a silly mistake, as you might imagine. Here if we pull this up again, I moved the location of the radio buttons, and now when I click them, I switch between a light and a dark CSS theme. And, if we take a look at the elements, we can see that the class changes from light to dark when I go back and forth. So the issue at play was I had added the radio buttons here above the footer, but it actually wasn't a part of this same element, the section element, that we attached our Vue. js component to. So the events that I was firing and the model I was trying to bind to was not actually part of the component. And so by moving the inputs inside the actual component, they were able to bind to the same model. So it was just the location of my markup in the DOM was misplaced. The rest of the code was working fine. So that is the piece for the TODO app for the theme switcher using Vue. js that we had before. And, the other bit that we asked you to take a look at going into the break was a Socket. IO and trying to incorporate Socket. IO into the updating flow of a todo item. The challenge that I placed to you guys going into the break --- and I didn't see any pull requests come in, which is okay. You're out having some Tai food, chowing down on a sandwich, catching up, totally fine. But the challenge I posed to you guys was using Socket. IO, update the Vue. js based user interface based on some activity that was going on on the server. Whenever a new todo was added, push an event using Socket. IO up to the server. So let's go ahead and do that. I didn't see the pull requests come in so over lunch I tried to get this going, so we can take a look at that. If we reload here, we have a list of todos. Maybe knock out a couple of those there. And there's our list. And we can still see that it is working and persisting stuff to a database, but now what we would like to have happen is when I add a todo list item over here and I just implement it for the add, I get another item showing up over here. Yo yo, if I add that over here, oh no! We do have a little. js error, so --- oh oops! A minor problem as I was finishing this up. One moment please. Just need to change that up right quick, reload both of these, and with any luck that will be the end of it. So, we add a todo list yup yup, and then we see that added on our other tab as well. So, the way that we implemented this was wiring up a socket server with our REST API endpoint, and when that add operation is executed, emit a message back up to the client with that todo that was added. So, let's break down that code here real quick. We'll start on the back-end. Now, here in the command where we actually start up our server, we added a little bit of logic here to actually initialize a Socket. IO server and associate that with the HTTP server for the application. So, we created our HTTP server like before, and now we create an instance of Socket. IO that we can use to manage incoming socket connections from our users. And then I pass that to a utility module that I created for handling socket communication over here, which has a method called init. And then we pass to that a Socket. IO object. Now if we look at that socket object defined at the same level as webapp. js, that init function just passes the Socket. IO instance and assigns that to a module-scoped variable called io. And then we export another function called todoAdded, which we intend to be called from our controller. It's going to be passed a todo item and then it's going to use that Socket. IO instance to emit this event, todoAdded, to the other connected clients. So, I guess the other bit on the server-side to point out is we just required that module here in our controller, and then in our create function if the todo was created successfully, then we call that function and emit that event in our Socket. IO server. Back in the Vue. js view, right now this is all implemented within the constructor for the Vue component. There are a few ways we could approach this, but I created a property that's not going to be observable on this Vue instance called socket, and I call that io function, which is added into the window global scope by including this Socket. IO JavaScript file, which is served up by Socket. IO on the server. And, after I create that socket, I add a listener for todoAdded. And because the scope of this does change within this function, I just stashed it away in a variable called self. And then when that todoAdded item is fired, I'm going to check the list of todos that I already have to see if I have that todo item. So I'll check the ID of all of the ones that have been successfully created against the one that just got added. And if I don't already have it in my list of todo items, I go ahead and push that into that array, which then will update the UI with the latest list of todo items. Any questions on that particular bit of code or Socket. IO stuff before we keep trucking? Yes? What is the of keyword? Is it basically serving the same purpose there as a foreach loop or how exactly are you doing that particular loop? It's sort of a variation on the traditional for loop where you have an iterator which is incremented. The primary reason I'm using this here rather than like an array foreach where you pass in a function with every element of the array is the foreach, you can't break out of a foreach loop. It's going to be executed for every member of the array, whereas in a for-of loop you can break out early, which I needed to do here. So, I'm sort of tracking the state there, I assign the current todo item to that t variable, and then start to work with it in the for loop. Other questions? Alright, cool beans. Let's keep moving. If you have any other questions as we go along, that's great. I'll push up that branch with the Socket. IO changes after we're sort of off to the races on our next exercise.

Production Monitoring
Locust and Rollbar
Now I'm going to talk a little bit about a few tools that we found really useful in monitoring and sort of preparing our applications for production. The two aspects that I'm going to focus on and give you a little demo of this afternoon are a load testing tool called Locust and an error tracking service that lots of teens use at Twilio actually called Rollbar, which will take exceptions in our application and put them in a service that will notify us if bad things happen in our application while it's running in production. So I'll do a quick demo of Locust, and then I'll turn you all loose on starting to add in some error handling in our application on the server-side, which we haven't actually done yet, if you might have noticed. So, the first thing I'm going to show you is a tool called Locust. There are load testing tools for just about every platform. This particular one is a Python-based tool that I've favored because it's relatively easy to use, it gives really good reporting output and is configurable through a web interface, which I'll show you here in a second. It's got a large community around it, and it allows you using Python code to actually simulate different types of usage scenarios on your website. So the flexibility of the configuration, and the maturity of the tool, as well as sort of the general usability are some reasons why I favor it over other things that are out there. The production monitoring tool we're going to take a look at is called Rollbar. It's a third-party service whose API you integrate into your application. And when handled exceptions are raised, you can send that exception data to this service and it'll send out notifications to whatever provider you happen to have configured. At Twilio, we use PagerDuty to create a schedule of folks that are response for responding to issues and outages on the website. So Rollbar does server-side stuff too? It's fully server-side. I mean yeah I think you might be able to do client-side, but we only use it for the server-side. You can use TrackJS on the client-side. I know you can use TrackJS on the client-side. Minor plug for our hometown hero, Todd Gardner. But yeah, so Rollbar is used across many services at Twilio to notify folks that things are going wrong, and it's something that I've really come to enjoy. If you have identifying information about a user at the time an exception is thrown, you can, in your Rollbar output, see which user and exception was generated from the origin. If you have Rollbar configured correctly, you can get a lot of very useful information about any exception in your application. So, definitely worth checking out, and it integrates very nicely with Express applications, as you will find out when you check out the exercise.

Load Testing Code Demo
But the first bit that I'm going to show you is this tool Locust, which we're going to use to beat up on that Elastic Beanstalk instance that we created yesterday with our TodoMVC application. So, the first bit that you're going to want to note about using Locust is if you're on a MAC or if you're on an operating system that limits the number of open files that any process can have at given time, you are going to need to raise that limit to use Locust because it does open a file on the file system for every simulated user that it throws against your system. On a MAC, the command to do that is ulimit -n 1000, and that, in the context of this terminal session, will raise the possible open files to, in this case, a 1000. You can raise it up a little bit more from there, but it has to be more than however many concurrent users you want to test in your application. If you haven't installed Locust, the easiest way to do it is to pip install locust. I think it might be actually locustio, one of those two. And it'll be installed using the pip package manager. And once you have it installed on your system, you'll have this command called locust to which you will pass a host that you want to test against, and it'll run a simulated load against the server that you pass in this command. But before you do that, you have to create something called a locust file. And I have a very simple one here that's going to generate a whole bunch of requests and push a whole bunch of fake data into our todos database. So, at the core of this is a Python class called a UserBehavior, and that class has a set of tasks that it can execute against your website. So, with each of these Python functions, they are passed an instance of the Locust object, and each Locust object has a HTTP client which you can use to send a post request or a get request to the host of your choosing. And you can also pass in parameters like, in this case, I'm generating some fake text to put into the todo items. So it's another package called faker, which generates some fake data, some lorem ipsum text that'll insert into a todo list item. Then we'll set the completed for each of these items to false. And then the other action we have is a read against our API, which will, by default, grab the last 1000 todo items from the database. In our behavior, we define that for every 1 create operation we want to execute 5 reads. And you can, in your Python code, kind of tweak this to be as close to an actual usage scenario for a user of your site as you possibly can. Question? Do they have any safeguards in the tool to prevent you from doing nefarious things with this? It seems like you could --- Like denial of service. Right. Not in the tool. No, I mean I think most applications probably will blacklist an IP eventually like if it seems to be involved in some evil doing. But generally, that's like --- yeah, I guess it's not Locust's problem. It's more of an our problem as application developers. But that's its job. It's job is to simulate high amounts of load against an application. So, here we have in the WebsiteUser class, we set our set of tasks to be some kind of user behavior, and then we specify a min and a max wait time, which will, in milliseconds, determine like a lower bound and an upper bound between requests from this simulated user in the application. And there's a ton more that you can do. You can send login requests. You can --- excuse me, let me shut these down here. You can send login requests so you can get authentication cookies back and execute authenticated requests. There are lots of different ways you can configure this simulated session. So, once we have our Locust file generated, we're going to use the Locast command to target this todomvc-plusplus instance running on elasticbeanstalk in the us-west-2 region. So, when we start that up, that's going to start a local web server that I can open up here in my browser on port 8089. And it's going to ask us for two things. It's going to be the number of users to simulate and the hatch rate, which is how often users are added to the list of users that are going to be spamming our application. So for starters, maybe we'll start with something relatively small for 100 users hatching at a rate of 10 per second. We're going to launch that load at our website and see how we're doing. So, as we are executing the test and we can execute it for as long as we like, we see the number of requests going into our get and our post endpoints. It starts to mount slowly over time. And with 100 users on prior tests, I think it tops out at around, with these settings, it'll be like around 20, 25 requests per second. And, we can see the min and the max time for a response, and that time is in milliseconds. And then we can look at the average and the median response times from the server as well. So for a load like this one, our server is usually able to respond in a reasonable amount of time. The post //todos is where you create a new one is quite a bit faster, returning in an average of about 127 milliseconds, but that read, because I have been spamming this quite a bit, there are quite a few records in the database, it's a little bit slower at about 400 milliseconds on average. And it's creeping up a little bit already as we're starting to tax our Elastic Beanstalk instances a little bit more. But, our instances do stay up. We haven't seen any failures or 500s yet, so that is a good thing. Now to see what's kind of happening on the other end of this, let's go into the Elastic Beanstalk administrative interface. If we look at the health section here, we can take a look at what's actually going on with the EC2 instances that are serving this traffic. So right now with our scaling roles as they are, we only have a single instance which is serving all of this traffic. And it's CPU utilization is hovering at right around 50%. So, at the moment, this single instance is not having too much trouble keeping up with the load, although our average response times do keep creeping up so that's probably something that we could investigate. If we turn up the heat a little bit, then we can start to see like where our application is going to break down. So I'll stop this test, and I'll create a new one. And this time, we'll simulate 900 concurrent users maybe hatching at a rate of about 25 per second, which is going to really start taxing our poor little T2 micro instance running on Elastic Beanstalk right now. So we can already see the average response times going way up as our single instance struggles to keep up with what is a pretty heavy load for its current configuration. And if we go back over here to our health check, we can see, yeah, it's already starting to blow up a little bit. Our CPU usage has creeped up to about 92%, so it's starting to be taxed a little more. But what we should see in a minute or so is Elastic Beanstalk reacting to this situation and our auto-scaling group is actually going to provision another instance of our application to start serving this traffic. It does take a minute, but we can configure those preferences. I've actually been messing around with this a little bit and this is something that you can do as you're load testing your application to see like what kind of scaling criteria you can bring to bear to ensure a consistent response time. So, if I look at the scaling configuration, I can take a look at like my auto scaling options. I fiddled with these a little bit from the default. The default was having a minimum of one instance available and a maximum of 4. I tuned up the maximum instances to 6. And I also tuned down this scaling cooldown. Sixty seconds is going to be far too short, but I wanted to make sure we saw some more instances getting provisioned during the test here. So this is basically the delay time. So after some condition is reached where it's decided that another instance needs to be provisioned, this is the cooldown in seconds that Amazon will for sure wait until it provisions another instance to start serving traffic. And the scaling triggers, these you'll want to tune based on what you're able to discover about your application. In the default settings, I look at the amount of data that you're pushing out over the network, but there are other bits here as well. The one that we might take a look at using instead for this application is maybe CPUUtilization where we could take a look at the upper threshold, which would be maybe 90. We can change the unit of measurement to percentage. So if the CPU threshold reaches 90 or 80%, that's when we'd actually start looking at spinning up another instance, and then we can also set the lower threshold for when it's time to rotate an instance out. So I'll cancel that for now, head back to the dashboard, and what we should see is our environment is okay, but as more instances are being available, Locust is going to continue to suck up the ability to pepper them with requests. And we can see their average response time is now up to about 17 seconds on average, which is not awesome. But, to help spread the load a little bit, we do have another instance that was just brought online 2 minutes ago. And it looks like it hasn't started serving very much traffic yet. It must have been provisioned recently, but now it is up to 92% utilization as well. So, we're going to probably, at this level of load, max out each of these tiny little servers pretty quick. But, the good news is we're not falling over. Well, it looks like we have had some failures, quite a few actually at this level. At the lower levels I was testing, I wasn't actually seeing any failures. But that's what you want to start to check and see like at what level your application is going to start to break down. And what we can look at when we have the failures over here is the error that we're getting from the HTTP client which is running the request. Here we can see we have a 504 error against our Elastic Beanstalk instance, which is probably related to just not having the --- the timeouts are happening because we just lack the capacity to service the number of requests that are coming up against the service. So, based on our criteria, we'll probably continue to provision instances up until 6. But the primary thing I wanted to show here was the ability to use this tool to at least understand how much traffic your website can take before it is completely brought to its knees. So, a site like --- there are a few like random statistics out there that you can find, like an open street map serves about 10-20 requests per second. There are a lot of sites which serve actually quite a bit more across many thousands of servers. But based on your site's traffic, you can ensure that at certain levels of concurrent users you're going to remain available at that level. So that is Locust. We'll let my poor Elastic Beanstalk instance relax a little bit. But after a test like this, we see that with the size of the instances we have, we definitely become CPU bound really fast at those high levels of traffic. So that is Locust. It's, again, my favorite tool that I've found for doing this kind of load testing stuff. Definitely recommend that you check it out.

Exercise 8: Production Monitoring with Rollbar
And the final bit that you are going to look at in the workshop session this afternoon is a tool called Rollbar. And, Rollbar, again, super useful. You can sign up for a free trial account to exercise it a little bit. But when you sign up, if you elect the programming language you're looking for, it gives you some handy-dandy instructions on how to get your module initialized. So you can start sending some test notifications pretty quickly, but you'll find that there is an Express middleware that you can actually plug directly into your application that will capture exceptions coming in to your Express app and send them directly to Rollbar. So your mission, if you would choose to accept it, is to sign up for a free Rollbar account and start sending any exceptions which are caught by Express to Rollbar. Now, one thing that you'll, I'm sure, figure out in your travels is that the order in which you add middleware to an Express application is significant. And, the errors that you put in place for 404 handling and for error handling are going to come after the routes that you define. So here in the routes file, this is --- oh, that's a different one. This is where your application routes are defined. We're probably not going to do it here. Where we're going to go is into webapp. js where we configure our other middleware. And, after our other routes in middleware have had a chance to try to handle the request, somewhere down beneath here we're probably going to add in that other middleware that's going to start capturing exceptions and sending them to Rollbar. You can test that by throwing an exception in one of the controllers and seeing that bubble into the middleware. The signature for an error handling middleware is going to be app. use, and then that function is going to have an arity of 4, I believe, which is going to be a request, response, the next handler, and then the error that was produced further up in the stack. The Rollbar module will, I believe, provide one of these types of middleware that you can just plug directly in. So, it actually ends up working out pretty great. You can also add your own custom error handling logic in this way as well. So the issue for that is out on GitHub, and that's going to be Issue #38. So, we're going to add some error handling into our Express application, and in the production environment, we're going to want to send that error information to Rollbar. So we should think about where it would make sense to store our Rollbar service key in our configuration and how to put the proper checks in place to make sure that it's only happening in our production environment rather than Dev or Test. So we'll take, I'm going to say, 20 minutes to break that down. I bet that's something we can get in place pretty quickly and then we can take a look at what that feedback looks like in the Rollbar interface. Yes Mark. Did the Beanstalk actually provision more instances? I don't know if I saw it actually provision more. Oh wait, yeah there's two, right? Yeah. So there was a third that got provisioned, and I think it could have gone up to six. But the network usage wasn't the best like indicator in this case because like we weren't actually sending all that much data over the wire, but it was more of like the deluge of requests and the CPU would have been a better indicator for when we should spin up more instances. And does it cool back down? Yeah, it will cool back down. Eventually, this list will drop to just one after it's determined that like we're not sending as much data over the wire. Pretty awesome! It's pretty handy. So you put a really heavy load on it. Can you hear the credit card machine running? Yeah. Can you have like a live websocket counter of all the money that's coming out of your account? I don't know. Hopefully --- I've been spamming it a lot, but it seems like I haven't used up my credits yet, so we'll see. I'm willing to take that one for the team though.

Exercise 8 Solution
The exercise that we were just working on was to integrate Rollbar into our error handling process for our application. Now, I did sort of a quick and dirty solution so I can show you what happens. But, Rollbar does provide a Node. js module that you can include in your application so you npm install rollbar. And, if you're doing an application based on Express, they have a connect middleware that you can plug in after the rest of your routes have been defined and it will pass off all the data that it can glean from the request off to Rollbar whenever an exception is thrown. So, I have my errorHandler here and my disposable account ID, which I'll terminate shortly. But I also have introduced a route in the application which I intend to throw an exception. So, when I open up my browser and hit error, in development, Express will actually show me the stack trace here in the browser. In production mode it won't do that. But I've generated an error, I got a stack trace, and I need to know what's going on. So, I'm going to head over to Rollbar and look in my Dashboard. And I've just signed up for this account so I have a lot of todo items. I can invite more people to my team, set up deploy tracking, which is actually super useful. When you do a deploy, you can possibly correlate errors to a particular deployment. There's lots of different stuff you can do. But on the Dashboard, I can see what errors have occurred from my application over the last 24 hours. So I can see I sent like the Hello World message during the onboarding experience, and now I'm starting to get an actual error. So if I click on that, I can see the status of that issue. It's actually possible to have your team resolve issues through Rollbar. We don't actually use Rollbar for that. We use PagerDuty to actually track down who is responsible for closing out an issue, but there are some workflow possibilities there. And if we scroll down, we get the entire trace back from our application showing us the error happened on line 37 of webapp. js. So it helps us to understand where the error occurred. It'll tell us when all the occurrences have been so we get a timestamp, the browser that generated it, the operating system if available, the route, which of our servers it came from, so if we're running the application on multiple servers, we can help track down the precise box that it came from, and we can glean a couple other bits from this. If we had a logged in user, we could configure our Rollbar integration to actually send the account information or some kind of account identifier for our user to Rollbar as well. So we could say, account 1234 just experienced this error in the production environment, like maybe we need to follow up. We can see some graphs about like the browser and OS. If you have deployments configured, you can see those deployments. You can see what the potentially suspicious deployment would be and other similar errors. Now, when one of these things happens, where the value of this comes in is the notification scheme because if an error happens and nobody is listening, then it doesn't really matter how effectively you log it. So, on a personal and on a project level, you can configure different notification channels. The only one that's active by default is email, but you can notify yourself of errors in many, many different ways as you can see. Again, the one we like to use is PagerDuty because PagerDuty has, itself, some very robust notification capabilities, and it can alert an entire chain of command essentially and escalate at each level if one person doesn't answer the page. So we usually have Rollbar pass through to PagerDuty when necessary. Any questions on Rollbar? A very simple integration. There's more data that you can send to help diagnose your errors, but very easy to get set up. Definitely recommend it if you're looking for a service like this one. It lets you know when bad stuff is happening. It's just one part of a notification scheme. The other things that you'll want to look at configuring are maybe a Pingdom configuration that'll check your servers from the outside world to ensure that they remain accessible. You can check the HTML that's coming back to ensure it's doing what you expect. And then with Elastic Beanstalk if you're running Beanstalk, there are some configurations around notifications as well. You can set up such that when certain events happen on your instances, you can see that happen. As you can see, since the load has died down, the other two instances that got spun up have been terminated, so I'm back down to my single instance. But here in Monitoring, you can dig into the data coming out of your instances, and then with the alarms, you can see any alarm configuration you have. So for like CPU utilization, let's see, if I click on this little alarm, we can send out a notification for if the CPU goes over a certain level, and we can specify an email address that gets that. There are also other services Amazon provides that are more robust, like CloudWatch, that can do more notifications about happenings going on inside your AWS account.

Web Analytics
Google Universal Analytics
What we're going to finish up with is a topic which is near and dear to my heart, which might be kind of strange considering I'm a developer by trade, but understanding what's going on in your application from a user's perspective is something I try to get better at every day and make part of my decision-making process as much as I possibly can. And the primary tool that we use to do that is Google Analytics. There are actually some other paid tools that you can use that are very cool. My new favorite toy is Heap analytics where you can retroactively create behavior statistics by essentially creating CSS selectors for elements on your page and attaching events to those. And you don't need to instrument those events ahead of time, you can actually retroactively pull those events out. So there's that. There's Optimizely, which is useful for AB testing. But the core of it all is definitely the data that we get out of Google Analytics. So what we'll do in this section is we'll learn a little bit about what kind of intelligence we can glean from Google Analytics, I'll hop into the UI and point out a couple of the key features that you might want to check out in your quest to find data about your users, and then we'll also take our last code break of the day to actually implement universal analytics on our TodoMVC application using a handy node module. So, why do you care about analytics in the first place? Mostly because you don't know everything. You have a strong hypothesis about what a user might need, about places they might go on your site, but all it is without data is your opinion. So, using Google Analytics as a part of your decision-making process as a developer for what you spend your time on ends up being a useful skill to acquire. Once you realize that you want to be working as smartly as you can and not just shipping code with your head down, Google Analytics can help you make the best decisions possible for what you're going to be focusing on. And when you look at some of the data coming from your application, you'll be able to serve your users better. You'll understand what they find useful, what they're searching for, and you can help them find it a little bit more easily. So are parts of Google Analytics free or is it all free or is it a --- You can sign up for Google Analytics. Yeah, it's free so you can integrate it on your site. There is a premium offering for Google Analytics, but it is --- what is it like? Like, it's in the six figures annually. I forget what the actual price tag is, but it is quite expensive. And you can get closer to real-time data and some more granular information. But the core of the offering is definitely free. Now, who in here is in Google Analytics regularly? So we've got James, we got Mark, a couple folks looking around. That's okay. As developers, it's not something that we necessarily do every day, but it can make all of us better at our jobs if we're developing for the web.

Google Analytics: Overview and Custom Reports
So what I'm going to do is just hop into the Google Analytics UI and show you a few things. My Google Analytics have been kind of broken on my personal website for a while, so the dataset here is going to be a lot smaller than it should be, and just nobody visits my website in general. It's a very low traffic place. I hardly update it. So, when you implement Google Analytics on your site, you'll have access to see some pre-canned reports just by integrating the tracking code on your site. You'll be able to see things like how many users or how many sessions are being initiated on your site over a given time period. And this date chooser up on the top right is constant across the custom reporting interface and this pre-canned reporting interface. So you can select the time scale for the data that you're looking at. A couple of the pre-canned reports, which are useful, are going to be the Content Drilldown, which is something that I use regularly, especially the primary website that I work on at Twilio is the documentation, so I'm very interested in which pages people are visiting most often, which pages have the highest bounce rate, which ones are the most popular landing pages, that sort of thing. So, I will frequently just kind of come into the Content Drilldown and maybe start at the root of the site and drill into these various areas to see where people are navigating in the site. So for me, the most hits I have right now, in terms of page views, are on the home page, and then the about where the recruiters can get some information to try to start to spam me, and then some other odds and ends on my very poorly maintained personal blog. The other bit that you're going to want to take a look at, which is part of one of the challenges that you can take on this afternoon, is custom events. And custom events allow you to track user interaction events in JavaScript while somebody is interacting with the site, so that if you are on frontendmasters. com and the video browsing section or whatever, you can fire off a custom event for a video play and understand how many people are playing videos on a given page. And the data that you can get in the pre-canned variety is going to be down here in the Behavior section under Events. If you go to Overview, you'll see a listing of events here. I'm not doing tracking on my personal website right now, but the way that you'll track custom events is with a category which consists of a family of events. So if you're tracking actions on video plays, you'll have a video category and then actions will be things like play, pause, rewind, scrub on the video players, and labels will be things like the actual title of the video. And you can use these custom events in reports in GA. Now the part you're going to probably use the most frequently is a custom report. And in fact, the newest version of the Google Analytics API, which you can use today, actually uses custom reports as the way in which you create a filter for the data that you're fetching from the API because actually creating a report ends up, in the GUI, does actually end up being a little simpler than it is trying to pull the same data out of the API. So when you create a custom report, you're going to be tracking metrics and across certain dimensions. So, a metric in Google Analytics is something you can measure, something that you can count. So some common ones would be for behavior you have, let's see, like the average time on page say. So you can track how long people are staying on a page. There's also things like the, oops, the bounce rate. So when a user lands on a page on your site and then immediately leaves, the percentage of people that do that make up the bounce rate. And generally speaking, a high bounce rate for a page is not all that great. So you can track a bounce rate for a page or other metrics that you care about. The other bit that you probably are going to care about a lot is goals. And I can show you that interface to create that very quickly after I'm done here. But in the custom report you can see how many people are reaching certain goals within your application. So, if you have a page that a user can land on after they successfully sign up for an account, you can configure that as a goal and you can track how people are reaching that account signup success page. And you can start to slice and dice data based on that. The dimension is going to be the other access of this report. So if the metric is the thing that we want to count, the dimension is the unit that is generating the metric. So for something like bounce rate or a goal conversion, the dimension that that would map to is a landing page. So, whichever page somebody comes to on your site, I'll just search for landing page, and there are lots of different dimensions you can use, so we can run a report for every landing page, which of them has the lowest bounce rate here. So, you can also add filters to only look at certain section of the site. So you can have a landing page filter where, and I do this a lot, so in this case, I only want to look at landing pages whose paths start with /docs. So if you're only interested in running a report for certain sections of the site, you can add a filter down there. But I will erase that for now. I'll just add a few more metrics in here so we can see what they look like, users, and then I can also look at say time on page, average time on page. So I have three metrics that we're going to track across all the different landing pages on my piddly little website. So here's our custom report, and we can see the number of users total across all the landing pages on the site. So, the landing page with the highest bounce rate is my description of Twilio signal conference, it has a very small amount of users that are coming there. The home page has the most amount of users coming to it. It also has a very high bounce rate. But for folks that land on the about page, that bounce rate is quite a bit lower. After they look at about, they're much more likely to view a second page or a third page on my site during the session. So, not large numbers here, but when you're trying to optimize landing pages for certain user goals, creating custom reports like this can tell you how close you are to the mark and will let you track your progress over time.

Google Analytics: Goals
The other bit that I'll show you in the administrative section is how to create a goal because a lot of the very useful stuff you can do in Google Analytics results is about like users reaching a goal. So if I go into the Goals section and create a new goal, a very common goal, there are lots of different kinds, but a goal that you're probably going to use a lot is --- what is this? This is new. I'm going to go with Custom because this scares me. So, a goal we're probably going to use a lot is arriving on a certain page. Whether it's like a marketing page that you're trying to drive a lot of traffic to or if it's a page like after a user signs up, it's a successful signup page, if people are reaching those goal pages, like that is an event that you want to track in your analytics. So, I could create a goal for a page that doesn't exist called signup. And you can only, in Google Analytics, this is another limitation, you can only set a certain amount of goals. I haven't used any of my goal slots yet, so I will go with Goal 1. And, the type of goal I'm going to create is a destination. So that's going to be a specific place that somebody can go on my site. And then, for the destination, you can have any page that begins with something or you can have a specific page like /signup-success. So, this is how you know that somebody successfully made it through your entire signup flow. And then there's other advanced stuff you can do here for funnels and what have you. But I'm going to create that goal even though I'm not going to actually catch anybody with that goal. But if I go back to my customization and my custom report, I can actually track that now as a metric in my report. So in this case, I won't name my report yet, but I will add as a metric here that specific goal that I just configured. So, it was Goal 1, which we called signup. So, there are a few different metrics you can track based on the goal. You can check the raw number, which is going to be signup completions. So that'll be the raw number of times somebody arrived on that page. And then you can also track the --- Goal 1, you can also track the conversion rate. So, this is one that you're going to use in conjunction with a landing page report like this one. So you can say across all landing pages on my site, show me how many signups each one of them generated, and then also show me the conversion rate, what percentage of sessions that began on this page resulted in the type of goal that I created. So this ends up being a powerful combination to start looking at specific sections of content in your site to understand like what pages are performing the best, what sections of the site are performing the best, and how good a job you're doing at converting users into doing the things that you care about on your site. Now the last bit I'll show you here in the UI is the ability to create segments. And a wise man once told me that segments are where Jesus lives in Google Analytics. And this is how you can really start to break down which sets of browsing sessions on your site are the most valuable. So if I open up this UI to create a new segment, I can create a customized view of a certain set of users and compare them to other kinds of users on my site. So for this one, I will say Russian Users because apparently my site, like there's some weird spammy stuff going on so like over half of the traffic to my site is from Russia. So, we'll separate them out as a segment and look at them in comparison to some others. So, some of the conditions we can do are demographics and location. So we can do country contains Russia. And that's going to be, as it updates, oh, it's only 48%, I guess, of users on my site are coming from Russia. So I've created a segment for just Russian users that visit my website. So I can look at just those users and then I can look at the inverse as well. So I can create a segment for non-Russians. And I can look at how the behavior of these two --- does not contain. And if our math is correct, --- (Inaudible) Oh I didn't --- oh oops, did I say contains? I think it changed it back when you chose the country. Oh oops. Does not contain. Good job team. And there we are back to 51%. And, let's edit this report back to something that might actually be like kind of interesting and look at some metrics like time on page, like average time on page and bounce rate, let's say. We'll save that up. So, it turns out that the Russian traffic, which I assumed to be almost completely due to shenanigans, is a much higher time on page and people from everywhere else in the world spend an average of 30 seconds on any given landing page on the site. So I can kind of sort and see which ones are performing the best based on average time on page. The about page is clearly out performing, but I can see that folks that are coming from outside of Russia actually bounce much less frequently so they're more likely to see more than one page during their browsing session. So, doing segments by geography can be useful. But it's also very useful to create segments by event. So, if you have a session which contained like a video play event, those types of users might behave very differently from users that didn't initiate like a video play say on your site. So, I know there's a ton to do in this tool. If you have more questions about it, I'm happy to geek out on this. I'm learning more about the tool all the time trying to get better at it. And the more I learn, the more I want to learn more about how users are interacting with the site. So, hopefully this will put a bug in your ear to get that same level of visibility into your own websites.

Universal Analytics in Node
So what do you have to do to your code to put flags or whatever in there to make it easier to find them on the reports? Man, are you on a two-wheeled machine that you lean to drive because that was an amazing segue, my friend, into what we are going to do next, which is talk about how we implement universal analytics in Node. js where we will be adding JavaScript code to the client-side of our application to track page views and custom events. We talked a little bit about the custom event data model. A category is a logical grouping for a kind of event like video events. An action is a play, pause, or a scrub. And then a label might be the specific video that's being played. But the general stats around page views are what you're going to be collecting first. And a page view event will send with it all kinds of other events to Google to populate all the rest of the data that you saw on that interface.

Exercise 9: Implement Google Analytics
Your challenge, should you choose to accept it, is to implement Google Analytics on our current TodoMVC site. And there's two basic pieces that we can look at implementing. And the one that you should do first is start tracking page views. So, initially when the page is loaded, like for the root URL, we should track a page view for the root URL. And then, there's actually a router within the application, if I go back here, like say if I filter for active events, or active todos, or all, these are actually different routes that are just happening client-side. And if you're doing a rich JavaScript application, you'll want to track these different routes as page view events as well. So, what we'll want to do is have page view events fire for all of these different routes, and even though they're just happening in JavaScript, we'll track them as page views just the same. The signing up for Google Analytics is very easy and it is free. And you'll be given a tracking code that you can use with this handy dandy module, which is universal analytics. Where is that at? Maybe I'll just go to npm directly. It's just called universal-analytics, I believe. And after you sign up, you'll be able to install this module, require it in your code, and then on initialization, pass in your Google Analytics account ID. So once you have that, you can use like some kind of unique user identifier. You won't want to actually pass in any personally identifying information, but if you have a UUID for a given user, you can use that. Otherwise, you can just initialize it with this tracking code. Once you have that, there is a bit of a delay before page view data starts showing up in Google Analytics. But you can also start adding some logic to track specific page views from within your application. And, the second task, if you get through this piece, will be tracking custom events. So, for a visitor, you can fire a custom event with a given category, and a given action, or with a label. And then there's also this optional value. And if there are events in your application that are higher value than others, you can kind of configure those weights within your event tracking as well. I actually don't use that piece of functionality right now, so I wouldn't be able to tell you very much about it, but that is a possibility. So the last code challenge of the day is going to be implementing Google Analytics' page view tracking in our application, but with the caveat that we only want to do it in the production environment. For testing, you could implement it for the development environment as well, but you'll want to have a way to only enable Google Analytics in your production environment so you're not firing off events and page views during development.

Exercise 9 Solution
So we'll take 20 minutes or so to go through that. We'll take a look at what this code could look like, and then we'll finish up with any questions or deeper dives into any of the topics that we covered over the last couple of days. Alright, so we'll at least go over the basic configuration in tracking the initial page view. And then the custom event tracking would happen within the guts of your Vue. js application as you're selecting different filters, if you're interested in tracking like which todos are being clicked and things like that. We'll kind of focus on the page view one right now. So, the first thing that you would need to do is npm intall universal-analytics, and then we would create a visitor, which we would have to propagate to other pieces of our application. So, we create a new visitor object with ua, and then we would pass in the tracking code that we created when we signed up for a new Google Analytics account. I actually already had one, so I just created a new property which gives me a tracking ID that I can use in my code. So I'm going to copy that and plop that guy right in there. Now, once I do that, the signature to actually track a page view is visitor. pageview and then we would pass in the current page being viewed. I think the default, if you don't pass one in, is just trying to get the location of the current page. But you could do something like window. location. href to pass in the current page. And then the event, the way that this particular API is set up, the event won't actually fire until you hit send. And then that will actually initially send off the page view to Google Analytics to track that event in your application. Now, the way that you would probably approach adding this to the router for the application is having this router function take, in addition to the Vue app as a parameter, also this visitor object. So if we go to the routes, in addition to taking the app, we'll accept a visitor as an argument. And then for each of those we'll configure the router to send a page view event when any of these routes are reached. So, within each of these in addition to setting this property on the Vue app that's going to be a part of the Vue model for the visibility of the current todos, we'll also call visitor. pageview and then we'll pass in / visibility, which is the shorthand for the slug that we're seeing. So we'll track all of those different filters as an individual page view in our application. And once again, we'll have to immediately hit send to actually send that off to Google. So in a single page application, this is typically what you would do if you are handling routing on the client-side. You'll need to manually send those page view events to Google or else every visit to your site, even if it's a single page application, will appear as if it was only a single page view when in fact they might be viewing multiple pages within your spa. So that's the basics of page views. And then also with that visitor object within your JavaScript that's going to be handling events for button clicks and other user-created events, you have the capacity to create those custom events, which you can then use in your Google Analytics reporting later on. But, that's pretty much all there is to it. The only bit that you probably are going to want to do, and that was part of the GitHub issue, was to only execute this logic if you're in production. So there's a couple ways you could do that, but probably the easiest is in the Vue. We already have some logic here that is conditionally executed based on the current environment, so inside this same conditional, we could place some additional JavaScript that would maybe just set like window. environmentType = production, or something like that. So we would conditionally create that JavaScript in our application and set that JavaScript property such that we could access it from our client-side code. So now, like say in our app. js, before we actually go to the trouble of creating this visitor, we could do a check, so if window. environmentType == production, then you would actually go through and create the visitor, otherwise you wouldn't because you won't want to track that stuff in development mode. Once you actually do get this implemented, there is a delay before the data starts coming into Google Analytics. It says it can be up to a couple of hours. In my experience, it can be anything from a few minutes to a couple hours. So it'll be kind of dependent on how things are moving through the Google tubes on a given day. But it could take quite some time before the data starts showing up. So, that is a one handy way to implement Google Analytics and integrate it within the rest of your JavaScript. if you're building a JavaScript application in this way.

Wrap-up
Any questions on Google Analytics before we wrap it up? There was somebody who was asking if you've used load balancing for the Postgres? Yeah, so there isn't a load balancer in front of Postgres. If you use RDS, thy do have a master slave setup and there's a fair degree of resilience that's just built into that as a service. But there isn't a load balancer on top of Postgres mostly because like it's not exactly the same as like an HTTP request. We actually have a number of open TCP connections between our EC2 instances and our Postgres instance. It can handle quite a few concurrent open connections, but it doesn't sort of handle traffic in that same way. So there isn't like a load balancer in front of Postgres per se. Cool! So, that's kind of all we're going to cover. Obviously, we haven't covered the entire universe of what's going to be necessary to get your application into production, but the hope is that the application that we've presented and worked on together over the last couple of days gives you the starting point for getting your application out the door. We've integrated in this application a lot of tools and technologies that you will bring to bear if you're going to actually operate this application in production, the build tooling, the technology stack, the monitoring, the ability to understand the performance implications of your application after it's been deployed, and infrastructure that with some tuning can be made to handle a whole bunch of load for a reasonably sized web application. So, what I hope you'll walk away from is a starting point to start building your own stack. But if you start from this one and just build on top of it, I think you'll ultimately be pretty happy. And if you have any questions as you go, my email address is kw@twilio. com. I'd be happy to answer any questions that you might have to the very best of my ability. I'm on the Twitters @kevinwhinnery, so you can follow me there as well where I mostly talk about nerd stuff. And I'm really grateful for having had the chance to spend this time with you over the last couple of days. And I'll hang out today. I know a couple of you are still trying to get stuff running, so I'm happy to stay here until everybody is up and running with as much of the stack as they would like. So, thank you very much. I appreciate it.
